{
  "unit": 12,
  "unitTitle": "Interview Execution & Design Communication",
  "chapter": 9,
  "chapterTitle": "Curated L6 Hard Set (Ambiguity, Trade-offs, Failure Narratives)",
  "chapterDescription": "Mixed cross-unit drill set curated for high-pressure L6 reasoning: ambiguous requirements, trade-off defense, and failure-mode diagnosis.",
  "problems": [
    {
      "id": "l6-hard-001",
      "type": "multi-select",
      "question": "During failover, which choices reduce correctness risk? (Select all that apply)",
      "options": [
        "Temporary stronger read path for critical entities",
        "Explicit degraded mode for non-critical reads",
        "Disable all write validation checks",
        "Clear client semantics for stale/unavailable states"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failover policies should protect invariants and make degraded semantics explicit.",
      "detailedExplanation": "The core signal here is \"during failover, which choices reduce correctness risk? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["consistency-coordination", "failure-narratives"],
      "source": {
        "unit": 8,
        "chapter": 1,
        "problemId": "cc-cm-076",
        "chapterTitle": "Consistency Models Fundamentals"
      }
    },
    {
      "id": "l6-hard-002",
      "type": "multi-select",
      "question": "During failover recovery, which checks are high value? (Select all that apply)",
      "options": [
        "Replica lag convergence",
        "Quorum success normalization",
        "Ignoring stale-read metrics",
        "Invariant validation samples"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Recovery should verify freshness, quorum health, and invariant integrity.",
      "detailedExplanation": "Read this as a scenario about \"during failover recovery, which checks are high value? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["consistency-coordination", "failure-narratives"],
      "source": {
        "unit": 8,
        "chapter": 2,
        "problemId": "cc-qr-076",
        "chapterTitle": "Quorums, Replication & Read/Write Paths"
      }
    },
    {
      "id": "l6-hard-003",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario: profile update event system reports failures related to mixed wall-clock and logical ordering assumptions. What is the primary diagnosis?",
          "options": [
            "Event-time and processing-time can be treated as identical in distributed paths.",
            "Ordering bugs in profile update event system indicate mismatch between causal requirements and current timestamp/queue assumptions around mixed wall-clock and logical ordering assumptions.",
            "Physical clocks are always sufficient for strict correctness ordering.",
            "Concurrent write conflicts disappear if retries are enabled."
          ],
          "correct": 1,
          "explanation": "The incident points to incorrect ordering assumptions: causal dependencies are not represented/enforced correctly.",
          "detailedExplanation": "The key clue in this question is \"scenario: profile update event system reports failures related to mixed wall-clock and\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change during failover recovery?",
          "options": [
            "Separate wall-clock display time from correctness ordering logic using logical clocks.",
            "Force one global synchronized timestamp source for every service write path.",
            "Drop delayed events silently to preserve local order appearance.",
            "Avoid metadata changes and accept occasional causal violations."
          ],
          "correct": 0,
          "explanation": "Adopt explicit causal/logical ordering controls before tuning secondary concerns.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis, what is the strongest next change during failover recovery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "If you keep \"time, Ordering & Causality\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "consistency-coordination",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 8,
        "chapter": 3,
        "problemId": "cc-tc-043",
        "chapterTitle": "Time, Ordering & Causality"
      }
    },
    {
      "id": "l6-hard-004",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario: multi-device messaging feed reports failures related to mixed wall-clock and logical ordering assumptions. What is the primary diagnosis?",
          "options": [
            "Physical clocks are always sufficient for strict correctness ordering.",
            "Concurrent write conflicts disappear if retries are enabled.",
            "Event-time and processing-time can be treated as identical in distributed paths.",
            "Ordering bugs in multi-device messaging feed indicate mismatch between causal requirements and current timestamp/queue assumptions around mixed wall-clock and logical ordering assumptions."
          ],
          "correct": 3,
          "explanation": "The incident points to incorrect ordering assumptions: causal dependencies are not represented/enforced correctly.",
          "detailedExplanation": "Start from \"scenario: multi-device messaging feed reports failures related to mixed wall-clock and\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with clear degraded semantics?",
          "options": [
            "Drop delayed events silently to preserve local order appearance.",
            "Avoid metadata changes and accept occasional causal violations.",
            "Separate wall-clock display time from correctness ordering logic using logical clocks.",
            "Force one global synchronized timestamp source for every service write path."
          ],
          "correct": 2,
          "explanation": "Adopt explicit causal/logical ordering controls before tuning secondary concerns.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis, what is the strongest next change with clear degraded\". Do not reset assumptions between stages; carry forward prior constraints directly. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior."
        }
      ],
      "detailedExplanation": "This prompt is really about \"time, Ordering & Causality\". Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "consistency-coordination",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 8,
        "chapter": 3,
        "problemId": "cc-tc-053",
        "chapterTitle": "Time, Ordering & Causality"
      }
    },
    {
      "id": "l6-hard-005",
      "type": "multi-select",
      "question": "Which are valid degraded behaviors when causal order is uncertain? (Select all that apply)",
      "options": [
        "Delay user-visible apply until dependency confirmed",
        "Show “pending sync” state",
        "Silently overwrite with uncertain update",
        "Escalate to stronger read path for validation"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Explicit degraded semantics are safer than silent incorrect ordering.",
      "detailedExplanation": "This prompt is really about \"valid degraded behaviors when causal order is uncertain? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "consistency-coordination",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 8,
        "chapter": 3,
        "problemId": "cc-tc-075",
        "chapterTitle": "Time, Ordering & Causality"
      }
    },
    {
      "id": "l6-hard-006",
      "type": "ordering",
      "question": "Order degraded responses when causal uncertainty appears.",
      "items": [
        "Mark state pending",
        "Escalate to stronger read/validation path",
        "Delay dependent actions",
        "Require manual/compensating resolution for high-risk cases"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Progress from low-cost caution to stronger safeguards for high-risk actions.",
      "detailedExplanation": "The decision turns on \"order degraded responses when causal uncertainty appears\". Place obvious extremes first, then sort the middle by pairwise comparison. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "consistency-coordination",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 8,
        "chapter": 3,
        "problemId": "cc-tc-098",
        "chapterTitle": "Time, Ordering & Causality"
      }
    },
    {
      "id": "l6-hard-007",
      "type": "numeric-input",
      "question": "Deadlock incidents are 72/day; each costs 35s to recover. Total deadlock recovery minutes/day?",
      "answer": 42,
      "unit": "minutes",
      "tolerance": 0.05,
      "explanation": "72*35 = 2,520 seconds = 42 minutes.",
      "detailedExplanation": "If you keep \"deadlock incidents are 72/day\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 72 and 35s should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "consistency-coordination",
        "failure-narratives",
        "trade-off-defense"
      ],
      "source": {
        "unit": 8,
        "chapter": 4,
        "problemId": "cc-ti-079",
        "chapterTitle": "Transactions & Isolation in Distributed Systems"
      }
    },
    {
      "id": "l6-hard-008",
      "type": "numeric-input",
      "question": "Lock acquisition attempts are 120,000/min with 3.5% contention failures. Failures/min?",
      "answer": 4200,
      "unit": "attempts",
      "tolerance": 0.02,
      "explanation": "0.035 * 120,000 = 4,200.",
      "detailedExplanation": "The key clue in this question is \"lock acquisition attempts are 120,000/min with 3\". Normalize units before computing so conversion mistakes do not propagate. Consistency decisions should be explicit about which conflicts are acceptable and why. If values like 120,000 and 3.5 appear, convert them into one unit basis before comparison. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["consistency-coordination", "failure-narratives"],
      "source": {
        "unit": 8,
        "chapter": 5,
        "problemId": "cc-cl-078",
        "chapterTitle": "Coordination Patterns & Distributed Locking"
      }
    },
    {
      "id": "l6-hard-009",
      "type": "numeric-input",
      "question": "Leader failover takes 18s and occurs 14 times/day. Total failover downtime minutes/day?",
      "answer": 4.2,
      "unit": "minutes",
      "tolerance": 0.05,
      "explanation": "18*14=252s = 4.2 minutes.",
      "detailedExplanation": "Start from \"leader failover takes 18s and occurs 14 times/day\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 18s and 14 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "difficulty": "L6",
      "tags": ["consistency-coordination", "failure-narratives"],
      "source": {
        "unit": 8,
        "chapter": 5,
        "problemId": "cc-cl-080",
        "chapterTitle": "Coordination Patterns & Distributed Locking"
      }
    },
    {
      "id": "l6-hard-010",
      "type": "multiple-choice",
      "question": "Case Psi: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat recommendation stream processor as a reliability-control decision, not an averages-only optimization. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is correct since it mitigates retry-amplified saturation after brownout while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "The core signal here is \"recommendation stream processor\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 1,
        "problemId": "rel-fd-023",
        "chapterTitle": "Failure Modes & Fault Domains"
      }
    },
    {
      "id": "l6-hard-011",
      "type": "multiple-choice",
      "question": "Case Forge: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Recommendation stream processor should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is strongest because it directly addresses retry-amplified saturation after brownout and improves repeatability under stress. This aligns with the extra condition (Legal/compliance constraints require explicit behavior in degraded mode).",
      "detailedExplanation": "The key clue in this question is \"recommendation stream processor\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 9,
        "chapter": 1,
        "problemId": "rel-fd-033",
        "chapterTitle": "Failure Modes & Fault Domains"
      }
    },
    {
      "id": "l6-hard-012",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Recommendation stream processor is a two-step reliability decision. At stage 1, \"The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around degraded dependency returning slow 200s.",
          "detailedExplanation": "Start from \"incident diagnosis for recommendation stream processor: signal points to degraded\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 200s appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for recommendation stream processor:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"failure Modes & Fault Domains\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 9,
        "chapter": 1,
        "problemId": "rel-fd-058",
        "chapterTitle": "Failure Modes & Fault Domains"
      }
    },
    {
      "id": "l6-hard-013",
      "type": "multiple-choice",
      "question": "Case Psi: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat profile update worker pool as a reliability-control decision, not an averages-only optimization. \"Bound concurrency at each dependency boundary and reject above safe capacity\" is correct since it mitigates brownout on non-critical features while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "The core signal here is \"profile update worker pool\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 3,
        "problemId": "rel-ca-023",
        "chapterTitle": "Circuit Breakers, Load Shedding & Admission Control"
      }
    },
    {
      "id": "l6-hard-014",
      "type": "multiple-choice",
      "question": "Case Forge: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Profile update worker pool should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Bound concurrency at each dependency boundary and reject above safe capacity\" is strongest because it directly addresses brownout on non-critical features and improves repeatability under stress. This aligns with the extra condition (Legal/compliance constraints require explicit behavior in degraded mode).",
      "detailedExplanation": "The key clue in this question is \"profile update worker pool\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 9,
        "chapter": 3,
        "problemId": "rel-ca-033",
        "chapterTitle": "Circuit Breakers, Load Shedding & Admission Control"
      }
    },
    {
      "id": "l6-hard-015",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which anti-patterns commonly enlarge outage blast radius.",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The core signal here is \"anti-patterns commonly enlarge outage blast radius? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 3,
        "problemId": "rel-ca-070",
        "chapterTitle": "Circuit Breakers, Load Shedding & Admission Control"
      }
    },
    {
      "id": "l6-hard-016",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what improves confidence in failover assumptions.",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Choose every valid option for this prompt: what improves confidence in failover assumptions, the highest-signal answer is a bundle of controls. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "If you keep \"improves confidence in failover assumptions? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "ambiguity-reasoning"],
      "source": {
        "unit": 9,
        "chapter": 3,
        "problemId": "rel-ca-071",
        "chapterTitle": "Circuit Breakers, Load Shedding & Admission Control"
      }
    },
    {
      "id": "l6-hard-017",
      "type": "multiple-choice",
      "question": "Case Phi: payments write cluster. Primary reliability risk is insufficient N+1 headroom in one AZ. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Design active/passive boundaries per workload and validate capacity with N+1 drills.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat payments write cluster as a reliability-control decision, not an averages-only optimization. \"Design active/passive boundaries per workload and validate capacity with N+1 drills\" is correct since it mitigates insufficient N+1 headroom in one AZ while keeping containment local. The decision remains valid given: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "The decision turns on \"payments write cluster\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 1 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "ambiguity-reasoning"],
      "source": {
        "unit": 9,
        "chapter": 4,
        "problemId": "rel-rf-021",
        "chapterTitle": "Redundancy, Replication & Failover Strategy"
      }
    },
    {
      "id": "l6-hard-018",
      "type": "multiple-choice",
      "question": "Case Zeta: notification preference API. Primary reliability risk is partial outage causing full-page failure. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply per-dependency timeouts and fallback chaining with bounded depth.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat notification preference API as a reliability-control decision, not an averages-only optimization. \"Apply per-dependency timeouts and fallback chaining with bounded depth\" is correct since it mitigates partial outage causing full-page failure while keeping containment local. The decision remains valid given: The previous mitigation improved averages but not tail behavior.",
      "detailedExplanation": "Read this as a scenario about \"notification preference API\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 5,
        "problemId": "rel-gd-006",
        "chapterTitle": "Graceful Degradation & Dependency Isolation"
      }
    },
    {
      "id": "l6-hard-019",
      "type": "multiple-choice",
      "question": "Case Pi: notification preference API. Primary reliability risk is partial outage causing full-page failure. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Apply per-dependency timeouts and fallback chaining with bounded depth."
      ],
      "correct": 3,
      "explanation": "Notification preference API should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Apply per-dependency timeouts and fallback chaining with bounded depth\" is strongest because it directly addresses partial outage causing full-page failure and improves repeatability under stress. This aligns with the extra condition (The team must preserve core write correctness under mitigation).",
      "detailedExplanation": "The key clue in this question is \"notification preference API\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 5,
        "problemId": "rel-gd-016",
        "chapterTitle": "Graceful Degradation & Dependency Isolation"
      }
    },
    {
      "id": "l6-hard-020",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation widget service: signal points to partial outage causing full-page failure. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Graceful Degradation & Dependency Isolation, the best answer is \"The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents\". It is the option most directly aligned to partial outage causing full-page failure while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"incident diagnosis for recommendation widget service: signal points to partial outage\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for recommendation widget service:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Test degradation scenarios in game days to validate user-journey continuity."
          ],
          "correct": 3,
          "explanation": "After diagnosing \"incident diagnosis for recommendation widget service:\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Test degradation scenarios in game days to validate user-journey continuity\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"graceful Degradation & Dependency Isolation\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 9,
        "chapter": 5,
        "problemId": "rel-gd-038",
        "chapterTitle": "Graceful Degradation & Dependency Isolation"
      }
    },
    {
      "id": "l6-hard-021",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation widget service: signal points to partial outage causing full-page failure. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Recommendation widget service is a two-step reliability decision. At stage 1, \"The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around partial outage causing full-page failure.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for recommendation widget service: signal points to partial outage\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for recommendation widget service:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Test degradation scenarios in game days to validate user-journey continuity.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Test degradation scenarios in game days to validate user-journey continuity\" best matches With root cause identified for \"incident diagnosis for recommendation widget service:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Start from \"graceful Degradation & Dependency Isolation\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "staged-reasoning"],
      "source": {
        "unit": 9,
        "chapter": 5,
        "problemId": "rel-gd-048",
        "chapterTitle": "Graceful Degradation & Dependency Isolation"
      }
    },
    {
      "id": "l6-hard-022",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation widget service: signal points to partial outage causing full-page failure. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents\" best matches recommendation widget service by targeting partial outage causing full-page failure and lowering repeat risk.",
          "detailedExplanation": "Use \"incident diagnosis for recommendation widget service: signal points to partial outage\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for recommendation widget service:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Test degradation scenarios in game days to validate user-journey continuity."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Graceful Degradation & Dependency Isolation: for Now that \"incident diagnosis for recommendation widget service:\" is diagnosed, which immediate adjustment best addresses the risk, \"Test degradation scenarios in game days to validate user-journey continuity\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"graceful Degradation & Dependency Isolation\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives", "staged-reasoning"],
      "source": {
        "unit": 9,
        "chapter": 5,
        "problemId": "rel-gd-058",
        "chapterTitle": "Graceful Degradation & Dependency Isolation"
      }
    },
    {
      "id": "l6-hard-023",
      "type": "multiple-choice",
      "question": "Case Delta: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Promote only replicas meeting freshness constraints and document failback guards."
      ],
      "correct": 3,
      "explanation": "For search brownout incident review, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Promote only replicas meeting freshness constraints and document failback guards\" outperforms the alternatives because it targets degraded mode not activated for optional paths and preserves safe recovery behavior. It is also the most compatible with Recent game-day results showed hidden cross-zone coupling.",
      "detailedExplanation": "Start from \"search brownout incident review\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 8,
        "problemId": "rel-scn-004",
        "chapterTitle": "Reliability Scenarios"
      }
    },
    {
      "id": "l6-hard-024",
      "type": "multiple-choice",
      "question": "Case Xi: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Promote only replicas meeting freshness constraints and document failback guards.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Reliability Scenarios, search brownout incident review fails mainly through degraded mode not activated for optional paths. The best choice is \"Promote only replicas meeting freshness constraints and document failback guards\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "The decision turns on \"search brownout incident review\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 8,
        "problemId": "rel-scn-014",
        "chapterTitle": "Reliability Scenarios"
      }
    },
    {
      "id": "l6-hard-025",
      "type": "multiple-choice",
      "question": "Case Phi: global checkout incident bridge. Primary reliability risk is retry storm during partial dependency outage. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Stabilize first with admission and shedding, then restore critical paths in dependency order.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Reliability Scenarios, global checkout incident bridge fails mainly through retry storm during partial dependency outage. The best choice is \"Stabilize first with admission and shedding, then restore critical paths in dependency order\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "If you keep \"global checkout incident bridge\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 8,
        "problemId": "rel-scn-021",
        "chapterTitle": "Reliability Scenarios"
      }
    },
    {
      "id": "l6-hard-026",
      "type": "multiple-choice",
      "question": "Case Omega: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Promote only replicas meeting freshness constraints and document failback guards."
      ],
      "correct": 3,
      "explanation": "Search brownout incident review should be solved at the failure boundary named in Reliability Scenarios. \"Promote only replicas meeting freshness constraints and document failback guards\" is strongest because it directly addresses degraded mode not activated for optional paths and improves repeatability under stress. This aligns with the extra condition (On-call needs mitigation that is observable by explicit metrics).",
      "detailedExplanation": "Read this as a scenario about \"search brownout incident review\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 8,
        "problemId": "rel-scn-024",
        "chapterTitle": "Reliability Scenarios"
      }
    },
    {
      "id": "l6-hard-027",
      "type": "multiple-choice",
      "question": "Case Harbor: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Promote only replicas meeting freshness constraints and document failback guards.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat search brownout incident review as a reliability-control decision, not an averages-only optimization. \"Promote only replicas meeting freshness constraints and document failback guards\" is correct since it mitigates degraded mode not activated for optional paths while keeping containment local. The decision remains valid given: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "Use \"search brownout incident review\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["reliability", "failure-narratives"],
      "source": {
        "unit": 9,
        "chapter": 8,
        "problemId": "rel-scn-034",
        "chapterTitle": "Reliability Scenarios"
      }
    },
    {
      "id": "l6-hard-028",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures\" best matches Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions by targeting global ordering assumptions broken across regions and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident review for timeline cache layer: signal points to global ordering assumptions\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for timeline cache layer: signal points\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Design delete fanout as priority workflow so visibility guarantees match product policy."
          ],
          "correct": 3,
          "explanation": "Now that \"incident review for timeline cache layer: signal points\" is diagnosed, which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Design delete fanout as priority workflow so visibility guarantees match product policy\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"twitter/X Timeline Write & Fanout\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives", "ambiguity-reasoning"],
      "source": {
        "unit": 10,
        "chapter": 1,
        "problemId": "cd-tw-050",
        "chapterTitle": "Twitter/X Timeline Write & Fanout"
      }
    },
    {
      "id": "l6-hard-029",
      "type": "multiple-choice",
      "question": "Case Xi: timeline serving API. Dominant risk is degraded dependency returning incomplete candidate sets. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Define degraded ranking modes that preserve safety and basic relevance under dependency failure.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, timeline serving API fails mainly through degraded dependency returning incomplete candidate sets. The best choice is \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A partial failure is masking itself as success in metrics.",
      "detailedExplanation": "Use \"timeline serving API\" as your starting point, then verify tradeoffs carefully. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives"],
      "source": {
        "unit": 10,
        "chapter": 2,
        "problemId": "cd-tr-014",
        "chapterTitle": "Twitter/X Timeline Ranking, Serving & Reliability"
      }
    },
    {
      "id": "l6-hard-030",
      "type": "multiple-choice",
      "question": "Case Harbor: timeline serving API. Dominant risk is degraded dependency returning incomplete candidate sets. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Define degraded ranking modes that preserve safety and basic relevance under dependency failure.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat timeline serving API as a reliability-control decision, not an averages-only optimization. \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\" is correct since it mitigates degraded dependency returning incomplete candidate sets while keeping containment local. The decision remains valid given: A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "The decision turns on \"timeline serving API\". Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives"],
      "source": {
        "unit": 10,
        "chapter": 2,
        "problemId": "cd-tr-034",
        "chapterTitle": "Twitter/X Timeline Ranking, Serving & Reliability"
      }
    },
    {
      "id": "l6-hard-031",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in URL Shortener Scale, Analytics & Operations: for Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events, \"The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures\" is correct because it addresses edge failover causing duplicated click events and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident review for fraud/bot filtering stage: signal points to edge failover causing\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for fraud/bot filtering stage: signal\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Test campaign surge scenarios with load + replay drills before peak events."
          ],
          "correct": 3,
          "explanation": "For stage 2 in URL Shortener Scale, Analytics & Operations, the best answer is \"Test campaign surge scenarios with load + replay drills before peak events\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"uRL Shortener Scale, Analytics & Operations\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives", "staged-reasoning"],
      "source": {
        "unit": 10,
        "chapter": 4,
        "problemId": "cd-ua-050",
        "chapterTitle": "URL Shortener Scale, Analytics & Operations"
      }
    },
    {
      "id": "l6-hard-032",
      "type": "ordering",
      "question": "Considering chat core messaging architecture, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Core Messaging Architecture should start with Scope blast radius and affected flows and end with Run recurrence checks and hardening actions. Considering chat core messaging architecture, order safe incident mitigation steps rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Read this as a scenario about \"order safe incident mitigation steps\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives"],
      "source": {
        "unit": 10,
        "chapter": 5,
        "problemId": "cd-cc-092",
        "chapterTitle": "Chat Core Messaging Architecture"
      }
    },
    {
      "id": "l6-hard-033",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility is a two-step reliability decision. At stage 1, \"The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures\" wins because it balances immediate containment with long-term prevention around attachment processing backlog delaying message visibility.",
          "detailedExplanation": "Use \"incident review for chat failover recovery controller: signal points to attachment\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident review for chat failover recovery controller:\", which next step is strongest under current constraints?",
          "options": [
            "Protect key-distribution flow with versioned key epochs and replay guards.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Protect key-distribution flow with versioned key epochs and replay guards\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 10,
        "chapter": 6,
        "problemId": "cd-cp-055",
        "chapterTitle": "Chat Presence, Sync & Reliability"
      }
    },
    {
      "id": "l6-hard-034",
      "type": "ordering",
      "question": "For chat presence, sync & reliability, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For chat presence, sync & reliability, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Start from \"order safe incident mitigation steps\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives"],
      "source": {
        "unit": 10,
        "chapter": 6,
        "problemId": "cd-cp-092",
        "chapterTitle": "Chat Presence, Sync & Reliability"
      }
    },
    {
      "id": "l6-hard-035",
      "type": "ordering",
      "question": "For notification system core architecture, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For notification system core architecture, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The decision turns on \"order safe incident mitigation steps\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives"],
      "source": {
        "unit": 10,
        "chapter": 7,
        "problemId": "cd-nc-092",
        "chapterTitle": "Notification System Core Architecture"
      }
    },
    {
      "id": "l6-hard-036",
      "type": "ordering",
      "question": "Order failover validation rigor. Focus on notification system core architecture tradeoffs.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Notification System Core Architecture should start with Host health check only and end with Staged shift plus failback rehearsal and rollback gates. Order failover validation rigor rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "This prompt is really about \"order failover validation rigor\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 10,
        "chapter": 7,
        "problemId": "cd-nc-095",
        "chapterTitle": "Notification System Core Architecture"
      }
    },
    {
      "id": "l6-hard-037",
      "type": "multiple-choice",
      "question": "Case Forge: provider outage failover workflow. Dominant risk is retry storm breaching provider quotas. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Gate retries with budgets and jitter so recovery does not cause quota collapse.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In Notification System Scale & Scenarios, provider outage failover workflow fails mainly through retry storm breaching provider quotas. The best choice is \"Gate retries with budgets and jitter so recovery does not cause quota collapse\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Stakeholders need clear trade-off rationale in the postmortem.",
      "detailedExplanation": "Use \"provider outage failover workflow\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 10,
        "chapter": 8,
        "problemId": "cd-ns-033",
        "chapterTitle": "Notification System Scale & Scenarios"
      }
    },
    {
      "id": "l6-hard-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for provider outage failover workflow: signal points to audit gaps obscuring delivery truth. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for provider outage failover workflow: signal points to audit gaps obscuring delivery truth is a two-step reliability decision. At stage 1, \"The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures\" wins because it balances immediate containment with long-term prevention around audit gaps obscuring delivery truth.",
          "detailedExplanation": "Use \"incident review for provider outage failover workflow: signal points to audit gaps\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident review for provider outage failover workflow:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Sequence recovery by user-impact class, then backfill deferred low-priority sends.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Sequence recovery by user-impact class, then backfill deferred low-priority sends\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"notification System Scale & Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 10,
        "chapter": 8,
        "problemId": "cd-ns-048",
        "chapterTitle": "Notification System Scale & Scenarios"
      }
    },
    {
      "id": "l6-hard-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for preference-sync inconsistency incident: signal points to regional outage amplifying backlog. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around preference-sync inconsistency incident mismatches regional outage amplifying backlog, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for preference-sync inconsistency incident: signal points to regional outage amplifying backlog is a two-step reliability decision. At stage 1, \"The current decomposition around preference-sync inconsistency incident mismatches regional outage amplifying backlog, creating repeated failures\" wins because it balances immediate containment with long-term prevention around regional outage amplifying backlog.",
          "detailedExplanation": "If you keep \"incident review for preference-sync inconsistency incident: signal points to regional\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for preference-sync inconsistency\", which immediate adjustment best addresses the risk?",
          "options": [
            "Stabilize with priority queue isolation before expanding throughput.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Stabilize with priority queue isolation before expanding throughput\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 10,
        "chapter": 8,
        "problemId": "cd-ns-051",
        "chapterTitle": "Notification System Scale & Scenarios"
      }
    },
    {
      "id": "l6-hard-040",
      "type": "ordering",
      "question": "For notification system scale & scenarios, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For notification system scale & scenarios, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The core signal here is \"order safe incident mitigation steps\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "difficulty": "L6",
      "tags": ["classic-designs", "failure-narratives"],
      "source": {
        "unit": 10,
        "chapter": 8,
        "problemId": "cd-ns-092",
        "chapterTitle": "Notification System Scale & Scenarios"
      }
    },
    {
      "id": "l6-hard-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "In webhook processing service, what is the highest-priority diagnosis given insufficient attacker capability modeling during a regional failover drill?",
          "options": [
            "unmapped trust boundary between edge and control plane",
            "internal-only endpoint exposed via misrouting",
            "insufficient attacker capability modeling",
            "unverified partner webhook source"
          ],
          "correct": 2,
          "explanation": "The best diagnosis is insufficient attacker capability modeling because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "The goal here is precise problem framing before intervention design. In webhook processing service, insufficient attacker capability modeling best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "OWASP Threat Modeling",
              "url": "https://owasp.org/www-community/Threat_Modeling"
            },
            {
              "title": "MITRE ATT&CK Framework",
              "url": "https://attack.mitre.org/"
            },
            {
              "title": "NIST SP 800-30 Rev.1 Risk Assessments",
              "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
            }
          ]
        },
        {
          "question": "With diagnosis confirmed in webhook processing service, which adjustment gives the best risk reduction?",
          "options": [
            "optimize blast radius later after incidents",
            "focus only on average latency instead of attack paths",
            "defer abuse-case analysis to on-call responders",
            "add compensating controls for unavoidable high-risk paths"
          ],
          "correct": 3,
          "explanation": "Given that diagnosis, add compensating controls for unavoidable high-risk paths is the strongest first change because it reduces insufficient attacker capability modeling quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "With diagnosis set, this step tests execution sequencing quality. In webhook processing service, add compensating controls for unavoidable high-risk paths is high leverage because it lowers insufficient attacker capability modeling without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "OWASP Threat Modeling",
              "url": "https://owasp.org/www-community/Threat_Modeling"
            },
            {
              "title": "MITRE ATT&CK Framework",
              "url": "https://attack.mitre.org/"
            },
            {
              "title": "NIST SP 800-30 Rev.1 Risk Assessments",
              "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for threat modeling and attack-surface reduction. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "A strong stage-1 answer narrows ambiguity to one testable hypothesis. Stage 2 rewards changes that are effective, measurable, and rollout-safe. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "OWASP Threat Modeling",
          "url": "https://owasp.org/www-community/Threat_Modeling"
        },
        {
          "title": "MITRE ATT&CK Framework",
          "url": "https://attack.mitre.org/"
        },
        {
          "title": "NIST SP 800-30 Rev.1 Risk Assessments",
          "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives", "ambiguity-reasoning"],
      "source": {
        "unit": 11,
        "chapter": 1,
        "problemId": "sec-tm-056",
        "chapterTitle": "Threat Modeling & Attack Surfaces"
      }
    },
    {
      "id": "l6-hard-042",
      "type": "numeric-input",
      "question": "fraud scoring engine rotates high-risk credentials every 6 hours under peak traffic. Approximately how many rotations occur in a 30-day month? During data-protection hardening.",
      "answer": 120,
      "unit": "rotations/month",
      "tolerance": 0.15,
      "explanation": "30 days × 24 hours ÷ 6 = 120 rotations/month. This estimate should drive a concrete data protection and secrets lifecycle discipline decision such as headroom, queue limits, or failover thresholds. Use this value to justify one concrete data protection and secrets lifecycle discipline decision.",
      "detailedExplanation": "Strong candidates show unit discipline before discussing architecture impact. In data protection and secrets lifecycle discipline, show units, assumptions, and headroom explicitly. Then map the result to a decision in this system, such as capacity limits, queue depth, or timeout budget.",
      "references": [
        {
          "title": "OWASP Secrets Management Cheat Sheet",
          "url": "https://cheatsheetseries.owasp.org/cheatsheets/Secrets_Management_Cheat_Sheet.html"
        },
        {
          "title": "OWASP Cryptographic Storage Cheat Sheet",
          "url": "https://cheatsheetseries.owasp.org/cheatsheets/Cryptographic_Storage_Cheat_Sheet.html"
        },
        {
          "title": "NIST SP 800-57 Part 1 Rev.5 Key Management",
          "url": "https://csrc.nist.gov/pubs/sp/800/57/pt1/r5/final"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse"],
      "source": {
        "unit": 11,
        "chapter": 3,
        "problemId": "sec-dp-080",
        "chapterTitle": "Data Protection & Secrets Management"
      }
    },
    {
      "id": "l6-hard-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of search indexing pipeline, which diagnosis is most defensible for insufficient abuse telemetry for response tuning when one dependency is degraded?",
          "options": [
            "scraper traffic evading naive IP rate limits",
            "abuse traffic hiding behind residential proxy pools",
            "API abuse through distributed low-and-slow automation",
            "insufficient abuse telemetry for response tuning"
          ],
          "correct": 3,
          "explanation": "The best diagnosis is insufficient abuse telemetry for response tuning because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "Pick the diagnosis that best explains both likelihood and blast radius. In this system, insufficient abuse telemetry for response tuning best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "NIST SP 800-63B Authentication and Lifecycle",
              "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
            },
            {
              "title": "RFC 9333: RateLimit Fields for HTTP",
              "url": "https://www.rfc-editor.org/rfc/rfc9333"
            },
            {
              "title": "OWASP Credential Stuffing Prevention Cheat Sheet",
              "url": "https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html"
            }
          ]
        },
        {
          "question": "After diagnosing search indexing pipeline, what should change first before broad rollout?",
          "options": [
            "define override workflows for high-confidence legitimate users",
            "disable anti-abuse controls during peak load",
            "apply one static CAPTCHA to all traffic forever",
            "rely solely on user reports for abuse detection"
          ],
          "correct": 0,
          "explanation": "Given that diagnosis, define override workflows for high-confidence legitimate users is the strongest first change because it reduces insufficient abuse telemetry for response tuning quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "Prioritize the smallest change that closes the largest risk gap. In this system, define override workflows for high-confidence legitimate users is high leverage because it lowers insufficient abuse telemetry for response tuning without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "NIST SP 800-63B Authentication and Lifecycle",
              "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
            },
            {
              "title": "RFC 9333: RateLimit Fields for HTTP",
              "url": "https://www.rfc-editor.org/rfc/rfc9333"
            },
            {
              "title": "OWASP Credential Stuffing Prevention Cheat Sheet",
              "url": "https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for abuse resistance and anti-automation controls. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "Stage 1 rewards causal clarity and evidence-based thinking. The right first step should be high ROI and low coordination overhead. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "NIST SP 800-63B Authentication and Lifecycle",
          "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
        },
        {
          "title": "RFC 9333: RateLimit Fields for HTTP",
          "url": "https://www.rfc-editor.org/rfc/rfc9333"
        },
        {
          "title": "OWASP Credential Stuffing Prevention Cheat Sheet",
          "url": "https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives", "ambiguity-reasoning"],
      "source": {
        "unit": 11,
        "chapter": 4,
        "problemId": "sec-ab-059",
        "chapterTitle": "Abuse Prevention & Anti-Automation"
      }
    },
    {
      "id": "l6-hard-044",
      "type": "numeric-input",
      "question": "fraud scoring engine rotates high-risk credentials every 6 hours under peak traffic. Approximately how many rotations occur in a 30-day month? During abuse-resilience design.",
      "answer": 120,
      "unit": "rotations/month",
      "tolerance": 0.15,
      "explanation": "30 days × 24 hours ÷ 6 = 120 rotations/month. This estimate should drive a concrete abuse resistance and anti-automation controls decision such as headroom, queue limits, or failover thresholds. Use this value to justify one concrete abuse resistance and anti-automation controls decision.",
      "detailedExplanation": "A good estimate closes with an actionable design consequence. In abuse resistance and anti-automation controls, show units, assumptions, and headroom explicitly. Then map the result to a decision in this system, such as capacity limits, queue depth, or timeout budget.",
      "references": [
        {
          "title": "OWASP Credential Stuffing Prevention Cheat Sheet",
          "url": "https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html"
        },
        {
          "title": "OWASP Automated Threats to Web Applications",
          "url": "https://owasp.org/www-project-automated-threats-to-web-applications/"
        },
        {
          "title": "NIST SP 800-63B Authentication and Lifecycle",
          "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse"],
      "source": {
        "unit": 11,
        "chapter": 4,
        "problemId": "sec-ab-080",
        "chapterTitle": "Abuse Prevention & Anti-Automation"
      }
    },
    {
      "id": "l6-hard-045",
      "type": "numeric-input",
      "question": "During mitigation in profile graph service, each request triggers 0.35 extra checks on average. At 2980 QPS, how many extra checks/sec are generated? During abuse-resilience design.",
      "answer": 1043,
      "unit": "checks/sec",
      "tolerance": 0.15,
      "explanation": "2980 × 0.35 = 1043 extra checks/sec. This estimate should drive a concrete abuse resistance and anti-automation controls decision such as headroom, queue limits, or failover thresholds. Use this value to justify one concrete abuse resistance and anti-automation controls decision.",
      "detailedExplanation": "Back-of-envelope math should be auditable and tied to thresholds. In abuse resistance and anti-automation controls, show units, assumptions, and headroom explicitly. Then map the result to a decision in this system, such as capacity limits, queue depth, or timeout budget.",
      "references": [
        {
          "title": "NIST SP 800-63B Authentication and Lifecycle",
          "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
        },
        {
          "title": "RFC 9333: RateLimit Fields for HTTP",
          "url": "https://www.rfc-editor.org/rfc/rfc9333"
        },
        {
          "title": "OWASP Credential Stuffing Prevention Cheat Sheet",
          "url": "https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse"],
      "source": {
        "unit": 11,
        "chapter": 4,
        "problemId": "sec-ab-082",
        "chapterTitle": "Abuse Prevention & Anti-Automation"
      }
    },
    {
      "id": "l6-hard-046",
      "type": "numeric-input",
      "question": "checkout API gateway rotates high-risk credentials every 4 hours under peak traffic. Approximately how many rotations occur in a 30-day month? During abuse-resilience design.",
      "answer": 180,
      "unit": "rotations/month",
      "tolerance": 0.15,
      "explanation": "30 days × 24 hours ÷ 4 = 180 rotations/month. This estimate should drive a concrete abuse resistance and anti-automation controls decision such as headroom, queue limits, or failover thresholds. Use this value to justify one concrete abuse resistance and anti-automation controls decision.",
      "detailedExplanation": "Keep the arithmetic simple, then explain the operational implication. In abuse resistance and anti-automation controls, show units, assumptions, and headroom explicitly. Then map the result to a decision in this system, such as capacity limits, queue depth, or timeout budget.",
      "references": [
        {
          "title": "NIST SP 800-63B Authentication and Lifecycle",
          "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
        },
        {
          "title": "RFC 9333: RateLimit Fields for HTTP",
          "url": "https://www.rfc-editor.org/rfc/rfc9333"
        },
        {
          "title": "OWASP Credential Stuffing Prevention Cheat Sheet",
          "url": "https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse"],
      "source": {
        "unit": 11,
        "chapter": 4,
        "problemId": "sec-ab-086",
        "chapterTitle": "Abuse Prevention & Anti-Automation"
      }
    },
    {
      "id": "l6-hard-047",
      "type": "numeric-input",
      "question": "During mitigation in fraud scoring engine, each request triggers 0.15 extra checks on average. At 3700 QPS, how many extra checks/sec are generated? During abuse-resilience design.",
      "answer": 555,
      "unit": "checks/sec",
      "tolerance": 0.15,
      "explanation": "3700 × 0.15 = 555 extra checks/sec. This estimate should drive a concrete abuse resistance and anti-automation controls decision such as headroom, queue limits, or failover thresholds. Use this value to justify one concrete abuse resistance and anti-automation controls decision.",
      "detailedExplanation": "A good estimate closes with an actionable design consequence. In abuse resistance and anti-automation controls, show units, assumptions, and headroom explicitly. Then map the result to a decision in this system, such as capacity limits, queue depth, or timeout budget.",
      "references": [
        {
          "title": "OWASP Credential Stuffing Prevention Cheat Sheet",
          "url": "https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html"
        },
        {
          "title": "OWASP Automated Threats to Web Applications",
          "url": "https://owasp.org/www-project-automated-threats-to-web-applications/"
        },
        {
          "title": "NIST SP 800-63B Authentication and Lifecycle",
          "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse"],
      "source": {
        "unit": 11,
        "chapter": 4,
        "problemId": "sec-ab-088",
        "chapterTitle": "Abuse Prevention & Anti-Automation"
      }
    },
    {
      "id": "l6-hard-048",
      "type": "multiple-choice",
      "question": "During incident preparation for experiment assignment service, highest-risk gap is manual-review queue saturation delaying high-risk decisions when one dependency is degraded. Which choice best improves the design?",
      "options": [
        "disable manual review under load pressure",
        "close the loop from analyst outcomes into model updates",
        "optimize only approval rate while ignoring fraud loss",
        "assume yesterday's top signal remains dominant forever"
      ],
      "correct": 1,
      "explanation": "close the loop from analyst outcomes into model updates is the right choice because it closes manual-review queue saturation delaying high-risk decisions and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "Treat this as a leverage decision: small change, large risk reduction. In experiment assignment service, close the loop from analyst outcomes into model updates should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "EBA RTS on SCA and Secure Communication (PSD2)",
          "url": "https://www.eba.europa.eu/regulation-and-policy/payment-services-and-electronic-money/regulatory-technical-standards-strong-customer-authentication-and-common-and-secure-open-standards-communication"
        },
        {
          "title": "NIST SP 800-63B Authentication and Lifecycle",
          "url": "https://pages.nist.gov/800-63-3/sp800-63b.html"
        },
        {
          "title": "PCI DSS v4.0",
          "url": "https://www.pcisecuritystandards.org/document_library?category=pcidss&document=pci_dss"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives"],
      "source": {
        "unit": 11,
        "chapter": 5,
        "problemId": "sec-fr-029",
        "chapterTitle": "Fraud Detection Signals & Controls"
      }
    },
    {
      "id": "l6-hard-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "For catalog read API, what is the most likely core problem behind forensic gaps in critical incident timeline when one dependency is degraded?",
          "options": [
            "dependency between incident and deployment control planes",
            "slow credential revocation during active compromise",
            "forensic gaps in critical incident timeline",
            "false closure before root-cause eradication complete"
          ],
          "correct": 2,
          "explanation": "The best diagnosis is forensic gaps in critical incident timeline because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "Stage 1 is diagnosis quality: identify the mechanism, not just the symptom. In catalog read API, forensic gaps in critical incident timeline best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
              "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
            },
            {
              "title": "Google SRE Book: Managing Incidents",
              "url": "https://sre.google/sre-book/managing-incidents/"
            },
            {
              "title": "MITRE ATT&CK Framework",
              "url": "https://attack.mitre.org/"
            }
          ]
        },
        {
          "question": "With diagnosis confirmed in catalog read API, which adjustment gives the best risk reduction?",
          "options": [
            "perform manual credential revocation at scale",
            "allow action items to remain unowned",
            "maintain current, tested incident playbooks with role clarity",
            "declare recovery once symptoms decrease temporarily"
          ],
          "correct": 2,
          "explanation": "Given that diagnosis, maintain current, tested incident playbooks with role clarity is the strongest first change because it reduces forensic gaps in critical incident timeline quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "This phase checks whether your mitigation plan is practical, not theoretical. In catalog read API, maintain current, tested incident playbooks with role clarity is high leverage because it lowers forensic gaps in critical incident timeline without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
              "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
            },
            {
              "title": "Google SRE Book: Managing Incidents",
              "url": "https://sre.google/sre-book/managing-incidents/"
            },
            {
              "title": "MITRE ATT&CK Framework",
              "url": "https://attack.mitre.org/"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for security incident execution and operational response. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "This step is about choosing the diagnosis that best fits constraints and signals. A strong stage-2 answer balances immediate impact with operational safety. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
          "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
        },
        {
          "title": "Google SRE Book: Managing Incidents",
          "url": "https://sre.google/sre-book/managing-incidents/"
        },
        {
          "title": "MITRE ATT&CK Framework",
          "url": "https://attack.mitre.org/"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives", "staged-reasoning"],
      "source": {
        "unit": 11,
        "chapter": 7,
        "problemId": "sec-ir-038",
        "chapterTitle": "Security Operations & Incident Response"
      }
    },
    {
      "id": "l6-hard-050",
      "type": "multi-select",
      "question": "For ad auction edge service, select the controls that most directly improve security-incident execution. (Select all that apply)",
      "options": [
        "capture immutable timelines for forensic and review quality",
        "treat every alert as equal severity",
        "track incident lifecycle metrics including detection-to-containment",
        "rely on undocumented tribal incident response knowledge",
        "define severity and ownership paths before incidents occur"
      ],
      "correctIndices": [0, 2, 4],
      "explanation": "The selected options are correct because together they reduce playbook drift from current architecture reality through layered mitigation and operational follow-through. The non-selected options leave meaningful gaps or add complexity without proportional risk reduction.",
      "detailedExplanation": "You are being tested on mitigation composition, not checkbox coverage. In ad auction edge service, defend the selected set in terms of coverage, operability, and residual risk. Mention one metric and one likely bypass path to show depth.",
      "references": [
        {
          "title": "MITRE ATT&CK Framework",
          "url": "https://attack.mitre.org/"
        },
        {
          "title": "CISA Incident Detection and Response",
          "url": "https://www.cisa.gov/topics/cyber-threats-and-advisories/incident-detection-and-response"
        },
        {
          "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
          "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives"],
      "source": {
        "unit": 11,
        "chapter": 7,
        "problemId": "sec-ir-061",
        "chapterTitle": "Security Operations & Incident Response"
      }
    },
    {
      "id": "l6-hard-051",
      "type": "multi-select",
      "question": "For catalog read API, which controls best strengthen security-incident execution? (Select all that apply)",
      "options": [
        "perform manual credential revocation at scale",
        "run regular incident drills against realistic attack scenarios",
        "prioritize containment actions by blast radius and asset criticality",
        "delay containment until complete root-cause certainty",
        "document explicit exit criteria before declaring recovery"
      ],
      "correctIndices": [1, 2, 4],
      "explanation": "The selected options are correct because together they reduce slow credential revocation during active compromise through layered mitigation and operational follow-through. The non-selected options leave meaningful gaps or add complexity without proportional risk reduction.",
      "detailedExplanation": "Choose the combination that remains resilient when assumptions shift. In catalog read API, defend the selected set in terms of coverage, operability, and residual risk. Mention one metric and one likely bypass path to show depth.",
      "references": [
        {
          "title": "Google SRE Book: Managing Incidents",
          "url": "https://sre.google/sre-book/managing-incidents/"
        },
        {
          "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
          "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
        },
        {
          "title": "CISA Incident Detection and Response",
          "url": "https://www.cisa.gov/topics/cyber-threats-and-advisories/incident-detection-and-response"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives"],
      "source": {
        "unit": 11,
        "chapter": 7,
        "problemId": "sec-ir-062",
        "chapterTitle": "Security Operations & Incident Response"
      }
    },
    {
      "id": "l6-hard-052",
      "type": "multi-select",
      "question": "In feature flag control plane, which actions should be combined to improve security-incident execution? (Select all that apply)",
      "options": [
        "rely on undocumented tribal incident response knowledge",
        "maintain current, tested incident playbooks with role clarity",
        "separate response controls from potentially compromised planes",
        "close post-incident actions with accountable owners and dates",
        "skip rehearsal because incidents are rare"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The selected options are correct because together they reduce incomplete blast-radius classification in first hour through layered mitigation and operational follow-through. The non-selected options leave meaningful gaps or add complexity without proportional risk reduction.",
      "detailedExplanation": "A strong set combines prevention, detection, and recovery readiness. In feature flag control plane, defend the selected set in terms of coverage, operability, and residual risk. Mention one metric and one likely bypass path to show depth.",
      "references": [
        {
          "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
          "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
        },
        {
          "title": "Google SRE Book: Managing Incidents",
          "url": "https://sre.google/sre-book/managing-incidents/"
        },
        {
          "title": "MITRE ATT&CK Framework",
          "url": "https://attack.mitre.org/"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives"],
      "source": {
        "unit": 11,
        "chapter": 7,
        "problemId": "sec-ir-063",
        "chapterTitle": "Security Operations & Incident Response"
      }
    },
    {
      "id": "l6-hard-053",
      "type": "multi-select",
      "question": "Which measures are most valuable for security-incident execution in admin operations console? (Select all that apply)",
      "options": [
        "automate high-confidence revocation and isolation workflows",
        "perform manual credential revocation at scale",
        "declare recovery once symptoms decrease temporarily",
        "track incident lifecycle metrics including detection-to-containment",
        "align executive and technical communication cadences"
      ],
      "correctIndices": [0, 3, 4],
      "explanation": "The selected options are correct because together they reduce dependency between incident and deployment control planes through layered mitigation and operational follow-through. The non-selected options leave meaningful gaps or add complexity without proportional risk reduction.",
      "detailedExplanation": "Your selected options should reduce bypass potential across different attack paths. In this system, defend the selected set in terms of coverage, operability, and residual risk. Mention one metric and one likely bypass path to show depth.",
      "references": [
        {
          "title": "CISA Incident Detection and Response",
          "url": "https://www.cisa.gov/topics/cyber-threats-and-advisories/incident-detection-and-response"
        },
        {
          "title": "MITRE ATT&CK Framework",
          "url": "https://attack.mitre.org/"
        },
        {
          "title": "Google SRE Book: Managing Incidents",
          "url": "https://sre.google/sre-book/managing-incidents/"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives"],
      "source": {
        "unit": 11,
        "chapter": 7,
        "problemId": "sec-ir-064",
        "chapterTitle": "Security Operations & Incident Response"
      }
    },
    {
      "id": "l6-hard-054",
      "type": "multi-select",
      "question": "Which measures are most valuable for integrated security tradeoff reasoning in document collaboration backend? (Select all that apply)",
      "options": [
        "review cross-domain dependencies before broad remediation",
        "communicate tradeoffs and residual risk as decisions evolve",
        "sequence containment actions by immediate risk-reduction yield",
        "discard forensics to speed immediate mitigation",
        "optimize only one control domain while ignoring others"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The selected options are correct because together they reduce policy conflict between fraud controls and accessibility through layered mitigation and operational follow-through. The non-selected options leave meaningful gaps or add complexity without proportional risk reduction.",
      "detailedExplanation": "The best selections should complement each other rather than overlap. In this system, defend the selected set in terms of coverage, operability, and residual risk. Mention one metric and one likely bypass path to show depth.",
      "references": [
        {
          "title": "OWASP ASVS",
          "url": "https://owasp.org/www-project-application-security-verification-standard/"
        },
        {
          "title": "NIST Cybersecurity Framework 2.0",
          "url": "https://www.nist.gov/cyberframework"
        },
        {
          "title": "NIST SP 800-30 Rev.1 Risk Assessments",
          "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "trade-off-defense"],
      "source": {
        "unit": 11,
        "chapter": 8,
        "problemId": "sec-scn-061",
        "chapterTitle": "Security & Abuse Scenarios"
      }
    },
    {
      "id": "l6-hard-055",
      "type": "multi-select",
      "question": "In profile graph service, which actions should be combined to improve integrated security tradeoff reasoning? (Select all that apply)",
      "options": [
        "isolate critical transaction paths from secondary feature impact",
        "validate rollback and recovery paths before policy expansion",
        "ship broad blocking policy without rollback path",
        "ignore user-impact telemetry during security response",
        "coordinate security, fraud, and reliability controls with shared timeline"
      ],
      "correctIndices": [0, 1, 4],
      "explanation": "The selected options are correct because together they reduce cross-team ownership gaps during high-severity abuse event through layered mitigation and operational follow-through. The non-selected options leave meaningful gaps or add complexity without proportional risk reduction.",
      "detailedExplanation": "This item rewards layered reasoning and explicit residual-risk thinking. In profile graph service, defend the selected set in terms of coverage, operability, and residual risk. Mention one metric and one likely bypass path to show depth.",
      "references": [
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "NIST SP 800-30 Rev.1 Risk Assessments",
          "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
        },
        {
          "title": "NIST Cybersecurity Framework 2.0",
          "url": "https://www.nist.gov/cyberframework"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "trade-off-defense"],
      "source": {
        "unit": 11,
        "chapter": 8,
        "problemId": "sec-scn-062",
        "chapterTitle": "Security & Abuse Scenarios"
      }
    },
    {
      "id": "l6-hard-056",
      "type": "multi-select",
      "question": "For analytics ingestion pipeline, which controls best strengthen integrated security tradeoff reasoning? (Select all that apply)",
      "options": [
        "preserve evidence quality while executing fast containment",
        "discard forensics to speed immediate mitigation",
        "allow temporary controls to become permanent drift",
        "reconcile privacy obligations with incident handling constraints",
        "monitor user-impact metrics alongside attack mitigation metrics"
      ],
      "correctIndices": [0, 3, 4],
      "explanation": "The selected options are correct because together they reduce stale threat model after major architecture migration through layered mitigation and operational follow-through. The non-selected options leave meaningful gaps or add complexity without proportional risk reduction.",
      "detailedExplanation": "Pick options that close distinct failure paths with minimal operational drag. In analytics ingestion pipeline, defend the selected set in terms of coverage, operability, and residual risk. Mention one metric and one likely bypass path to show depth.",
      "references": [
        {
          "title": "NIST SP 800-30 Rev.1 Risk Assessments",
          "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "OWASP ASVS",
          "url": "https://owasp.org/www-project-application-security-verification-standard/"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "trade-off-defense"],
      "source": {
        "unit": 11,
        "chapter": 8,
        "problemId": "sec-scn-063",
        "chapterTitle": "Security & Abuse Scenarios"
      }
    },
    {
      "id": "l6-hard-057",
      "type": "ordering",
      "question": "Arrange these controls by real-world robustness under failure (lowest to highest) for integrated security tradeoff reasoning.",
      "items": [
        "Clarify requirement",
        "Evaluate tradeoffs",
        "Summarize risks",
        "State assumptions",
        "Propose design"
      ],
      "correctOrder": [0, 3, 4, 1, 2],
      "explanation": "This sequence is best because it moves from objective framing to option evaluation, then implementation planning and residual-risk review. In integrated security and abuse trade-off decisions, changing this order often causes premature commitment and weaker validation.",
      "detailedExplanation": "Sequence quality is a proxy for how you run real projects under pressure. In integrated security and abuse trade-off decisions, explain why each transition is next and what risk it prevents. That reasoning is what interviewers use to judge design maturity.",
      "references": [
        {
          "title": "NIST SP 800-30 Rev.1 Risk Assessments",
          "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "OWASP ASVS",
          "url": "https://owasp.org/www-project-application-security-verification-standard/"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 11,
        "chapter": 8,
        "problemId": "sec-scn-092",
        "chapterTitle": "Security & Abuse Scenarios"
      }
    },
    {
      "id": "l6-hard-058",
      "type": "ordering",
      "question": "Order these controls by how strongly they reduce repeat incidents (lowest to highest) for integrated security tradeoff reasoning.",
      "items": [
        "Define objective",
        "Select approach",
        "Review residual risk",
        "Plan migration",
        "Compare alternatives"
      ],
      "correctOrder": [0, 4, 1, 3, 2],
      "explanation": "This sequence is best because it moves from objective framing to option evaluation, then implementation planning and residual-risk review. In integrated security and abuse trade-off decisions, changing this order often causes premature commitment and weaker validation.",
      "detailedExplanation": "Ordering items measure execution discipline more than terminology recall. In integrated security and abuse trade-off decisions, explain why each transition is next and what risk it prevents. That reasoning is what interviewers use to judge design maturity.",
      "references": [
        {
          "title": "OWASP ASVS",
          "url": "https://owasp.org/www-project-application-security-verification-standard/"
        },
        {
          "title": "NIST Cybersecurity Framework 2.0",
          "url": "https://www.nist.gov/cyberframework"
        },
        {
          "title": "NIST SP 800-30 Rev.1 Risk Assessments",
          "url": "https://csrc.nist.gov/pubs/sp/800/30/r1/final"
        }
      ],
      "difficulty": "L6",
      "tags": ["security-abuse", "failure-narratives", "trade-off-defense"],
      "source": {
        "unit": 11,
        "chapter": 8,
        "problemId": "sec-scn-094",
        "chapterTitle": "Security & Abuse Scenarios"
      }
    },
    {
      "id": "l6-hard-059",
      "type": "multiple-choice",
      "question": "During incident preparation for document collaboration backend, highest-risk gap is ignoring write/read ratio uncertainty during a regional failover drill. Which choice best improves the design?",
      "options": [
        "argue constraints instead of negotiating tradeoffs",
        "summarize agreed scope before proposing components",
        "treat non-functional constraints as optional",
        "promise to solve every edge case upfront"
      ],
      "correct": 1,
      "explanation": "summarize agreed scope before proposing components is the best first move because it directly mitigates ignoring write/read ratio uncertainty and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "This is primarily a sequencing problem, not a tooling problem. In document collaboration backend, summarize agreed scope before proposing components should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 1,
        "problemId": "int-sr-031",
        "chapterTitle": "Scoping & Requirement Negotiation"
      }
    },
    {
      "id": "l6-hard-060",
      "type": "multiple-choice",
      "question": "During incident preparation for document collaboration backend, highest-risk gap is failure to update plan after assumption changes with frequent schema evolution. Which choice best improves the design?",
      "options": [
        "ignore conflicting constraints until Q&A",
        "check assumption consistency across subsystems",
        "optimize for one assumption only",
        "treat assumption tracking as overhead"
      ],
      "correct": 1,
      "explanation": "check assumption consistency across subsystems is highest leverage because it mitigates failure to update plan after assumption changes and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "A high-quality response balances speed, safety, and ownership. In document collaboration backend, check assumption consistency across subsystems should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "Brewer: CAP Twelve Years Later",
          "url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-12.html"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 2,
        "problemId": "int-ca-005",
        "chapterTitle": "Constraint Framing & Assumptions"
      }
    },
    {
      "id": "l6-hard-061",
      "type": "multiple-choice",
      "question": "During incident preparation for feature flag control plane, highest-risk gap is constraint tradeoffs not surfaced clearly when one dependency is degraded. Which choice best improves the design?",
      "options": [
        "mix confirmed facts with speculation",
        "restate assumption deltas before revising design",
        "hide assumptions to appear confident",
        "skip fallback planning for risky assumptions"
      ],
      "correct": 1,
      "explanation": "restate assumption deltas before revising design is the best first move because it directly mitigates constraint tradeoffs not surfaced clearly and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "The strongest answer starts with causal reasoning, not buzzwords. In feature flag control plane, restate assumption deltas before revising design should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "Brewer: CAP Twelve Years Later",
          "url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-12.html"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "trade-off-defense"
      ],
      "source": {
        "unit": 12,
        "chapter": 2,
        "problemId": "int-ca-012",
        "chapterTitle": "Constraint Framing & Assumptions"
      }
    },
    {
      "id": "l6-hard-062",
      "type": "multiple-choice",
      "question": "During incident preparation for document collaboration backend, highest-risk gap is failure to update plan after assumption changes under peak traffic. Which choice best improves the design?",
      "options": [
        "optimize for one assumption only",
        "ignore conflicting constraints until Q&A",
        "check assumption consistency across subsystems",
        "treat assumption tracking as overhead"
      ],
      "correct": 2,
      "explanation": "check assumption consistency across subsystems is the right choice because it closes failure to update plan after assumption changes and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "This is primarily a sequencing problem, not a tooling problem. In document collaboration backend, check assumption consistency across subsystems should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "Brewer: CAP Twelve Years Later",
          "url": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-12.html"
        },
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        },
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 2,
        "problemId": "int-ca-029",
        "chapterTitle": "Constraint Framing & Assumptions"
      }
    },
    {
      "id": "l6-hard-063",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of inventory reservation service, which diagnosis is most defensible for failure to update plan after assumption changes with strict compliance scope?",
          "options": [
            "missing dependency constraints on external systems",
            "assumption list not prioritized by impact",
            "no fallback if assumption proves wrong",
            "failure to update plan after assumption changes"
          ],
          "correct": 3,
          "explanation": "The best diagnosis is failure to update plan after assumption changes because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "Stage 1 is diagnosis quality: identify the mechanism, not just the symptom. In this system, failure to update plan after assumption changes best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            }
          ]
        },
        {
          "question": "Now that root cause is clear for inventory reservation service, what is the strongest immediate response?",
          "options": [
            "check assumption consistency across subsystems",
            "treat assumption tracking as overhead",
            "ignore conflicting constraints until Q&A",
            "optimize for one assumption only"
          ],
          "correct": 0,
          "explanation": "Given that diagnosis, check assumption consistency across subsystems is the strongest first change because it reduces failure to update plan after assumption changes quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "This phase checks whether your mitigation plan is practical, not theoretical. In this system, check assumption consistency across subsystems is high leverage because it lowers failure to update plan after assumption changes without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for assumption quality and explicit constraint framing. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "This step is about choosing the diagnosis that best fits constraints and signals. A strong stage-2 answer balances immediate impact with operational safety. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 2,
        "problemId": "int-ca-040",
        "chapterTitle": "Constraint Framing & Assumptions"
      }
    },
    {
      "id": "l6-hard-064",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of profile graph service, which diagnosis is most defensible for inconsistent assumptions across components when one dependency is degraded?",
          "options": [
            "overly precise numbers without confidence ranges",
            "constraint tradeoffs not surfaced clearly",
            "inconsistent assumptions across components",
            "equating guesses with confirmed requirements"
          ],
          "correct": 2,
          "explanation": "The best diagnosis is inconsistent assumptions across components because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "A defensible diagnosis should be falsifiable with concrete telemetry. In this system, inconsistent assumptions across components best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            }
          ]
        },
        {
          "question": "After diagnosing profile graph service, what should change first before broad rollout?",
          "options": [
            "mix confirmed facts with speculation",
            "skip fallback planning for risky assumptions",
            "update architecture when interviewer changes constraints",
            "refuse to revise architecture after new information"
          ],
          "correct": 2,
          "explanation": "Given that diagnosis, update architecture when interviewer changes constraints is the strongest first change because it reduces inconsistent assumptions across components quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "This is a prioritization question: what should ship first and why. In this system, update architecture when interviewer changes constraints is high leverage because it lowers inconsistent assumptions across components without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for assumption quality and explicit constraint framing. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "A strong stage-1 answer narrows ambiguity to one testable hypothesis. Stage 2 rewards changes that are effective, measurable, and rollout-safe. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 2,
        "problemId": "int-ca-053",
        "chapterTitle": "Constraint Framing & Assumptions"
      }
    },
    {
      "id": "l6-hard-065",
      "type": "two-stage",
      "stages": [
        {
          "question": "For support workflow automation, what is the most likely core problem behind insufficient explanation of synchronous vs async boundaries when one dependency is degraded?",
          "options": [
            "insufficient explanation of synchronous vs async boundaries",
            "unclear API contract between core services",
            "inconsistent naming across diagrams and APIs",
            "unclear ownership of data transformations"
          ],
          "correct": 0,
          "explanation": "The best diagnosis is insufficient explanation of synchronous vs async boundaries because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "The goal here is precise problem framing before intervention design. In support workflow automation, insufficient explanation of synchronous vs async boundaries best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "RFC 6749: OAuth 2.0 Authorization Framework",
              "url": "https://www.rfc-editor.org/rfc/rfc6749"
            }
          ]
        },
        {
          "question": "Given that diagnosis in support workflow automation, which next change should be prioritized first?",
          "options": [
            "assume interviewer infers state transitions",
            "ignore compatibility concerns in interface evolution",
            "treat data validation as implementation detail",
            "show event contract versioning and compatibility path"
          ],
          "correct": 3,
          "explanation": "Given that diagnosis, show event contract versioning and compatibility path is the strongest first change because it reduces insufficient explanation of synchronous vs async boundaries quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "With diagnosis set, this step tests execution sequencing quality. In support workflow automation, show event contract versioning and compatibility path is high leverage because it lowers insufficient explanation of synchronous vs async boundaries without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "RFC 6749: OAuth 2.0 Authorization Framework",
              "url": "https://www.rfc-editor.org/rfc/rfc6749"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for interface clarity and explainable data-flow narratives. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "This step is about choosing the diagnosis that best fits constraints and signals. A strong stage-2 answer balances immediate impact with operational safety. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "RFC 6749: OAuth 2.0 Authorization Framework",
          "url": "https://www.rfc-editor.org/rfc/rfc6749"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 3,
        "problemId": "int-df-046",
        "chapterTitle": "Interface and Data Flow Articulation"
      }
    },
    {
      "id": "l6-hard-066",
      "type": "two-stage",
      "stages": [
        {
          "question": "In billing reconciliation batch, what is the highest-priority diagnosis given tradeoff discussion detached from constraints during a regional failover drill?",
          "options": [
            "tradeoff discussion detached from constraints",
            "ignoring downside of chosen architecture",
            "uncertain communication under follow-up pressure",
            "over-indexing on favorite technology"
          ],
          "correct": 0,
          "explanation": "The best diagnosis is tradeoff discussion detached from constraints because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "Start by naming the core risk driver that unlocks a mitigation path. In billing reconciliation batch, tradeoff discussion detached from constraints best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            }
          ]
        },
        {
          "question": "Given that diagnosis in billing reconciliation batch, which next change should be prioritized first?",
          "options": [
            "argue technology preference without constraints",
            "avoid absolutist language in design claims",
            "change position without explaining why",
            "ignore migration or rollout complexity"
          ],
          "correct": 1,
          "explanation": "Given that diagnosis, avoid absolutist language in design claims is the strongest first change because it reduces tradeoff discussion detached from constraints quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "Choose the action that creates momentum for subsequent hardening work. In billing reconciliation batch, avoid absolutist language in design claims is high leverage because it lowers tradeoff discussion detached from constraints without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for trade-off articulation with explicit downside analysis. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "Stage 1 rewards causal clarity and evidence-based thinking. The right first step should be high ROI and low coordination overhead. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "trade-off-defense"
      ],
      "source": {
        "unit": 12,
        "chapter": 4,
        "problemId": "int-tn-045",
        "chapterTitle": "Trade-off Narratives Under Pressure"
      }
    },
    {
      "id": "l6-hard-067",
      "type": "two-stage",
      "stages": [
        {
          "question": "In search indexing pipeline, what is the highest-priority diagnosis given uncertain communication under follow-up pressure when one dependency is degraded?",
          "options": [
            "uncertain communication under follow-up pressure",
            "missing migration path from current state",
            "over-indexing on favorite technology",
            "ignoring downside of chosen architecture"
          ],
          "correct": 0,
          "explanation": "The best diagnosis is uncertain communication because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "A defensible diagnosis should be falsifiable with concrete telemetry. In search indexing pipeline, uncertain communication best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "Google SRE Book: Service Level Objectives",
              "url": "https://sre.google/sre-book/service-level-objectives/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            }
          ]
        },
        {
          "question": "With diagnosis confirmed in search indexing pipeline, which adjustment gives the best risk reduction?",
          "options": [
            "present one option as universally best",
            "close with a decisive recommendation and rationale",
            "ignore migration or rollout complexity",
            "argue technology preference without constraints"
          ],
          "correct": 1,
          "explanation": "Given that diagnosis, close with a decisive recommendation and rationale is the strongest first change because it reduces uncertain communication quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "This is a prioritization question: what should ship first and why. In search indexing pipeline, close with a decisive recommendation and rationale is high leverage because it lowers uncertain communication without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "Google SRE Book: Service Level Objectives",
              "url": "https://sre.google/sre-book/service-level-objectives/"
            },
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for trade-off articulation with explicit downside analysis. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "A strong stage-1 answer narrows ambiguity to one testable hypothesis. Stage 2 rewards changes that are effective, measurable, and rollout-safe. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 4,
        "problemId": "int-tn-059",
        "chapterTitle": "Trade-off Narratives Under Pressure"
      }
    },
    {
      "id": "l6-hard-068",
      "type": "multiple-choice",
      "question": "During incident preparation for webhook processing service, highest-risk gap is dependency failure handling left implicit during a regional failover drill. Which choice best improves the design?",
      "options": [
        "declare solution complete without verification plan",
        "defer degradation behavior to implementation phase",
        "protect critical invariants during degraded operation",
        "assume retries fix most failures"
      ],
      "correct": 2,
      "explanation": "protect critical invariants during degraded operation is the best first move because it directly mitigates dependency failure handling left implicit and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "The strongest answer starts with causal reasoning, not buzzwords. In webhook processing service, protect critical invariants during degraded operation should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
          "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
        },
        {
          "title": "Google SRE Book: Managing Incidents",
          "url": "https://sre.google/sre-book/managing-incidents/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        }
      ],
      "difficulty": "L6",
      "tags": ["interview-execution", "failure-narratives"],
      "source": {
        "unit": 12,
        "chapter": 6,
        "problemId": "int-fm-006",
        "chapterTitle": "Failure-Mode Discussion Quality"
      }
    },
    {
      "id": "l6-hard-069",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of document collaboration backend, which diagnosis is most defensible for mitigation sequencing unclear during outages while operating across two regions?",
          "options": [
            "incomplete blast-radius reasoning",
            "unclear ownership during incident response",
            "mitigation sequencing unclear during outages",
            "dependency failure handling left implicit"
          ],
          "correct": 2,
          "explanation": "The best diagnosis is mitigation sequencing unclear because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "This phase tests whether your mental model is coherent under pressure. In this system, mitigation sequencing unclear best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
              "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
            },
            {
              "title": "Google SRE Book: Managing Incidents",
              "url": "https://sre.google/sre-book/managing-incidents/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            }
          ]
        },
        {
          "question": "Now that root cause is clear for document collaboration backend, what is the strongest immediate response?",
          "options": [
            "sequence mitigation by risk reduction and reversibility",
            "defer degradation behavior to implementation phase",
            "treat monitoring as separate from design",
            "declare solution complete without verification plan"
          ],
          "correct": 0,
          "explanation": "Given that diagnosis, sequence mitigation by risk reduction and reversibility is the strongest first change because it reduces mitigation sequencing unclear quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "Stage 2 is first-move prioritization under real delivery constraints. In this system, sequence mitigation by risk reduction and reversibility is high leverage because it lowers mitigation sequencing unclear without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
              "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
            },
            {
              "title": "Google SRE Book: Managing Incidents",
              "url": "https://sre.google/sre-book/managing-incidents/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for failure analysis depth and mitigation sequencing. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "Start by naming the core risk driver that unlocks a mitigation path. Choose the action that creates momentum for subsequent hardening work. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "NIST SP 800-61 Rev.2 Incident Handling Guide",
          "url": "https://csrc.nist.gov/pubs/sp/800/61/r2/final"
        },
        {
          "title": "Google SRE Book: Managing Incidents",
          "url": "https://sre.google/sre-book/managing-incidents/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        }
      ],
      "difficulty": "L6",
      "tags": ["interview-execution", "failure-narratives", "staged-reasoning"],
      "source": {
        "unit": 12,
        "chapter": 6,
        "problemId": "int-fm-037",
        "chapterTitle": "Failure-Mode Discussion Quality"
      }
    },
    {
      "id": "l6-hard-070",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of analytics ingestion pipeline, which diagnosis is most defensible for dependency failure handling left implicit during a regional failover drill?",
          "options": [
            "unclear ownership during incident response",
            "insufficient validation after incident fixes",
            "dependency failure handling left implicit",
            "no plan for partial data corruption scenarios"
          ],
          "correct": 2,
          "explanation": "The best diagnosis is dependency failure handling left implicit because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "A strong stage-1 answer narrows ambiguity to one testable hypothesis. In this system, dependency failure handling left implicit best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "Google SRE Book: Service Level Objectives",
              "url": "https://sre.google/sre-book/service-level-objectives/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Google SRE Book: Managing Incidents",
              "url": "https://sre.google/sre-book/managing-incidents/"
            }
          ]
        },
        {
          "question": "Now that root cause is clear for analytics ingestion pipeline, what is the strongest immediate response?",
          "options": [
            "protect critical invariants during degraded operation",
            "declare solution complete without verification plan",
            "assume retries fix most failures",
            "defer degradation behavior to implementation phase"
          ],
          "correct": 0,
          "explanation": "Given that diagnosis, protect critical invariants during degraded operation is the strongest first change because it reduces dependency failure handling left implicit quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "Stage 2 rewards changes that are effective, measurable, and rollout-safe. In this system, protect critical invariants during degraded operation is high leverage because it lowers dependency failure handling left implicit without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "Google SRE Book: Service Level Objectives",
              "url": "https://sre.google/sre-book/service-level-objectives/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Google SRE Book: Managing Incidents",
              "url": "https://sre.google/sre-book/managing-incidents/"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for failure analysis depth and mitigation sequencing. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "The goal here is precise problem framing before intervention design. With diagnosis set, this step tests execution sequencing quality. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "Google SRE Book: Managing Incidents",
          "url": "https://sre.google/sre-book/managing-incidents/"
        }
      ],
      "difficulty": "L6",
      "tags": ["interview-execution", "failure-narratives", "staged-reasoning"],
      "source": {
        "unit": 12,
        "chapter": 6,
        "problemId": "int-fm-041",
        "chapterTitle": "Failure-Mode Discussion Quality"
      }
    },
    {
      "id": "l6-hard-071",
      "type": "multiple-choice",
      "question": "During incident preparation for model inference endpoint, highest-risk gap is tradeoff regressions introduced by patch fixes when one dependency is degraded. Which choice best improves the design?",
      "options": [
        "avoid decisive recommendation after revisions",
        "summarize unresolved risks with mitigation plan",
        "lose track of original goals after pivots",
        "change design without explaining tradeoff impact"
      ],
      "correct": 1,
      "explanation": "summarize unresolved risks with mitigation plan is the right choice because it closes tradeoff regressions introduced by patch fixes and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "A high-quality response balances speed, safety, and ownership. In model inference endpoint, summarize unresolved risks with mitigation plan should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "trade-off-defense"
      ],
      "source": {
        "unit": 12,
        "chapter": 7,
        "problemId": "int-rd-007",
        "chapterTitle": "Review, Defend, and Iterate the Design"
      }
    },
    {
      "id": "l6-hard-072",
      "type": "two-stage",
      "stages": [
        {
          "question": "In ad auction edge service, what is the highest-priority diagnosis given defensive posture when assumptions are challenged when one dependency is degraded?",
          "options": [
            "defensive posture when assumptions are challenged",
            "tradeoff regressions introduced by patch fixes",
            "loss of narrative structure after pivots",
            "feedback integrated without preserving core invariants"
          ],
          "correct": 0,
          "explanation": "The best diagnosis is defensive posture because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "This step is about choosing the diagnosis that best fits constraints and signals. In ad auction edge service, defensive posture best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            }
          ]
        },
        {
          "question": "With diagnosis confirmed in ad auction edge service, which adjustment gives the best risk reduction?",
          "options": [
            "treat iteration as cosmetic diagram changes",
            "acknowledge critique and restate constraints before revising",
            "change design without explaining tradeoff impact",
            "avoid decisive recommendation after revisions"
          ],
          "correct": 1,
          "explanation": "Given that diagnosis, acknowledge critique and restate constraints before revising is the strongest first change because it reduces defensive posture quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "A strong stage-2 answer balances immediate impact with operational safety. In ad auction edge service, acknowledge critique and restate constraints before revising is high leverage because it lowers defensive posture without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for design defense and iterative refinement in discussion. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "The goal here is precise problem framing before intervention design. With diagnosis set, this step tests execution sequencing quality. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 7,
        "problemId": "int-rd-048",
        "chapterTitle": "Review, Defend, and Iterate the Design"
      }
    },
    {
      "id": "l6-hard-073",
      "type": "multiple-choice",
      "question": "During incident preparation for ad auction edge service, highest-risk gap is tradeoff narrative weak under interviewer challenge when one dependency is degraded. Which choice best improves the design?",
      "options": [
        "treat mock scenarios as isolated trivia questions",
        "ignore interviewer feedback to preserve initial design",
        "adapt design under challenge while preserving coherence",
        "optimize one metric while ignoring core requirements"
      ],
      "correct": 2,
      "explanation": "adapt design under challenge while preserving coherence is the best first move because it directly mitigates tradeoff narrative weak and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "A high-quality response balances speed, safety, and ownership. In ad auction edge service, adapt design under challenge while preserving coherence should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "trade-off-defense"
      ],
      "source": {
        "unit": 12,
        "chapter": 8,
        "problemId": "int-ms-016",
        "chapterTitle": "Mock Interview Scenarios"
      }
    },
    {
      "id": "l6-hard-074",
      "type": "multiple-choice",
      "question": "During incident preparation for identity token service, highest-risk gap is inability to prioritize when constraints conflict during a regional failover drill. Which choice best improves the design?",
      "options": [
        "optimize one metric while ignoring core requirements",
        "leave assumptions implicit in final answer",
        "close with crisp recap of decisions and open risks",
        "ignore interviewer feedback to preserve initial design"
      ],
      "correct": 2,
      "explanation": "close with crisp recap of decisions and open risks is the best first move because it directly mitigates inability to prioritize and can be rolled out with measurable impact. Alternatives are weaker because they defer mitigation, rely on brittle assumptions, or increase operational risk.",
      "detailedExplanation": "The strongest answer starts with causal reasoning, not buzzwords. In identity token service, close with crisp recap of decisions and open risks should be justified by expected risk reduction speed, blast-radius containment, and operational cost. A strong answer also names one residual risk and one post-rollout verification signal.",
      "references": [
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "trade-off-defense"
      ],
      "source": {
        "unit": 12,
        "chapter": 8,
        "problemId": "int-ms-034",
        "chapterTitle": "Mock Interview Scenarios"
      }
    },
    {
      "id": "l6-hard-075",
      "type": "two-stage",
      "stages": [
        {
          "question": "For inventory reservation service, what is the most likely core problem behind assumption drift causes inconsistent component choices during a regional failover drill?",
          "options": [
            "feedback integration breaks previous constraints",
            "summary fails to reflect key decisions and risks",
            "assumption drift causes inconsistent component choices",
            "tradeoff narrative weak under interviewer challenge"
          ],
          "correct": 2,
          "explanation": "The best diagnosis is assumption drift causes inconsistent component choices because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "This step is about choosing the diagnosis that best fits constraints and signals. In inventory reservation service, assumption drift causes inconsistent component choices best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            }
          ]
        },
        {
          "question": "With diagnosis confirmed in inventory reservation service, which adjustment gives the best risk reduction?",
          "options": [
            "ignore interviewer feedback to preserve initial design",
            "leave assumptions implicit in final answer",
            "optimize one metric while ignoring core requirements",
            "keep assumptions and constraints visible throughout discussion"
          ],
          "correct": 3,
          "explanation": "Given that diagnosis, keep assumptions and constraints visible throughout discussion is the strongest first change because it reduces assumption drift causes inconsistent component choices quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "A strong stage-2 answer balances immediate impact with operational safety. In inventory reservation service, keep assumptions and constraints visible throughout discussion is high leverage because it lowers assumption drift causes inconsistent component choices without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "AWS Well-Architected Framework",
              "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for integrated interview execution from scope to defense. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "Stage 1 is diagnosis quality: identify the mechanism, not just the symptom. This phase checks whether your mitigation plan is practical, not theoretical. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "AWS Well-Architected Framework",
          "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "ambiguity-reasoning"
      ],
      "source": {
        "unit": 12,
        "chapter": 8,
        "problemId": "int-ms-037",
        "chapterTitle": "Mock Interview Scenarios"
      }
    },
    {
      "id": "l6-hard-076",
      "type": "two-stage",
      "stages": [
        {
          "question": "For partner integration gateway, what is the most likely core problem behind feedback integration breaks previous constraints when one dependency is degraded?",
          "options": [
            "inability to prioritize when constraints conflict",
            "lack of confidence in final defense under pushback",
            "feedback integration breaks previous constraints",
            "summary fails to reflect key decisions and risks"
          ],
          "correct": 2,
          "explanation": "The best diagnosis is feedback integration breaks previous constraints because it explains the observed behavior and identifies a testable mitigation path under the stated constraints.",
          "detailedExplanation": "This first step checks whether you can isolate the root driver of risk. In partner integration gateway, feedback integration breaks previous constraints best matches the evidence and gives a falsifiable next step. State which telemetry or logs you would inspect first to validate the diagnosis.",
          "references": [
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            }
          ]
        },
        {
          "question": "With diagnosis confirmed in partner integration gateway, which adjustment gives the best risk reduction?",
          "options": [
            "treat mock scenarios as isolated trivia questions",
            "optimize one metric while ignoring core requirements",
            "leave assumptions implicit in final answer",
            "budget time for final risk and mitigation summary"
          ],
          "correct": 3,
          "explanation": "Given that diagnosis, budget time for final risk and mitigation summary is the strongest first change because it reduces feedback integration breaks previous constraints quickly and can be deployed with clear guardrails.",
          "detailedExplanation": "The best response reduces exposure quickly without destabilizing the system. In partner integration gateway, budget time for final risk and mitigation summary is high leverage because it lowers feedback integration breaks previous constraints without requiring a full redesign. Mention rollout safety checks and a rollback trigger to complete the answer.",
          "references": [
            {
              "title": "The C4 Model for Software Architecture",
              "url": "https://c4model.com/"
            },
            {
              "title": "Google SRE Workbook",
              "url": "https://sre.google/workbook/table-of-contents/"
            },
            {
              "title": "Architecture Decision Records (ADR)",
              "url": "https://adr.github.io/"
            }
          ]
        }
      ],
      "explanation": "This two-stage problem evaluates diagnosis accuracy first and mitigation prioritization second for integrated interview execution from scope to defense. High-quality responses keep stage-2 action explicitly tied to stage-1 risk.",
      "detailedExplanation": "Pick the diagnosis that best explains both likelihood and blast radius. Prioritize the smallest change that closes the largest risk gap. The strongest answers connect diagnosis, first action, and validation criteria as one coherent plan.",
      "references": [
        {
          "title": "The C4 Model for Software Architecture",
          "url": "https://c4model.com/"
        },
        {
          "title": "Google SRE Workbook",
          "url": "https://sre.google/workbook/table-of-contents/"
        },
        {
          "title": "Architecture Decision Records (ADR)",
          "url": "https://adr.github.io/"
        }
      ],
      "difficulty": "L6",
      "tags": [
        "interview-execution",
        "failure-narratives",
        "trade-off-defense"
      ],
      "source": {
        "unit": 12,
        "chapter": 8,
        "problemId": "int-ms-041",
        "chapterTitle": "Mock Interview Scenarios"
      }
    }
  ]
}
