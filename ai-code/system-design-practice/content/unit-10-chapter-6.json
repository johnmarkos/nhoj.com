{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 6,
  "chapterTitle": "Chat Presence, Sync & Reliability",
  "chapterDescription": "Decompose presence state, multi-device sync, media handling, and resilience controls for chat platforms.",
  "problems": [
    {
      "id": "cd-cp-001",
      "type": "multiple-choice",
      "question": "Case Alpha: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For presence heartbeat service, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Model presence as soft-state with expiration windows and anti-flap damping\" outperforms the alternatives because it targets presence flapping due to heartbeat jitter and preserves safe recovery behavior. It is also the most compatible with The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "This prompt is really about \"presence heartbeat service\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-002",
      "type": "multiple-choice",
      "question": "Case Beta: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use per-device sync cursors and deterministic merge rules for unread counters.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat multi-device sync coordinator as a reliability-control decision, not an averages-only optimization. \"Use per-device sync cursors and deterministic merge rules for unread counters\" is correct since it mitigates multi-device unread count divergence while keeping containment local. The decision remains valid given: Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "If you keep \"multi-device sync coordinator\" in view, the correct answer separates faster. Prefer the choice that remains viable across the planning horizon, not just today. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: treating compounding as linear change.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "cd-cp-003",
      "type": "multiple-choice",
      "question": "Case Gamma: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Decouple media processing from text delivery and show pending attachment state.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Read-receipt pipeline should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Decouple media processing from text delivery and show pending attachment state\" is strongest because it directly addresses attachment processing backlog delaying message visibility and improves repeatability under stress. This aligns with the extra condition (Leadership asked for a fix that reduces recurrence, not just MTTR).",
      "detailedExplanation": "The core signal here is \"read-receipt pipeline\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-004",
      "type": "multiple-choice",
      "question": "Case Delta: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply monotonic sequence checks for receipts and read-state transitions."
      ],
      "correct": 3,
      "explanation": "In Chat Presence, Sync & Reliability, attachment upload workflow fails mainly through read-receipt reorder causing inconsistent user state. The best choice is \"Apply monotonic sequence checks for receipts and read-state transitions\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A previous rollback fixed averages but not tail impact.",
      "detailedExplanation": "The key clue in this question is \"attachment upload workflow\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For media thumbnail service, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Protect key-distribution flow with versioned key epochs and replay guards\" outperforms the alternatives because it targets encryption key refresh race across devices and preserves safe recovery behavior. It is also the most compatible with User trust risk is highest on this path.",
      "detailedExplanation": "Start from \"media thumbnail service\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-006",
      "type": "multiple-choice",
      "question": "Case Zeta: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Run failover recovery with dedupe tokens for presence and receipt events.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat unread counter aggregator as a reliability-control decision, not an averages-only optimization. \"Run failover recovery with dedupe tokens for presence and receipt events\" is correct since it mitigates failover replay duplicating old presence events while keeping containment local. The decision remains valid given: A shared dependency has uncertain health right now.",
      "detailedExplanation": "The decision turns on \"unread counter aggregator\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-007",
      "type": "multiple-choice",
      "question": "Case Eta: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Device state reconciliation worker should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Throttle reconnect sync requests with adaptive backoff and server hints\" is strongest because it directly addresses state sync storms after mobile reconnect wave and improves repeatability under stress. This aligns with the extra condition (The change must preserve cost discipline during peak).",
      "detailedExplanation": "Read this as a scenario about \"device state reconciliation worker\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-008",
      "type": "multiple-choice",
      "question": "Case Theta: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
      ],
      "correct": 3,
      "explanation": "In Chat Presence, Sync & Reliability, encryption key distribution path fails mainly through push fallback sending alerts for already-read messages. The best choice is \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry shows risk concentrated in one partition class.",
      "detailedExplanation": "Use \"encryption key distribution path\" as your starting point, then verify tradeoffs carefully. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-009",
      "type": "multiple-choice",
      "question": "Case Iota: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For push-notification fallback path, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Add degraded mode where attachment services fail without blocking core chat\" outperforms the alternatives because it targets attachment CDN outage blocking chat critical path and preserves safe recovery behavior. It is also the most compatible with The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "This prompt is really about \"push-notification fallback path\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-010",
      "type": "multiple-choice",
      "question": "Case Kappa: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Shard counter updates and batch aggregation for large-room scalability.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Chat failover recovery controller should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Shard counter updates and batch aggregation for large-room scalability\" is strongest because it directly addresses counter hot spots for high-traffic group chats and improves repeatability under stress. This aligns with the extra condition (Current runbooks are missing explicit ownership for this boundary).",
      "detailedExplanation": "Read this as a scenario about \"chat failover recovery controller\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-011",
      "type": "multiple-choice",
      "question": "Case Lambda: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Chat Presence, Sync & Reliability, presence heartbeat service fails mainly through presence flapping due to heartbeat jitter. The best choice is \"Model presence as soft-state with expiration windows and anti-flap damping\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A cross-region path recently changed behavior after migration.",
      "detailedExplanation": "The decision turns on \"presence heartbeat service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-012",
      "type": "multiple-choice",
      "question": "Case Mu: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use per-device sync cursors and deterministic merge rules for unread counters."
      ],
      "correct": 3,
      "explanation": "For multi-device sync coordinator, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Use per-device sync cursors and deterministic merge rules for unread counters\" outperforms the alternatives because it targets multi-device unread count divergence and preserves safe recovery behavior. It is also the most compatible with Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "Start from \"multi-device sync coordinator\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-013",
      "type": "multiple-choice",
      "question": "Case Nu: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Decouple media processing from text delivery and show pending attachment state.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat read-receipt pipeline as a reliability-control decision, not an averages-only optimization. \"Decouple media processing from text delivery and show pending attachment state\" is correct since it mitigates attachment processing backlog delaying message visibility while keeping containment local. The decision remains valid given: Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "The key clue in this question is \"read-receipt pipeline\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-014",
      "type": "multiple-choice",
      "question": "Case Xi: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply monotonic sequence checks for receipts and read-state transitions.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Attachment upload workflow should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Apply monotonic sequence checks for receipts and read-state transitions\" is strongest because it directly addresses read-receipt reorder causing inconsistent user state and improves repeatability under stress. This aligns with the extra condition (A partial failure is masking itself as success in metrics).",
      "detailedExplanation": "The core signal here is \"attachment upload workflow\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-015",
      "type": "multiple-choice",
      "question": "Case Omicron: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Chat Presence, Sync & Reliability, media thumbnail service fails mainly through encryption key refresh race across devices. The best choice is \"Protect key-distribution flow with versioned key epochs and replay guards\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This fix must hold under celebrity or campaign spike conditions.",
      "detailedExplanation": "If you keep \"media thumbnail service\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-016",
      "type": "multiple-choice",
      "question": "Case Pi: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Run failover recovery with dedupe tokens for presence and receipt events."
      ],
      "correct": 3,
      "explanation": "For unread counter aggregator, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Run failover recovery with dedupe tokens for presence and receipt events\" outperforms the alternatives because it targets failover replay duplicating old presence events and preserves safe recovery behavior. It is also the most compatible with SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "This prompt is really about \"unread counter aggregator\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-017",
      "type": "multiple-choice",
      "question": "Case Rho: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat device state reconciliation worker as a reliability-control decision, not an averages-only optimization. \"Throttle reconnect sync requests with adaptive backoff and server hints\" is correct since it mitigates state sync storms after mobile reconnect wave while keeping containment local. The decision remains valid given: On-call requested a reversible operational first step.",
      "detailedExplanation": "Use \"device state reconciliation worker\" as your starting point, then verify tradeoffs carefully. Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-018",
      "type": "multiple-choice",
      "question": "Case Sigma: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Encryption key distribution path should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" is strongest because it directly addresses push fallback sending alerts for already-read messages and improves repeatability under stress. This aligns with the extra condition (The system mixes strict and eventual paths with unclear contracts).",
      "detailedExplanation": "Read this as a scenario about \"encryption key distribution path\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-019",
      "type": "multiple-choice",
      "question": "Case Tau: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Chat Presence, Sync & Reliability, push-notification fallback path fails mainly through attachment CDN outage blocking chat critical path. The best choice is \"Add degraded mode where attachment services fail without blocking core chat\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A hot-key pattern is likely from real traffic skew.",
      "detailedExplanation": "The decision turns on \"push-notification fallback path\". Prefer the answer that correctly handles unit conversion and link-capacity limits. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Shard counter updates and batch aggregation for large-room scalability."
      ],
      "correct": 3,
      "explanation": "Treat chat failover recovery controller as a reliability-control decision, not an averages-only optimization. \"Shard counter updates and batch aggregation for large-room scalability\" is correct since it mitigates counter hot spots for high-traffic group chats while keeping containment local. The decision remains valid given: The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "The decision turns on \"chat failover recovery controller\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-021",
      "type": "multiple-choice",
      "question": "Case Phi: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Presence heartbeat service should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Model presence as soft-state with expiration windows and anti-flap damping\" is strongest because it directly addresses presence flapping due to heartbeat jitter and improves repeatability under stress. This aligns with the extra condition (Compliance requires explicit behavior for edge-case failures).",
      "detailedExplanation": "Read this as a scenario about \"presence heartbeat service\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-022",
      "type": "multiple-choice",
      "question": "Case Chi: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use per-device sync cursors and deterministic merge rules for unread counters.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In Chat Presence, Sync & Reliability, multi-device sync coordinator fails mainly through multi-device unread count divergence. The best choice is \"Use per-device sync cursors and deterministic merge rules for unread counters\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This boundary has failed during the last two game days.",
      "detailedExplanation": "The key clue in this question is \"multi-device sync coordinator\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-023",
      "type": "multiple-choice",
      "question": "Case Psi: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Decouple media processing from text delivery and show pending attachment state.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For read-receipt pipeline, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Decouple media processing from text delivery and show pending attachment state\" outperforms the alternatives because it targets attachment processing backlog delaying message visibility and preserves safe recovery behavior. It is also the most compatible with A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "Start from \"read-receipt pipeline\", then pressure-test the result against the options. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "cd-cp-024",
      "type": "multiple-choice",
      "question": "Case Omega: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply monotonic sequence checks for receipts and read-state transitions."
      ],
      "correct": 3,
      "explanation": "Treat attachment upload workflow as a reliability-control decision, not an averages-only optimization. \"Apply monotonic sequence checks for receipts and read-state transitions\" is correct since it mitigates read-receipt reorder causing inconsistent user state while keeping containment local. The decision remains valid given: Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "If you keep \"attachment upload workflow\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-025",
      "type": "multiple-choice",
      "question": "Case Atlas: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Media thumbnail service should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Protect key-distribution flow with versioned key epochs and replay guards\" is strongest because it directly addresses encryption key refresh race across devices and improves repeatability under stress. This aligns with the extra condition (The fix should avoid broad architectural rewrites this quarter).",
      "detailedExplanation": "The core signal here is \"media thumbnail service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-026",
      "type": "multiple-choice",
      "question": "Case Nova: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Run failover recovery with dedupe tokens for presence and receipt events.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In Chat Presence, Sync & Reliability, unread counter aggregator fails mainly through failover replay duplicating old presence events. The best choice is \"Run failover recovery with dedupe tokens for presence and receipt events\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Current metrics hide per-tenant variance that matters.",
      "detailedExplanation": "Use \"unread counter aggregator\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-027",
      "type": "multiple-choice",
      "question": "Case Orion: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For device state reconciliation worker, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Throttle reconnect sync requests with adaptive backoff and server hints\" outperforms the alternatives because it targets state sync storms after mobile reconnect wave and preserves safe recovery behavior. It is also the most compatible with The fallback path is under-tested in production-like load.",
      "detailedExplanation": "This prompt is really about \"device state reconciliation worker\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-028",
      "type": "multiple-choice",
      "question": "Case Vega: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
      ],
      "correct": 3,
      "explanation": "Treat encryption key distribution path as a reliability-control decision, not an averages-only optimization. \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" is correct since it mitigates push fallback sending alerts for already-read messages while keeping containment local. The decision remains valid given: A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "The decision turns on \"encryption key distribution path\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-029",
      "type": "multiple-choice",
      "question": "Case Helios: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Push-notification fallback path should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Add degraded mode where attachment services fail without blocking core chat\" is strongest because it directly addresses attachment CDN outage blocking chat critical path and improves repeatability under stress. This aligns with the extra condition (The system must preserve critical events over bulk traffic).",
      "detailedExplanation": "Read this as a scenario about \"push-notification fallback path\". Eliminate options that ignore sustained peak transfer constraints. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Common pitfall: planning on average transfer while peak bursts dominate.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-030",
      "type": "multiple-choice",
      "question": "Case Aurora: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Shard counter updates and batch aggregation for large-room scalability.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For chat failover recovery controller, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Shard counter updates and batch aggregation for large-room scalability\" outperforms the alternatives because it targets counter hot spots for high-traffic group chats and preserves safe recovery behavior. It is also the most compatible with Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "Start from \"chat failover recovery controller\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat presence heartbeat service as a reliability-control decision, not an averages-only optimization. \"Model presence as soft-state with expiration windows and anti-flap damping\" is correct since it mitigates presence flapping due to heartbeat jitter while keeping containment local. The decision remains valid given: The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "The key clue in this question is \"presence heartbeat service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-032",
      "type": "multiple-choice",
      "question": "Case Pulse: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use per-device sync cursors and deterministic merge rules for unread counters."
      ],
      "correct": 3,
      "explanation": "Multi-device sync coordinator should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Use per-device sync cursors and deterministic merge rules for unread counters\" is strongest because it directly addresses multi-device unread count divergence and improves repeatability under stress. This aligns with the extra condition (Operational complexity is rising faster than team onboarding).",
      "detailedExplanation": "Read this as a scenario about \"multi-device sync coordinator\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-033",
      "type": "multiple-choice",
      "question": "Case Forge: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Decouple media processing from text delivery and show pending attachment state.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In Chat Presence, Sync & Reliability, read-receipt pipeline fails mainly through attachment processing backlog delaying message visibility. The best choice is \"Decouple media processing from text delivery and show pending attachment state\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Stakeholders need clear trade-off rationale in the postmortem.",
      "detailedExplanation": "The decision turns on \"read-receipt pipeline\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-034",
      "type": "multiple-choice",
      "question": "Case Harbor: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply monotonic sequence checks for receipts and read-state transitions.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For attachment upload workflow, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Apply monotonic sequence checks for receipts and read-state transitions\" outperforms the alternatives because it targets read-receipt reorder causing inconsistent user state and preserves safe recovery behavior. It is also the most compatible with A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "This prompt is really about \"attachment upload workflow\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-035",
      "type": "multiple-choice",
      "question": "Case Vector: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat media thumbnail service as a reliability-control decision, not an averages-only optimization. \"Protect key-distribution flow with versioned key epochs and replay guards\" is correct since it mitigates encryption key refresh race across devices while keeping containment local. The decision remains valid given: Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "Use \"media thumbnail service\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures\". It is the option most directly aligned to read-receipt reorder causing inconsistent user state while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident review for presence heartbeat service: signal points to read-receipt reorder\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for presence heartbeat service: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Run failover recovery with dedupe tokens for presence and receipt events.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for In the \"incident review for presence heartbeat service: signal\" scenario, what should change first before wider rollout, \"Run failover recovery with dedupe tokens for presence and receipt events\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices is a two-step reliability decision. At stage 1, \"The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures\" wins because it balances immediate containment with long-term prevention around encryption key refresh race across devices.",
          "detailedExplanation": "The key clue in this question is \"incident review for multi-device sync coordinator: signal points to encryption key\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for multi-device sync coordinator:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Throttle reconnect sync requests with adaptive backoff and server hints\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures\" best matches Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events by targeting failover replay duplicating old presence events and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident review for read-receipt pipeline: signal points to failover replay duplicating\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for read-receipt pipeline: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
          ],
          "correct": 3,
          "explanation": "Using the diagnosis from \"incident review for read-receipt pipeline: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave, \"The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures\" is correct because it addresses state sync storms after mobile reconnect wave and improves controllability.",
          "detailedExplanation": "If you keep \"incident review for attachment upload workflow: signal points to state sync storms\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", what should change first before wider rollout?",
          "options": [
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Add degraded mode where attachment services fail without blocking core chat\" best matches With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages is a two-step reliability decision. At stage 1, \"The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures\" wins because it balances immediate containment with long-term prevention around push fallback sending alerts for already-read messages.",
          "detailedExplanation": "If you keep \"incident review for media thumbnail service: signal points to push fallback sending\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for media thumbnail service: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Shard counter updates and batch aggregation for large-room scalability.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Shard counter updates and batch aggregation for large-room scalability\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures\" best matches Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path by targeting attachment CDN outage blocking chat critical path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident review for unread counter aggregator: signal points to attachment CDN outage\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for unread counter aggregator: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Model presence as soft-state with expiration windows and anti-flap damping.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident review for unread counter aggregator: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Model presence as soft-state with expiration windows and anti-flap damping\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats, \"The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures\" is correct because it addresses counter hot spots for high-traffic group chats and improves controllability.",
          "detailedExplanation": "Use \"incident review for device state reconciliation worker: signal points to counter hot\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for device state reconciliation worker:\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Use per-device sync cursors and deterministic merge rules for unread counters."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use per-device sync cursors and deterministic merge rules for unread counters\" best matches For \"incident review for device state reconciliation worker:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures\". It is the option most directly aligned to presence flapping due to heartbeat jitter while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident review for encryption key distribution path: signal points to presence\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for encryption key distribution path:\", what should change first before wider rollout?",
          "options": [
            "Decouple media processing from text delivery and show pending attachment state.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Given the diagnosis in \"incident review for encryption key distribution path:\", what should change first before wider rollout, \"Decouple media processing from text delivery and show pending attachment state\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence is a two-step reliability decision. At stage 1, \"The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures\" wins because it balances immediate containment with long-term prevention around multi-device unread count divergence.",
          "detailedExplanation": "The decision turns on \"incident review for push-notification fallback path: signal points to multi-device\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for push-notification fallback path:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Apply monotonic sequence checks for receipts and read-state transitions.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Apply monotonic sequence checks for receipts and read-state transitions\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"chat Presence, Sync & Reliability\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures\" best matches Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility by targeting attachment processing backlog delaying message visibility and lowering repeat risk.",
          "detailedExplanation": "Start from \"incident review for chat failover recovery controller: signal points to attachment\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for chat failover recovery controller:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Protect key-distribution flow with versioned key epochs and replay guards.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "With root cause identified for \"incident review for chat failover recovery controller:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Protect key-distribution flow with versioned key epochs and replay guards\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state, \"The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures\" is correct because it addresses read-receipt reorder causing inconsistent user state and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident review for presence heartbeat service: signal points to read-receipt reorder\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for presence heartbeat service: signal\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Run failover recovery with dedupe tokens for presence and receipt events."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Run failover recovery with dedupe tokens for presence and receipt events\" best matches Now that \"incident review for presence heartbeat service: signal\" is diagnosed, which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures\". It is the option most directly aligned to encryption key refresh race across devices while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident review for multi-device sync coordinator: signal points to encryption key\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for multi-device sync coordinator:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Now that \"incident review for multi-device sync coordinator:\" is diagnosed, which immediate adjustment best addresses the risk, \"Throttle reconnect sync requests with adaptive backoff and server hints\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events is a two-step reliability decision. At stage 1, \"The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures\" wins because it balances immediate containment with long-term prevention around failover replay duplicating old presence events.",
          "detailedExplanation": "If you keep \"incident review for read-receipt pipeline: signal points to failover replay duplicating\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident review for read-receipt pipeline: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures\" best matches Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave by targeting state sync storms after mobile reconnect wave and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident review for attachment upload workflow: signal points to state sync storms\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Add degraded mode where attachment services fail without blocking core chat\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures\". It is the option most directly aligned to push fallback sending alerts for already-read messages while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident review for media thumbnail service: signal points to push fallback sending\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for media thumbnail service: signal\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Shard counter updates and batch aggregation for large-room scalability."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Now that \"incident review for media thumbnail service: signal\" is diagnosed, what should change first before wider rollout, \"Shard counter updates and batch aggregation for large-room scalability\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path is a two-step reliability decision. At stage 1, \"The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures\" wins because it balances immediate containment with long-term prevention around attachment CDN outage blocking chat critical path.",
          "detailedExplanation": "The key clue in this question is \"incident review for unread counter aggregator: signal points to attachment CDN outage\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for unread counter aggregator: signal\", what should change first before wider rollout?",
          "options": [
            "Model presence as soft-state with expiration windows and anti-flap damping.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Model presence as soft-state with expiration windows and anti-flap damping\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures\" best matches Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats by targeting counter hot spots for high-traffic group chats and lowering repeat risk.",
          "detailedExplanation": "Start from \"incident review for device state reconciliation worker: signal points to counter hot\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for device state reconciliation worker:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Use per-device sync cursors and deterministic merge rules for unread counters.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "After diagnosing \"incident review for device state reconciliation worker:\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Use per-device sync cursors and deterministic merge rules for unread counters\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter, \"The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures\" is correct because it addresses presence flapping due to heartbeat jitter and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident review for encryption key distribution path: signal points to presence\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for encryption key distribution path:\" scenario, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Decouple media processing from text delivery and show pending attachment state.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Decouple media processing from text delivery and show pending attachment state\" best matches In the \"incident review for encryption key distribution path:\" scenario, which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"chat Presence, Sync & Reliability\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures\". It is the option most directly aligned to multi-device unread count divergence while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident review for push-notification fallback path: signal points to multi-device\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for push-notification fallback path:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Apply monotonic sequence checks for receipts and read-state transitions."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for For \"incident review for push-notification fallback path:\", which immediate adjustment best addresses the risk, \"Apply monotonic sequence checks for receipts and read-state transitions\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility is a two-step reliability decision. At stage 1, \"The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures\" wins because it balances immediate containment with long-term prevention around attachment processing backlog delaying message visibility.",
          "detailedExplanation": "Use \"incident review for chat failover recovery controller: signal points to attachment\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident review for chat failover recovery controller:\", which next step is strongest under current constraints?",
          "options": [
            "Protect key-distribution flow with versioned key epochs and replay guards.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Protect key-distribution flow with versioned key epochs and replay guards\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures\" best matches Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state by targeting read-receipt reorder causing inconsistent user state and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident review for presence heartbeat service: signal points to read-receipt reorder\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for presence heartbeat service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Run failover recovery with dedupe tokens for presence and receipt events.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident review for presence heartbeat service: signal\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Run failover recovery with dedupe tokens for presence and receipt events\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices, \"The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures\" is correct because it addresses encryption key refresh race across devices and improves controllability.",
          "detailedExplanation": "If you keep \"incident review for multi-device sync coordinator: signal points to encryption key\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for multi-device sync coordinator:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Throttle reconnect sync requests with adaptive backoff and server hints\" best matches Given the diagnosis in \"incident review for multi-device sync coordinator:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures\". It is the option most directly aligned to failover replay duplicating old presence events while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident review for read-receipt pipeline: signal points to failover replay duplicating\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for read-receipt pipeline: signal\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Now that \"incident review for read-receipt pipeline: signal\" is diagnosed, which next step is strongest under current constraints, \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave is a two-step reliability decision. At stage 1, \"The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures\" wins because it balances immediate containment with long-term prevention around state sync storms after mobile reconnect wave.",
          "detailedExplanation": "The key clue in this question is \"incident review for attachment upload workflow: signal points to state sync storms\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for attachment upload workflow: signal\", which next change should be prioritized first?",
          "options": [
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Add degraded mode where attachment services fail without blocking core chat\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages, \"The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures\" is correct because it addresses push fallback sending alerts for already-read messages and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident review for media thumbnail service: signal points to push fallback sending\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for media thumbnail service: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Shard counter updates and batch aggregation for large-room scalability.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Shard counter updates and batch aggregation for large-room scalability\" best matches Using the diagnosis from \"incident review for media thumbnail service: signal\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-061",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: which signals best identify decomposition boundary mistakes is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The core signal here is \"signals best identify decomposition boundary mistakes? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-062",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: which controls improve safety on critical write paths needs layered controls, not one silver bullet. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Use \"controls improve safety on critical write paths? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-063",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Select all options that correctly address this: which practices reduce hot-key or hot-partition impact, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "This prompt is really about \"practices reduce hot-key or hot-partition impact? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-064",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The decision turns on \"improves reliability when mixing sync and async paths? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-065",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: which choices usually lower operational risk at scale is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Read this as a scenario about \"choices usually lower operational risk at scale? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-066",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: what should be explicit in API/service contracts for this design needs layered controls, not one silver bullet. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The key clue in this question is \"be explicit in API/service contracts for this design? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-067",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Select all options that correctly address this: which anti-patterns often cause incident recurrence, the highest-signal answer is a bundle of controls. The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Start from \"anti-patterns often cause incident recurrence? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-068",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "If you keep \"increases confidence before broad traffic rollout? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-069",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: which controls protect high-priority traffic during spikes is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The core signal here is \"controls protect high-priority traffic during spikes? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-070",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Select all options that correctly address this: which telemetry dimensions are most actionable for design triage, the highest-signal answer is a bundle of controls. The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "This prompt is really about \"telemetry dimensions are most actionable for design triage? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-071",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Use \"governance actions improve cross-team reliability ownership? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-072",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: what helps prevent retry amplification cascades is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The core signal here is \"helps prevent retry amplification cascades? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-073",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: which fallback strategies are strong when dependencies degrade needs layered controls, not one silver bullet. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "If you keep \"fallback strategies are strong when dependencies degrade? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-074",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Select all options that correctly address this: what reduces data-quality regressions in eventual pipelines, the highest-signal answer is a bundle of controls. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Start from \"reduces data-quality regressions in eventual pipelines? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-075",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The key clue in this question is \"runbook components improve incident execution quality? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-076",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Select all options that correctly address this: which architecture choices improve blast-radius containment is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Read this as a scenario about \"architecture choices improve blast-radius containment? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-077",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: what evidence demonstrates a fix worked beyond short-term recovery needs layered controls, not one silver bullet. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The decision turns on \"evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Compute failures/day.",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "Use first-pass reliability arithmetic for A critical path handles 5,400,000 requests/day and 0: 9720 requests. Answers within +/-3% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "This prompt is really about \"critical path handles 5,400,000 requests/day and 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 5,400 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Compute net drain rate.",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "The operational math for Queue ingest is 2,200 events/min and drain is 2,530 events/min gives 330 events/min. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Use \"queue ingest is 2,200 events/min and drain is 2,530 events/min\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Compute effective attempts/sec.",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "For Retries add 0, the computed target in Chat Presence, Sync & Reliability is 96000 attempts/sec. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Use \"retries add 0\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Compute total failover seconds/day.",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Failover takes 16s and occurs 24 times/day: 384 seconds. Answers within +/-0% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "This prompt is really about \"failover takes 16s and occurs 24 times/day\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 16s and 24 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Compute percent over target.",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for Target p99 is 650ms; observed p99 is 845ms gives 30 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "If you keep \"target p99 is 650ms\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 650ms and 845ms in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "cd-cp-083",
      "type": "numeric-input",
      "question": "Which answer best fits: if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "Chat Presence, Sync & Reliability expects quick quantitative triage: Which answer best fits: if 34% of 130,000 req/min are high-priority, how many high-priority req/min evaluates to 44200 requests/min. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "The core signal here is \"if 34% of 130,000 req/min are high-priority, how many high-priority req/min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 34 and 130,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Compute percent reduction.",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Error rate drops from 1, the computed target in Chat Presence, Sync & Reliability is 78 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "The key clue in this question is \"error rate drops from 1\". Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 1.0 and 0.22 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Compute minimum acknowledgements.",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for A 9-node quorum cluster requires majority writes: 5 acks. Answers within +/-0% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "Start from \"9-node quorum cluster requires majority writes\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. Consistency decisions should be explicit about which conflicts are acceptable and why. If values like 9 and 5 appear, convert them into one unit basis before comparison. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "cd-cp-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Compute minutes to clear.",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "The operational math for Backlog is 56,000 tasks with net drain 350 tasks/min gives 160 minutes. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "The decision turns on \"backlog is 56,000 tasks with net drain 350 tasks/min\". Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Compute percent remaining available.",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Chat Presence, Sync & Reliability expects quick quantitative triage: A fleet has 18 zones and 3 are unavailable evaluates to 83.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "Read this as a scenario about \"fleet has 18 zones and 3 are unavailable\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 18 and 3 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Compute percent reduction.",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For MTTR improved from 52 min to 34 min, the computed target in Chat Presence, Sync & Reliability is 34.62 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "Use \"mTTR improved from 52 min to 34 min\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 52 min and 34 min should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-089",
      "type": "numeric-input",
      "question": "Which answer best fits: if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for Which answer best fits: if 11% of 2,800,000 daily ops need manual checks, checks/day: 308000 operations. Answers within +/-2% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "This prompt is really about \"if 11% of 2,800,000 daily ops need manual checks, checks/day\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 11 and 2,800 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-090",
      "type": "ordering",
      "question": "Order a classic-design decomposition workflow. Use a chat presence, sync & reliability perspective.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Identify critical user journey and invariants must happen before Validate with load/failure drills and refine. That ordering matches incident-safe flow in Chat Presence, Sync & Reliability.",
      "detailedExplanation": "Read this as a scenario about \"order a classic-design decomposition workflow\". Place obvious extremes first, then sort the middle by pairwise comparison. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-091",
      "type": "ordering",
      "question": "Order by increasing design risk. (chat presence, sync & reliability lens)",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Presence, Sync & Reliability should start with Explicit boundaries with contracts and end with Implicit coupling with no ownership. Order by increasing design risk rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The decision turns on \"order by increasing design risk\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-092",
      "type": "ordering",
      "question": "For chat presence, sync & reliability, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For chat presence, sync & reliability, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Start from \"order safe incident mitigation steps\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-093",
      "type": "ordering",
      "question": "Order by increasing retry-control maturity. Focus on chat presence, sync & reliability tradeoffs.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Chat Presence, Sync & Reliability emphasizes safe recovery order. Beginning at Fixed immediate retries and finishing at Jittered retries with retry budgets and telemetry keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The key clue in this question is \"order by increasing retry-control maturity\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-094",
      "type": "ordering",
      "question": "In this chat presence, sync & reliability context, order fallback sophistication.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Implicit fallback behavior must happen before Policy-driven automated fallback with tests. That ordering matches incident-safe flow in Chat Presence, Sync & Reliability.",
      "detailedExplanation": "The core signal here is \"order fallback sophistication\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-095",
      "type": "ordering",
      "question": "Within chat presence, sync & reliability, order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Presence, Sync & Reliability should start with Host health check only and end with Staged shift plus failback rehearsal and rollback gates. Within chat presence, sync & reliability, order failover validation rigor rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "If you keep \"order failover validation rigor\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-096",
      "type": "ordering",
      "question": "Sequence these from minimal to maximal blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Sequence these from minimal to maximal blast radius, the correct ordering runs from Single process failure to Cross-region control-plane failure. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "This prompt is really about \"order by increasing blast radius\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-097",
      "type": "ordering",
      "question": "Considering chat presence, sync & reliability, order data-path durability confidence.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Chat Presence, Sync & Reliability emphasizes safe recovery order. Beginning at In-memory only acknowledgment and finishing at Replicated durable write plus replay/integrity verification keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Use \"order data-path durability confidence\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-098",
      "type": "ordering",
      "question": "Order by increasing operational discipline. Use a chat presence, sync & reliability perspective.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc incident response must happen before Role-based response plus action closure tracking. That ordering matches incident-safe flow in Chat Presence, Sync & Reliability.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing operational discipline\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-099",
      "type": "ordering",
      "question": "Order rollout safety for major design changes. (chat presence, sync & reliability lens)",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Presence, Sync & Reliability should start with Canary small cohort and end with Finalize runbook and ownership updates. Order rollout safety for major design changes rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The decision turns on \"order rollout safety for major design changes\". Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-100",
      "type": "ordering",
      "question": "From a chat presence, sync & reliability viewpoint, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a chat presence, sync & reliability viewpoint, order evidence strength for fix success, the correct ordering runs from Single successful test run to Sustained recovery plus failure-drill pass. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "This prompt is really about \"order evidence strength for fix success\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    }
  ]
}
