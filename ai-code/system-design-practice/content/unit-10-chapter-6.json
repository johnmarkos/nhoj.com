{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 6,
  "chapterTitle": "Chat Presence, Sync & Reliability",
  "chapterDescription": "Decompose presence state, multi-device sync, media handling, and resilience controls for chat platforms.",
  "problems": [
    {
      "id": "cd-cp-001",
      "type": "multiple-choice",
      "question": "Case Alpha: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"presence heartbeat service\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-002",
      "type": "multiple-choice",
      "question": "Case Beta: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use per-device sync cursors and deterministic merge rules for unread counters.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"multi-device sync coordinator\" in view, the correct answer separates faster. Prefer the choice that remains viable across the planning horizon, not just today. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: treating compounding as linear change.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "cd-cp-003",
      "type": "multiple-choice",
      "question": "Case Gamma: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Decouple media processing from text delivery and show pending attachment state.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"read-receipt pipeline\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-004",
      "type": "multiple-choice",
      "question": "Case Delta: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply monotonic sequence checks for receipts and read-state transitions."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"attachment upload workflow\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"media thumbnail service\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-006",
      "type": "multiple-choice",
      "question": "Case Zeta: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Run failover recovery with dedupe tokens for presence and receipt events.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"unread counter aggregator\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-007",
      "type": "multiple-choice",
      "question": "Case Eta: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"device state reconciliation worker\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-008",
      "type": "multiple-choice",
      "question": "Case Theta: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"encryption key distribution path\" as your starting point, then verify tradeoffs carefully. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-009",
      "type": "multiple-choice",
      "question": "Case Iota: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"push-notification fallback path\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-010",
      "type": "multiple-choice",
      "question": "Case Kappa: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Shard counter updates and batch aggregation for large-room scalability.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"chat failover recovery controller\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-011",
      "type": "multiple-choice",
      "question": "Case Lambda: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"presence heartbeat service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-012",
      "type": "multiple-choice",
      "question": "Case Mu: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use per-device sync cursors and deterministic merge rules for unread counters."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"multi-device sync coordinator\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-013",
      "type": "multiple-choice",
      "question": "Case Nu: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Decouple media processing from text delivery and show pending attachment state.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"read-receipt pipeline\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-014",
      "type": "multiple-choice",
      "question": "Case Xi: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply monotonic sequence checks for receipts and read-state transitions.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"attachment upload workflow\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-015",
      "type": "multiple-choice",
      "question": "Case Omicron: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"media thumbnail service\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-016",
      "type": "multiple-choice",
      "question": "Case Pi: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Run failover recovery with dedupe tokens for presence and receipt events."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"unread counter aggregator\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-017",
      "type": "multiple-choice",
      "question": "Case Rho: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"device state reconciliation worker\" as your starting point, then verify tradeoffs carefully. Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-018",
      "type": "multiple-choice",
      "question": "Case Sigma: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"encryption key distribution path\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-019",
      "type": "multiple-choice",
      "question": "Case Tau: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"push-notification fallback path\". Prefer the answer that correctly handles unit conversion and link-capacity limits. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Shard counter updates and batch aggregation for large-room scalability."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"chat failover recovery controller\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-021",
      "type": "multiple-choice",
      "question": "Case Phi: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"presence heartbeat service\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-022",
      "type": "multiple-choice",
      "question": "Case Chi: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use per-device sync cursors and deterministic merge rules for unread counters.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"multi-device sync coordinator\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-023",
      "type": "multiple-choice",
      "question": "Case Psi: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Decouple media processing from text delivery and show pending attachment state.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"read-receipt pipeline\", then pressure-test the result against the options. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "cd-cp-024",
      "type": "multiple-choice",
      "question": "Case Omega: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply monotonic sequence checks for receipts and read-state transitions."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"attachment upload workflow\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-025",
      "type": "multiple-choice",
      "question": "Case Atlas: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"media thumbnail service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-026",
      "type": "multiple-choice",
      "question": "Case Nova: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Run failover recovery with dedupe tokens for presence and receipt events.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"unread counter aggregator\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-027",
      "type": "multiple-choice",
      "question": "Case Orion: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"device state reconciliation worker\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cp-028",
      "type": "multiple-choice",
      "question": "Case Vega: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"encryption key distribution path\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-029",
      "type": "multiple-choice",
      "question": "Case Helios: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"push-notification fallback path\". Eliminate options that ignore sustained peak transfer constraints. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Common pitfall: planning on average transfer while peak bursts dominate.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-030",
      "type": "multiple-choice",
      "question": "Case Aurora: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Shard counter updates and batch aggregation for large-room scalability.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"chat failover recovery controller\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"presence heartbeat service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ]
    },
    {
      "id": "cd-cp-032",
      "type": "multiple-choice",
      "question": "Case Pulse: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use per-device sync cursors and deterministic merge rules for unread counters."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"multi-device sync coordinator\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-033",
      "type": "multiple-choice",
      "question": "Case Forge: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Decouple media processing from text delivery and show pending attachment state.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"read-receipt pipeline\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-034",
      "type": "multiple-choice",
      "question": "Case Harbor: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply monotonic sequence checks for receipts and read-state transitions.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"attachment upload workflow\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-035",
      "type": "multiple-choice",
      "question": "Case Vector: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"media thumbnail service\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for presence heartbeat service: signal points to read-receipt reorder\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Run failover recovery with dedupe tokens for presence and receipt events.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for multi-device sync coordinator: signal points to encryption key\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for read-receipt pipeline: signal points to failover replay duplicating\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for attachment upload workflow: signal points to state sync storms\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for media thumbnail service: signal points to push fallback sending\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Shard counter updates and batch aggregation for large-room scalability.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for unread counter aggregator: signal points to attachment CDN outage\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Model presence as soft-state with expiration windows and anti-flap damping.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for device state reconciliation worker: signal points to counter hot\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Use per-device sync cursors and deterministic merge rules for unread counters."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for encryption key distribution path: signal points to presence\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Decouple media processing from text delivery and show pending attachment state.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for push-notification fallback path: signal points to multi-device\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Apply monotonic sequence checks for receipts and read-state transitions.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"chat Presence, Sync & Reliability\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for chat failover recovery controller: signal points to attachment\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Protect key-distribution flow with versioned key epochs and replay guards.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for presence heartbeat service: signal points to read-receipt reorder\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Run failover recovery with dedupe tokens for presence and receipt events."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for multi-device sync coordinator: signal points to encryption key\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for read-receipt pipeline: signal points to failover replay duplicating\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for attachment upload workflow: signal points to state sync storms\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for media thumbnail service: signal points to push fallback sending\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Shard counter updates and batch aggregation for large-room scalability."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for unread counter aggregator: signal points to attachment CDN outage\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Model presence as soft-state with expiration windows and anti-flap damping.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for device state reconciliation worker: signal points to counter hot\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Use per-device sync cursors and deterministic merge rules for unread counters.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for encryption key distribution path: signal points to presence\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Decouple media processing from text delivery and show pending attachment state.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"chat Presence, Sync & Reliability\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for push-notification fallback path: signal points to multi-device\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Apply monotonic sequence checks for receipts and read-state transitions."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for chat failover recovery controller: signal points to attachment\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Protect key-distribution flow with versioned key epochs and replay guards.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Presence, Sync & Reliability\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for presence heartbeat service: signal points to read-receipt reorder\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Run failover recovery with dedupe tokens for presence and receipt events.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Start from \"chat Presence, Sync & Reliability\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for multi-device sync coordinator: signal points to encryption key\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Presence, Sync & Reliability\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for read-receipt pipeline: signal points to failover replay duplicating\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Presence, Sync & Reliability\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for attachment upload workflow: signal points to state sync storms\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for media thumbnail service: signal points to push fallback sending\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Shard counter updates and batch aggregation for large-room scalability.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"chat Presence, Sync & Reliability\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-061",
      "type": "multi-select",
      "question": "Which signals best identify decomposition boundary mistakes? (Select all that apply)",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The core signal here is \"signals best identify decomposition boundary mistakes? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-062",
      "type": "multi-select",
      "question": "Which controls improve safety on critical write paths? (Select all that apply)",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Use \"controls improve safety on critical write paths? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-063",
      "type": "multi-select",
      "question": "Which practices reduce hot-key or hot-partition impact? (Select all that apply)",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "This prompt is really about \"practices reduce hot-key or hot-partition impact? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-064",
      "type": "multi-select",
      "question": "What improves reliability when mixing sync and async paths? (Select all that apply)",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The decision turns on \"improves reliability when mixing sync and async paths? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-065",
      "type": "multi-select",
      "question": "Which choices usually lower operational risk at scale? (Select all that apply)",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Read this as a scenario about \"choices usually lower operational risk at scale? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-066",
      "type": "multi-select",
      "question": "What should be explicit in API/service contracts for this design? (Select all that apply)",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The key clue in this question is \"be explicit in API/service contracts for this design? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cp-067",
      "type": "multi-select",
      "question": "Which anti-patterns often cause incident recurrence? (Select all that apply)",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Start from \"anti-patterns often cause incident recurrence? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-068",
      "type": "multi-select",
      "question": "What increases confidence before broad traffic rollout? (Select all that apply)",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "If you keep \"increases confidence before broad traffic rollout? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-069",
      "type": "multi-select",
      "question": "Which controls protect high-priority traffic during spikes? (Select all that apply)",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The core signal here is \"controls protect high-priority traffic during spikes? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-070",
      "type": "multi-select",
      "question": "Which telemetry dimensions are most actionable for design triage? (Select all that apply)",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "This prompt is really about \"telemetry dimensions are most actionable for design triage? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-071",
      "type": "multi-select",
      "question": "Which governance actions improve cross-team reliability ownership? (Select all that apply)",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Use \"governance actions improve cross-team reliability ownership? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-072",
      "type": "multi-select",
      "question": "What helps prevent retry amplification cascades? (Select all that apply)",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The core signal here is \"helps prevent retry amplification cascades? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-073",
      "type": "multi-select",
      "question": "Which fallback strategies are strong when dependencies degrade? (Select all that apply)",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "If you keep \"fallback strategies are strong when dependencies degrade? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-074",
      "type": "multi-select",
      "question": "What reduces data-quality regressions in eventual pipelines? (Select all that apply)",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Start from \"reduces data-quality regressions in eventual pipelines? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-075",
      "type": "multi-select",
      "question": "Which runbook components improve incident execution quality? (Select all that apply)",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The key clue in this question is \"runbook components improve incident execution quality? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-076",
      "type": "multi-select",
      "question": "Which architecture choices improve blast-radius containment? (Select all that apply)",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Read this as a scenario about \"architecture choices improve blast-radius containment? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-077",
      "type": "multi-select",
      "question": "What evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The decision turns on \"evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Failures/day?",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "0.0018*5,400,000=9,720.",
      "detailedExplanation": "This prompt is really about \"critical path handles 5,400,000 requests/day and 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 5,400 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Net drain rate?",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "2,530-2,200=330.",
      "detailedExplanation": "Use \"queue ingest is 2,200 events/min and drain is 2,530 events/min\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Effective attempts/sec?",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "75,000*1.28=96,000.",
      "detailedExplanation": "Use \"retries add 0\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Total failover seconds/day?",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "16*24=384.",
      "detailedExplanation": "This prompt is really about \"failover takes 16s and occurs 24 times/day\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 16s and 24 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Percent over target?",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(845-650)/650=30%.",
      "detailedExplanation": "If you keep \"target p99 is 650ms\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 650ms and 845ms in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "cd-cp-083",
      "type": "numeric-input",
      "question": "If 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "0.34*130,000=44,200.",
      "detailedExplanation": "The core signal here is \"if 34% of 130,000 req/min are high-priority, how many high-priority req/min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 34 and 130,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Percent reduction?",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1.0-0.22)/1.0=78%.",
      "detailedExplanation": "The key clue in this question is \"error rate drops from 1\". Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 1.0 and 0.22 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Minimum acknowledgements?",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Majority of 9 is 5.",
      "detailedExplanation": "Start from \"9-node quorum cluster requires majority writes\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. Consistency decisions should be explicit about which conflicts are acceptable and why. If values like 9 and 5 appear, convert them into one unit basis before comparison. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "cd-cp-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Minutes to clear?",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "56,000/350=160.",
      "detailedExplanation": "The decision turns on \"backlog is 56,000 tasks with net drain 350 tasks/min\". Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Percent remaining available?",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "15/18=83.33%.",
      "detailedExplanation": "Read this as a scenario about \"fleet has 18 zones and 3 are unavailable\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 18 and 3 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Percent reduction?",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(52-34)/52=34.62%.",
      "detailedExplanation": "Use \"mTTR improved from 52 min to 34 min\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 52 min and 34 min should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-089",
      "type": "numeric-input",
      "question": "If 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "0.11*2,800,000=308,000.",
      "detailedExplanation": "This prompt is really about \"if 11% of 2,800,000 daily ops need manual checks, checks/day\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 11 and 2,800 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-090",
      "type": "ordering",
      "question": "Order a classic-design decomposition workflow.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start with outcomes/invariants, then decompose, control, and validate.",
      "detailedExplanation": "Read this as a scenario about \"order a classic-design decomposition workflow\". Place obvious extremes first, then sort the middle by pairwise comparison. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-091",
      "type": "ordering",
      "question": "Order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Risk rises as boundaries and ownership degrade.",
      "detailedExplanation": "The decision turns on \"order by increasing design risk\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-092",
      "type": "ordering",
      "question": "Order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope, contain, fix, then harden.",
      "detailedExplanation": "Start from \"order safe incident mitigation steps\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-093",
      "type": "ordering",
      "question": "Order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Maturity improves with safeguards and observability.",
      "detailedExplanation": "The key clue in this question is \"order by increasing retry-control maturity\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cp-094",
      "type": "ordering",
      "question": "Order fallback sophistication.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sophistication rises with explicit policy and automation.",
      "detailedExplanation": "The core signal here is \"order fallback sophistication\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-095",
      "type": "ordering",
      "question": "Order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rigor increases with validation and reversible control.",
      "detailedExplanation": "If you keep \"order failover validation rigor\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-096",
      "type": "ordering",
      "question": "Order by increasing blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Blast radius expands from local failures to regional control issues.",
      "detailedExplanation": "This prompt is really about \"order by increasing blast radius\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-097",
      "type": "ordering",
      "question": "Order data-path durability confidence.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Durability confidence grows with replication and verification.",
      "detailedExplanation": "Use \"order data-path durability confidence\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-098",
      "type": "ordering",
      "question": "Order by increasing operational discipline.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Discipline improves with structure and accountability.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing operational discipline\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-099",
      "type": "ordering",
      "question": "Order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rollout should progress from limited exposure to institutionalization.",
      "detailedExplanation": "The decision turns on \"order rollout safety for major design changes\". Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cp-100",
      "type": "ordering",
      "question": "Order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Evidence strength rises with sustained production and drill results.",
      "detailedExplanation": "This prompt is really about \"order evidence strength for fix success\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    }
  ]
}
