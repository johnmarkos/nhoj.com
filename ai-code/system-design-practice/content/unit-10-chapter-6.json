{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 6,
  "chapterTitle": "Chat Presence, Sync & Reliability",
  "chapterDescription": "Decompose presence state, multi-device sync, media handling, and resilience controls for chat platforms.",
  "problems": [
    {
      "id": "cd-cp-001",
      "type": "multiple-choice",
      "question": "Case Alpha: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For presence heartbeat service, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Model presence as soft-state with expiration windows and anti-flap damping\" outperforms the alternatives because it targets presence flapping due to heartbeat jitter and preserves safe recovery behavior. It is also the most compatible with The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-002",
      "type": "multiple-choice",
      "question": "Case Beta: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Use per-device sync cursors and deterministic merge rules for unread counters.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat multi-device sync coordinator as a reliability-control decision, not an averages-only optimization. \"Use per-device sync cursors and deterministic merge rules for unread counters\" is correct since it mitigates multi-device unread count divergence while keeping containment local. The decision remains valid given: Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer the approach that remains viable across the planning horizon, not just today. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: treating compounding as linear change.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-003",
      "type": "multiple-choice",
      "question": "Case Gamma: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Decouple media processing from text delivery and show pending attachment state.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Read-receipt pipeline should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Decouple media processing from text delivery and show pending attachment state\" is strongest because it directly addresses attachment processing backlog delaying message visibility and improves repeatability under stress. This aligns with the extra condition (Leadership asked for a fix that reduces recurrence, not just MTTR).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-004",
      "type": "multiple-choice",
      "question": "Case Delta: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Apply monotonic sequence checks for receipts and read-state transitions."
      ],
      "correct": 3,
      "explanation": "In Chat Presence, Sync & Reliability, attachment upload workflow fails mainly through read-receipt reorder causing inconsistent user state. The best choice is \"Apply monotonic sequence checks for receipts and read-state transitions\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A previous rollback fixed averages but not tail impact.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For media thumbnail service, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Protect key-distribution flow with versioned key epochs and replay guards\" outperforms the alternatives because it targets encryption key refresh race across devices and preserves safe recovery behavior. It is also the most compatible with User trust risk is highest on this path.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-006",
      "type": "multiple-choice",
      "question": "Case Zeta: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Run failover recovery with dedupe tokens for presence and receipt events.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat unread counter aggregator as a reliability-control decision, not an averages-only optimization. \"Run failover recovery with dedupe tokens for presence and receipt events\" is correct since it mitigates failover replay duplicating old presence events while keeping containment local. The decision remains valid given: A shared dependency has uncertain health right now.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-007",
      "type": "multiple-choice",
      "question": "Case Eta: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Device state reconciliation worker should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Throttle reconnect sync requests with adaptive backoff and server hints\" is strongest because it directly addresses state sync storms after mobile reconnect wave and improves repeatability under stress. This aligns with the extra condition (The change must preserve cost discipline during peak).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-008",
      "type": "multiple-choice",
      "question": "Case Theta: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
      ],
      "correct": 3,
      "explanation": "In Chat Presence, Sync & Reliability, encryption key distribution path fails mainly through push fallback sending alerts for already-read messages. The best choice is \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry shows risk concentrated in one partition class.",
      "detailedExplanation": "Generalize from encryption key distribution path to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-009",
      "type": "multiple-choice",
      "question": "Case Iota: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For push-notification fallback path, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Add degraded mode where attachment services fail without blocking core chat\" outperforms the alternatives because it targets attachment CDN outage blocking chat critical path and preserves safe recovery behavior. It is also the most compatible with The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-010",
      "type": "multiple-choice",
      "question": "Case Kappa: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Shard counter updates and batch aggregation for large-room scalability.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Chat failover recovery controller should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Shard counter updates and batch aggregation for large-room scalability\" is strongest because it directly addresses counter hot spots for high-traffic group chats and improves repeatability under stress. This aligns with the extra condition (Current runbooks are missing explicit ownership for this boundary).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-011",
      "type": "multiple-choice",
      "question": "Case Lambda: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Chat Presence, Sync & Reliability, presence heartbeat service fails mainly through presence flapping due to heartbeat jitter. The best choice is \"Model presence as soft-state with expiration windows and anti-flap damping\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A cross-region path recently changed behavior after migration.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-012",
      "type": "multiple-choice",
      "question": "Case Mu: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Use per-device sync cursors and deterministic merge rules for unread counters."
      ],
      "correct": 3,
      "explanation": "For multi-device sync coordinator, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Use per-device sync cursors and deterministic merge rules for unread counters\" outperforms the alternatives because it targets multi-device unread count divergence and preserves safe recovery behavior. It is also the most compatible with Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-013",
      "type": "multiple-choice",
      "question": "Case Nu: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Decouple media processing from text delivery and show pending attachment state.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat read-receipt pipeline as a reliability-control decision, not an averages-only optimization. \"Decouple media processing from text delivery and show pending attachment state\" is correct since it mitigates attachment processing backlog delaying message visibility while keeping containment local. The decision remains valid given: Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-014",
      "type": "multiple-choice",
      "question": "Case Xi: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Apply monotonic sequence checks for receipts and read-state transitions.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Attachment upload workflow should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Apply monotonic sequence checks for receipts and read-state transitions\" is strongest because it directly addresses read-receipt reorder causing inconsistent user state and improves repeatability under stress. This aligns with the extra condition (A partial failure is masking itself as success in metrics).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-015",
      "type": "multiple-choice",
      "question": "Case Omicron: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Chat Presence, Sync & Reliability, media thumbnail service fails mainly through encryption key refresh race across devices. The best choice is \"Protect key-distribution flow with versioned key epochs and replay guards\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This fix must hold under celebrity or campaign spike conditions.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-016",
      "type": "multiple-choice",
      "question": "Case Pi: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Run failover recovery with dedupe tokens for presence and receipt events."
      ],
      "correct": 3,
      "explanation": "For unread counter aggregator, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Run failover recovery with dedupe tokens for presence and receipt events\" outperforms the alternatives because it targets failover replay duplicating old presence events and preserves safe recovery behavior. It is also the most compatible with SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-017",
      "type": "multiple-choice",
      "question": "Case Rho: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat device state reconciliation worker as a reliability-control decision, not an averages-only optimization. \"Throttle reconnect sync requests with adaptive backoff and server hints\" is correct since it mitigates state sync storms after mobile reconnect wave while keeping containment local. The decision remains valid given: On-call requested a reversible operational first step.",
      "detailedExplanation": "Generalize from device state reconciliation worker to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-018",
      "type": "multiple-choice",
      "question": "Case Sigma: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Encryption key distribution path should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" is strongest because it directly addresses push fallback sending alerts for already-read messages and improves repeatability under stress. This aligns with the extra condition (The system mixes strict and eventual paths with unclear contracts).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-019",
      "type": "multiple-choice",
      "question": "Case Tau: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Chat Presence, Sync & Reliability, push-notification fallback path fails mainly through attachment CDN outage blocking chat critical path. The best choice is \"Add degraded mode where attachment services fail without blocking core chat\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A hot-key pattern is likely from real traffic skew.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the answer that correctly handles unit conversion and link-capacity limits. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Shard counter updates and batch aggregation for large-room scalability."
      ],
      "correct": 3,
      "explanation": "Treat chat failover recovery controller as a reliability-control decision, not an averages-only optimization. \"Shard counter updates and batch aggregation for large-room scalability\" is correct since it mitigates counter hot spots for high-traffic group chats while keeping containment local. The decision remains valid given: The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-021",
      "type": "multiple-choice",
      "question": "Case Phi: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Presence heartbeat service should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Model presence as soft-state with expiration windows and anti-flap damping\" is strongest because it directly addresses presence flapping due to heartbeat jitter and improves repeatability under stress. This aligns with the extra condition (Compliance requires explicit behavior for edge-case failures).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-022",
      "type": "multiple-choice",
      "question": "Case Chi: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Use per-device sync cursors and deterministic merge rules for unread counters.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Chat Presence, Sync & Reliability, multi-device sync coordinator fails mainly through multi-device unread count divergence. The best choice is \"Use per-device sync cursors and deterministic merge rules for unread counters\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This boundary has failed during the last two game days.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-023",
      "type": "multiple-choice",
      "question": "Case Psi: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Decouple media processing from text delivery and show pending attachment state.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For read-receipt pipeline, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Decouple media processing from text delivery and show pending attachment state\" outperforms the alternatives because it targets attachment processing backlog delaying message visibility and preserves safe recovery behavior. It is also the most compatible with A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-024",
      "type": "multiple-choice",
      "question": "Case Omega: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Apply monotonic sequence checks for receipts and read-state transitions."
      ],
      "correct": 3,
      "explanation": "Treat attachment upload workflow as a reliability-control decision, not an averages-only optimization. \"Apply monotonic sequence checks for receipts and read-state transitions\" is correct since it mitigates read-receipt reorder causing inconsistent user state while keeping containment local. The decision remains valid given: Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-025",
      "type": "multiple-choice",
      "question": "Case Atlas: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Media thumbnail service should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Protect key-distribution flow with versioned key epochs and replay guards\" is strongest because it directly addresses encryption key refresh race across devices and improves repeatability under stress. This aligns with the extra condition (The fix should avoid broad architectural rewrites this quarter).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-026",
      "type": "multiple-choice",
      "question": "Case Nova: unread counter aggregator. Dominant risk is failover replay duplicating old presence events. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Run failover recovery with dedupe tokens for presence and receipt events.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Chat Presence, Sync & Reliability, unread counter aggregator fails mainly through failover replay duplicating old presence events. The best choice is \"Run failover recovery with dedupe tokens for presence and receipt events\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Current metrics hide per-tenant variance that matters.",
      "detailedExplanation": "Generalize from unread counter aggregator to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-027",
      "type": "multiple-choice",
      "question": "Case Orion: device state reconciliation worker. Dominant risk is state sync storms after mobile reconnect wave. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Throttle reconnect sync requests with adaptive backoff and server hints.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For device state reconciliation worker, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Throttle reconnect sync requests with adaptive backoff and server hints\" outperforms the alternatives because it targets state sync storms after mobile reconnect wave and preserves safe recovery behavior. It is also the most compatible with The fallback path is under-tested in production-like load.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-028",
      "type": "multiple-choice",
      "question": "Case Vega: encryption key distribution path. Dominant risk is push fallback sending alerts for already-read messages. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
      ],
      "correct": 3,
      "explanation": "Treat encryption key distribution path as a reliability-control decision, not an averages-only optimization. \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" is correct since it mitigates push fallback sending alerts for already-read messages while keeping containment local. The decision remains valid given: A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-029",
      "type": "multiple-choice",
      "question": "Case Helios: push-notification fallback path. Dominant risk is attachment CDN outage blocking chat critical path. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Add degraded mode where attachment services fail without blocking core chat.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Push-notification fallback path should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Add degraded mode where attachment services fail without blocking core chat\" is strongest because it directly addresses attachment CDN outage blocking chat critical path and improves repeatability under stress. This aligns with the extra condition (The system must preserve critical events over bulk traffic).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that ignore sustained peak transfer constraints. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Common pitfall: planning on average transfer while peak bursts dominate.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-030",
      "type": "multiple-choice",
      "question": "Case Aurora: chat failover recovery controller. Dominant risk is counter hot spots for high-traffic group chats. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Shard counter updates and batch aggregation for large-room scalability.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For chat failover recovery controller, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Shard counter updates and batch aggregation for large-room scalability\" outperforms the alternatives because it targets counter hot spots for high-traffic group chats and preserves safe recovery behavior. It is also the most compatible with Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: presence heartbeat service. Dominant risk is presence flapping due to heartbeat jitter. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Model presence as soft-state with expiration windows and anti-flap damping.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat presence heartbeat service as a reliability-control decision, not an averages-only optimization. \"Model presence as soft-state with expiration windows and anti-flap damping\" is correct since it mitigates presence flapping due to heartbeat jitter while keeping containment local. The decision remains valid given: The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-032",
      "type": "multiple-choice",
      "question": "Case Pulse: multi-device sync coordinator. Dominant risk is multi-device unread count divergence. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Use per-device sync cursors and deterministic merge rules for unread counters."
      ],
      "correct": 3,
      "explanation": "Multi-device sync coordinator should be solved at the failure boundary named in Chat Presence, Sync & Reliability. \"Use per-device sync cursors and deterministic merge rules for unread counters\" is strongest because it directly addresses multi-device unread count divergence and improves repeatability under stress. This aligns with the extra condition (Operational complexity is rising faster than team onboarding).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-033",
      "type": "multiple-choice",
      "question": "Case Forge: read-receipt pipeline. Dominant risk is attachment processing backlog delaying message visibility. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Decouple media processing from text delivery and show pending attachment state.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In Chat Presence, Sync & Reliability, read-receipt pipeline fails mainly through attachment processing backlog delaying message visibility. The best choice is \"Decouple media processing from text delivery and show pending attachment state\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Stakeholders need clear trade-off rationale in the postmortem.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-034",
      "type": "multiple-choice",
      "question": "Case Harbor: attachment upload workflow. Dominant risk is read-receipt reorder causing inconsistent user state. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Apply monotonic sequence checks for receipts and read-state transitions.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For attachment upload workflow, prefer the option that prevents reoccurrence in Chat Presence, Sync & Reliability. \"Apply monotonic sequence checks for receipts and read-state transitions\" outperforms the alternatives because it targets read-receipt reorder causing inconsistent user state and preserves safe recovery behavior. It is also the most compatible with A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-035",
      "type": "multiple-choice",
      "question": "Case Vector: media thumbnail service. Dominant risk is encryption key refresh race across devices. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Protect key-distribution flow with versioned key epochs and replay guards.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat media thumbnail service as a reliability-control decision, not an averages-only optimization. \"Protect key-distribution flow with versioned key epochs and replay guards\" is correct since it mitigates encryption key refresh race across devices while keeping containment local. The decision remains valid given: Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "Generalize from media thumbnail service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures\". It is the option most directly aligned to read-receipt reorder causing inconsistent user state while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for presence heartbeat service: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Run failover recovery with dedupe tokens for presence and receipt events.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for In the \"incident review for presence heartbeat service: signal\" scenario, what should change first before wider rollout, \"Run failover recovery with dedupe tokens for presence and receipt events\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices is a two-step reliability decision. At stage 1, \"The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures\" wins because it balances immediate containment with long-term prevention around encryption key refresh race across devices.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for multi-device sync coordinator:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Throttle reconnect sync requests with adaptive backoff and server hints\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures\" best matches Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events by targeting failover replay duplicating old presence events and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for read-receipt pipeline: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
          ],
          "correct": 3,
          "explanation": "Using the diagnosis from \"incident review for read-receipt pipeline: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave, \"The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures\" is correct because it addresses state sync storms after mobile reconnect wave and improves controllability.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", what should change first before wider rollout?",
          "options": [
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Add degraded mode where attachment services fail without blocking core chat\" best matches With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages is a two-step reliability decision. At stage 1, \"The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures\" wins because it balances immediate containment with long-term prevention around push fallback sending alerts for already-read messages.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for media thumbnail service: signal\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Shard counter updates and batch aggregation for large-room scalability.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Shard counter updates and batch aggregation for large-room scalability\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures\" best matches Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path by targeting attachment CDN outage blocking chat critical path and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for unread counter aggregator: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Model presence as soft-state with expiration windows and anti-flap damping.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident review for unread counter aggregator: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Model presence as soft-state with expiration windows and anti-flap damping\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats, \"The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures\" is correct because it addresses counter hot spots for high-traffic group chats and improves controllability.",
          "detailedExplanation": "Generalize from incident review for device state reconciliation worker: signal points to counter hot to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for device state reconciliation worker:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Use per-device sync cursors and deterministic merge rules for unread counters."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use per-device sync cursors and deterministic merge rules for unread counters\" best matches For \"incident review for device state reconciliation worker:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures\". It is the option most directly aligned to presence flapping due to heartbeat jitter while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for encryption key distribution path:\", what should change first before wider rollout?",
          "options": [
            "Decouple media processing from text delivery and show pending attachment state.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Given the diagnosis in \"incident review for encryption key distribution path:\", what should change first before wider rollout, \"Decouple media processing from text delivery and show pending attachment state\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence is a two-step reliability decision. At stage 1, \"The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures\" wins because it balances immediate containment with long-term prevention around multi-device unread count divergence.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for push-notification fallback path:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Apply monotonic sequence checks for receipts and read-state transitions.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Apply monotonic sequence checks for receipts and read-state transitions\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from chat Presence, Sync & Reliability to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures\" best matches Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility by targeting attachment processing backlog delaying message visibility and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for chat failover recovery controller:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Protect key-distribution flow with versioned key epochs and replay guards.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "With root cause identified for \"incident review for chat failover recovery controller:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Protect key-distribution flow with versioned key epochs and replay guards\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state, \"The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures\" is correct because it addresses read-receipt reorder causing inconsistent user state and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for presence heartbeat service: signal\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Run failover recovery with dedupe tokens for presence and receipt events."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Run failover recovery with dedupe tokens for presence and receipt events\" best matches Now that \"incident review for presence heartbeat service: signal\" is diagnosed, which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures\". It is the option most directly aligned to encryption key refresh race across devices while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for multi-device sync coordinator:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Now that \"incident review for multi-device sync coordinator:\" is diagnosed, which immediate adjustment best addresses the risk, \"Throttle reconnect sync requests with adaptive backoff and server hints\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events is a two-step reliability decision. At stage 1, \"The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures\" wins because it balances immediate containment with long-term prevention around failover replay duplicating old presence events.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident review for read-receipt pipeline: signal\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures\" best matches Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave by targeting state sync storms after mobile reconnect wave and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "With diagnosis confirmed in \"incident review for attachment upload workflow: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Add degraded mode where attachment services fail without blocking core chat\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures\". It is the option most directly aligned to push fallback sending alerts for already-read messages while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for media thumbnail service: signal\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Shard counter updates and batch aggregation for large-room scalability."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Now that \"incident review for media thumbnail service: signal\" is diagnosed, what should change first before wider rollout, \"Shard counter updates and batch aggregation for large-room scalability\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for unread counter aggregator: signal points to attachment CDN outage blocking chat critical path is a two-step reliability decision. At stage 1, \"The current decomposition around unread counter aggregator mismatches attachment CDN outage blocking chat critical path, creating repeated failures\" wins because it balances immediate containment with long-term prevention around attachment CDN outage blocking chat critical path.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for unread counter aggregator: signal\", what should change first before wider rollout?",
          "options": [
            "Model presence as soft-state with expiration windows and anti-flap damping.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Model presence as soft-state with expiration windows and anti-flap damping\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around device state reconciliation worker mismatches counter hot spots for high-traffic group chats, creating repeated failures\" best matches Incident review for device state reconciliation worker: signal points to counter hot spots for high-traffic group chats by targeting counter hot spots for high-traffic group chats and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for device state reconciliation worker:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Use per-device sync cursors and deterministic merge rules for unread counters.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "After diagnosing \"incident review for device state reconciliation worker:\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Use per-device sync cursors and deterministic merge rules for unread counters\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for encryption key distribution path: signal points to presence flapping due to heartbeat jitter, \"The current decomposition around encryption key distribution path mismatches presence flapping due to heartbeat jitter, creating repeated failures\" is correct because it addresses presence flapping due to heartbeat jitter and improves controllability.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for encryption key distribution path:\" scenario, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Decouple media processing from text delivery and show pending attachment state.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Decouple media processing from text delivery and show pending attachment state\" best matches In the \"incident review for encryption key distribution path:\" scenario, which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from chat Presence, Sync & Reliability to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push-notification fallback path: signal points to multi-device unread count divergence. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around push-notification fallback path mismatches multi-device unread count divergence, creating repeated failures\". It is the option most directly aligned to multi-device unread count divergence while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for push-notification fallback path:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Apply monotonic sequence checks for receipts and read-state transitions."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for \"incident review for push-notification fallback path:\", which immediate adjustment best addresses the risk, \"Apply monotonic sequence checks for receipts and read-state transitions\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for chat failover recovery controller: signal points to attachment processing backlog delaying message visibility is a two-step reliability decision. At stage 1, \"The current decomposition around chat failover recovery controller mismatches attachment processing backlog delaying message visibility, creating repeated failures\" wins because it balances immediate containment with long-term prevention around attachment processing backlog delaying message visibility.",
          "detailedExplanation": "Generalize from incident review for chat failover recovery controller: signal points to attachment to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident review for chat failover recovery controller:\", which next step is strongest under current constraints?",
          "options": [
            "Protect key-distribution flow with versioned key epochs and replay guards.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Protect key-distribution flow with versioned key epochs and replay guards\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around presence heartbeat service mismatches read-receipt reorder causing inconsistent user state, creating repeated failures\" best matches Incident review for presence heartbeat service: signal points to read-receipt reorder causing inconsistent user state by targeting read-receipt reorder causing inconsistent user state and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for presence heartbeat service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Run failover recovery with dedupe tokens for presence and receipt events.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident review for presence heartbeat service: signal\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Run failover recovery with dedupe tokens for presence and receipt events\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for multi-device sync coordinator: signal points to encryption key refresh race across devices, \"The current decomposition around multi-device sync coordinator mismatches encryption key refresh race across devices, creating repeated failures\" is correct because it addresses encryption key refresh race across devices and improves controllability.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for multi-device sync coordinator:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Throttle reconnect sync requests with adaptive backoff and server hints.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Throttle reconnect sync requests with adaptive backoff and server hints\" best matches Given the diagnosis in \"incident review for multi-device sync coordinator:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for read-receipt pipeline: signal points to failover replay duplicating old presence events. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Chat Presence, Sync & Reliability, the best answer is \"The current decomposition around read-receipt pipeline mismatches failover replay duplicating old presence events, creating repeated failures\". It is the option most directly aligned to failover replay duplicating old presence events while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for read-receipt pipeline: signal\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Gate push fallbacks on fresh read-state checks to avoid stale alerts."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Chat Presence, Sync & Reliability: for Now that \"incident review for read-receipt pipeline: signal\" is diagnosed, which next step is strongest under current constraints, \"Gate push fallbacks on fresh read-state checks to avoid stale alerts\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for attachment upload workflow: signal points to state sync storms after mobile reconnect wave is a two-step reliability decision. At stage 1, \"The current decomposition around attachment upload workflow mismatches state sync storms after mobile reconnect wave, creating repeated failures\" wins because it balances immediate containment with long-term prevention around state sync storms after mobile reconnect wave.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for attachment upload workflow: signal\", which next change should be prioritized first?",
          "options": [
            "Add degraded mode where attachment services fail without blocking core chat.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Chat Presence, Sync & Reliability, the best answer is \"Add degraded mode where attachment services fail without blocking core chat\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Chat Presence, Sync & Reliability: for Incident review for media thumbnail service: signal points to push fallback sending alerts for already-read messages, \"The current decomposition around media thumbnail service mismatches push fallback sending alerts for already-read messages, creating repeated failures\" is correct because it addresses push fallback sending alerts for already-read messages and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for media thumbnail service: signal\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Shard counter updates and batch aggregation for large-room scalability.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Shard counter updates and batch aggregation for large-room scalability\" best matches Using the diagnosis from \"incident review for media thumbnail service: signal\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-061",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: which signals best identify decomposition boundary mistakes is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-062",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: which controls improve safety on critical write paths needs layered controls, not one silver bullet. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize from controls improve safety on critical write paths? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-063",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Select all options that correctly address this: which practices reduce hot-key or hot-partition impact, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-064",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-065",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: which choices usually lower operational risk at scale is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-066",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: what should be explicit in API/service contracts for this design needs layered controls, not one silver bullet. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-067",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Select all options that correctly address this: which anti-patterns often cause incident recurrence, the highest-signal answer is a bundle of controls. The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-068",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-069",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: which controls protect high-priority traffic during spikes is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-070",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Select all options that correctly address this: which telemetry dimensions are most actionable for design triage, the highest-signal answer is a bundle of controls. The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-071",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Generalize from governance actions improve cross-team reliability ownership? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-072",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: what helps prevent retry amplification cascades is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-073",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: which fallback strategies are strong when dependencies degrade needs layered controls, not one silver bullet. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-074",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Select all options that correctly address this: what reduces data-quality regressions in eventual pipelines, the highest-signal answer is a bundle of controls. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-075",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Chat Presence, Sync & Reliability: The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-076",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Select all options that correctly address this: which architecture choices improve blast-radius containment is intentionally multi-dimensional in Chat Presence, Sync & Reliability. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-077",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Chat Presence, Sync & Reliability, Select all options that correctly address this: what evidence demonstrates a fix worked beyond short-term recovery needs layered controls, not one silver bullet. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Compute failures/day.",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "Use first-pass reliability arithmetic for A critical path handles 5,400,000 requests/day and 0: 9720 requests. Answers within +/-3% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 5,400 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Compute net drain rate.",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "The operational math for Queue ingest is 2,200 events/min and drain is 2,530 events/min gives 330 events/min. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Generalize from queue ingest is 2,200 events/min and drain is 2,530 events/min to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Compute effective attempts/sec.",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "For Retries add 0, the computed target in Chat Presence, Sync & Reliability is 96000 attempts/sec. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize from retries add 0 to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Compute total failover seconds/day.",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Failover takes 16s and occurs 24 times/day: 384 seconds. Answers within +/-0% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 16s and 24 in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Compute percent over target.",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for Target p99 is 650ms; observed p99 is 845ms gives 30 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 650ms and 845ms in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-083",
      "type": "numeric-input",
      "question": "Which answer best fits: if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "Chat Presence, Sync & Reliability expects quick quantitative triage: Which answer best fits: if 34% of 130,000 req/min are high-priority, how many high-priority req/min evaluates to 44200 requests/min. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 34 and 130,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Compute percent reduction.",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Error rate drops from 1, the computed target in Chat Presence, Sync & Reliability is 78 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 1.0 and 0.22 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Compute minimum acknowledgements.",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for A 9-node quorum cluster requires majority writes: 5 acks. Answers within +/-0% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. Consistency decisions should be explicit about which conflicts are acceptable and why. If values like 9 and 5 appear, convert them into one unit basis before comparison. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Compute minutes to clear.",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "The operational math for Backlog is 56,000 tasks with net drain 350 tasks/min gives 160 minutes. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Compute percent remaining available.",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Chat Presence, Sync & Reliability expects quick quantitative triage: A fleet has 18 zones and 3 are unavailable evaluates to 83.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 18 and 3 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Compute percent reduction.",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For MTTR improved from 52 min to 34 min, the computed target in Chat Presence, Sync & Reliability is 34.62 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize from mTTR improved from 52 min to 34 min to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 52 min and 34 min should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-089",
      "type": "numeric-input",
      "question": "Which answer best fits: if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for Which answer best fits: if 11% of 2,800,000 daily ops need manual checks, checks/day: 308000 operations. Answers within +/-2% show correct directional reasoning for Chat Presence, Sync & Reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 11 and 2,800 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-090",
      "type": "ordering",
      "question": "Order a classic-design decomposition workflow. Use a chat presence, sync & reliability perspective.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Identify critical user journey and invariants must happen before Validate with load/failure drills and refine. That ordering matches incident-safe flow in Chat Presence, Sync & Reliability.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-091",
      "type": "ordering",
      "question": "Order by increasing design risk. (chat presence, sync & reliability lens)",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Presence, Sync & Reliability should start with Explicit boundaries with contracts and end with Implicit coupling with no ownership. Order by increasing design risk rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-092",
      "type": "ordering",
      "question": "For chat presence, sync & reliability, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For chat presence, sync & reliability, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Place obvious extremes first, then sort the middle by pairwise comparison. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-093",
      "type": "ordering",
      "question": "Order by increasing retry-control maturity. Focus on chat presence, sync & reliability tradeoffs.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Chat Presence, Sync & Reliability emphasizes safe recovery order. Beginning at Fixed immediate retries and finishing at Jittered retries with retry budgets and telemetry keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-094",
      "type": "ordering",
      "question": "In this chat presence, sync & reliability context, order fallback sophistication.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Implicit fallback behavior must happen before Policy-driven automated fallback with tests. That ordering matches incident-safe flow in Chat Presence, Sync & Reliability.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-095",
      "type": "ordering",
      "question": "Within chat presence, sync & reliability, order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Presence, Sync & Reliability should start with Host health check only and end with Staged shift plus failback rehearsal and rollback gates. Within chat presence, sync & reliability, order failover validation rigor rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-096",
      "type": "ordering",
      "question": "Sequence these from minimal to maximal blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Sequence these from minimal to maximal blast radius, the correct ordering runs from Single process failure to Cross-region control-plane failure. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-097",
      "type": "ordering",
      "question": "Considering chat presence, sync & reliability, order data-path durability confidence.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Chat Presence, Sync & Reliability emphasizes safe recovery order. Beginning at In-memory only acknowledgment and finishing at Replicated durable write plus replay/integrity verification keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Generalize from order data-path durability confidence to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-098",
      "type": "ordering",
      "question": "Order by increasing operational discipline. Use a chat presence, sync & reliability perspective.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc incident response must happen before Role-based response plus action closure tracking. That ordering matches incident-safe flow in Chat Presence, Sync & Reliability.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-099",
      "type": "ordering",
      "question": "Order rollout safety for major design changes. (chat presence, sync & reliability lens)",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Presence, Sync & Reliability should start with Canary small cohort and end with Finalize runbook and ownership updates. Order rollout safety for major design changes rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-cp-100",
      "type": "ordering",
      "question": "From a chat presence, sync & reliability viewpoint, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a chat presence, sync & reliability viewpoint, order evidence strength for fix success, the correct ordering runs from Single successful test run to Sustained recovery plus failure-drill pass. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "chat-presence-sync-and-reliability"],
      "difficulty": "staff-level"
    }
  ]
}
