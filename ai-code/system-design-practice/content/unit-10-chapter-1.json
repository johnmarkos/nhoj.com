{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 1,
  "chapterTitle": "Twitter/X Timeline Write & Fanout",
  "chapterDescription": "Decompose tweet ingest, fanout strategy, and home timeline assembly decisions under realistic scale constraints.",
  "problems": [
    {
      "id": "cd-tw-001",
      "type": "multiple-choice",
      "question": "Case Alpha: tweet publish API. Dominant risk is fanout queue saturation during burst posting. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Split fanout policy by account class and use fanout-on-read for celebrity traffic.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For tweet publish API, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Split fanout policy by account class and use fanout-on-read for celebrity traffic\" outperforms the alternatives because it targets fanout queue saturation during burst posting and preserves safe recovery behavior. It is also the most compatible with The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-002",
      "type": "multiple-choice",
      "question": "Case Beta: home timeline service. Dominant risk is celebrity write amplification overwhelming worker pools. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Twitter/X Timeline Write & Fanout, home timeline service fails mainly through celebrity write amplification overwhelming worker pools. The best choice is \"Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-003",
      "type": "multiple-choice",
      "question": "Case Gamma: celebrity account fanout path. Dominant risk is stale timeline cache entries after deletes. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use timeline segment caches with bounded staleness and targeted invalidation on deletes.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Celebrity account fanout path should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Use timeline segment caches with bounded staleness and targeted invalidation on deletes\" is strongest because it directly addresses stale timeline cache entries after deletes and improves repeatability under stress. This aligns with the extra condition (Leadership asked for a fix that reduces recurrence, not just MTTR).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-004",
      "type": "multiple-choice",
      "question": "Case Delta: follower-graph store. Dominant risk is hot partition in follower graph lookups. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Shard follower graph by stable user key and add hot-key replication for skewed accounts."
      ],
      "correct": 3,
      "explanation": "Treat follower-graph store as a reliability-control decision, not an averages-only optimization. \"Shard follower graph by stable user key and add hot-key replication for skewed accounts\" is correct since it mitigates hot partition in follower graph lookups while keeping containment local. The decision remains valid given: A previous rollback fixed averages but not tail impact.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: timeline cache layer. Dominant risk is read path latency spikes from fragmented data fetches. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Assemble home feed with tiered fetch strategy to cap tail latency under partial misses.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For timeline cache layer, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Assemble home feed with tiered fetch strategy to cap tail latency under partial misses\" outperforms the alternatives because it targets read path latency spikes from fragmented data fetches and preserves safe recovery behavior. It is also the most compatible with User trust risk is highest on this path.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-006",
      "type": "multiple-choice",
      "question": "Case Zeta: feed backfill worker. Dominant risk is duplicate fanout jobs causing timeline inconsistencies. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Twitter/X Timeline Write & Fanout, feed backfill worker fails mainly through duplicate fanout jobs causing timeline inconsistencies. The best choice is \"Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A shared dependency has uncertain health right now.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-007",
      "type": "multiple-choice",
      "question": "Case Eta: content moderation pre-check. Dominant risk is write/read path coupling causing cascading slowdowns. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Content moderation pre-check should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets\" is strongest because it directly addresses write/read path coupling causing cascading slowdowns and improves repeatability under stress. This aligns with the extra condition (The change must preserve cost discipline during peak).",
      "detailedExplanation": "Generalize from content moderation pre-check to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the answer that survives a sanity check against known anchor numbers. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-008",
      "type": "multiple-choice",
      "question": "Case Theta: tweet metadata pipeline. Dominant risk is global ordering assumptions broken across regions. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Use per-region ordering guarantees with explicit cross-region reconciliation semantics."
      ],
      "correct": 3,
      "explanation": "Treat tweet metadata pipeline as a reliability-control decision, not an averages-only optimization. \"Use per-region ordering guarantees with explicit cross-region reconciliation semantics\" is correct since it mitigates global ordering assumptions broken across regions while keeping containment local. The decision remains valid given: Telemetry shows risk concentrated in one partition class.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-009",
      "type": "multiple-choice",
      "question": "Case Iota: cold-start timeline builder. Dominant risk is backfill lag exceeding freshness SLO. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For cold-start timeline builder, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated\" outperforms the alternatives because it targets backfill lag exceeding freshness SLO and preserves safe recovery behavior. It is also the most compatible with The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-010",
      "type": "multiple-choice",
      "question": "Case Kappa: regional timeline edge cache. Dominant risk is delete propagation delays violating product expectations. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Design delete fanout as priority workflow so visibility guarantees match product policy.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Regional timeline edge cache should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Design delete fanout as priority workflow so visibility guarantees match product policy\" is strongest because it directly addresses delete propagation delays violating product expectations and improves repeatability under stress. This aligns with the extra condition (Current runbooks are missing explicit ownership for this boundary).",
      "detailedExplanation": "Generalize from regional timeline edge cache to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-011",
      "type": "multiple-choice",
      "question": "Case Lambda: tweet publish API. Dominant risk is fanout queue saturation during burst posting. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Split fanout policy by account class and use fanout-on-read for celebrity traffic.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat tweet publish API as a reliability-control decision, not an averages-only optimization. \"Split fanout policy by account class and use fanout-on-read for celebrity traffic\" is correct since it mitigates fanout queue saturation during burst posting while keeping containment local. The decision remains valid given: A cross-region path recently changed behavior after migration.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-012",
      "type": "multiple-choice",
      "question": "Case Mu: home timeline service. Dominant risk is celebrity write amplification overwhelming worker pools. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs."
      ],
      "correct": 3,
      "explanation": "For home timeline service, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs\" outperforms the alternatives because it targets celebrity write amplification overwhelming worker pools and preserves safe recovery behavior. It is also the most compatible with Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-013",
      "type": "multiple-choice",
      "question": "Case Nu: celebrity account fanout path. Dominant risk is stale timeline cache entries after deletes. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Use timeline segment caches with bounded staleness and targeted invalidation on deletes.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In Twitter/X Timeline Write & Fanout, celebrity account fanout path fails mainly through stale timeline cache entries after deletes. The best choice is \"Use timeline segment caches with bounded staleness and targeted invalidation on deletes\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-014",
      "type": "multiple-choice",
      "question": "Case Xi: follower-graph store. Dominant risk is hot partition in follower graph lookups. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Shard follower graph by stable user key and add hot-key replication for skewed accounts.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Follower-graph store should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Shard follower graph by stable user key and add hot-key replication for skewed accounts\" is strongest because it directly addresses hot partition in follower graph lookups and improves repeatability under stress. This aligns with the extra condition (A partial failure is masking itself as success in metrics).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-015",
      "type": "multiple-choice",
      "question": "Case Omicron: timeline cache layer. Dominant risk is read path latency spikes from fragmented data fetches. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Assemble home feed with tiered fetch strategy to cap tail latency under partial misses.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat timeline cache layer as a reliability-control decision, not an averages-only optimization. \"Assemble home feed with tiered fetch strategy to cap tail latency under partial misses\" is correct since it mitigates read path latency spikes from fragmented data fetches while keeping containment local. The decision remains valid given: This fix must hold under celebrity or campaign spike conditions.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-016",
      "type": "multiple-choice",
      "question": "Case Pi: feed backfill worker. Dominant risk is duplicate fanout jobs causing timeline inconsistencies. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts."
      ],
      "correct": 3,
      "explanation": "For feed backfill worker, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts\" outperforms the alternatives because it targets duplicate fanout jobs causing timeline inconsistencies and preserves safe recovery behavior. It is also the most compatible with SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-017",
      "type": "multiple-choice",
      "question": "Case Rho: content moderation pre-check. Dominant risk is write/read path coupling causing cascading slowdowns. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In Twitter/X Timeline Write & Fanout, content moderation pre-check fails mainly through write/read path coupling causing cascading slowdowns. The best choice is \"Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: On-call requested a reversible operational first step.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard options that depend on unrealistic assumptions hidden in the estimate. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-018",
      "type": "multiple-choice",
      "question": "Case Sigma: tweet metadata pipeline. Dominant risk is global ordering assumptions broken across regions. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Use per-region ordering guarantees with explicit cross-region reconciliation semantics.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Tweet metadata pipeline should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Use per-region ordering guarantees with explicit cross-region reconciliation semantics\" is strongest because it directly addresses global ordering assumptions broken across regions and improves repeatability under stress. This aligns with the extra condition (The system mixes strict and eventual paths with unclear contracts).",
      "detailedExplanation": "Generalize from tweet metadata pipeline to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Eliminate approaches that hand-wave conflict resolution or quorum behavior. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-019",
      "type": "multiple-choice",
      "question": "Case Tau: cold-start timeline builder. Dominant risk is backfill lag exceeding freshness SLO. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat cold-start timeline builder as a reliability-control decision, not an averages-only optimization. \"Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated\" is correct since it mitigates backfill lag exceeding freshness SLO while keeping containment local. The decision remains valid given: A hot-key pattern is likely from real traffic skew.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: regional timeline edge cache. Dominant risk is delete propagation delays violating product expectations. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Design delete fanout as priority workflow so visibility guarantees match product policy."
      ],
      "correct": 3,
      "explanation": "In Twitter/X Timeline Write & Fanout, regional timeline edge cache fails mainly through delete propagation delays violating product expectations. The best choice is \"Design delete fanout as priority workflow so visibility guarantees match product policy\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-021",
      "type": "multiple-choice",
      "question": "Case Phi: tweet publish API. Dominant risk is fanout queue saturation during burst posting. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Split fanout policy by account class and use fanout-on-read for celebrity traffic.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Tweet publish API should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Split fanout policy by account class and use fanout-on-read for celebrity traffic\" is strongest because it directly addresses fanout queue saturation during burst posting and improves repeatability under stress. This aligns with the extra condition (Compliance requires explicit behavior for edge-case failures).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-022",
      "type": "multiple-choice",
      "question": "Case Chi: home timeline service. Dominant risk is celebrity write amplification overwhelming worker pools. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat home timeline service as a reliability-control decision, not an averages-only optimization. \"Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs\" is correct since it mitigates celebrity write amplification overwhelming worker pools while keeping containment local. The decision remains valid given: This boundary has failed during the last two game days.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-023",
      "type": "multiple-choice",
      "question": "Case Psi: celebrity account fanout path. Dominant risk is stale timeline cache entries after deletes. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use timeline segment caches with bounded staleness and targeted invalidation on deletes.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For celebrity account fanout path, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Use timeline segment caches with bounded staleness and targeted invalidation on deletes\" outperforms the alternatives because it targets stale timeline cache entries after deletes and preserves safe recovery behavior. It is also the most compatible with A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-024",
      "type": "multiple-choice",
      "question": "Case Omega: follower-graph store. Dominant risk is hot partition in follower graph lookups. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Shard follower graph by stable user key and add hot-key replication for skewed accounts."
      ],
      "correct": 3,
      "explanation": "In Twitter/X Timeline Write & Fanout, follower-graph store fails mainly through hot partition in follower graph lookups. The best choice is \"Shard follower graph by stable user key and add hot-key replication for skewed accounts\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-025",
      "type": "multiple-choice",
      "question": "Case Atlas: timeline cache layer. Dominant risk is read path latency spikes from fragmented data fetches. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Assemble home feed with tiered fetch strategy to cap tail latency under partial misses.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Timeline cache layer should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Assemble home feed with tiered fetch strategy to cap tail latency under partial misses\" is strongest because it directly addresses read path latency spikes from fragmented data fetches and improves repeatability under stress. This aligns with the extra condition (The fix should avoid broad architectural rewrites this quarter).",
      "detailedExplanation": "Generalize from timeline cache layer to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-026",
      "type": "multiple-choice",
      "question": "Case Nova: feed backfill worker. Dominant risk is duplicate fanout jobs causing timeline inconsistencies. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat feed backfill worker as a reliability-control decision, not an averages-only optimization. \"Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts\" is correct since it mitigates duplicate fanout jobs causing timeline inconsistencies while keeping containment local. The decision remains valid given: Current metrics hide per-tenant variance that matters.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-027",
      "type": "multiple-choice",
      "question": "Case Orion: content moderation pre-check. Dominant risk is write/read path coupling causing cascading slowdowns. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For content moderation pre-check, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets\" outperforms the alternatives because it targets write/read path coupling causing cascading slowdowns and preserves safe recovery behavior. It is also the most compatible with The fallback path is under-tested in production-like load.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-028",
      "type": "multiple-choice",
      "question": "Case Vega: tweet metadata pipeline. Dominant risk is global ordering assumptions broken across regions. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Use per-region ordering guarantees with explicit cross-region reconciliation semantics."
      ],
      "correct": 3,
      "explanation": "In Twitter/X Timeline Write & Fanout, tweet metadata pipeline fails mainly through global ordering assumptions broken across regions. The best choice is \"Use per-region ordering guarantees with explicit cross-region reconciliation semantics\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-029",
      "type": "multiple-choice",
      "question": "Case Helios: cold-start timeline builder. Dominant risk is backfill lag exceeding freshness SLO. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Cold-start timeline builder should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated\" is strongest because it directly addresses backfill lag exceeding freshness SLO and improves repeatability under stress. This aligns with the extra condition (The system must preserve critical events over bulk traffic).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-030",
      "type": "multiple-choice",
      "question": "Case Aurora: regional timeline edge cache. Dominant risk is delete propagation delays violating product expectations. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Design delete fanout as priority workflow so visibility guarantees match product policy.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For regional timeline edge cache, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Design delete fanout as priority workflow so visibility guarantees match product policy\" outperforms the alternatives because it targets delete propagation delays violating product expectations and preserves safe recovery behavior. It is also the most compatible with Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: tweet publish API. Dominant risk is fanout queue saturation during burst posting. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Split fanout policy by account class and use fanout-on-read for celebrity traffic.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Twitter/X Timeline Write & Fanout, tweet publish API fails mainly through fanout queue saturation during burst posting. The best choice is \"Split fanout policy by account class and use fanout-on-read for celebrity traffic\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-032",
      "type": "multiple-choice",
      "question": "Case Pulse: home timeline service. Dominant risk is celebrity write amplification overwhelming worker pools. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs."
      ],
      "correct": 3,
      "explanation": "Home timeline service should be solved at the failure boundary named in Twitter/X Timeline Write & Fanout. \"Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs\" is strongest because it directly addresses celebrity write amplification overwhelming worker pools and improves repeatability under stress. This aligns with the extra condition (Operational complexity is rising faster than team onboarding).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-033",
      "type": "multiple-choice",
      "question": "Case Forge: celebrity account fanout path. Dominant risk is stale timeline cache entries after deletes. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Use timeline segment caches with bounded staleness and targeted invalidation on deletes.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat celebrity account fanout path as a reliability-control decision, not an averages-only optimization. \"Use timeline segment caches with bounded staleness and targeted invalidation on deletes\" is correct since it mitigates stale timeline cache entries after deletes while keeping containment local. The decision remains valid given: Stakeholders need clear trade-off rationale in the postmortem.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-034",
      "type": "multiple-choice",
      "question": "Case Harbor: follower-graph store. Dominant risk is hot partition in follower graph lookups. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Shard follower graph by stable user key and add hot-key replication for skewed accounts.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For follower-graph store, prefer the option that prevents reoccurrence in Twitter/X Timeline Write & Fanout. \"Shard follower graph by stable user key and add hot-key replication for skewed accounts\" outperforms the alternatives because it targets hot partition in follower graph lookups and preserves safe recovery behavior. It is also the most compatible with A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject designs that improve throughput while weakening reliability guarantees. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-035",
      "type": "multiple-choice",
      "question": "Case Vector: timeline cache layer. Dominant risk is read path latency spikes from fragmented data fetches. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Assemble home feed with tiered fetch strategy to cap tail latency under partial misses.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Twitter/X Timeline Write & Fanout, timeline cache layer fails mainly through read path latency spikes from fragmented data fetches. The best choice is \"Assemble home feed with tiered fetch strategy to cap tail latency under partial misses\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for tweet publish API: signal points to hot partition in follower graph lookups. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around tweet publish API mismatches hot partition in follower graph lookups, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around tweet publish API mismatches hot partition in follower graph lookups, creating repeated failures\" best matches Incident review for tweet publish API: signal points to hot partition in follower graph lookups by targeting hot partition in follower graph lookups and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for tweet publish API: signal points to\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For \"incident review for tweet publish API: signal points to\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize from twitter/X Timeline Write & Fanout to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for home timeline service: signal points to read path latency spikes from fragmented data fetches. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around home timeline service mismatches read path latency spikes from fragmented data fetches, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Twitter/X Timeline Write & Fanout: for Incident review for home timeline service: signal points to read path latency spikes from fragmented data fetches, \"The current decomposition around home timeline service mismatches read path latency spikes from fragmented data fetches, creating repeated failures\" is correct because it addresses read path latency spikes from fragmented data fetches and improves controllability.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident review for home timeline service: signal\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets\" best matches Given the diagnosis in \"incident review for home timeline service: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for celebrity account fanout path: signal points to duplicate fanout jobs causing timeline inconsistencies. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around celebrity account fanout path mismatches duplicate fanout jobs causing timeline inconsistencies, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Twitter/X Timeline Write & Fanout, the best answer is \"The current decomposition around celebrity account fanout path mismatches duplicate fanout jobs causing timeline inconsistencies, creating repeated failures\". It is the option most directly aligned to duplicate fanout jobs causing timeline inconsistencies while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident review for celebrity account fanout path: signal points to duplicate fanout to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for celebrity account fanout path:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Use per-region ordering guarantees with explicit cross-region reconciliation semantics."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Twitter/X Timeline Write & Fanout: for With diagnosis confirmed in \"incident review for celebrity account fanout path:\", what first move gives the best reliability impact, \"Use per-region ordering guarantees with explicit cross-region reconciliation semantics\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for follower-graph store: signal points to write/read path coupling causing cascading slowdowns. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around follower-graph store mismatches write/read path coupling causing cascading slowdowns, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for follower-graph store: signal points to write/read path coupling causing cascading slowdowns is a two-step reliability decision. At stage 1, \"The current decomposition around follower-graph store mismatches write/read path coupling causing cascading slowdowns, creating repeated failures\" wins because it balances immediate containment with long-term prevention around write/read path coupling causing cascading slowdowns.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for follower-graph store: signal points\", what is the highest-leverage change to make now?",
          "options": [
            "Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Twitter/X Timeline Write & Fanout, the best answer is \"Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Twitter/X Timeline Write & Fanout: for Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions, \"The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures\" is correct because it addresses global ordering assumptions broken across regions and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for timeline cache layer: signal points\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Design delete fanout as priority workflow so visibility guarantees match product policy.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Design delete fanout as priority workflow so visibility guarantees match product policy\" best matches With root cause identified for \"incident review for timeline cache layer: signal points\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for feed backfill worker: signal points to backfill lag exceeding freshness SLO. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around feed backfill worker mismatches backfill lag exceeding freshness SLO, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Twitter/X Timeline Write & Fanout, the best answer is \"The current decomposition around feed backfill worker mismatches backfill lag exceeding freshness SLO, creating repeated failures\". It is the option most directly aligned to backfill lag exceeding freshness SLO while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for feed backfill worker: signal points\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Split fanout policy by account class and use fanout-on-read for celebrity traffic.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Twitter/X Timeline Write & Fanout: for After diagnosing \"incident review for feed backfill worker: signal points\", which next change should be prioritized first, \"Split fanout policy by account class and use fanout-on-read for celebrity traffic\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for content moderation pre-check: signal points to delete propagation delays violating product expectations. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around content moderation pre-check mismatches delete propagation delays violating product expectations, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Incident review for content moderation pre-check: signal points to delete propagation delays violating product expectations is a two-step reliability decision. At stage 1, \"The current decomposition around content moderation pre-check mismatches delete propagation delays violating product expectations, creating repeated failures\" wins because it balances immediate containment with long-term prevention around delete propagation delays violating product expectations.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for content moderation pre-check:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Twitter/X Timeline Write & Fanout, the best answer is \"Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for tweet metadata pipeline: signal points to fanout queue saturation during burst posting. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around tweet metadata pipeline mismatches fanout queue saturation during burst posting, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around tweet metadata pipeline mismatches fanout queue saturation during burst posting, creating repeated failures\" best matches Incident review for tweet metadata pipeline: signal points to fanout queue saturation during burst posting by targeting fanout queue saturation during burst posting and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for tweet metadata pipeline: signal\", what should change first before wider rollout?",
          "options": [
            "Use timeline segment caches with bounded staleness and targeted invalidation on deletes.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For \"incident review for tweet metadata pipeline: signal\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Use timeline segment caches with bounded staleness and targeted invalidation on deletes\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize from twitter/X Timeline Write & Fanout to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for cold-start timeline builder: signal points to celebrity write amplification overwhelming worker pools. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around cold-start timeline builder mismatches celebrity write amplification overwhelming worker pools, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Twitter/X Timeline Write & Fanout: for Incident review for cold-start timeline builder: signal points to celebrity write amplification overwhelming worker pools, \"The current decomposition around cold-start timeline builder mismatches celebrity write amplification overwhelming worker pools, creating repeated failures\" is correct because it addresses celebrity write amplification overwhelming worker pools and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for cold-start timeline builder: signal\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Shard follower graph by stable user key and add hot-key replication for skewed accounts.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Shard follower graph by stable user key and add hot-key replication for skewed accounts\" best matches Using the diagnosis from \"incident review for cold-start timeline builder: signal\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional timeline edge cache: signal points to stale timeline cache entries after deletes. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around regional timeline edge cache mismatches stale timeline cache entries after deletes, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Twitter/X Timeline Write & Fanout, the best answer is \"The current decomposition around regional timeline edge cache mismatches stale timeline cache entries after deletes, creating repeated failures\". It is the option most directly aligned to stale timeline cache entries after deletes while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident review for regional timeline edge cache: signal points to stale timeline cache to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for regional timeline edge cache:\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Assemble home feed with tiered fetch strategy to cap tail latency under partial misses.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Twitter/X Timeline Write & Fanout: for With diagnosis confirmed in \"incident review for regional timeline edge cache:\", which next change should be prioritized first, \"Assemble home feed with tiered fetch strategy to cap tail latency under partial misses\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for tweet publish API: signal points to hot partition in follower graph lookups. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around tweet publish API mismatches hot partition in follower graph lookups, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Incident review for tweet publish API: signal points to hot partition in follower graph lookups is a two-step reliability decision. At stage 1, \"The current decomposition around tweet publish API mismatches hot partition in follower graph lookups, creating repeated failures\" wins because it balances immediate containment with long-term prevention around hot partition in follower graph lookups.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for tweet publish API: signal points to\" scenario, what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Twitter/X Timeline Write & Fanout, the best answer is \"Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for home timeline service: signal points to read path latency spikes from fragmented data fetches. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around home timeline service mismatches read path latency spikes from fragmented data fetches, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around home timeline service mismatches read path latency spikes from fragmented data fetches, creating repeated failures\" best matches Incident review for home timeline service: signal points to read path latency spikes from fragmented data fetches by targeting read path latency spikes from fragmented data fetches and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for home timeline service: signal\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Now that \"incident review for home timeline service: signal\" is diagnosed, what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for celebrity account fanout path: signal points to duplicate fanout jobs causing timeline inconsistencies. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around celebrity account fanout path mismatches duplicate fanout jobs causing timeline inconsistencies, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Twitter/X Timeline Write & Fanout: for Incident review for celebrity account fanout path: signal points to duplicate fanout jobs causing timeline inconsistencies, \"The current decomposition around celebrity account fanout path mismatches duplicate fanout jobs causing timeline inconsistencies, creating repeated failures\" is correct because it addresses duplicate fanout jobs causing timeline inconsistencies and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for celebrity account fanout path:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Use per-region ordering guarantees with explicit cross-region reconciliation semantics.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use per-region ordering guarantees with explicit cross-region reconciliation semantics\" best matches With root cause identified for \"incident review for celebrity account fanout path:\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for follower-graph store: signal points to write/read path coupling causing cascading slowdowns. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around follower-graph store mismatches write/read path coupling causing cascading slowdowns, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Twitter/X Timeline Write & Fanout, the best answer is \"The current decomposition around follower-graph store mismatches write/read path coupling causing cascading slowdowns, creating repeated failures\". It is the option most directly aligned to write/read path coupling causing cascading slowdowns while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for follower-graph store: signal points\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Twitter/X Timeline Write & Fanout: for After diagnosing \"incident review for follower-graph store: signal points\", what is the highest-leverage change to make now, \"Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures\" best matches Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions by targeting global ordering assumptions broken across regions and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for timeline cache layer: signal points\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Design delete fanout as priority workflow so visibility guarantees match product policy."
          ],
          "correct": 3,
          "explanation": "Now that \"incident review for timeline cache layer: signal points\" is diagnosed, which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Design delete fanout as priority workflow so visibility guarantees match product policy\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for feed backfill worker: signal points to backfill lag exceeding freshness SLO. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around feed backfill worker mismatches backfill lag exceeding freshness SLO, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Twitter/X Timeline Write & Fanout: for Incident review for feed backfill worker: signal points to backfill lag exceeding freshness SLO, \"The current decomposition around feed backfill worker mismatches backfill lag exceeding freshness SLO, creating repeated failures\" is correct because it addresses backfill lag exceeding freshness SLO and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for feed backfill worker: signal points\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Split fanout policy by account class and use fanout-on-read for celebrity traffic.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Split fanout policy by account class and use fanout-on-read for celebrity traffic\" best matches In the \"incident review for feed backfill worker: signal points\" scenario, which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for content moderation pre-check: signal points to delete propagation delays violating product expectations. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around content moderation pre-check mismatches delete propagation delays violating product expectations, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Twitter/X Timeline Write & Fanout, the best answer is \"The current decomposition around content moderation pre-check mismatches delete propagation delays violating product expectations, creating repeated failures\". It is the option most directly aligned to delete propagation delays violating product expectations while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident review for content moderation pre-check: signal points to delete propagation to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for content moderation pre-check:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Twitter/X Timeline Write & Fanout: for With diagnosis confirmed in \"incident review for content moderation pre-check:\", what should change first before wider rollout, \"Precompute timelines for normal accounts while rate-limiting high-amplification fanout jobs\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for tweet metadata pipeline: signal points to fanout queue saturation during burst posting. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around tweet metadata pipeline mismatches fanout queue saturation during burst posting, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Incident review for tweet metadata pipeline: signal points to fanout queue saturation during burst posting is a two-step reliability decision. At stage 1, \"The current decomposition around tweet metadata pipeline mismatches fanout queue saturation during burst posting, creating repeated failures\" wins because it balances immediate containment with long-term prevention around fanout queue saturation during burst posting.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for tweet metadata pipeline: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Use timeline segment caches with bounded staleness and targeted invalidation on deletes.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Twitter/X Timeline Write & Fanout, the best answer is \"Use timeline segment caches with bounded staleness and targeted invalidation on deletes\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for cold-start timeline builder: signal points to celebrity write amplification overwhelming worker pools. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around cold-start timeline builder mismatches celebrity write amplification overwhelming worker pools, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around cold-start timeline builder mismatches celebrity write amplification overwhelming worker pools, creating repeated failures\" best matches Incident review for cold-start timeline builder: signal points to celebrity write amplification overwhelming worker pools by targeting celebrity write amplification overwhelming worker pools and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for cold-start timeline builder: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Shard follower graph by stable user key and add hot-key replication for skewed accounts."
          ],
          "correct": 3,
          "explanation": "For \"incident review for cold-start timeline builder: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Shard follower graph by stable user key and add hot-key replication for skewed accounts\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize from twitter/X Timeline Write & Fanout to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional timeline edge cache: signal points to stale timeline cache entries after deletes. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around regional timeline edge cache mismatches stale timeline cache entries after deletes, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Twitter/X Timeline Write & Fanout: for Incident review for regional timeline edge cache: signal points to stale timeline cache entries after deletes, \"The current decomposition around regional timeline edge cache mismatches stale timeline cache entries after deletes, creating repeated failures\" is correct because it addresses stale timeline cache entries after deletes and improves controllability.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for regional timeline edge cache:\", which next step is strongest under current constraints?",
          "options": [
            "Assemble home feed with tiered fetch strategy to cap tail latency under partial misses.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Assemble home feed with tiered fetch strategy to cap tail latency under partial misses\" best matches Given the diagnosis in \"incident review for regional timeline edge cache:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for tweet publish API: signal points to hot partition in follower graph lookups. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around tweet publish API mismatches hot partition in follower graph lookups, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Twitter/X Timeline Write & Fanout, the best answer is \"The current decomposition around tweet publish API mismatches hot partition in follower graph lookups, creating repeated failures\". It is the option most directly aligned to hot partition in follower graph lookups while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for tweet publish API: signal points to\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Twitter/X Timeline Write & Fanout: for After diagnosing \"incident review for tweet publish API: signal points to\", what should change first before wider rollout, \"Enforce idempotent fanout jobs with dedupe keys to avoid duplicate timeline inserts\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for home timeline service: signal points to read path latency spikes from fragmented data fetches. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around home timeline service mismatches read path latency spikes from fragmented data fetches, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Incident review for home timeline service: signal points to read path latency spikes from fragmented data fetches is a two-step reliability decision. At stage 1, \"The current decomposition around home timeline service mismatches read path latency spikes from fragmented data fetches, creating repeated failures\" wins because it balances immediate containment with long-term prevention around read path latency spikes from fragmented data fetches.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for home timeline service: signal\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Twitter/X Timeline Write & Fanout, the best answer is \"Decouple publish path from heavy enrichment using async pipelines and strict timeout budgets\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for celebrity account fanout path: signal points to duplicate fanout jobs causing timeline inconsistencies. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around celebrity account fanout path mismatches duplicate fanout jobs causing timeline inconsistencies, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around celebrity account fanout path mismatches duplicate fanout jobs causing timeline inconsistencies, creating repeated failures\" best matches Incident review for celebrity account fanout path: signal points to duplicate fanout jobs causing timeline inconsistencies by targeting duplicate fanout jobs causing timeline inconsistencies and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for celebrity account fanout path:\" is diagnosed, what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Use per-region ordering guarantees with explicit cross-region reconciliation semantics."
          ],
          "correct": 3,
          "explanation": "Now that \"incident review for celebrity account fanout path:\" is diagnosed, what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Use per-region ordering guarantees with explicit cross-region reconciliation semantics\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for follower-graph store: signal points to write/read path coupling causing cascading slowdowns. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around follower-graph store mismatches write/read path coupling causing cascading slowdowns, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Twitter/X Timeline Write & Fanout: for Incident review for follower-graph store: signal points to write/read path coupling causing cascading slowdowns, \"The current decomposition around follower-graph store mismatches write/read path coupling causing cascading slowdowns, creating repeated failures\" is correct because it addresses write/read path coupling causing cascading slowdowns and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for follower-graph store: signal points\" scenario, what should change first before wider rollout?",
          "options": [
            "Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Track backfill lag as a first-class SLO and throttle non-critical recomputation when saturated\" best matches In the \"incident review for follower-graph store: signal points\" scenario, what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for timeline cache layer: signal points to global ordering assumptions broken across regions is a two-step reliability decision. At stage 1, \"The current decomposition around timeline cache layer mismatches global ordering assumptions broken across regions, creating repeated failures\" wins because it balances immediate containment with long-term prevention around global ordering assumptions broken across regions.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident review for timeline cache layer: signal points\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Design delete fanout as priority workflow so visibility guarantees match product policy.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Twitter/X Timeline Write & Fanout, the best answer is \"Design delete fanout as priority workflow so visibility guarantees match product policy\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-061",
      "type": "multi-select",
      "question": "Which signals best identify decomposition boundary mistakes? (Select all that apply)",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Which signals best identify decomposition boundary mistakes is intentionally multi-dimensional in Twitter/X Timeline Write & Fanout. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from signals best identify decomposition boundary mistakes? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-062",
      "type": "multi-select",
      "question": "Which controls improve safety on critical write paths? (Select all that apply)",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Twitter/X Timeline Write & Fanout: The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-063",
      "type": "multi-select",
      "question": "Which practices reduce hot-key or hot-partition impact? (Select all that apply)",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Which practices reduce hot-key or hot-partition impact, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-064",
      "type": "multi-select",
      "question": "What improves reliability when mixing sync and async paths? (Select all that apply)",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Twitter/X Timeline Write & Fanout, What improves reliability when mixing sync and async paths needs layered controls, not one silver bullet. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-065",
      "type": "multi-select",
      "question": "Which choices usually lower operational risk at scale? (Select all that apply)",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Which choices usually lower operational risk at scale is intentionally multi-dimensional in Twitter/X Timeline Write & Fanout. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-066",
      "type": "multi-select",
      "question": "What should be explicit in API/service contracts for this design? (Select all that apply)",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Twitter/X Timeline Write & Fanout: The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-067",
      "type": "multi-select",
      "question": "Which anti-patterns often cause incident recurrence? (Select all that apply)",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Which anti-patterns often cause incident recurrence, the highest-signal answer is a bundle of controls. The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-068",
      "type": "multi-select",
      "question": "What increases confidence before broad traffic rollout? (Select all that apply)",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Twitter/X Timeline Write & Fanout, What increases confidence before broad traffic rollout needs layered controls, not one silver bullet. The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-069",
      "type": "multi-select",
      "question": "Which controls protect high-priority traffic during spikes? (Select all that apply)",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Which controls protect high-priority traffic during spikes is intentionally multi-dimensional in Twitter/X Timeline Write & Fanout. The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from controls protect high-priority traffic during spikes? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-070",
      "type": "multi-select",
      "question": "Which telemetry dimensions are most actionable for design triage? (Select all that apply)",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Which telemetry dimensions are most actionable for design triage, the highest-signal answer is a bundle of controls. The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-071",
      "type": "multi-select",
      "question": "Which governance actions improve cross-team reliability ownership? (Select all that apply)",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "In Twitter/X Timeline Write & Fanout, Which governance actions improve cross-team reliability ownership needs layered controls, not one silver bullet. The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-072",
      "type": "multi-select",
      "question": "What helps prevent retry amplification cascades? (Select all that apply)",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "What helps prevent retry amplification cascades is intentionally multi-dimensional in Twitter/X Timeline Write & Fanout. The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from helps prevent retry amplification cascades? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-073",
      "type": "multi-select",
      "question": "Which fallback strategies are strong when dependencies degrade? (Select all that apply)",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Twitter/X Timeline Write & Fanout: The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-074",
      "type": "multi-select",
      "question": "What reduces data-quality regressions in eventual pipelines? (Select all that apply)",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For What reduces data-quality regressions in eventual pipelines, the highest-signal answer is a bundle of controls. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-075",
      "type": "multi-select",
      "question": "Which runbook components improve incident execution quality? (Select all that apply)",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Twitter/X Timeline Write & Fanout, Which runbook components improve incident execution quality needs layered controls, not one silver bullet. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-076",
      "type": "multi-select",
      "question": "Which architecture choices improve blast-radius containment? (Select all that apply)",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Which architecture choices improve blast-radius containment is intentionally multi-dimensional in Twitter/X Timeline Write & Fanout. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-077",
      "type": "multi-select",
      "question": "What evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Twitter/X Timeline Write & Fanout: The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Failures/day?",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "Use first-pass reliability arithmetic for A critical path handles 5,400,000 requests/day and 0: 9720 requests. Answers within +/-3% show correct directional reasoning for Twitter/X Timeline Write & Fanout.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 5,400 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Net drain rate?",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "For Queue ingest is 2,200 events/min and drain is 2,530 events/min, the computed target in Twitter/X Timeline Write & Fanout is 330 events/min. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Effective attempts/sec?",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "The operational math for Retries add 0 gives 96000 attempts/sec. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 0.28 and 75,000 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Total failover seconds/day?",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Failover takes 16s and occurs 24 times/day: 384 seconds. Answers within +/-0% show correct directional reasoning for Twitter/X Timeline Write & Fanout.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 16s and 24 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Percent over target?",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Target p99 is 650ms; observed p99 is 845ms, the computed target in Twitter/X Timeline Write & Fanout is 30 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 650ms and 845ms in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-083",
      "type": "numeric-input",
      "question": "If 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "Twitter/X Timeline Write & Fanout expects quick quantitative triage: If 34% of 130,000 req/min are high-priority, how many high-priority req/min evaluates to 44200 requests/min. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 34 and 130,000 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Percent reduction?",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for Error rate drops from 1 gives 78 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 1.0 and 0.22 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Minimum acknowledgements?",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for A 9-node quorum cluster requires majority writes: 5 acks. Answers within +/-0% show correct directional reasoning for Twitter/X Timeline Write & Fanout.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Strong answers connect quorum/coordination settings to concrete correctness goals. If values like 9 and 5 appear, convert them into one unit basis before comparison. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Minutes to clear?",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "For Backlog is 56,000 tasks with net drain 350 tasks/min, the computed target in Twitter/X Timeline Write & Fanout is 160 minutes. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Percent remaining available?",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Twitter/X Timeline Write & Fanout expects quick quantitative triage: A fleet has 18 zones and 3 are unavailable evaluates to 83.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "Generalize from fleet has 18 zones and 3 are unavailable to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 18 and 3 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Percent reduction?",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for MTTR improved from 52 min to 34 min gives 34.62 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 52 min and 34 min in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-089",
      "type": "numeric-input",
      "question": "If 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for If 11% of 2,800,000 daily ops need manual checks, checks/day: 308000 operations. Answers within +/-2% show correct directional reasoning for Twitter/X Timeline Write & Fanout.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 11 and 2,800 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-090",
      "type": "ordering",
      "question": "From a twitter/x timeline write & fanout viewpoint, order a classic-design decomposition workflow.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Identify critical user journey and invariants must happen before Validate with load/failure drills and refine. That ordering matches incident-safe flow in Twitter/X Timeline Write & Fanout.",
      "detailedExplanation": "Generalize from order a classic-design decomposition workflow to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-091",
      "type": "ordering",
      "question": "Order by increasing design risk. (twitter/x timeline write & fanout lens)",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Twitter/X Timeline Write & Fanout emphasizes safe recovery order. Beginning at Explicit boundaries with contracts and finishing at Implicit coupling with no ownership keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-092",
      "type": "ordering",
      "question": "In this twitter/x timeline write & fanout context, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For In this twitter/x timeline write & fanout context, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-093",
      "type": "ordering",
      "question": "Considering twitter/x timeline write & fanout, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Twitter/X Timeline Write & Fanout should start with Fixed immediate retries and end with Jittered retries with retry budgets and telemetry. Considering twitter/x timeline write & fanout, order by increasing retry-control maturity rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Place obvious extremes first, then sort the middle by pairwise comparison. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-094",
      "type": "ordering",
      "question": "For twitter/x timeline write & fanout, order fallback sophistication.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Implicit fallback behavior must happen before Policy-driven automated fallback with tests. That ordering matches incident-safe flow in Twitter/X Timeline Write & Fanout.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-095",
      "type": "ordering",
      "question": "Within twitter/x timeline write & fanout, order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Twitter/X Timeline Write & Fanout emphasizes safe recovery order. Beginning at Host health check only and finishing at Staged shift plus failback rehearsal and rollback gates keeps blast radius controlled while restoring service.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Build the rank from biggest differences first, then refine with adjacent checks. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-096",
      "type": "ordering",
      "question": "Rank these from smallest to largest blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Rank these from smallest to largest blast radius, the correct ordering runs from Single process failure to Cross-region control-plane failure. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-097",
      "type": "ordering",
      "question": "Order data-path durability confidence. Focus on twitter/x timeline write & fanout tradeoffs.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Twitter/X Timeline Write & Fanout should start with In-memory only acknowledgment and end with Replicated durable write plus replay/integrity verification. Order data-path durability confidence rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-098",
      "type": "ordering",
      "question": "From a twitter/x timeline write & fanout viewpoint, order by increasing operational discipline.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc incident response must happen before Role-based response plus action closure tracking. That ordering matches incident-safe flow in Twitter/X Timeline Write & Fanout.",
      "detailedExplanation": "Generalize from order by increasing operational discipline to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-099",
      "type": "ordering",
      "question": "Order rollout safety for major design changes. (twitter/x timeline write & fanout lens)",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Twitter/X Timeline Write & Fanout emphasizes safe recovery order. Beginning at Canary small cohort and finishing at Finalize runbook and ownership updates keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tw-100",
      "type": "ordering",
      "question": "Order evidence strength for fix success. Use a twitter/x timeline write & fanout perspective.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order evidence strength for fix success, the correct ordering runs from Single successful test run to Sustained recovery plus failure-drill pass. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "twitter-x-timeline-write-and-fanout"],
      "difficulty": "staff-level"
    }
  ]
}
