{
  "unit": 2,
  "unitTitle": "Data Modeling",
  "chapter": 8,
  "chapterTitle": "Modeling Scenarios",
  "chapterDescription": "Compound problems — model real systems end-to-end: booking, social feed, e-commerce, and more.",
  "problems": [
    {
      "id": "scenario-001",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a hotel booking system. Which entities do you need at minimum?",
          "options": [
            "Hotels, Rooms, Guests, Bookings",
            "Hotels, Bookings",
            "Rooms, Guests",
            "Hotels, Rooms, Bookings, Guests, Payments, RoomTypes"
          ],
          "correct": 0,
          "explanation": "Minimum viable: Hotels (properties), Rooms (specific units), Guests (who books), Bookings (reservation linking guest to room with dates). Payments and RoomTypes are important but second-priority.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "The Bookings table needs to prevent double-booking (same room, overlapping dates). Which constraint enforces this at the database level?",
          "options": [
            "UNIQUE on (room_id, check_in, check_out)",
            "An exclusion constraint: EXCLUDE USING gist (room_id WITH =, daterange(check_in, check_out) WITH &&)",
            "CHECK (check_in < check_out)",
            "A trigger that checks for overlaps"
          ],
          "correct": 1,
          "explanation": "An exclusion constraint with GiST index prevents overlapping date ranges for the same room at the database level. UNIQUE on exact dates wouldn't catch overlaps (Jan 1-5 and Jan 3-7 have different check_in/check_out but overlap).",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-002",
      "type": "multiple-choice",
      "question": "In the hotel booking system, a Room belongs to a Hotel and has a RoomType (Standard, Deluxe, Suite). What's the relationship pattern?",
      "options": [
        "Room has two foreign keys: hotel_id and room_type_id — both are 1:N relationships",
        "Room embeds Hotel and RoomType as JSON",
        "Three-way junction table between Room, Hotel, and RoomType",
        "Room only references Hotel; RoomType is stored as a string"
      ],
      "correct": 0,
      "explanation": "Room → Hotel (1:N, each room belongs to one hotel). Room → RoomType (1:N, each room has one type). Two foreign keys on Room is the standard pattern. RoomType as a lookup table allows adding types without schema changes.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-003",
      "type": "ordering",
      "question": "For the hotel booking system, rank the access patterns from MOST FREQUENT to LEAST FREQUENT:",
      "items": [
        "Search available rooms for dates/location",
        "View booking details",
        "Create a new booking",
        "Generate monthly revenue report"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Search (browse): highest frequency — many searches per booking. View details: frequent (confirmation pages, check-in). Create booking: lower (conversion from search). Reports: infrequent (admin only, periodic).",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-004",
      "type": "multi-select",
      "question": "For a hotel availability search (rooms available for dates X-Y in city Z), which indexes are essential?",
      "options": [
        "Index on Hotels(city) for location filtering",
        "Composite index on Bookings(room_id, check_in, check_out) for overlap detection",
        "Index on Rooms(hotel_id, room_type_id) for room listing",
        "Full-text index on hotel descriptions"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "City index: filter hotels by location. Booking date index: efficiently check room availability (no overlapping bookings). Room index: list rooms per hotel with type. Full-text search is a nice-to-have, not essential for availability.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-005",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a social media feed (Twitter/X-like). Users follow other users and see their posts. What's the core entity model?",
          "options": [
            "Users, Posts, Follows",
            "Users, Posts, Follows, Likes, Comments",
            "Users, Posts",
            "Users, Follows, Feed"
          ],
          "correct": 0,
          "explanation": "Core minimum: Users (who), Posts (content), Follows (who follows whom — M:N self-referential on Users). Likes, Comments, and Feed are important features but built on top of the core.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "The Follows table represents a directed M:N relationship (User A follows User B doesn't mean B follows A). What's the table structure?",
          "options": [
            "Follows(follower_id, followed_id) — composite primary key, both FK to Users",
            "Follows(user_id_1, user_id_2) with a bidirectional flag",
            "A self-referential FK on Users(follows_user_id)",
            "A JSON array of followed user IDs on the Users table"
          ],
          "correct": 0,
          "explanation": "Follows(follower_id, followed_id) with PK (follower_id, followed_id). Directed: the pair is ordered. To check if A follows B: WHERE follower_id = A AND followed_id = B. To get A's feed: WHERE followed_id IN (SELECT followed_id FROM follows WHERE follower_id = A).",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-006",
      "type": "multiple-choice",
      "question": "For the social feed, the primary read pattern is: 'Get the latest 50 posts from people I follow.' The naive query joins Posts and Follows. At scale (millions of users), what's the standard optimization?",
      "options": [
        "Add more indexes to Posts and Follows",
        "Fan-out on write: precompute each user's feed by pushing posts into their feed timeline at write time",
        "Use a graph database for the follow relationship",
        "Paginate with OFFSET"
      ],
      "correct": 1,
      "explanation": "Fan-out on write: when User A posts, push the post into every follower's precomputed feed (a Feed table or Redis sorted set). Reading the feed becomes a simple, fast lookup — no join needed.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-007",
      "type": "ordering",
      "question": "For social media, rank these denormalization decisions from MOST BENEFICIAL to LEAST BENEFICIAL:",
      "items": [
        "Precomputed feed timeline (avoids expensive join on every feed load)",
        "follower_count / following_count on Users (avoids COUNT queries)",
        "author_name on Posts (avoids user lookup per post)",
        "Embedded comments in Post documents (avoids comments join)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Feed timeline: massive read savings (most frequent operation). Follower counts: high-traffic profile pages. Author name: avoids a join per post in listings. Embedded comments: risky (comments can be unbounded, frequently added).",
      "detailedExplanation": "Compare relative scale first, then confirm neighboring items pairwise to lock in the order. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-008",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're modeling an e-commerce product catalog. Products have: name, description, price, category, and variable attributes (clothing has size/color, electronics have specs). How do you model variable attributes?",
          "options": [
            "Add all possible attribute columns to the Products table",
            "Use an EAV (Entity-Attribute-Value) pattern: ProductAttributes(product_id, attribute_name, attribute_value)",
            "Store variable attributes as a JSONB column on Products",
            "Create a separate table for each product category"
          ],
          "correct": 2,
          "explanation": "JSONB column: Products(id, name, price, category_id, attributes JSONB). Flexible (any key-value), queryable (GIN index on JSONB in PostgreSQL), and simpler than EAV. Example: {\"color\": \"red\", \"size\": \"L\", \"material\": \"cotton\"}.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why is JSONB generally preferred over EAV for variable product attributes?",
          "options": [
            "JSONB is always faster",
            "JSONB keeps all attributes in one row (no joins), supports indexing (GIN), and is easier to query than pivoting EAV rows into columns",
            "EAV doesn't work in PostgreSQL",
            "JSONB uses less storage"
          ],
          "correct": 1,
          "explanation": "EAV requires: JOIN to get attributes, pivot/GROUP BY to reconstruct the attribute set, and has poor query ergonomics (WHERE attribute_name = 'color' AND attribute_value = 'red'). JSONB: attributes->>'color' = 'red' — one row, indexed, readable.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-009",
      "type": "multi-select",
      "question": "An e-commerce system needs these entities. Which have M:N relationships?",
      "options": [
        "Products and Categories (a product can be in multiple categories)",
        "Products and Tags",
        "Orders and Products (an order has multiple products, a product appears in multiple orders)",
        "Users and Addresses (a user can have multiple addresses)"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Products-Categories: M:N (multi-category products). Products-Tags: M:N (multiple tags per product, shared tags). Orders-Products: M:N (via OrderItems junction table). Users-Addresses: 1:N (addresses belong to one user).",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-010",
      "type": "multiple-choice",
      "question": "In the e-commerce order model, OrderItems(order_id, product_id, quantity, unit_price) stores the price at time of purchase. Why is unit_price stored here instead of looked up from Products?",
      "options": [
        "It's faster to query",
        "Point-in-time snapshot: the product price may change later, but the order should reflect the price the customer actually paid",
        "Products table doesn't have a price column",
        "To avoid joins"
      ],
      "correct": 1,
      "explanation": "Intentional denormalization via snapshot. If the product price goes from $10 to $15, orders placed at $10 must still show $10. Storing unit_price on OrderItems freezes the price at purchase time. This is a correctness requirement, not just an optimization.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-011",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a ride-sharing system (Uber-like). What are the core entities?",
          "options": [
            "Riders, Drivers, Rides, Vehicles",
            "Riders, Drivers, Rides, Vehicles, Payments, Locations, Ratings",
            "Users, Rides",
            "Riders, Drivers"
          ],
          "correct": 0,
          "explanation": "Core: Riders (passengers), Drivers (who drive), Rides (a ride request matched to a driver, with status tracking), Vehicles (what drivers drive). Payments, Locations, and Ratings build on top of these.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "A Ride has a lifecycle: requested → matched → en_route → in_progress → completed/cancelled. How should the status be modeled?",
          "options": [
            "A status ENUM/VARCHAR column on Rides with CHECK constraint",
            "A separate RideStatusHistory table logging every transition with timestamps",
            "Both: status column for current state (fast queries) AND a history table for audit trail",
            "A boolean for each status (is_requested, is_matched, etc.)"
          ],
          "correct": 2,
          "explanation": "Both: the status column gives instant access to current state (WHERE status = 'in_progress'). The history table (ride_id, status, transitioned_at) provides audit trail, analytics, and debugging. Use both — they serve different access patterns.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-012",
      "type": "ordering",
      "question": "In a ride-sharing system, rank these access patterns from MOST LATENCY-SENSITIVE to LEAST:",
      "items": [
        "Match a rider with the nearest available driver",
        "Track driver's real-time location",
        "Display ride history for a user",
        "Generate driver earnings report"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Driver matching: sub-second (user is waiting). Real-time tracking: near-real-time (UI updates). Ride history: seconds acceptable (browsing). Earnings report: minutes acceptable (admin/batch). Latency requirements drive storage choices.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-013",
      "type": "multiple-choice",
      "question": "For ride-sharing driver matching ('find nearest available driver'), what storage is appropriate for real-time location data?",
      "options": [
        "PostgreSQL with PostGIS extension for geospatial queries",
        "Redis with geospatial commands (GEOADD, GEORADIUS)",
        "Both: Redis for real-time matching (sub-ms lookups), PostgreSQL for persistent location history",
        "A flat file of GPS coordinates"
      ],
      "correct": 2,
      "explanation": "Redis GEORADIUS: sub-millisecond nearest-neighbor queries for real-time matching. PostgreSQL/PostGIS: persistent storage for historical locations, analytics, and complex spatial queries. Each serves different access patterns.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-014",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a messaging system (Slack-like). Core entities: Users, Channels, Messages. A Channel has multiple members (Users). What's the relationship?",
          "options": [
            "1:N — a user belongs to one channel",
            "M:N — Users and Channels are linked via ChannelMembers(channel_id, user_id, role, joined_at)",
            "1:1 — each user has one channel",
            "Embedded users in channels"
          ],
          "correct": 1,
          "explanation": "M:N: a user is in many channels, a channel has many users. The junction table ChannelMembers holds membership data (role: admin/member, joined_at, notification preferences).",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "The primary read pattern is: 'Get the latest 50 messages in channel X.' What index serves this?",
          "options": [
            "INDEX on Messages(id)",
            "INDEX on Messages(channel_id, created_at DESC)",
            "INDEX on Messages(created_at)",
            "INDEX on Messages(channel_id)"
          ],
          "correct": 1,
          "explanation": "Composite index (channel_id, created_at DESC): jump to the channel, walk backward in time, stop after 50. One index seek, no sort step. The access pattern directly dictates the index design.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-015",
      "type": "multi-select",
      "question": "For a messaging system, which denormalized fields on the Channels table improve the channel list view?",
      "options": [
        "last_message_text (preview of latest message)",
        "last_message_at (timestamp for sorting channels by activity)",
        "member_count (avoid COUNT query)",
        "unread_count per user (per-user, not per-channel — needs a different approach)"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "last_message_text, last_message_at, and member_count are channel-level aggregates — stored on Channels. unread_count is per-user-per-channel — it needs a separate table (ChannelMembers.unread_count or a UserChannelState table).",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-016",
      "type": "multiple-choice",
      "question": "You're designing an event ticketing system (Ticketmaster-like). The critical problem: 10,000 tickets, 100,000 users trying to buy simultaneously. What data modeling concern is paramount?",
      "options": [
        "Entity relationships",
        "Concurrency control — preventing overselling (two users buying the same ticket) and handling high contention on a limited inventory",
        "Schema normalization",
        "Index design"
      ],
      "correct": 1,
      "explanation": "Inventory contention: the core modeling challenge. 100K users competing for 10K tickets = extreme row-level contention. The schema and transaction design must prevent double-selling while maintaining throughput.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-017",
      "type": "two-stage",
      "stages": [
        {
          "question": "For the ticketing system, which approach prevents overselling?",
          "options": [
            "Optimistic locking: read ticket count, attempt to decrement, retry on conflict",
            "Pessimistic locking: SELECT ... FOR UPDATE on the inventory row, then decrement",
            "Use a queue: serialize ticket requests and process one at a time",
            "All three are valid approaches with different tradeoffs"
          ],
          "correct": 3,
          "explanation": "All three work. Optimistic: high throughput but many retries under contention. Pessimistic: guaranteed consistency but serializes access (bottleneck). Queue: serializes cleanly but adds latency. The choice depends on traffic patterns and tolerance for retries.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "At extreme scale (100K simultaneous requests), which approach is most practical?",
          "options": [
            "Pessimistic locking on a single row",
            "Pre-partition inventory: split 10K tickets across 10 'buckets' of 1K each, reducing contention per row by 10x, combined with optimistic locking",
            "Let users buy and resolve overselling later",
            "Rate limit to 1 request per second"
          ],
          "correct": 1,
          "explanation": "Pre-partitioning distributes contention. Instead of 100K users fighting over 1 row, it's 10K users per bucket. Combined with optimistic locking (atomic decrement), this provides both correctness and throughput.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-018",
      "type": "ordering",
      "question": "For an event ticketing system, rank the entity model from CORE (build first) to EXTENDED (build later):",
      "items": [
        "Events, Venues, TicketTypes, Tickets",
        "Orders, Payments, Users",
        "Seating maps, Sections, Rows",
        "Promotions, Discounts, Waitlists"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Core inventory (events, venues, tickets) first. Then transactional entities (orders, payments). Then physical layout (seating). Finally, business features (promotions, waitlists). Build in layers of increasing complexity.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-019",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a food delivery system (DoorDash-like). What are the key entities?",
          "options": [
            "Restaurants, Menus, MenuItems, Orders, OrderItems, Drivers, Customers",
            "Restaurants, Orders, Drivers",
            "Food, Delivery, Payment",
            "Restaurants, Menus, Drivers, Customers"
          ],
          "correct": 0,
          "explanation": "Full entity set: Restaurants (where), Menus/MenuItems (what's available), Orders/OrderItems (what was ordered), Drivers (who delivers), Customers (who ordered). Each has distinct relationships and access patterns.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "A Restaurant's Menu changes frequently (items added, removed, prices changed). But historical orders must reflect the price at time of order. How do you model this?",
          "options": [
            "Always look up current menu prices",
            "Snapshot pricing: OrderItems stores item_name, unit_price, and quantity — frozen at order time. MenuItems has the current price.",
            "Version the entire menu on each change",
            "Use a time-series table for prices"
          ],
          "correct": 1,
          "explanation": "OrderItems captures the price at purchase time (snapshot). MenuItems has the live, current price. This is the same pattern as e-commerce: orders are historical records that must not change when current prices change.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-020",
      "type": "multiple-choice",
      "question": "In the food delivery system, the order lifecycle is: placed → confirmed_by_restaurant → preparing → ready_for_pickup → picked_up → delivered. What's the access pattern for 'show active orders for driver X'?",
      "options": [
        "Full table scan of Orders",
        "INDEX on Orders(driver_id, status) — query: WHERE driver_id = X AND status IN ('picked_up', 'ready_for_pickup')",
        "INDEX on Orders(status) only",
        "No index needed — active orders are few"
      ],
      "correct": 1,
      "explanation": "Composite index (driver_id, status): narrow to the specific driver, then filter by active statuses. A partial index (WHERE status NOT IN ('delivered', 'cancelled')) would be even more targeted — only indexing active orders.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-021",
      "type": "multi-select",
      "question": "For a food delivery system, which real-time data needs special storage (beyond a relational DB)?",
      "options": [
        "Driver GPS locations (frequent updates, geospatial queries)",
        "Estimated delivery times (computed in real-time, cached)",
        "Restaurant open/closed status (changes infrequently)",
        "Order status updates (event stream for real-time UI updates)"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "GPS locations: Redis geospatial or dedicated location service (high frequency). ETAs: computed and cached (volatile). Order status events: event stream (Kafka/SSE for real-time push to UI). Restaurant status: changes rarely, fits in the relational DB.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "scenario-022",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a URL shortener (bit.ly-like). What's the minimum data model?",
          "options": [
            "URLs(short_code, original_url, created_at, user_id)",
            "URLs(id, url)",
            "URLs, Users, Clicks, Analytics",
            "ShortCodes(code, destination)"
          ],
          "correct": 0,
          "explanation": "Minimum: short_code (the unique key, e.g., 'abc123'), original_url (redirect destination), created_at, and user_id (who created it). This handles the core redirect use case.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "The primary access pattern is: given short_code 'abc123', redirect to the original URL. What's the optimal schema design for this?",
          "options": [
            "Full table scan on short_code",
            "short_code as the PRIMARY KEY — direct key lookup, O(1) via hash index or B-tree",
            "Secondary index on short_code with id as PK",
            "Cache all URLs in memory"
          ],
          "correct": 1,
          "explanation": "short_code as PK: direct key lookup for every redirect. In a key-value store, this is the natural design. In SQL, primary key = clustered index = fastest possible lookup. The access pattern is a single key lookup — optimize for exactly that.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-023",
      "type": "numeric-input",
      "question": "A URL shortener uses a 7-character alphanumeric code (a-z, A-Z, 0-9 = 62 characters). How many unique short URLs can it generate?",
      "answer": 3521614606208,
      "unit": "URLs",
      "tolerance": 0.01,
      "explanation": "62^7 = 3,521,614,606,208 (about 3.5 trillion). More than enough for most URL shorteners. Even at 1 million URLs/day, it would take ~9,600 years to exhaust.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-024",
      "type": "multiple-choice",
      "question": "For the URL shortener, click analytics are needed: how many clicks, when, from where. How do you model click tracking without slowing down redirects?",
      "options": [
        "UPDATE urls SET click_count = click_count + 1 on every redirect (synchronous)",
        "Log clicks asynchronously: redirect immediately, write to a Clicks event log (Kafka or async queue), aggregate later",
        "Count clicks at query time from access logs",
        "Don't track clicks"
      ],
      "correct": 1,
      "explanation": "Async click logging: the redirect happens immediately (sub-ms). A click event is published asynchronously. A consumer aggregates clicks into a summary table. The redirect path stays fast; analytics are eventually consistent.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-025",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a calendar/scheduling system (Google Calendar-like). What's the core data model?",
          "options": [
            "Users, Calendars, Events, Attendees (EventAttendees junction table)",
            "Users, Events",
            "Calendars, TimeSlots",
            "Users, Calendars, Events, Recurrence rules, Reminders, Attendees"
          ],
          "correct": 0,
          "explanation": "Core: Users (who), Calendars (containers — a user can have multiple), Events (time-bound items on a calendar), Attendees (M:N between Events and Users with response status). Recurrence and reminders are important extensions.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Recurring events (e.g., 'Weekly team meeting every Monday at 10am') are notoriously hard to model. What's the standard approach?",
          "options": [
            "Create individual event rows for every occurrence (materialized instances)",
            "Store a recurrence rule (RFC 5545 RRULE) and compute occurrences at query time",
            "Hybrid: store the rule, materialize instances for a rolling window (e.g., next 6 months), and expand on demand",
            "Store as a JSON array of dates"
          ],
          "correct": 2,
          "explanation": "Hybrid: the RRULE stores the pattern compactly. Materialized instances (for the near future) enable efficient querying (WHERE start_time BETWEEN ...). On-demand expansion handles far-future queries. Pure materialization = storage explosion; pure computation = slow queries.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-026",
      "type": "multi-select",
      "question": "In a calendar system, which access patterns drive schema design?",
      "options": [
        "Show all events for user X in date range Y-Z",
        "Find free/busy times for a group of users",
        "Create/update/delete a single event",
        "Search events by title or description"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All four drive design. Date-range query: primary read pattern (index on user_id + start_time). Free/busy: cross-user availability (index on attendee + time). CRUD: transactional writes. Search: full-text capability needed.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-027",
      "type": "ordering",
      "question": "For the calendar system, rank these indexes by importance (MOST CRITICAL to LEAST):",
      "items": [
        "Events(calendar_id, start_time) — for date range queries",
        "EventAttendees(user_id, event_id) — for user's events",
        "Events(created_at) — for recently created events",
        "GIN index on Events(title) — for text search"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Calendar view (date range): the most frequent query. User's events: needed for attendee lookups. Text search: useful but less frequent. Recently created: rarely needed as a primary access pattern.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-028",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a content management system (CMS) for a news website. Articles go through a workflow: draft → review → published → archived. What schema pattern supports this?",
          "options": [
            "A status column with CHECK constraint",
            "Separate tables for each status (Drafts, Reviews, Published, Archived)",
            "A status column plus a revision/version history table",
            "A workflow engine table"
          ],
          "correct": 2,
          "explanation": "Status column for current state + ArticleRevisions(article_id, version, content, editor_id, created_at) for history. This supports: current state queries (WHERE status = 'published'), version history, and rollback to previous versions.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "The CMS needs to support 'preview before publish' — an editor modifies a published article, and the changes are visible only in preview until published. How do you model this?",
          "options": [
            "Edit the article directly and hope for the best",
            "Maintain a draft_content column alongside published_content — edits go to draft_content, publish copies draft to published",
            "Create a copy of the article for editing",
            "Use a feature flag per article"
          ],
          "correct": 1,
          "explanation": "Dual-content pattern: published_content (what readers see) and draft_content (editor's work in progress). 'Publish' copies draft to published. This avoids creating duplicate articles and keeps the editing workflow simple.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-029",
      "type": "multiple-choice",
      "question": "A multi-tenant SaaS application (e.g., project management tool) has Tenants, Users, Projects, and Tasks. What's the fundamental schema decision for multi-tenancy?",
      "options": [
        "Separate database per tenant",
        "Shared database, shared schema with tenant_id on every table",
        "Shared database, separate schema per tenant",
        "All three are valid — the choice depends on isolation requirements, scale, and operational complexity"
      ],
      "correct": 3,
      "explanation": "All three are valid: Separate DB = maximum isolation, highest cost. Shared schema + tenant_id = easiest to manage, lowest isolation. Schema-per-tenant = middle ground. The choice depends on compliance requirements, number of tenants, and operational budget.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-030",
      "type": "ordering",
      "question": "Rank these multi-tenancy approaches from MOST SHARED (cheapest) to MOST ISOLATED (most expensive):",
      "items": [
        "Shared database, shared tables, tenant_id column",
        "Shared database, separate schema per tenant",
        "Separate database per tenant",
        "Separate database server per tenant"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Shared everything: cheapest, least isolated. Separate schema: some isolation (different tables). Separate database: strong isolation, same server. Separate server: maximum isolation, highest cost — often for compliance (data residency, HIPAA).",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-031",
      "type": "two-stage",
      "stages": [
        {
          "question": "In shared-schema multi-tenancy (tenant_id on every table), what's the biggest risk?",
          "options": [
            "Performance degradation",
            "Data leakage: a bug in a query that forgets the WHERE tenant_id = ? clause exposes another tenant's data",
            "Schema conflicts between tenants",
            "Too many rows per table"
          ],
          "correct": 1,
          "explanation": "Cross-tenant data leakage is the critical risk. One missing tenant_id filter and you're showing Tenant A's data to Tenant B. This is a security and compliance nightmare.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "How do you prevent accidental cross-tenant queries?",
          "options": [
            "Trust developers to always include tenant_id",
            "Use Row-Level Security (RLS) policies that automatically filter by tenant_id — the database enforces isolation regardless of query correctness",
            "Add tenant_id to every index",
            "Use a different database user per tenant"
          ],
          "correct": 1,
          "explanation": "Row-Level Security (PostgreSQL, SQL Server): CREATE POLICY tenant_isolation ON tasks USING (tenant_id = current_setting('app.current_tenant')). The database automatically applies the filter. Even if code forgets WHERE tenant_id = ?, RLS catches it.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-032",
      "type": "multi-select",
      "question": "A SaaS project management tool (like Jira) needs: Projects, Tasks (in a Project), Sprints (time-boxed iteration), and Task assignments. Which are M:N relationships?",
      "options": [
        "Tasks and Users (a task can have multiple assignees)",
        "Tasks and Labels/Tags",
        "Tasks and Sprints (a task might span sprints or be moved between them)",
        "Projects and Users (project members)"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Task-Users: M:N (multiple assignees via TaskAssignments). Task-Labels: M:N (multiple labels per task via TaskLabels). Project-Users: M:N (project membership via ProjectMembers). Task-Sprint: typically 1:N (a task is in one sprint at a time).",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-033",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a payment/wallet system (Venmo/PayPal-like). What entities are needed?",
          "options": [
            "Users, Wallets, Transactions",
            "Users, Wallets, Transactions, Ledger entries",
            "Users, Money",
            "Accounts, Transfers"
          ],
          "correct": 1,
          "explanation": "Users (who), Wallets (balance per user), Transactions (payment events), Ledger entries (double-entry bookkeeping — debits and credits). The ledger is critical for financial accuracy and auditability.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why use double-entry bookkeeping (Ledger entries) instead of just updating wallet balances directly?",
          "options": [
            "It's faster",
            "Every transaction creates two entries (debit from sender, credit to receiver) — the sum of all entries for a wallet always equals the balance, providing a built-in audit trail and error detection",
            "It uses less storage",
            "It's required by law in all countries"
          ],
          "correct": 1,
          "explanation": "Double-entry: debit (sender -$50) + credit (receiver +$50). Total debits = total credits (system invariant). If the wallet balance doesn't match the sum of ledger entries, you know something is wrong. This is fundamental for financial integrity.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-034",
      "type": "multiple-choice",
      "question": "In the payment system, transferring $50 from User A to User B must be atomic (both debit and credit succeed, or neither does). What database feature ensures this?",
      "options": [
        "Eventual consistency",
        "A database transaction (BEGIN → debit A → credit B → COMMIT) — if either operation fails, ROLLBACK undoes both",
        "Two separate UPDATE statements",
        "Optimistic locking"
      ],
      "correct": 1,
      "explanation": "ACID transaction: BEGIN; UPDATE wallets SET balance = balance - 50 WHERE user_id = A; UPDATE wallets SET balance = balance + 50 WHERE user_id = B; INSERT INTO ledger ...; COMMIT. If any step fails, ROLLBACK. Money is never created or destroyed.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-035",
      "type": "ordering",
      "question": "For a payment system, rank these data integrity measures from MOST CRITICAL to LEAST CRITICAL:",
      "items": [
        "ACID transactions for transfers",
        "Double-entry ledger with balance reconciliation",
        "Idempotency keys to prevent duplicate payments",
        "Audit log of all balance changes"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "ACID transactions: foundational (money conservation). Double-entry ledger: audit trail and error detection. Idempotency: prevents double-charging (critical for retries). Audit log: compliance and debugging.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a notification system. Notifications go to users via multiple channels (in-app, email, push). What's the data model?",
          "options": [
            "Notifications(id, user_id, type, message, channel, status, created_at)",
            "Notifications(id, user_id, type, message) + NotificationDeliveries(notification_id, channel, status, delivered_at)",
            "One table per channel (InAppNotifications, EmailNotifications, PushNotifications)",
            "A single event queue"
          ],
          "correct": 1,
          "explanation": "Separate the notification (what) from the delivery (how). One notification can have multiple deliveries (in-app + email + push). Each delivery tracks its own status (pending, sent, failed). This avoids duplicating notification content per channel.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "The primary access pattern is: 'Get user's unread in-app notifications, newest first.' What query serves this?",
          "options": [
            "Full scan of Notifications",
            "SELECT n.* FROM notifications n JOIN notification_deliveries d ON n.id = d.notification_id WHERE n.user_id = ? AND d.channel = 'in_app' AND d.status = 'unread' ORDER BY n.created_at DESC",
            "SELECT * FROM notifications WHERE user_id = ? AND read = false",
            "Query the email server"
          ],
          "correct": 1,
          "explanation": "Join notifications with deliveries, filter by user, channel='in_app', status='unread', sort by newest. Index: notifications(user_id, created_at DESC) and notification_deliveries(notification_id, channel, status).",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-037",
      "type": "numeric-input",
      "question": "A notification system sends to 3 channels (in-app, email, push). An app has 1M daily active users, each receiving an average of 10 notifications/day. How many NotificationDelivery rows are created per day?",
      "answer": 30000000,
      "unit": "rows",
      "tolerance": 0.1,
      "explanation": "1M users × 10 notifications × 3 channels = 30M delivery rows/day. The Notifications table has 10M rows/day (one per notification), but deliveries are 3x that. This volume informs partitioning and retention decisions.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ]
    },
    {
      "id": "scenario-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're modeling a permission system (RBAC — Role-Based Access Control). What are the core entities?",
          "options": [
            "Users, Roles, Permissions, UserRoles (junction), RolePermissions (junction)",
            "Users, Permissions",
            "Users, Roles",
            "Users, Groups, Policies"
          ],
          "correct": 0,
          "explanation": "RBAC: Users are assigned Roles (via UserRoles M:N). Roles have Permissions (via RolePermissions M:N). To check if a user can do X: find their roles, check if any role has permission X. Two M:N relationships.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible."
        },
        {
          "question": "Checking permissions on every request requires: User → UserRoles → Roles → RolePermissions → Permissions. This is 4 joins. How do you optimize?",
          "options": [
            "Denormalize: cache user permissions in a flat set (Redis or application cache), invalidated when roles/permissions change",
            "Accept the 4 joins — they're fast",
            "Store permissions as a JSON array on the User row",
            "Use a graph database"
          ],
          "correct": 0,
          "explanation": "Cache the resolved permission set per user (e.g., Redis SET: user:123:permissions → {read_posts, write_posts, admin}). On every request, check the cache (O(1)). Invalidate when roles or permissions change. This eliminates 4 joins on the hot path.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-039",
      "type": "multi-select",
      "question": "Beyond basic RBAC, which permission model extensions add complexity?",
      "options": [
        "Resource-level permissions (user can edit THEIR posts but not others')",
        "Hierarchical roles (admin inherits all editor permissions)",
        "Time-limited permissions (temporary access that expires)",
        "Negative permissions (explicit denials that override grants)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All add complexity. Resource-level: WHERE author_id = current_user (data filtering). Hierarchical: role inheritance trees. Time-limited: expiry tracking. Negative permissions: deny overrides grant — requires precedence rules.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-040",
      "type": "multiple-choice",
      "question": "You're designing a survey/form builder (Google Forms-like). The challenge: users create forms with arbitrary questions of different types (text, multiple choice, rating, etc.). How do you model questions?",
      "options": [
        "One table per question type (TextQuestions, MCQuestions, RatingQuestions)",
        "Questions(id, form_id, type, position, config JSONB) — the JSONB config holds type-specific data",
        "A single Questions table with columns for every possible question type attribute",
        "Store the entire form as one JSON document"
      ],
      "correct": 1,
      "explanation": "JSONB config column: Questions has common fields (id, form_id, type, position, required) and a JSONB config for type-specific data. MC config: {options: ['A', 'B', 'C']}. Rating config: {min: 1, max: 5}. Flexible, queryable, no table sprawl.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "For the survey builder, how do you model responses? Each response is to a specific form, answering multiple questions.",
          "options": [
            "Responses(id, form_id, respondent_id, submitted_at) + Answers(response_id, question_id, value JSONB)",
            "One Responses table with a column per question",
            "Store all answers as a JSON blob on Responses",
            "A separate table for each question type's answers"
          ],
          "correct": 0,
          "explanation": "Responses (form submission metadata) + Answers (individual question answers). JSONB value accommodates different answer types: text string, selected option index, rating number, file upload URL. One table handles all question types.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "A form has 10 questions. 50,000 people respond. How many Answer rows are created?",
          "options": ["50,000", "500,000", "10", "5,000,000"],
          "correct": 1,
          "explanation": "50,000 responses × 10 questions = 500,000 answer rows. This scales linearly with both respondents and questions. For large surveys, partitioning by form_id keeps queries efficient.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-042",
      "type": "ordering",
      "question": "For a blog platform, rank these entities by how early they should be modeled (FIRST to LAST):",
      "items": [
        "Posts (core content)",
        "Users/Authors (who writes)",
        "Comments (engagement)",
        "Tags and Categories (organization)"
      ],
      "correctOrder": [1, 0, 3, 2],
      "explanation": "Users first (FK target for everything). Posts next (core content). Tags/Categories (organization, affects browsing). Comments last (engagement layer, can be added later). Build the content model before the interaction model.",
      "detailedExplanation": "Order by relative impact rather than exact values, then verify the sequence one boundary at a time. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a file storage system (Google Drive-like). Files are organized in folders (hierarchical). What's the data model for the hierarchy?",
          "options": [
            "Files(id, name, parent_folder_id, owner_id, type, size, created_at) — self-referential FK for parent",
            "Folders and Files as separate tables with a junction table",
            "A single Files table with a 'path' column (e.g., '/docs/reports/q1.pdf')",
            "Nested sets model"
          ],
          "correct": 0,
          "explanation": "Self-referential FK: parent_folder_id references Files(id). Folders are Files with type='folder'. This supports: 'list contents of folder X' (WHERE parent_folder_id = X), moving files (UPDATE parent_folder_id), and nested depth.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "Listing the full path of a file (root → Documents → Reports → Q1.pdf) requires traversing parent pointers. What's the performance concern?",
          "options": [
            "Path lookup is always fast",
            "Recursive query (WITH RECURSIVE) to walk parent pointers — depth determines number of joins. For deep hierarchies, this can be slow",
            "It requires a full table scan",
            "PostgreSQL can't do recursive queries"
          ],
          "correct": 1,
          "explanation": "WITH RECURSIVE walks parent_folder_id up to the root. Each level is a join. For depth 5, it's 5 iterations — fine. For depth 50, it's expensive. Mitigation: cache the materialized_path ('/docs/reports/') on each file as a denormalized column.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-044",
      "type": "multi-select",
      "question": "For a file storage hierarchy, which alternative models to adjacency list (parent_id) exist?",
      "options": [
        "Materialized path: store full path string ('/docs/reports/q1.pdf')",
        "Nested sets: store left/right boundaries for tree traversal",
        "Closure table: store all ancestor-descendant pairs",
        "Graph database: store hierarchy as edges"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are valid hierarchy models. Materialized path: simple, fast path lookups, but moves require updating many paths. Nested sets: fast subtree queries, expensive inserts. Closure table: fast all-ancestor/descendant queries, more storage. Graph DB: native traversal.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ]
    },
    {
      "id": "scenario-045",
      "type": "multiple-choice",
      "question": "You're designing an auction system (eBay-like). The critical data modeling concern is bidding. What must the Bids model guarantee?",
      "options": [
        "Bids are stored in alphabetical order",
        "Each bid must be higher than the current highest bid, enforced atomically — no two bids can tie, and the highest bidder is always deterministic",
        "Bids can be any amount",
        "Bids are processed in batch"
      ],
      "correct": 1,
      "explanation": "Bid integrity: new_bid > current_max_bid, enforced atomically (no race condition where two equal bids are accepted). Use a transaction: check current max, insert bid only if higher, update item's current_price — all atomically.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "For the auction system, Items have: start_price, current_price, end_time. Bids reference the Item and the Bidder. What constraint prevents bids after the auction ends?",
          "options": [
            "Application-level check only",
            "CHECK constraint: created_at <= (SELECT end_time FROM items WHERE id = item_id) — but cross-table checks aren't supported in CHECK constraints",
            "Use a trigger or application-level check within a transaction that also verifies end_time > NOW()",
            "Set a database event to close the auction"
          ],
          "correct": 2,
          "explanation": "CHECK constraints can't reference other tables. Use a transaction: BEGIN, SELECT end_time FROM items WHERE id = ? FOR UPDATE, verify end_time > NOW(), INSERT bid, UPDATE items SET current_price, COMMIT. Application + transaction ensures correctness.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible."
        },
        {
          "question": "The auction ends. How do you determine the winner and prevent late bids?",
          "options": [
            "Query for the highest bid after end_time",
            "At end_time, a scheduled job marks the auction as 'closed' (status column) and records the winner — new bid attempts check status = 'open' within their transaction",
            "The last person to bid wins",
            "Manual review by admin"
          ],
          "correct": 1,
          "explanation": "A scheduled job (or application-level cron) closes the auction at end_time: UPDATE items SET status = 'closed', winner_id = (SELECT bidder_id FROM bids WHERE item_id = ? ORDER BY amount DESC LIMIT 1). Bid transactions check status = 'open'.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-047",
      "type": "numeric-input",
      "question": "An auction site has 100K active auctions. Each receives an average of 50 bids. How many rows are in the Bids table for active auctions?",
      "answer": 5000000,
      "unit": "rows",
      "tolerance": "exact",
      "explanation": "100,000 auctions × 50 bids = 5,000,000 bid rows. Partitioning by auction status (active vs completed) or archiving completed auction bids keeps the active table fast.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing an analytics event tracking system (like Segment or Mixpanel). Events come in at 50K/sec with varying structures: page views, clicks, purchases, etc. What's the data model?",
          "options": [
            "One table per event type",
            "Events(id, event_type, user_id, timestamp, properties JSONB) — semi-structured with a JSONB column for event-specific data",
            "A single table with columns for every possible event property",
            "Store as raw log files only"
          ],
          "correct": 1,
          "explanation": "Semi-structured: common fields (type, user, time) as columns for efficient filtering/indexing. Event-specific data in JSONB (page_url for page views, product_id for purchases). One table handles all event types with flexibility.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        },
        {
          "question": "At 50K events/sec, what storage considerations apply?",
          "options": [
            "Standard PostgreSQL handles this easily",
            "Time-partition the table (daily/weekly), use minimal indexes on the write path, and consider a columnar store (ClickHouse, BigQuery) for analytical queries",
            "Store in memory only",
            "Use a flat file per second"
          ],
          "correct": 1,
          "explanation": "50K/sec = 4.3B events/day. Time-partitioning: drop old partitions easily. Minimal write-path indexes: reduce write amplification. Columnar stores: compress well and query billions of rows efficiently. Standard PostgreSQL alone won't handle this volume for analytics.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-049",
      "type": "ordering",
      "question": "For a high-volume event tracking system, rank these storage technologies from BEST FIT to WORST FIT:",
      "items": [
        "ClickHouse (columnar, optimized for analytics on billions of rows)",
        "Apache Kafka (event streaming) → data warehouse pipeline",
        "PostgreSQL with time partitioning",
        "MongoDB with TTL indexes"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "ClickHouse: purpose-built for this exact workload (analytical queries on massive event data). Kafka → warehouse: production-grade pipeline. PostgreSQL: works up to moderate scale with careful design. MongoDB: workable but not optimized for analytical queries.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a recommendation engine data model. Users interact with Items (views, purchases, ratings). The engine needs to compute: 'users who liked X also liked Y.' What data do you need?",
          "options": [
            "UserInteractions(user_id, item_id, interaction_type, timestamp)",
            "A similarity matrix between all items",
            "Both: raw interactions for computing recommendations, and a precomputed similarity/recommendation table for serving",
            "Just purchase history"
          ],
          "correct": 2,
          "explanation": "Raw interactions: the training data for computing recommendations (batch or real-time ML). Precomputed recommendations: the serving data (user_id → [recommended_item_ids]). Separate computation from serving — different access patterns.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why separate the computation data model from the serving data model?",
          "options": [
            "They need different databases",
            "Computation needs full interaction history (billions of rows, batch processing). Serving needs fast key→value lookups for recommendations. Different access patterns require different optimizations.",
            "It's more organized",
            "Legal requirements"
          ],
          "correct": 1,
          "explanation": "Computation: scan all interactions, compute similarities — heavy batch/ML workload (data warehouse). Serving: given user_id, return top 20 recommendations — low-latency key lookup (Redis, DynamoDB). Two fundamentally different access patterns.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-051",
      "type": "multiple-choice",
      "question": "You're modeling a multi-warehouse inventory system. Products exist across multiple warehouses, each with different stock levels. What's the data model?",
      "options": [
        "Products(id, stock_count) — one global count",
        "Products(id, ...) + Inventory(product_id, warehouse_id, quantity) — stock tracked per warehouse",
        "One Products table per warehouse",
        "Warehouses(id, products JSON)"
      ],
      "correct": 1,
      "explanation": "Separate Inventory table: (product_id, warehouse_id, quantity) with composite PK. One product has multiple inventory records (one per warehouse). Total stock = SUM(quantity) across warehouses. This supports fulfillment from the nearest warehouse.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-052",
      "type": "multi-select",
      "question": "For the multi-warehouse inventory system, which access patterns drive schema design?",
      "options": [
        "Check stock at a specific warehouse for a product",
        "Find the nearest warehouse with product X in stock (geospatial)",
        "Reserve inventory (decrement atomically during checkout)",
        "Get total stock across all warehouses for a product"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All four: specific warehouse stock (point lookup), nearest warehouse (geo + availability join), reservation (atomic update with concurrency control), total stock (aggregation across warehouses). Each requires different indexing.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a subscription/billing system (Stripe-like). What are the core entities?",
          "options": [
            "Customers, Plans, Subscriptions, Invoices, Payments",
            "Users, Payments",
            "Customers, Subscriptions",
            "Plans, Invoices"
          ],
          "correct": 0,
          "explanation": "Customers (who pays), Plans (what they pay for — pricing, features), Subscriptions (customer + plan + status + period), Invoices (billing records generated per period), Payments (actual money transfers for invoices).",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "A Subscription has: customer_id, plan_id, status (active/cancelled/past_due), current_period_start, current_period_end, cancelled_at. Why track current_period separately from status?",
          "options": [
            "They're redundant",
            "A subscription can be cancelled but still active until the period ends — 'cancel at end of period' means status changes to cancelled but access continues until current_period_end",
            "For billing calculations only",
            "Database normalization requires it"
          ],
          "correct": 1,
          "explanation": "Cancel ≠ immediate loss of access. A user cancels mid-month but has access until month end. Status = 'cancelled', current_period_end = Jan 31. After Jan 31, status → 'expired'. This distinction is critical for billing and access control.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-054",
      "type": "ordering",
      "question": "For the subscription billing system, rank the data integrity requirements from MOST CRITICAL to LEAST:",
      "items": [
        "Payment amounts must exactly match invoice totals",
        "Subscription status must accurately reflect payment state",
        "Plan changes must be prorated correctly",
        "Usage metrics must be tracked precisely"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Payment accuracy: money is involved — no errors. Status accuracy: controls access — wrong status = lost revenue or unauthorized access. Proration: financial accuracy for mid-cycle changes. Usage tracking: important but slight imprecision is more tolerable.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a learning management system (LMS — like Coursera). Core entities: Courses, Modules (chapters within a course), Lessons (content within a module), Users, Enrollments. What's the hierarchy?",
          "options": [
            "Course → Module → Lesson (1:N:N hierarchy), Users enroll in Courses",
            "Course → Lesson (flat)",
            "Course → Module → Lesson → Quiz → Question (deep hierarchy)",
            "Users → Courses (direct relationship)"
          ],
          "correct": 0,
          "explanation": "Three-level content hierarchy: Course contains Modules, Modules contain Lessons. Users enroll in Courses (Enrollments junction table with progress tracking). This provides natural organization while keeping the hierarchy manageable.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "How do you track a user's progress through a course?",
          "options": [
            "A single 'percent_complete' on the Enrollment",
            "LessonProgress(user_id, lesson_id, status, completed_at) — granular per-lesson tracking, with course-level completion derived from lesson progress",
            "Store completed lessons as a JSON array on Enrollment",
            "A boolean 'completed' on Enrollment"
          ],
          "correct": 1,
          "explanation": "Per-lesson tracking: LessonProgress records the status of each lesson (not_started, in_progress, completed). Course completion = all lessons completed. This enables: resume where you left off, progress bars, and analytics on which lessons users struggle with.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-056",
      "type": "multiple-choice",
      "question": "You're modeling a warehouse management system with Products, Locations (shelves/bins), and Inventory. A product can be in multiple locations. What's the relationship?",
      "options": [
        "1:1 — one product, one location",
        "M:N — Inventory(product_id, location_id, quantity) junction table",
        "1:N — each product has one location",
        "Products embed location data"
      ],
      "correct": 1,
      "explanation": "M:N via Inventory junction table: the same product can be in multiple locations (bin A5 has 50 units, bin B3 has 30 units). Each Inventory row = a specific product at a specific location with a quantity.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a real-time collaborative document editor (Google Docs-like). What's the data modeling challenge unique to this system?",
          "options": [
            "Storing large documents",
            "Handling concurrent edits by multiple users on the same document without conflicts — operational transformation or CRDT-based conflict resolution",
            "User authentication",
            "Document search"
          ],
          "correct": 1,
          "explanation": "Concurrent editing is the defining challenge. Multiple users type simultaneously. The system must resolve conflicts in real-time: operational transformation (OT) or CRDTs (Conflict-free Replicated Data Types) handle concurrent operations.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "For storing document state, which approach supports real-time collaboration?",
          "options": [
            "Store the full document text, overwrite on each save",
            "Store an ordered list of operations (insert char 'a' at position 5, delete chars 10-15), with the current document derived by replaying operations",
            "Store each user's version separately and merge on read",
            "Use a relational table with one row per character"
          ],
          "correct": 1,
          "explanation": "Operation log (event sourcing for documents): each edit is an operation. The current state is derived by replaying operations. Operations can be merged (OT) or automatically resolved (CRDT). Periodic snapshots prevent unbounded replay.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-058",
      "type": "multi-select",
      "question": "For a collaborative editor, which storage components are needed?",
      "options": [
        "Document metadata (title, owner, shared_with, created_at)",
        "Operation log (ordered sequence of edits)",
        "Periodic snapshots (full document state at checkpoints)",
        "Real-time presence data (who's currently editing, cursor positions)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "Metadata: relational DB. Operation log: append-only store (database or event log). Snapshots: reduces replay time on document open. Presence data: ephemeral, in-memory (Redis pub/sub or WebSocket state). All four serve different aspects.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "scenario-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're modeling a healthcare appointment system. Entities: Patients, Doctors, Appointments, MedicalRecords. Doctors have a weekly schedule with available time slots. How do you model availability?",
          "options": [
            "DoctorSchedules(doctor_id, day_of_week, start_time, end_time) for recurring weekly patterns, with Appointments blocking specific slots",
            "A flat list of all possible appointment times",
            "Doctors manually create each open slot",
            "Store availability as a boolean per hour"
          ],
          "correct": 0,
          "explanation": "Weekly schedule template: DoctorSchedules defines recurring availability (Mon 9am-5pm). Appointments book specific time ranges within that availability. To check openness: compute available slots from schedule minus booked appointments.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "Medical records have strict privacy requirements (HIPAA in the US). How does this affect the data model?",
          "options": [
            "No impact — same as any other data",
            "Encryption at rest, audit logging of all access, strict access controls (only the patient's doctor can see records), and potentially separate storage for PHI (Protected Health Information)",
            "Store medical records in a different country",
            "Use a blockchain"
          ],
          "correct": 1,
          "explanation": "HIPAA requires: encryption (at rest and in transit), access controls (role-based, minimum necessary), audit trails (who accessed what, when), and potentially data isolation (separate schema or database for PHI). The data model must support auditability.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-060",
      "type": "ordering",
      "question": "For a generic SaaS application, rank the order in which you'd model these concerns (FIRST to LAST):",
      "items": [
        "Core domain entities (the business objects)",
        "Authentication and authorization (users, roles, permissions)",
        "Audit logging and compliance",
        "Analytics and reporting"
      ],
      "correctOrder": [1, 0, 2, 3],
      "explanation": "Auth first (users are referenced by everything). Core domain next (the actual business logic). Audit/compliance third (tracks access to domain objects). Analytics last (derived from domain data, often in a separate store).",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-061",
      "type": "multiple-choice",
      "question": "You're modeling an email system (Gmail-like). Emails can be in multiple labels (Inbox, Starred, Important — like tags). What relationship is this?",
      "options": [
        "1:N — each email has one label",
        "M:N — EmailLabels(email_id, label_id) — an email has multiple labels, a label contains multiple emails",
        "1:1 — each email matches one label",
        "Emails store labels as a comma-separated string"
      ],
      "correct": 1,
      "explanation": "M:N: an email can be in 'Inbox' AND 'Starred' AND 'Work' simultaneously. Labels are like tags. EmailLabels junction table enables: 'show all emails with label X' and 'show all labels for email Y'.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-062",
      "type": "two-stage",
      "stages": [
        {
          "question": "Email threading: replies belong to a conversation thread. How do you model threads?",
          "options": [
            "Emails(id, thread_id, in_reply_to, ...) — thread_id groups messages, in_reply_to tracks the reply chain",
            "Store threads as nested JSON",
            "Create a separate Threads table with all messages embedded",
            "Each reply creates a new email with no connection to the original"
          ],
          "correct": 0,
          "explanation": "thread_id: groups all messages in a conversation. in_reply_to: links to the specific email being replied to (forms a tree within the thread). This supports both flat thread view (WHERE thread_id = ?) and reply-chain view (walk in_reply_to).",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        },
        {
          "question": "The 'Inbox' view shows threads sorted by most recent message. How do you serve this efficiently?",
          "options": [
            "JOIN emails with labels, GROUP BY thread_id, ORDER BY MAX(created_at)",
            "Denormalize: store last_message_at on a Threads table (or materialized thread metadata), updated on each new message in the thread",
            "Sort emails by thread_id",
            "Full table scan with sorting"
          ],
          "correct": 1,
          "explanation": "Denormalize: Threads(id, subject, last_message_at, message_count, snippet). Update on each new email in the thread. The Inbox query becomes: SELECT * FROM threads JOIN email_labels ON ... WHERE label = 'inbox' ORDER BY last_message_at DESC.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-063",
      "type": "numeric-input",
      "question": "An email system stores 1 billion emails across 100 million threads (average 10 emails per thread). If 5% of threads are in a user's Inbox, how many threads does the average Inbox contain?",
      "answer": 5000000,
      "unit": "threads",
      "tolerance": 0.1,
      "explanation": "100M threads × 5% = 5M threads in the Inbox. This is system-wide; a single user's inbox is much smaller. But the label-based query (WHERE label = 'inbox' AND user_id = ?) must be efficiently indexed.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-064",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing an inventory reservation system for a flash sale. 1000 items, 50K users hit 'buy' simultaneously. What pattern prevents overselling while maintaining throughput?",
          "options": [
            "First-come-first-served with single-row locking",
            "Queue-based: put all requests in a queue, process sequentially, confirm or reject",
            "Optimistic locking with retry",
            "Pre-allocate inventory into 10 'pools' of 100 items each, with separate counters — reduces contention by 10x"
          ],
          "correct": 3,
          "explanation": "Pre-partitioning: split 1000 items into 10 pools. Each pool has its own counter row. 50K requests are distributed across 10 counters (~5K/counter). 10x less contention than a single counter. Combine with atomic decrement (SET qty = qty - 1 WHERE qty > 0).",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "A pool's counter hits 0 but other pools still have stock. How do you handle the request?",
          "options": [
            "Reject it — user must retry",
            "Try the next pool: if pool 3 is empty, try pool 4. If all pools are empty, reject. This is a form of consistent hashing with fallback.",
            "Rebalance all pools instantly",
            "Wait for a cancellation"
          ],
          "correct": 1,
          "explanation": "Fallback chain: try the assigned pool first (fast path). If empty, try others. This redistributes demand from exhausted pools to remaining ones. Eventually, all pools empty = truly sold out.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-065",
      "type": "multiple-choice",
      "question": "You're modeling a content recommendation feed (TikTok-like). The feed mixes: followed creators' posts, algorithmic recommendations, and ads. How do you model the feed?",
      "options": [
        "A single chronological Posts table",
        "FeedItems(user_id, item_id, item_type, source, rank, created_at) — a precomputed table mixing different content types, with source indicating 'followed', 'recommended', or 'ad'",
        "Three separate feeds merged on the client",
        "A machine learning model serves the feed directly with no database"
      ],
      "correct": 1,
      "explanation": "Precomputed feed table: each user's feed is materialized with mixed content sources. item_type distinguishes posts vs ads. source tracks provenance. rank orders display. This enables fast, single-table feed reads while supporting diverse content mixing.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-066",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a multi-currency pricing system. Products have prices in multiple currencies. What's the data model?",
          "options": [
            "Products stores a single price with currency conversion at display time",
            "ProductPrices(product_id, currency_code, price) — explicit prices per currency, not computed from exchange rates",
            "Store all prices in USD and convert",
            "A JSON object of prices on the Product row"
          ],
          "correct": 1,
          "explanation": "Explicit per-currency pricing: the business sets prices in each currency (not just converting from a base). $99 USD might be €89 EUR (not the exchange rate equivalent). This is standard in international e-commerce — prices are marketing decisions, not math.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Exchange rates change daily. Some prices ARE derived from a base currency. How do you handle both explicit and derived pricing?",
          "options": [
            "Store everything as explicit",
            "ProductPrices has a 'source' column: 'explicit' (manually set) or 'derived' (computed from base price × exchange rate). A nightly job updates derived prices using current exchange rates.",
            "Compute all prices in real-time",
            "Use a single currency"
          ],
          "correct": 1,
          "explanation": "Hybrid: some currencies have explicitly set prices (key markets). Others are derived from the base price × exchange rate. A batch job refreshes derived prices. This balances business control (explicit) with coverage (derived for all other currencies).",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-067",
      "type": "multi-select",
      "question": "When modeling a data system end-to-end, which cross-cutting concerns apply to EVERY scenario?",
      "options": [
        "Sharding strategy for every table",
        "Audit logging (who changed what, when)",
        "Timestamps on every table (created_at, updated_at)",
        "Consistent primary key strategy (auto-increment, UUID, etc.)"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Audit logging, timestamps, and PK strategy are cross-cutting decisions that affect every table. Decide these upfront — retrofitting later requires migrations across all tables. Sharding does NOT apply to every scenario — many systems run on a single database and never need sharding.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-068",
      "type": "ordering",
      "question": "When starting a new system design, rank these modeling activities from FIRST to LAST:",
      "items": [
        "Identify entities and their relationships",
        "Define access patterns (key queries and write operations)",
        "Choose primary key strategy and cross-cutting conventions",
        "Optimize with indexes, denormalization, and caching"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "Entities first (what exists). Conventions (PKs, timestamps — applied to all entities). Access patterns (how the system is used). Optimization last (only after you know what to optimize). Don't optimize prematurely.",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-069",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a social network with posts, comments, and nested replies (Reddit-like). Comments can be replies to other comments (tree structure). What's the model?",
          "options": [
            "Comments(id, post_id, parent_comment_id, author_id, body, created_at) — adjacency list with self-referential FK",
            "Store comments as nested JSON in the Post document",
            "Flat comments only (no nesting)",
            "A separate table for each nesting level"
          ],
          "correct": 0,
          "explanation": "Adjacency list: parent_comment_id IS NULL for top-level comments, references parent for replies. WITH RECURSIVE builds the tree. This is the standard approach — simple schema, handles arbitrary depth.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Displaying a post with 500 comments requires building the tree. What optimization avoids repeated recursive queries?",
          "options": [
            "Load all comments in one flat query, build the tree in application code",
            "Use a closure table for O(1) subtree retrieval",
            "Load one level at a time (lazy loading)",
            "All three are valid depending on the comment volume"
          ],
          "correct": 3,
          "explanation": "Small volume (< 1K comments): load all flat, build tree in app (one query, simple). Medium: closure table (precomputed ancestor-descendant pairs). Large: lazy-load (expand on click). The right choice depends on typical comment volume per post.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-070",
      "type": "multiple-choice",
      "question": "Across all these scenarios, what is the single most important principle of data modeling?",
      "options": [
        "Always normalize to 3NF",
        "Use NoSQL for everything",
        "Understand your access patterns first — then model the schema to serve them efficiently, making deliberate tradeoffs between normalization, performance, and complexity",
        "Follow the ORM's conventions exactly"
      ],
      "correct": 2,
      "explanation": "Access patterns drive everything: table structure, indexes, denormalization decisions, technology choices, and consistency guarantees. A schema that looks elegant on paper but can't serve the actual queries is useless. Model for how data is used, not just what data exists.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-071",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a customer support ticket system (Zendesk-like). Tickets have: requester, assignee (agent), status, priority, tags, and a conversation thread. What's the core model?",
          "options": [
            "Tickets(id, requester_id, assignee_id, status, priority, created_at) + TicketMessages(ticket_id, author_id, body, created_at) + TicketTags(ticket_id, tag_id)",
            "Tickets with embedded messages",
            "A single table with all data",
            "Tickets and Users only"
          ],
          "correct": 0,
          "explanation": "Tickets (metadata), TicketMessages (conversation thread — public replies and internal notes), TicketTags (M:N categorization). This separates the ticket state from the conversation content, enabling different access patterns.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        },
        {
          "question": "Agents need a queue view: 'Show my open tickets, sorted by priority then age.' What index serves this?",
          "options": [
            "INDEX(assignee_id)",
            "INDEX(assignee_id, status, priority, created_at) — filter by agent + open status, sort by priority then age",
            "INDEX(priority)",
            "INDEX(status, created_at)"
          ],
          "correct": 1,
          "explanation": "Composite index: jump to the agent's tickets (assignee_id), filter to open (status = 'open'), sort by priority (highest first) then created_at (oldest first). The access pattern directly maps to the index column order.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-072",
      "type": "multi-select",
      "question": "For the ticket system, which SLA (Service Level Agreement) tracking requires data modeling support?",
      "options": [
        "First response time (time from ticket creation to first agent reply)",
        "Resolution time (time from creation to status = 'resolved')",
        "Business hours calculation (SLA pauses outside 9am-5pm)",
        "Escalation tracking (ticket automatically escalated if SLA breached)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All four: first_response_at timestamp, resolved_at timestamp, business hours calendar table for SLA calculation, escalation rules + tracking. SLA compliance is a core feature that requires explicit data model support.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-073",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a coupon/discount system for e-commerce. What entities are needed?",
          "options": [
            "Coupons(id, code, discount_type, discount_value, min_order_amount, max_uses, used_count, valid_from, valid_until)",
            "Coupons(code, discount_percent)",
            "Discounts table only",
            "Coupons stored as JSON in Orders"
          ],
          "correct": 0,
          "explanation": "Coupons need: unique code, discount mechanics (percentage or fixed amount), constraints (min order, max uses, validity period), and tracking (used_count). This enables validation at checkout and prevents abuse.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Two users try to use a coupon with max_uses = 1 simultaneously. How do you prevent double-use?",
          "options": [
            "Application-level check then update",
            "Atomic conditional update: UPDATE coupons SET used_count = used_count + 1 WHERE code = 'SAVE20' AND used_count < max_uses. If 0 rows updated, the coupon is already used.",
            "Lock the entire coupons table",
            "Queue coupon redemptions"
          ],
          "correct": 1,
          "explanation": "Atomic conditional update: the WHERE clause ensures only one request succeeds. The second request sees used_count = max_uses and the UPDATE affects 0 rows — coupon rejected. No race condition, no explicit locking.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-074",
      "type": "multiple-choice",
      "question": "You're designing a geographic points-of-interest (POI) system (Google Maps-like). POIs have: name, category, location (lat/lng), reviews, photos. What index type is essential?",
      "options": [
        "B-tree on latitude and longitude separately",
        "Spatial index (GiST/R-tree) on a geometry/geography column — enables 'find all restaurants within 1km of this point'",
        "Full-text index on names",
        "Hash index on category"
      ],
      "correct": 1,
      "explanation": "Spatial index: enables geographic queries (nearby, within radius, bounding box). B-tree on lat/lng separately can't answer 'within 1km' efficiently. Use PostGIS (PostgreSQL) or SPATIAL index (MySQL) on a POINT/GEOGRAPHY column.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-075",
      "type": "ordering",
      "question": "For the POI system, rank these access patterns from MOST COMMON to LEAST COMMON:",
      "items": [
        "Find POIs near a location (map view)",
        "View POI details with reviews",
        "Search POIs by name",
        "Admin bulk-import POI data"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Map view (nearby search): the primary interaction. POI details: when a user taps a pin. Name search: occasional. Bulk import: admin-only, rare. The nearby search pattern drives the choice of spatial indexing and storage.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-076",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a logging/monitoring system (Datadog-like). Metrics come in at 1M data points/sec: (service, metric_name, value, timestamp, tags). What storage fits?",
          "options": [
            "PostgreSQL with indexes",
            "A time-series database (InfluxDB, TimescaleDB, Prometheus TSDB) — purpose-built for high-volume, time-indexed metrics",
            "MongoDB",
            "Elasticsearch"
          ],
          "correct": 1,
          "explanation": "Time-series databases are optimized for: high ingest rate (millions/sec), time-based queries (last 1 hour, aggregation by minute), and efficient compression of time-ordered data. General-purpose databases can't match their write throughput and query patterns.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "Metrics older than 90 days should be downsampled (e.g., 1-minute resolution → 1-hour resolution) to save storage. How do you model this?",
          "options": [
            "Delete old data",
            "Multiple resolution tables: metrics_1min (0-7 days), metrics_5min (7-30 days), metrics_1hour (30-90 days), metrics_1day (90+ days) — aggregated by rollup jobs",
            "Keep all data at full resolution",
            "Archive to CSV files"
          ],
          "correct": 1,
          "explanation": "Multi-resolution tiering: recent data at full resolution, older data progressively downsampled. Rollup jobs aggregate and move data between tiers. This balances detail (recent) with storage cost (historical).",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-077",
      "type": "numeric-input",
      "question": "A monitoring system ingests 1M metrics/sec. Each metric is 50 bytes. How much raw data is generated per day?",
      "answer": 4320,
      "unit": "GB",
      "tolerance": 0.1,
      "explanation": "1,000,000 metrics/sec × 50 bytes × 86,400 sec/day = 4,320,000,000,000 bytes ≈ 4,320 GB ≈ 4.3 TB/day. This illustrates why time-series databases need efficient compression and downsampling.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-078",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a voting/polling system. Requirements: each user can vote once per poll, votes are anonymous (can't be traced back to the voter), results must be accurate. What's the modeling tension?",
          "options": [
            "No tension — standard table design works",
            "You need to prevent duplicate votes (requires knowing who voted) while keeping votes anonymous (requires NOT knowing who voted) — these conflict",
            "Anonymity isn't important",
            "Accuracy isn't important"
          ],
          "correct": 1,
          "explanation": "The tension: preventing duplicates requires storing user_id (to check if they already voted). But anonymity means the vote choice shouldn't be linkable to the voter. These are contradictory requirements.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible."
        },
        {
          "question": "How do you resolve this? (Prevent duplicates AND maintain anonymity)",
          "options": [
            "Store user_id and vote together (not anonymous)",
            "Two separate records: VoteReceipts(poll_id, user_id) tracks WHO voted (for deduplication). Votes(poll_id, choice) stores WHAT was chosen (no user reference). The link between them is severed.",
            "Trust users not to vote twice",
            "Use blockchain"
          ],
          "correct": 1,
          "explanation": "Separate records: VoteReceipts proves the user voted (deduplication). Votes stores the choice (aggregation). There's no foreign key between them — the system can't link a receipt to a specific choice. This is how real anonymous voting systems work.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-079",
      "type": "multi-select",
      "question": "Across all modeling scenarios, which patterns appear repeatedly?",
      "options": [
        "Junction tables for M:N relationships",
        "Status columns with lifecycle tracking",
        "Denormalized counters/aggregates for display",
        "Point-in-time snapshots for historical accuracy"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All four are universal patterns: M:N junctions (users-roles, products-categories). Status tracking (orders, subscriptions, tickets). Counter caches (follower_count, review_count). Snapshots (order prices, addresses). Recognize these patterns across domains.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-080",
      "type": "ordering",
      "question": "When tackling a new modeling scenario in an interview, rank these steps from FIRST to LAST:",
      "items": [
        "Clarify requirements and access patterns",
        "Identify core entities and relationships",
        "Design the schema (tables, keys, indexes)",
        "Discuss tradeoffs and optimizations"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Requirements first (what does the system do?). Entities second (what data exists?). Schema third (how is it stored?). Tradeoffs last (what are the alternatives?). This progression shows structured thinking — interviewers value the approach as much as the answer.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-081",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing an appointment scheduling API (Calendly-like). The core challenge: a time slot can only be booked once. Multiple people viewing the same slot simultaneously. How do you prevent double-booking?",
          "options": [
            "First-come-first-served with no locking",
            "Optimistic concurrency: attempt to INSERT the booking with a UNIQUE constraint on (host_id, time_slot) — the second INSERT fails with a constraint violation",
            "Lock the entire calendar during booking",
            "Show slots as unavailable immediately when anyone views them"
          ],
          "correct": 1,
          "explanation": "UNIQUE constraint on (host_id, time_slot): the database enforces single-booking. The first INSERT succeeds. Concurrent INSERTs for the same slot get a unique violation error. The application catches this and shows 'slot no longer available.'",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "Between viewing available slots and completing booking, time passes (user fills in details). The slot might be taken. What UX pattern handles this?",
          "options": [
            "Lock the slot for 10 minutes when someone starts booking (temporary reservation)",
            "Let users book; handle conflicts at insert time (show 'slot taken, please choose another')",
            "Both approaches are valid — temporary reservations for high-demand scheduling, conflict-at-insert for low-demand",
            "Don't show availability — just let them pick any time"
          ],
          "correct": 2,
          "explanation": "Low demand: conflict-at-insert is simpler (rare conflicts). High demand (popular time slots): temporary reservation prevents frustration. The temporary reservation pattern needs a TTL (auto-release after 10 min if not confirmed).",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-082",
      "type": "multiple-choice",
      "question": "You're modeling a feature flag system. FeatureFlags have: name, rollout_percentage, and optional user targeting rules. What's the data model?",
      "options": [
        "FeatureFlags(id, name, enabled BOOLEAN)",
        "FeatureFlags(id, name, rollout_percentage, targeting_rules JSONB, created_at, updated_at) — JSONB for flexible targeting rules like user segments, geos, etc.",
        "A config file (not a database)",
        "One table per feature flag"
      ],
      "correct": 1,
      "explanation": "JSONB targeting_rules: {\"user_ids\": [1,2,3], \"countries\": [\"US\", \"CA\"], \"percentage\": 10}. Flexible enough for any targeting logic. The application evaluates the rules at runtime against the current user context.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-083",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a job queue system (like Sidekiq/Celery). Jobs are enqueued, processed by workers, and marked complete or failed. What's the schema?",
          "options": [
            "Jobs(id, queue, payload JSONB, status, attempts, max_attempts, run_at, locked_by, locked_at, completed_at, failed_at)",
            "Jobs(id, data)",
            "A message queue (Kafka/RabbitMQ) instead of a database",
            "Tasks table with a boolean 'done' column"
          ],
          "correct": 0,
          "explanation": "Full job schema: queue (routing), payload (job data), status tracking, retry support (attempts, max_attempts), scheduling (run_at), worker locking (locked_by, locked_at to prevent double-processing), and completion tracking.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "Workers poll for jobs: 'Give me the next unlocked job in queue X.' How do you prevent two workers from picking the same job?",
          "options": [
            "Application-level lock flag",
            "SELECT ... FOR UPDATE SKIP LOCKED: atomically select and lock a row, skipping rows already locked by other workers — built-in PostgreSQL feature for job queues",
            "Assign jobs to specific workers",
            "Use a separate lock table"
          ],
          "correct": 1,
          "explanation": "FOR UPDATE SKIP LOCKED: each worker's SELECT skips rows locked by others and locks the row it selects. No contention, no double-processing. This is why PostgreSQL is a viable job queue backend for moderate throughput.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-084",
      "type": "multi-select",
      "question": "Which real-world systems commonly use PostgreSQL as a job queue (instead of dedicated queue like Redis/RabbitMQ)?",
      "options": [
        "Ruby on Rails apps with Delayed Job or Que",
        "Django apps with django-db-queue",
        "Systems needing transactional enqueue (enqueue job in same transaction as the triggering write)",
        "High-throughput systems processing 1M+ jobs/sec"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "PostgreSQL job queues work well for: Rails/Django apps (simple setup), transactional enqueue (critical for consistency — job is enqueued only if the triggering DB write commits). At 1M+/sec, dedicated queues (Redis, Kafka) are needed.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "scenario-085",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing an e-commerce returns/refunds system. What entities extend the existing order model?",
          "options": [
            "Returns(id, order_id, reason, status, created_at) + ReturnItems(return_id, order_item_id, quantity, condition)",
            "A 'returned' boolean on Orders",
            "Refunds table only",
            "Delete the order and create a new one"
          ],
          "correct": 0,
          "explanation": "Returns (the return request) + ReturnItems (which items from the order are being returned, in what quantity and condition). This links back to the original order while tracking the return lifecycle separately.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "A return can result in: refund, exchange, or store credit. How do you model the resolution?",
          "options": [
            "A 'resolution_type' ENUM on the Returns table",
            "ReturnResolutions(return_id, type, amount, credit_id/refund_id) — separate entity since different resolutions have different data (refund has payment_id, credit has credit_balance_id)",
            "Three separate return tables",
            "Let the payment system handle it"
          ],
          "correct": 1,
          "explanation": "ReturnResolutions: links the return to its outcome. Different resolution types reference different entities (Refunds, StoreCredits, ExchangeOrders). This cleanly separates the return request from how it was resolved.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-086",
      "type": "ordering",
      "question": "For an e-commerce system, rank the entity model layers from CORE (innermost) to EDGE (outermost):",
      "items": [
        "Products, Categories (catalog)",
        "Users, Addresses (customers)",
        "Orders, OrderItems, Payments (transactions)",
        "Returns, Refunds, Reviews (post-purchase)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Catalog first (what's for sale). Customers next (who's buying). Transactions (the purchase). Post-purchase (returns, refunds, reviews). Each layer depends on the previous ones. Build from the center outward.",
      "detailedExplanation": "Order by relative impact rather than exact values, then verify the sequence one boundary at a time. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-087",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a rate-limiting system. Access pattern: 'Has user X exceeded 100 requests in the last minute?' What data structure fits?",
          "options": [
            "A database table of all requests",
            "A sliding window counter in Redis: key = user:{id}:requests:{minute}, INCR on each request, EXPIRE after 60s",
            "A blockchain ledger",
            "Application-level counter in memory"
          ],
          "correct": 1,
          "explanation": "Redis counter with TTL: O(1) check per request. Key auto-expires, so no cleanup needed. For sliding windows (more precise), use Redis sorted sets with timestamps. For fixed windows (simpler), just INCR + EXPIRE.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "The system has 10 application servers. Why can't you use in-memory counters?",
          "options": [
            "In-memory counters are too slow",
            "Each server has its own counter — a user hitting 10 servers uses 10 of their 100 limit per server (100 total = 1000 actual requests). The counter must be shared across servers.",
            "In-memory counters can't handle the data type",
            "Servers don't have enough RAM"
          ],
          "correct": 1,
          "explanation": "Distributed counting problem: per-server counters fragment the limit. User makes 90 requests to server A (under limit) and 90 to server B (under limit) — 180 total, way over the 100 limit. Redis provides a centralized, atomic counter.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-088",
      "type": "multiple-choice",
      "question": "You're modeling a social graph. Users can follow, block, and mute other users. How many relationships do you need?",
      "options": [
        "One table: UserRelationships(user_id, target_id, type) where type ∈ ('follow', 'block', 'mute')",
        "Three tables: Follows, Blocks, Mutes",
        "Both approaches work — one table is simpler, three tables are more explicit and allow different indexes per relationship type",
        "A graph database is the only option"
      ],
      "correct": 2,
      "explanation": "Both are valid. Single table: simpler schema, one place to check all relationships. Separate tables: clearer semantics, independent indexes, can add type-specific columns (e.g., follow has 'notifications_enabled'). Choose based on access patterns.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-089",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a task dependency system (like a build system). Tasks can depend on other tasks (DAG — Directed Acyclic Graph). What's the data model?",
          "options": [
            "Tasks(id, name, status) + TaskDependencies(task_id, depends_on_task_id)",
            "Tasks with a parent_task_id column",
            "Tasks with a JSON array of dependency IDs",
            "A separate Dependencies table with no task reference"
          ],
          "correct": 0,
          "explanation": "Tasks + TaskDependencies junction: each dependency is an explicit row. A task can depend on multiple tasks (fan-in), and a task can be depended on by multiple others (fan-out). This is a DAG modeled as an adjacency list.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "To execute tasks in the correct order, you need topological sort. What query finds tasks with all dependencies satisfied?",
          "options": [
            "SELECT all tasks ORDER BY id",
            "SELECT tasks WHERE NOT EXISTS (SELECT 1 FROM task_dependencies td JOIN tasks dep ON td.depends_on_task_id = dep.id WHERE td.task_id = tasks.id AND dep.status != 'completed')",
            "SELECT tasks WHERE status = 'pending' ORDER BY created_at",
            "Recursive CTE from root to leaves"
          ],
          "correct": 1,
          "explanation": "Find tasks where all dependencies are completed (no unsatisfied dependency exists). This is the 'ready to run' set. Process these, mark completed, repeat — the frontier advances through the DAG.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-090",
      "type": "numeric-input",
      "question": "A build system has 500 tasks. On average, each task has 3 dependencies. How many rows are in the TaskDependencies table?",
      "answer": 1500,
      "unit": "rows",
      "tolerance": "exact",
      "explanation": "500 tasks × 3 dependencies each = 1,500 dependency rows. This is small — the complexity is in the graph traversal (topological sort), not the data volume.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-091",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a content moderation system. User-generated content (posts, comments, images) needs to be reviewed for policy violations. What's the data model?",
          "options": [
            "ModerationQueue(id, content_type, content_id, reason, status, assigned_to, reviewed_at, decision, notes)",
            "A 'flagged' boolean on each content table",
            "Separate moderation tables per content type",
            "No database — use an external moderation API"
          ],
          "correct": 0,
          "explanation": "A unified ModerationQueue: polymorphic (content_type + content_id points to any content table), tracks the full moderation lifecycle (reason, assignment, decision, notes). This supports: agent assignment, SLA tracking, appeal handling.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "Content can be flagged by users, automated systems (ML), or manually by moderators. How do you track the source?",
          "options": [
            "A 'source' column: 'user_report', 'auto_detected', 'manual_review'",
            "ModerationReports(moderation_item_id, reporter_type, reporter_id, reason, created_at) — multiple reports can be filed against the same content",
            "Don't track the source",
            "Separate queues per source"
          ],
          "correct": 1,
          "explanation": "Multiple reports per content: 10 users might report the same post. ModerationReports tracks each individual report. The ModerationQueue item aggregates them. This enables: 'auto-escalate content with 5+ reports' and reporter trustworthiness scoring.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-092",
      "type": "multi-select",
      "question": "Which scenarios from this chapter involve inventory/resource contention as a core modeling concern?",
      "options": [
        "Hotel room booking (preventing double-booking)",
        "Event ticketing (preventing overselling)",
        "Appointment scheduling (preventing double-booking of time slots)",
        "Social media feed (no finite resource)"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Hotels: rooms are finite resources. Tickets: limited inventory. Appointments: time slots are scarce. Social feed: no contention — unlimited users can read the feed. Contention patterns require: atomic operations, pessimistic/optimistic locking, or pre-partitioning.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-093",
      "type": "ordering",
      "question": "Rank these system types by the complexity of their data model (SIMPLEST to MOST COMPLEX):",
      "items": [
        "URL shortener (key-value + analytics)",
        "Blog platform (posts, comments, tags)",
        "E-commerce (catalog, cart, orders, payments, returns)",
        "Ride-sharing (real-time matching, geospatial, pricing, payments)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "URL shortener: essentially a key-value store. Blog: standard CRUD with relationships. E-commerce: multi-entity with financial transactions. Ride-sharing: real-time, geospatial, multi-party coordination, surge pricing — most complex data model.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "scenario-094",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing an A/B testing platform. Experiments target users, assign them to variants, and track conversion metrics. What's the core data model?",
          "options": [
            "Experiments(id, name, status, traffic_percentage) + Variants(experiment_id, name, weight) + ExperimentAssignments(experiment_id, user_id, variant_id, assigned_at)",
            "Experiments with a JSON config",
            "A single Analytics table",
            "Feature flags only (no explicit experiments)"
          ],
          "correct": 0,
          "explanation": "Experiments (the test), Variants (A/B/C options with traffic weights), Assignments (which user sees which variant — must be sticky/consistent). Conversions tracked via events linked to assignments.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "A user must always see the same variant (sticky assignment). How do you ensure this?",
          "options": [
            "Random assignment on every page load",
            "Store the assignment in ExperimentAssignments(user_id, experiment_id, variant_id) — check before assigning, create only if not exists",
            "Use cookies only (lost on device switch)",
            "Hash the user_id deterministically (no database lookup needed)"
          ],
          "correct": 3,
          "explanation": "Deterministic hashing: variant = hash(user_id + experiment_id) % 100. Always produces the same result for the same user+experiment. No database lookup needed. The database stores assignments for analytics, but the assignment decision is stateless.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-095",
      "type": "multiple-choice",
      "question": "Across all these scenarios, when should you choose a NoSQL database over relational?",
      "options": [
        "Always — NoSQL is newer and better",
        "When access patterns are simple (key-value lookups), data is naturally document-shaped, relationships are minimal, and you need extreme write throughput or schema flexibility",
        "Never — relational handles everything",
        "Only for social media"
      ],
      "correct": 1,
      "explanation": "NoSQL fits: simple access patterns (key-value: sessions, cache), document-shaped data (product catalogs with varying attributes), minimal relationships (no complex joins), extreme write volume (IoT, logging). Relational fits: complex queries, strong consistency, many relationships.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "scenario-096",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're in a system design interview. The interviewer says: 'Design the data model for a parking lot system.' What's your first response?",
          "options": [
            "Start drawing tables immediately",
            "Ask clarifying questions: How many lots? Single or multi-level? Reservations or first-come? Payment integrated? Dynamic pricing?",
            "Say it's too simple to need a data model",
            "Ask which database to use"
          ],
          "correct": 1,
          "explanation": "Always clarify requirements first. A single-lot, first-come system is trivial. A multi-location, multi-level, reservation-supporting, dynamically-priced system is complex. The requirements determine the model complexity.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "After clarification (multi-location, reservations supported, hourly pricing), what are the core entities?",
          "options": [
            "ParkingLots, Levels, Spots, Vehicles, Bookings, PricingRules",
            "ParkingLots, Cars",
            "Spots, Reservations",
            "A single ParkingEvents table"
          ],
          "correct": 0,
          "explanation": "ParkingLots (locations) → Levels (floors) → Spots (individual spaces with type: compact/regular/handicap). Vehicles (license plates). Bookings (reservation or active session). PricingRules (per lot, per hour, peak/off-peak). Clean hierarchy with booking as the transactional entity.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-097",
      "type": "multi-select",
      "question": "Which data modeling mistakes are common in system design interviews?",
      "options": [
        "Starting with the schema before understanding access patterns",
        "Over-normalizing when denormalization would serve the primary use case better",
        "Ignoring data growth and volume considerations",
        "Using too many tables (interviewers prefer fewer, wider tables)"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Schema-first (should be access-pattern-first), over-normalization (pragmatism matters), and ignoring scale (changes everything) are common interview mistakes. Using 'too many tables' is not a mistake — more tables often means better normalization. Interviewers don't penalize proper table decomposition.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-098",
      "type": "ordering",
      "question": "Rank these interview communication strategies from MOST to LEAST effective:",
      "items": [
        "Think out loud, explaining WHY you make each modeling decision",
        "Draw the entity-relationship diagram as you explain",
        "Discuss tradeoffs: 'I chose X because of Y, but the alternative Z would be better if...'",
        "Present a perfect schema with no explanation of the reasoning"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "Thinking aloud: shows reasoning process (most valued). Tradeoffs: shows depth of understanding. Diagrams: visual communication aids clarity. Silent perfect answer: doesn't demonstrate the thinking — which is what interviewers actually assess.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "scenario-099",
      "type": "multiple-choice",
      "question": "What's the most important takeaway from data modeling for system design interviews?",
      "options": [
        "Memorize common schemas",
        "There's always one correct answer",
        "Data modeling is about tradeoffs — understand the access patterns, make deliberate choices, and be able to articulate why you chose one approach over another",
        "Use the most complex model possible to impress the interviewer"
      ],
      "correct": 2,
      "explanation": "Data modeling is tradeoff engineering: normalization vs performance, consistency vs availability, simplicity vs flexibility. The 'right' model depends on the specific requirements. Being able to articulate these tradeoffs is the skill interviewers test.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "scenario-100",
      "type": "two-stage",
      "stages": [
        {
          "question": "Final comprehensive question: You're asked to 'design the data model for Twitter.' What do you do first?",
          "options": [
            "List all entities: Users, Tweets, Follows, Likes, Retweets, DMs, Lists, Trending, Notifications, Search...",
            "Scope it down: 'Let me focus on the core: posting tweets and reading a personalized feed. I'll expand to other features if time permits.'",
            "Ask what database to use",
            "Start with the Users table"
          ],
          "correct": 1,
          "explanation": "Scope first. 'Design Twitter' is too broad for a 45-minute interview. Pick the core (tweet creation + feed) and go deep. You can mention other entities as extensions. Going deep on 3 entities beats going shallow on 15.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "For the core (tweets + feed), what's the key modeling decision you should discuss?",
          "options": [
            "Whether to use UUID or auto-increment for IDs",
            "Fan-out on write vs fan-out on read for feed generation — this is the defining architectural tradeoff and demonstrates the deepest understanding of the system's data modeling",
            "How to store tweet text",
            "Which database vendor to use"
          ],
          "correct": 1,
          "explanation": "Fan-out on write (precompute feeds) vs on read (compute at query time) is THE defining data modeling decision for Twitter. It touches: denormalization, access patterns, scale, consistency, and celebrity user handling. Discussing this tradeoff demonstrates mastery.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    }
  ]
}
