{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 6,
  "chapterTitle": "Data Safety, Durability & Recovery",
  "chapterDescription": "Prevent irreversible data loss and plan restoration with explicit RPO/RTO guarantees.",
  "problems": [
    {
      "id": "rel-dr-001",
      "type": "multiple-choice",
      "question": "Case Alpha: transaction ledger database. Primary reliability risk is backup gaps exceeding RPO. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Define explicit RPO/RTO per data class and align backup cadence to each tier.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat transaction ledger database as a reliability-control decision, not an averages-only optimization. \"Define explicit RPO/RTO per data class and align backup cadence to each tier\" is correct since it mitigates backup gaps exceeding RPO while keeping containment local. The decision remains valid given: A rollback window is still available for the next 15 minutes.",
      "detailedExplanation": "The key clue in this question is \"transaction ledger database\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 15 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-dr-002",
      "type": "multiple-choice",
      "question": "Case Beta: customer profile store. Primary reliability risk is restore procedure untested at scale. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Run regular restore drills with production-like volume and verification checks.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For customer profile store, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Run regular restore drills with production-like volume and verification checks\" outperforms the alternatives because it targets restore procedure untested at scale and preserves safe recovery behavior. It is also the most compatible with Leadership asked for an action that lowers recurrence, not just symptoms.",
      "detailedExplanation": "Read this as a scenario about \"customer profile store\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-003",
      "type": "multiple-choice",
      "question": "Case Gamma: event stream archive. Primary reliability risk is snapshot corruption discovered late. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use immutable backup storage with integrity manifests and checksum validation.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Data Safety, Durability & Recovery, event stream archive fails mainly through snapshot corruption discovered late. The best choice is \"Use immutable backup storage with integrity manifests and checksum validation\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "The decision turns on \"event stream archive\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-dr-004",
      "type": "multiple-choice",
      "question": "Case Delta: object metadata catalog. Primary reliability risk is replica acknowledged before durable write. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Separate durability acknowledgements from in-memory replication signals."
      ],
      "correct": 3,
      "explanation": "Object metadata catalog should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Separate durability acknowledgements from in-memory replication signals\" is strongest because it directly addresses replica acknowledged before durable write and improves repeatability under stress. This aligns with the extra condition (Recent game-day results showed hidden cross-zone coupling).",
      "detailedExplanation": "This prompt is really about \"object metadata catalog\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-dr-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: multi-tenant billing records. Primary reliability risk is point-in-time recovery window misconfigured. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Implement point-in-time recovery and test rollback to known incident timestamps.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat multi-tenant billing records as a reliability-control decision, not an averages-only optimization. \"Implement point-in-time recovery and test rollback to known incident timestamps\" is correct since it mitigates point-in-time recovery window misconfigured while keeping containment local. The decision remains valid given: Customer impact is concentrated on invariant-critical transactions.",
      "detailedExplanation": "Use \"multi-tenant billing records\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-006",
      "type": "multiple-choice",
      "question": "Case Zeta: search index snapshots. Primary reliability risk is checksum verification skipped in restore. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Protect backup credentials and keys with isolated recovery procedures.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For search index snapshots, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Protect backup credentials and keys with isolated recovery procedures\" outperforms the alternatives because it targets checksum verification skipped in restore and preserves safe recovery behavior. It is also the most compatible with The previous mitigation improved averages but not tail behavior.",
      "detailedExplanation": "The core signal here is \"search index snapshots\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-007",
      "type": "multiple-choice",
      "question": "Case Eta: audit trail warehouse. Primary reliability risk is encryption key loss blocks recovery. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Continuously audit backup completeness and alert on coverage gaps.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Data Safety, Durability & Recovery, audit trail warehouse fails mainly through encryption key loss blocks recovery. The best choice is \"Continuously audit backup completeness and alert on coverage gaps\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "If you keep \"audit trail warehouse\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-008",
      "type": "multiple-choice",
      "question": "Case Theta: session persistence cluster. Primary reliability risk is cross-region backup drift. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Retain forensic logs long enough to support incident reconstruction."
      ],
      "correct": 3,
      "explanation": "Session persistence cluster should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Retain forensic logs long enough to support incident reconstruction\" is strongest because it directly addresses cross-region backup drift and improves repeatability under stress. This aligns with the extra condition (Operations wants a reversible step before broader architecture changes).",
      "detailedExplanation": "Start from \"session persistence cluster\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-dr-009",
      "type": "multiple-choice",
      "question": "Case Iota: feature flag history store. Primary reliability risk is logical delete propagated without guard. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Use staged recovery with read-only validation before write re-enable.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat feature flag history store as a reliability-control decision, not an averages-only optimization. \"Use staged recovery with read-only validation before write re-enable\" is correct since it mitigates logical delete propagated without guard while keeping containment local. The decision remains valid given: SLO burn rate accelerated after a config rollout this morning.",
      "detailedExplanation": "The key clue in this question is \"feature flag history store\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-010",
      "type": "multiple-choice",
      "question": "Case Kappa: support ticket history. Primary reliability risk is retention policy removes needed history. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Track data-loss near misses and close process gaps before next incident.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Data Safety, Durability & Recovery, support ticket history fails mainly through retention policy removes needed history. The best choice is \"Track data-loss near misses and close process gaps before next incident\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "The decision turns on \"support ticket history\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-dr-011",
      "type": "multiple-choice",
      "question": "Case Lambda: transaction ledger database. Primary reliability risk is backup gaps exceeding RPO. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Define explicit RPO/RTO per data class and align backup cadence to each tier.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Transaction ledger database should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Define explicit RPO/RTO per data class and align backup cadence to each tier\" is strongest because it directly addresses backup gaps exceeding RPO and improves repeatability under stress. This aligns with the extra condition (The incident review highlighted missing boundary ownership).",
      "detailedExplanation": "Read this as a scenario about \"transaction ledger database\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-dr-012",
      "type": "multiple-choice",
      "question": "Case Mu: customer profile store. Primary reliability risk is restore procedure untested at scale. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Run regular restore drills with production-like volume and verification checks."
      ],
      "correct": 3,
      "explanation": "Treat customer profile store as a reliability-control decision, not an averages-only optimization. \"Run regular restore drills with production-like volume and verification checks\" is correct since it mitigates restore procedure untested at scale while keeping containment local. The decision remains valid given: Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "detailedExplanation": "The key clue in this question is \"customer profile store\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-013",
      "type": "multiple-choice",
      "question": "Case Nu: event stream archive. Primary reliability risk is snapshot corruption discovered late. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Use immutable backup storage with integrity manifests and checksum validation.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For event stream archive, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Use immutable backup storage with integrity manifests and checksum validation\" outperforms the alternatives because it targets snapshot corruption discovered late and preserves safe recovery behavior. It is also the most compatible with A canary can be deployed immediately if the strategy is clear.",
      "detailedExplanation": "Start from \"event stream archive\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-dr-014",
      "type": "multiple-choice",
      "question": "Case Xi: object metadata catalog. Primary reliability risk is replica acknowledged before durable write. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Separate durability acknowledgements from in-memory replication signals.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Data Safety, Durability & Recovery, object metadata catalog fails mainly through replica acknowledged before durable write. The best choice is \"Separate durability acknowledgements from in-memory replication signals\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "If you keep \"object metadata catalog\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-dr-015",
      "type": "multiple-choice",
      "question": "Case Omicron: multi-tenant billing records. Primary reliability risk is point-in-time recovery window misconfigured. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Implement point-in-time recovery and test rollback to known incident timestamps.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Multi-tenant billing records should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Implement point-in-time recovery and test rollback to known incident timestamps\" is strongest because it directly addresses point-in-time recovery window misconfigured and improves repeatability under stress. This aligns with the extra condition (Client retries are already elevated and could amplify mistakes).",
      "detailedExplanation": "The core signal here is \"multi-tenant billing records\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-016",
      "type": "multiple-choice",
      "question": "Case Pi: search index snapshots. Primary reliability risk is checksum verification skipped in restore. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Protect backup credentials and keys with isolated recovery procedures."
      ],
      "correct": 3,
      "explanation": "Treat search index snapshots as a reliability-control decision, not an averages-only optimization. \"Protect backup credentials and keys with isolated recovery procedures\" is correct since it mitigates checksum verification skipped in restore while keeping containment local. The decision remains valid given: The team must preserve core write correctness under mitigation.",
      "detailedExplanation": "Use \"search index snapshots\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-017",
      "type": "multiple-choice",
      "question": "Case Rho: audit trail warehouse. Primary reliability risk is encryption key loss blocks recovery. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Continuously audit backup completeness and alert on coverage gaps.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For audit trail warehouse, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Continuously audit backup completeness and alert on coverage gaps\" outperforms the alternatives because it targets encryption key loss blocks recovery and preserves safe recovery behavior. It is also the most compatible with Recent staffing changes require simpler operational controls.",
      "detailedExplanation": "This prompt is really about \"audit trail warehouse\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-018",
      "type": "multiple-choice",
      "question": "Case Sigma: session persistence cluster. Primary reliability risk is cross-region backup drift. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Retain forensic logs long enough to support incident reconstruction.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Data Safety, Durability & Recovery, session persistence cluster fails mainly through cross-region backup drift. The best choice is \"Retain forensic logs long enough to support incident reconstruction\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Cross-region latency variance increased during the event.",
      "detailedExplanation": "The decision turns on \"session persistence cluster\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-dr-019",
      "type": "multiple-choice",
      "question": "Case Tau: feature flag history store. Primary reliability risk is logical delete propagated without guard. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use staged recovery with read-only validation before write re-enable.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Feature flag history store should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Use staged recovery with read-only validation before write re-enable\" is strongest because it directly addresses logical delete propagated without guard and improves repeatability under stress. This aligns with the extra condition (This path mixes latency-sensitive and correctness-sensitive requests).",
      "detailedExplanation": "Read this as a scenario about \"feature flag history store\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: support ticket history. Primary reliability risk is retention policy removes needed history. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Track data-loss near misses and close process gaps before next incident."
      ],
      "correct": 3,
      "explanation": "For support ticket history, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Track data-loss near misses and close process gaps before next incident\" outperforms the alternatives because it targets retention policy removes needed history and preserves safe recovery behavior. It is also the most compatible with The service has one hidden shared component with no backup path.",
      "detailedExplanation": "Read this as a scenario about \"support ticket history\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-dr-021",
      "type": "multiple-choice",
      "question": "Case Phi: transaction ledger database. Primary reliability risk is backup gaps exceeding RPO. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Define explicit RPO/RTO per data class and align backup cadence to each tier.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Data Safety, Durability & Recovery, transaction ledger database fails mainly through backup gaps exceeding RPO. The best choice is \"Define explicit RPO/RTO per data class and align backup cadence to each tier\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "The decision turns on \"transaction ledger database\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-dr-022",
      "type": "multiple-choice",
      "question": "Case Chi: customer profile store. Primary reliability risk is restore procedure untested at scale. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Run regular restore drills with production-like volume and verification checks.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Customer profile store should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Run regular restore drills with production-like volume and verification checks\" is strongest because it directly addresses restore procedure untested at scale and improves repeatability under stress. This aligns with the extra condition (Change approval favors narrowly scoped policies over global flips).",
      "detailedExplanation": "Start from \"customer profile store\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-023",
      "type": "multiple-choice",
      "question": "Case Psi: event stream archive. Primary reliability risk is snapshot corruption discovered late. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use immutable backup storage with integrity manifests and checksum validation.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat event stream archive as a reliability-control decision, not an averages-only optimization. \"Use immutable backup storage with integrity manifests and checksum validation\" is correct since it mitigates snapshot corruption discovered late while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "The key clue in this question is \"event stream archive\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-dr-024",
      "type": "multiple-choice",
      "question": "Case Omega: object metadata catalog. Primary reliability risk is replica acknowledged before durable write. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Separate durability acknowledgements from in-memory replication signals."
      ],
      "correct": 3,
      "explanation": "For object metadata catalog, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Separate durability acknowledgements from in-memory replication signals\" outperforms the alternatives because it targets replica acknowledged before durable write and preserves safe recovery behavior. It is also the most compatible with On-call needs mitigation that is observable by explicit metrics.",
      "detailedExplanation": "The core signal here is \"object metadata catalog\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-dr-025",
      "type": "multiple-choice",
      "question": "Case Atlas: multi-tenant billing records. Primary reliability risk is point-in-time recovery window misconfigured. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Implement point-in-time recovery and test rollback to known incident timestamps.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Data Safety, Durability & Recovery, multi-tenant billing records fails mainly through point-in-time recovery window misconfigured. The best choice is \"Implement point-in-time recovery and test rollback to known incident timestamps\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "If you keep \"multi-tenant billing records\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-026",
      "type": "multiple-choice",
      "question": "Case Nova: search index snapshots. Primary reliability risk is checksum verification skipped in restore. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Protect backup credentials and keys with isolated recovery procedures.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Search index snapshots should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Protect backup credentials and keys with isolated recovery procedures\" is strongest because it directly addresses checksum verification skipped in restore and improves repeatability under stress. This aligns with the extra condition (Business impact is highest in the top 5% of critical flows).",
      "detailedExplanation": "This prompt is really about \"search index snapshots\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-027",
      "type": "multiple-choice",
      "question": "Case Orion: audit trail warehouse. Primary reliability risk is encryption key loss blocks recovery. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Continuously audit backup completeness and alert on coverage gaps.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat audit trail warehouse as a reliability-control decision, not an averages-only optimization. \"Continuously audit backup completeness and alert on coverage gaps\" is correct since it mitigates encryption key loss blocks recovery while keeping containment local. The decision remains valid given: Regional failover is possible but expensive if used prematurely.",
      "detailedExplanation": "Use \"audit trail warehouse\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-028",
      "type": "multiple-choice",
      "question": "Case Vega: session persistence cluster. Primary reliability risk is cross-region backup drift. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Retain forensic logs long enough to support incident reconstruction."
      ],
      "correct": 3,
      "explanation": "For session persistence cluster, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Retain forensic logs long enough to support incident reconstruction\" outperforms the alternatives because it targets cross-region backup drift and preserves safe recovery behavior. It is also the most compatible with A hot tenant currently consumes disproportionate worker capacity.",
      "detailedExplanation": "Read this as a scenario about \"session persistence cluster\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-dr-029",
      "type": "multiple-choice",
      "question": "Case Helios: feature flag history store. Primary reliability risk is logical delete propagated without guard. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Use staged recovery with read-only validation before write re-enable.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Data Safety, Durability & Recovery, feature flag history store fails mainly through logical delete propagated without guard. The best choice is \"Use staged recovery with read-only validation before write re-enable\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "The decision turns on \"feature flag history store\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-030",
      "type": "multiple-choice",
      "question": "Case Aurora: support ticket history. Primary reliability risk is retention policy removes needed history. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Track data-loss near misses and close process gaps before next incident.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat support ticket history as a reliability-control decision, not an averages-only optimization. \"Track data-loss near misses and close process gaps before next incident\" is correct since it mitigates retention policy removes needed history while keeping containment local. The decision remains valid given: Queue age is rising even though average CPU appears normal.",
      "detailedExplanation": "Use \"support ticket history\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-dr-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: transaction ledger database. Primary reliability risk is backup gaps exceeding RPO. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Define explicit RPO/RTO per data class and align backup cadence to each tier.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For transaction ledger database, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Define explicit RPO/RTO per data class and align backup cadence to each tier\" outperforms the alternatives because it targets backup gaps exceeding RPO and preserves safe recovery behavior. It is also the most compatible with A control-plane API is healthy but data-plane errors are increasing.",
      "detailedExplanation": "This prompt is really about \"transaction ledger database\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "rel-dr-032",
      "type": "multiple-choice",
      "question": "Case Pulse: customer profile store. Primary reliability risk is restore procedure untested at scale. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Run regular restore drills with production-like volume and verification checks."
      ],
      "correct": 3,
      "explanation": "In Data Safety, Durability & Recovery, customer profile store fails mainly through restore procedure untested at scale. The best choice is \"Run regular restore drills with production-like volume and verification checks\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "If you keep \"customer profile store\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-033",
      "type": "multiple-choice",
      "question": "Case Forge: event stream archive. Primary reliability risk is snapshot corruption discovered late. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Use immutable backup storage with integrity manifests and checksum validation.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Event stream archive should be solved at the failure boundary named in Data Safety, Durability & Recovery. \"Use immutable backup storage with integrity manifests and checksum validation\" is strongest because it directly addresses snapshot corruption discovered late and improves repeatability under stress. This aligns with the extra condition (Legal/compliance constraints require explicit behavior in degraded mode).",
      "detailedExplanation": "The core signal here is \"event stream archive\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-dr-034",
      "type": "multiple-choice",
      "question": "Case Harbor: object metadata catalog. Primary reliability risk is replica acknowledged before durable write. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Separate durability acknowledgements from in-memory replication signals.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat object metadata catalog as a reliability-control decision, not an averages-only optimization. \"Separate durability acknowledgements from in-memory replication signals\" is correct since it mitigates replica acknowledged before durable write while keeping containment local. The decision remains valid given: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "The key clue in this question is \"object metadata catalog\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-dr-035",
      "type": "multiple-choice",
      "question": "Case Vector: multi-tenant billing records. Primary reliability risk is point-in-time recovery window misconfigured. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Implement point-in-time recovery and test rollback to known incident timestamps.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For multi-tenant billing records, prefer the option that prevents reoccurrence in Data Safety, Durability & Recovery. \"Implement point-in-time recovery and test rollback to known incident timestamps\" outperforms the alternatives because it targets point-in-time recovery window misconfigured and preserves safe recovery behavior. It is also the most compatible with User trust impact is tied to visible inconsistency, not only downtime.",
      "detailedExplanation": "Start from \"multi-tenant billing records\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for transaction ledger database: signal points to replica acknowledged before durable write. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for transaction ledger database is mismatched to replica acknowledged before durable write, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Transaction ledger database is a two-step reliability decision. At stage 1, \"The design for transaction ledger database is mismatched to replica acknowledged before durable write, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around replica acknowledged before durable write.",
          "detailedExplanation": "Use \"incident diagnosis for transaction ledger database: signal points to replica\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for transaction ledger database:\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Protect backup credentials and keys with isolated recovery procedures.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Protect backup credentials and keys with isolated recovery procedures\" best matches Now that \"incident diagnosis for transaction ledger database:\" is diagnosed, which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The decision turns on \"data Safety, Durability & Recovery\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for customer profile store: signal points to point-in-time recovery window misconfigured. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for customer profile store is mismatched to point-in-time recovery window misconfigured, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Data Safety, Durability & Recovery, the best answer is \"The design for customer profile store is mismatched to point-in-time recovery window misconfigured, creating repeat reliability incidents\". It is the option most directly aligned to point-in-time recovery window misconfigured while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for customer profile store: signal points to point-in-time recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for customer profile store: signal\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Continuously audit backup completeness and alert on coverage gaps.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In the \"incident diagnosis for customer profile store: signal\" scenario, which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Continuously audit backup completeness and alert on coverage gaps\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"data Safety, Durability & Recovery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for event stream archive: signal points to checksum verification skipped in restore. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for event stream archive is mismatched to checksum verification skipped in restore, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Data Safety, Durability & Recovery: for event stream archive, \"The design for event stream archive is mismatched to checksum verification skipped in restore, creating repeat reliability incidents\" is correct because it addresses checksum verification skipped in restore and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for event stream archive: signal points to checksum verification\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for event stream archive: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Retain forensic logs long enough to support incident reconstruction."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Data Safety, Durability & Recovery, the best answer is \"Retain forensic logs long enough to support incident reconstruction\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"data Safety, Durability & Recovery\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for object metadata catalog: signal points to encryption key loss blocks recovery. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for object metadata catalog is mismatched to encryption key loss blocks recovery, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for object metadata catalog is mismatched to encryption key loss blocks recovery, creating repeat reliability incidents\" best matches object metadata catalog by targeting encryption key loss blocks recovery and lowering repeat risk.",
          "detailedExplanation": "Start from \"incident diagnosis for object metadata catalog: signal points to encryption key loss\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for object metadata catalog: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Use staged recovery with read-only validation before write re-enable.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Data Safety, Durability & Recovery: for Using the diagnosis from \"incident diagnosis for object metadata catalog: signal\", what is the highest-leverage change to make now, \"Use staged recovery with read-only validation before write re-enable\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "This prompt is really about \"data Safety, Durability & Recovery\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for multi-tenant billing records: signal points to cross-region backup drift. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for multi-tenant billing records is mismatched to cross-region backup drift, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Data Safety, Durability & Recovery, the best answer is \"The design for multi-tenant billing records is mismatched to cross-region backup drift, creating repeat reliability incidents\". It is the option most directly aligned to cross-region backup drift while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"incident diagnosis for multi-tenant billing records: signal points to cross-region\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for multi-tenant billing records:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Track data-loss near misses and close process gaps before next incident.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Using the diagnosis from \"incident diagnosis for multi-tenant billing records:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Track data-loss near misses and close process gaps before next incident\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "This prompt is really about \"data Safety, Durability & Recovery\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search index snapshots: signal points to logical delete propagated without guard. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for search index snapshots is mismatched to logical delete propagated without guard, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Data Safety, Durability & Recovery: for search index snapshots, \"The design for search index snapshots is mismatched to logical delete propagated without guard, creating repeat reliability incidents\" is correct because it addresses logical delete propagated without guard and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for search index snapshots: signal points to logical delete\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for search index snapshots: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Define explicit RPO/RTO per data class and align backup cadence to each tier.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Data Safety, Durability & Recovery, the best answer is \"Define explicit RPO/RTO per data class and align backup cadence to each tier\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"data Safety, Durability & Recovery\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for audit trail warehouse: signal points to retention policy removes needed history. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for audit trail warehouse is mismatched to retention policy removes needed history, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for audit trail warehouse is mismatched to retention policy removes needed history, creating repeat reliability incidents\" best matches audit trail warehouse by targeting retention policy removes needed history and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for audit trail warehouse: signal points to retention policy removes\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for audit trail warehouse: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Run regular restore drills with production-like volume and verification checks."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Data Safety, Durability & Recovery: for Given the diagnosis in \"incident diagnosis for audit trail warehouse: signal\", which immediate adjustment best addresses the risk, \"Run regular restore drills with production-like volume and verification checks\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"data Safety, Durability & Recovery\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session persistence cluster: signal points to backup gaps exceeding RPO. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for session persistence cluster is mismatched to backup gaps exceeding RPO, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Session persistence cluster is a two-step reliability decision. At stage 1, \"The design for session persistence cluster is mismatched to backup gaps exceeding RPO, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around backup gaps exceeding RPO.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for session persistence cluster: signal points to backup gaps\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for session persistence cluster:\", which next step is strongest under current constraints?",
          "options": [
            "Use immutable backup storage with integrity manifests and checksum validation.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use immutable backup storage with integrity manifests and checksum validation\" best matches For \"incident diagnosis for session persistence cluster:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "If you keep \"data Safety, Durability & Recovery\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature flag history store: signal points to restore procedure untested at scale. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for feature flag history store is mismatched to restore procedure untested at scale, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Data Safety, Durability & Recovery, the best answer is \"The design for feature flag history store is mismatched to restore procedure untested at scale, creating repeat reliability incidents\". It is the option most directly aligned to restore procedure untested at scale while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for feature flag history store: signal points to restore procedure\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for feature flag history store:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Separate durability acknowledgements from in-memory replication signals.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "With root cause identified for \"incident diagnosis for feature flag history store:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Separate durability acknowledgements from in-memory replication signals\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Start from \"data Safety, Durability & Recovery\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for support ticket history: signal points to snapshot corruption discovered late. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for support ticket history is mismatched to snapshot corruption discovered late, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Data Safety, Durability & Recovery: for support ticket history, \"The design for support ticket history is mismatched to snapshot corruption discovered late, creating repeat reliability incidents\" is correct because it addresses snapshot corruption discovered late and improves controllability.",
          "detailedExplanation": "If you keep \"incident diagnosis for support ticket history: signal points to snapshot corruption\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for support ticket history: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Implement point-in-time recovery and test rollback to known incident timestamps.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Data Safety, Durability & Recovery, the best answer is \"Implement point-in-time recovery and test rollback to known incident timestamps\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for transaction ledger database: signal points to replica acknowledged before durable write. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for transaction ledger database is mismatched to replica acknowledged before durable write, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for transaction ledger database is mismatched to replica acknowledged before durable write, creating repeat reliability incidents\" best matches transaction ledger database by targeting replica acknowledged before durable write and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for transaction ledger database: signal points to replica\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for transaction ledger database:\" scenario, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Protect backup credentials and keys with isolated recovery procedures."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Data Safety, Durability & Recovery: for In the \"incident diagnosis for transaction ledger database:\" scenario, which immediate adjustment best addresses the risk, \"Protect backup credentials and keys with isolated recovery procedures\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for customer profile store: signal points to point-in-time recovery window misconfigured. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for customer profile store is mismatched to point-in-time recovery window misconfigured, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Customer profile store is a two-step reliability decision. At stage 1, \"The design for customer profile store is mismatched to point-in-time recovery window misconfigured, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around point-in-time recovery window misconfigured.",
          "detailedExplanation": "Use \"incident diagnosis for customer profile store: signal points to point-in-time recovery\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for customer profile store: signal\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Continuously audit backup completeness and alert on coverage gaps.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Continuously audit backup completeness and alert on coverage gaps\" best matches Now that \"incident diagnosis for customer profile store: signal\" is diagnosed, which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The decision turns on \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for event stream archive: signal points to checksum verification skipped in restore. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for event stream archive is mismatched to checksum verification skipped in restore, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Data Safety, Durability & Recovery, the best answer is \"The design for event stream archive is mismatched to checksum verification skipped in restore, creating repeat reliability incidents\". It is the option most directly aligned to checksum verification skipped in restore while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"incident diagnosis for event stream archive: signal points to checksum verification\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for event stream archive: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Retain forensic logs long enough to support incident reconstruction.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Using the diagnosis from \"incident diagnosis for event stream archive: signal\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Retain forensic logs long enough to support incident reconstruction\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "This prompt is really about \"data Safety, Durability & Recovery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for object metadata catalog: signal points to encryption key loss blocks recovery. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for object metadata catalog is mismatched to encryption key loss blocks recovery, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Data Safety, Durability & Recovery: for object metadata catalog, \"The design for object metadata catalog is mismatched to encryption key loss blocks recovery, creating repeat reliability incidents\" is correct because it addresses encryption key loss blocks recovery and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for object metadata catalog: signal points to encryption key loss\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for object metadata catalog: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Use staged recovery with read-only validation before write re-enable.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Data Safety, Durability & Recovery, the best answer is \"Use staged recovery with read-only validation before write re-enable\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"data Safety, Durability & Recovery\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for multi-tenant billing records: signal points to cross-region backup drift. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for multi-tenant billing records is mismatched to cross-region backup drift, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Multi-tenant billing records is a two-step reliability decision. At stage 1, \"The design for multi-tenant billing records is mismatched to cross-region backup drift, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around cross-region backup drift.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for multi-tenant billing records: signal points to cross-region\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for multi-tenant billing records:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Track data-loss near misses and close process gaps before next incident."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Track data-loss near misses and close process gaps before next incident\" best matches For \"incident diagnosis for multi-tenant billing records:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "If you keep \"data Safety, Durability & Recovery\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search index snapshots: signal points to logical delete propagated without guard. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for search index snapshots is mismatched to logical delete propagated without guard, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Data Safety, Durability & Recovery, the best answer is \"The design for search index snapshots is mismatched to logical delete propagated without guard, creating repeat reliability incidents\". It is the option most directly aligned to logical delete propagated without guard while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for search index snapshots: signal points to logical delete\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for search index snapshots: signal\", which next change should be prioritized first?",
          "options": [
            "Define explicit RPO/RTO per data class and align backup cadence to each tier.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Given the diagnosis in \"incident diagnosis for search index snapshots: signal\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Define explicit RPO/RTO per data class and align backup cadence to each tier\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"data Safety, Durability & Recovery\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for audit trail warehouse: signal points to retention policy removes needed history. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for audit trail warehouse is mismatched to retention policy removes needed history, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Data Safety, Durability & Recovery: for audit trail warehouse, \"The design for audit trail warehouse is mismatched to retention policy removes needed history, creating repeat reliability incidents\" is correct because it addresses retention policy removes needed history and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for audit trail warehouse: signal points to retention policy removes\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for audit trail warehouse: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Run regular restore drills with production-like volume and verification checks.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Data Safety, Durability & Recovery, the best answer is \"Run regular restore drills with production-like volume and verification checks\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"data Safety, Durability & Recovery\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session persistence cluster: signal points to backup gaps exceeding RPO. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for session persistence cluster is mismatched to backup gaps exceeding RPO, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for session persistence cluster is mismatched to backup gaps exceeding RPO, creating repeat reliability incidents\" best matches session persistence cluster by targeting backup gaps exceeding RPO and lowering repeat risk.",
          "detailedExplanation": "Start from \"incident diagnosis for session persistence cluster: signal points to backup gaps\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for session persistence cluster:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Use immutable backup storage with integrity manifests and checksum validation.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Data Safety, Durability & Recovery: for Using the diagnosis from \"incident diagnosis for session persistence cluster:\", which immediate adjustment best addresses the risk, \"Use immutable backup storage with integrity manifests and checksum validation\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "This prompt is really about \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature flag history store: signal points to restore procedure untested at scale. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for feature flag history store is mismatched to restore procedure untested at scale, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Feature flag history store is a two-step reliability decision. At stage 1, \"The design for feature flag history store is mismatched to restore procedure untested at scale, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around restore procedure untested at scale.",
          "detailedExplanation": "Use \"incident diagnosis for feature flag history store: signal points to restore procedure\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for feature flag history store:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Separate durability acknowledgements from in-memory replication signals."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Separate durability acknowledgements from in-memory replication signals\" best matches Now that \"incident diagnosis for feature flag history store:\" is diagnosed, which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for support ticket history: signal points to snapshot corruption discovered late. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for support ticket history is mismatched to snapshot corruption discovered late, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Data Safety, Durability & Recovery, the best answer is \"The design for support ticket history is mismatched to snapshot corruption discovered late, creating repeat reliability incidents\". It is the option most directly aligned to snapshot corruption discovered late while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for support ticket history: signal points to snapshot corruption\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for support ticket history: signal\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Implement point-in-time recovery and test rollback to known incident timestamps.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Now that \"incident diagnosis for support ticket history: signal\" is diagnosed, which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Implement point-in-time recovery and test rollback to known incident timestamps\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for transaction ledger database: signal points to replica acknowledged before durable write. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for transaction ledger database is mismatched to replica acknowledged before durable write, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Data Safety, Durability & Recovery: for transaction ledger database, \"The design for transaction ledger database is mismatched to replica acknowledged before durable write, creating repeat reliability incidents\" is correct because it addresses replica acknowledged before durable write and improves controllability.",
          "detailedExplanation": "If you keep \"incident diagnosis for transaction ledger database: signal points to replica\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for transaction ledger database:\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Protect backup credentials and keys with isolated recovery procedures.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Data Safety, Durability & Recovery, the best answer is \"Protect backup credentials and keys with isolated recovery procedures\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for customer profile store: signal points to point-in-time recovery window misconfigured. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for customer profile store is mismatched to point-in-time recovery window misconfigured, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for customer profile store is mismatched to point-in-time recovery window misconfigured, creating repeat reliability incidents\" best matches customer profile store by targeting point-in-time recovery window misconfigured and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for customer profile store: signal points to point-in-time recovery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for customer profile store: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Continuously audit backup completeness and alert on coverage gaps.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Data Safety, Durability & Recovery: for After diagnosing \"incident diagnosis for customer profile store: signal\", what first move gives the best reliability impact, \"Continuously audit backup completeness and alert on coverage gaps\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"data Safety, Durability & Recovery\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for event stream archive: signal points to checksum verification skipped in restore. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for event stream archive is mismatched to checksum verification skipped in restore, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Event stream archive is a two-step reliability decision. At stage 1, \"The design for event stream archive is mismatched to checksum verification skipped in restore, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around checksum verification skipped in restore.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for event stream archive: signal points to checksum verification\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for event stream archive: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Retain forensic logs long enough to support incident reconstruction."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Retain forensic logs long enough to support incident reconstruction\" best matches With root cause identified for \"incident diagnosis for event stream archive: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"data Safety, Durability & Recovery\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for object metadata catalog: signal points to encryption key loss blocks recovery. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for object metadata catalog is mismatched to encryption key loss blocks recovery, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Data Safety, Durability & Recovery, the best answer is \"The design for object metadata catalog is mismatched to encryption key loss blocks recovery, creating repeat reliability incidents\". It is the option most directly aligned to encryption key loss blocks recovery while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for object metadata catalog: signal points to encryption key loss\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for object metadata catalog: signal\", what first move gives the best reliability impact?",
          "options": [
            "Use staged recovery with read-only validation before write re-enable.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For \"incident diagnosis for object metadata catalog: signal\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Use staged recovery with read-only validation before write re-enable\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"data Safety, Durability & Recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for multi-tenant billing records: signal points to cross-region backup drift. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for multi-tenant billing records is mismatched to cross-region backup drift, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for multi-tenant billing records is mismatched to cross-region backup drift, creating repeat reliability incidents\" best matches multi-tenant billing records by targeting cross-region backup drift and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for multi-tenant billing records: signal points to cross-region\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for multi-tenant billing records:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Track data-loss near misses and close process gaps before next incident.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Data Safety, Durability & Recovery: for For \"incident diagnosis for multi-tenant billing records:\", what is the highest-leverage change to make now, \"Track data-loss near misses and close process gaps before next incident\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The core signal here is \"data Safety, Durability & Recovery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-061",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which indicators most directly reveal cross-domain blast radius.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Data Safety, Durability & Recovery, Select all options that correctly address this: which indicators most directly reveal cross-domain blast radius needs layered controls, not one silver bullet. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "If you keep \"indicators most directly reveal cross-domain blast radius? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-062",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which controls reduce hidden single points of failure.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Select all options that correctly address this: which controls reduce hidden single points of failure is intentionally multi-dimensional in Data Safety, Durability & Recovery. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "This prompt is really about \"controls reduce hidden single points of failure? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-063",
      "type": "multi-select",
      "question": "Select all options that correctly address this: during partial failures, which practices improve diagnosis quality.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Data Safety, Durability & Recovery: The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Use \"during partial failures, which practices improve diagnosis quality? (Select all that\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-064",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what belongs in a useful dependency failure taxonomy.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Select all options that correctly address this: what belongs in a useful dependency failure taxonomy, the highest-signal answer is a bundle of controls. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Read this as a scenario about \"belongs in a useful dependency failure taxonomy? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-065",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which patterns limit correlated failures across zones.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Data Safety, Durability & Recovery, Select all options that correctly address this: which patterns limit correlated failures across zones needs layered controls, not one silver bullet. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The decision turns on \"patterns limit correlated failures across zones? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-066",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which runbook elements increase incident execution reliability.",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Select all options that correctly address this: which runbook elements increase incident execution reliability is intentionally multi-dimensional in Data Safety, Durability & Recovery. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Start from \"runbook elements increase incident execution reliability? (Select all that apply)\", then pressure-test the result against the options. Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-067",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which signals should trigger graceful isolation first.",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Data Safety, Durability & Recovery: The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The key clue in this question is \"signals should trigger graceful isolation first? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-068",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which architectural choices help contain tenant-induced overload.",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Select all options that correctly address this: which architectural choices help contain tenant-induced overload, the highest-signal answer is a bundle of controls. The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The core signal here is \"architectural choices help contain tenant-induced overload? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-069",
      "type": "multi-select",
      "question": "Select all options that correctly address this: for reliability policies, which items should be explicit per endpoint.",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Data Safety, Durability & Recovery, Select all options that correctly address this: for reliability policies, which items should be explicit per endpoint needs layered controls, not one silver bullet. The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "If you keep \"for reliability policies, which items should be explicit per endpoint? (Select all that\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "rel-dr-070",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which anti-patterns commonly enlarge outage blast radius.",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Data Safety, Durability & Recovery: The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The key clue in this question is \"anti-patterns commonly enlarge outage blast radius? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-071",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what improves confidence in failover assumptions.",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Select all options that correctly address this: what improves confidence in failover assumptions, the highest-signal answer is a bundle of controls. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Start from \"improves confidence in failover assumptions? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-072",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which data is essential when classifying partial vs fail-stop incidents.",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Data Safety, Durability & Recovery, Select all options that correctly address this: which data is essential when classifying partial vs fail-stop incidents needs layered controls, not one silver bullet. The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The decision turns on \"data is essential when classifying partial vs fail-stop incidents? (Select all that\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-073",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which controls improve safety when control-plane health is uncertain.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: which controls improve safety when control-plane health is uncertain is intentionally multi-dimensional in Data Safety, Durability & Recovery. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Read this as a scenario about \"controls improve safety when control-plane health is uncertain? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-074",
      "type": "multi-select",
      "question": "Select all options that correctly address this: for critical writes, which guardrails reduce corruption risk under faults.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Data Safety, Durability & Recovery: The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Use \"for critical writes, which guardrails reduce corruption risk under faults? (Select all\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-075",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which recurring reviews keep reliability boundaries accurate over time.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Select all options that correctly address this: which recurring reviews keep reliability boundaries accurate over time, the highest-signal answer is a bundle of controls. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "This prompt is really about \"recurring reviews keep reliability boundaries accurate over time? (Select all that\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-076",
      "type": "multi-select",
      "question": "Select all options that correctly address this: which decisions help teams align on reliability trade-offs during incidents.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Data Safety, Durability & Recovery, Select all options that correctly address this: which decisions help teams align on reliability trade-offs during incidents needs layered controls, not one silver bullet. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "If you keep \"decisions help teams align on reliability trade-offs during incidents? (Select all that\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-077",
      "type": "multi-select",
      "question": "Select all options that correctly address this: what evidence best shows a mitigation reduced recurrence risk.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all options that correctly address this: what evidence best shows a mitigation reduced recurrence risk is intentionally multi-dimensional in Data Safety, Durability & Recovery. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The core signal here is \"evidence best shows a mitigation reduced recurrence risk? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-078",
      "type": "numeric-input",
      "question": "A service processes 4,200,000 requests/day and 0.22% violate reliability SLO. Compute how many violations/day.",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for A service processes 4,200,000 requests/day and 0 gives 9240 requests. In interview pacing, hitting this value within +/-3% is the pass condition.",
      "detailedExplanation": "The key clue in this question is \"service processes 4,200,000 requests/day and 0\". Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 4,200 and 000 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-079",
      "type": "numeric-input",
      "question": "Incident queue receives 1,800 items/min and drains 2,050 items/min. Compute net drain rate.",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Incident queue receives 1,800 items/min and drains 2,050 items/min: 250 items/min. Answers within +/-0% show correct directional reasoning for Data Safety, Durability & Recovery.",
      "detailedExplanation": "Start from \"incident queue receives 1,800 items/min and drains 2,050 items/min\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 1,800 and 2,050 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-dr-080",
      "type": "numeric-input",
      "question": "Retry policy adds 0.35 extra attempts per request at 60,000 req/sec. Compute effective attempts/sec.",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Data Safety, Durability & Recovery expects quick quantitative triage: Retry policy adds 0 evaluates to 81000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "Start from \"retry policy adds 0\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 0.35 and 60,000 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-081",
      "type": "numeric-input",
      "question": "Failover takes 18 seconds and happens 21 times/day. Compute total failover seconds/day.",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for Failover takes 18 seconds and happens 21 times/day gives 378 seconds. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "The key clue in this question is \"failover takes 18 seconds and happens 21 times/day\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 18 seconds and 21 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-082",
      "type": "numeric-input",
      "question": "Target p99 latency is 700ms; observed p99 is 980ms. Compute percent over target.",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Target p99 latency is 700ms; observed p99 is 980ms: 40 %. Answers within +/-30% show correct directional reasoning for Data Safety, Durability & Recovery.",
      "detailedExplanation": "Read this as a scenario about \"target p99 latency is 700ms\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 700ms and 980ms should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-083",
      "type": "numeric-input",
      "question": "Which answer best fits: if 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For Which answer best fits: if 31% of 120,000 requests/min are critical-path, how many critical requests/min, the computed target in Data Safety, Durability & Recovery is 37200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "The decision turns on \"if 31% of 120,000 requests/min are critical-path, how many critical requests/min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 31 and 120,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.2% to 0.3%. Compute percent reduction.",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Data Safety, Durability & Recovery expects quick quantitative triage: Error rate drops from 1 evaluates to 75 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "This prompt is really about \"error rate drops from 1\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 1.2 and 0.3 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-085",
      "type": "numeric-input",
      "question": "A 7-node quorum system requires majority writes. Compute minimum acknowledgements required.",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for A 7-node quorum system requires majority writes gives 4 acks. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Use \"7-node quorum system requires majority writes\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 7 and 4 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-086",
      "type": "numeric-input",
      "question": "Backlog is 48,000 tasks and net drain is 320 tasks/min. Compute minutes to clear backlog.",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Backlog is 48,000 tasks and net drain is 320 tasks/min: 150 minutes. Answers within +/-0% show correct directional reasoning for Data Safety, Durability & Recovery.",
      "detailedExplanation": "The core signal here is \"backlog is 48,000 tasks and net drain is 320 tasks/min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 48,000 and 320 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-087",
      "type": "numeric-input",
      "question": "A system with 14 zones has 2 unavailable. Compute what percent remain available.",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For A system with 14 zones has 2 unavailable, the computed target in Data Safety, Durability & Recovery is 85.71 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "If you keep \"system with 14 zones has 2 unavailable\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 14 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-088",
      "type": "numeric-input",
      "question": "MTTR improved from 45 min to 30 min. Compute percent reduction.",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Data Safety, Durability & Recovery expects quick quantitative triage: MTTR improved from 45 min to 30 min evaluates to 33.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "Start from \"mTTR improved from 45 min to 30 min\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 45 min and 30 min should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-089",
      "type": "numeric-input",
      "question": "Which answer best fits: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for Which answer best fits: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day gives 225000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "The key clue in this question is \"if 9% of 2,500,000 daily operations need manual recovery checks, checks/day\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 9 and 2,500 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle. (data safety, durability & recovery lens)",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Data Safety, Durability & Recovery should start with Detect and scope affected fault domains and end with Validate recovery and harden recurrence defenses. Order a reliability response lifecycle rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The decision turns on \"order a reliability response lifecycle\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-091",
      "type": "ordering",
      "question": "Order from lowest to highest reliability risk. Use a data safety, durability & recovery perspective.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Isolated dependency with fallback and budget must happen before Implicit dependency with no failure policy. That ordering matches incident-safe flow in Data Safety, Durability & Recovery.",
      "detailedExplanation": "Read this as a scenario about \"order from lowest to highest reliability risk\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-092",
      "type": "ordering",
      "question": "Order failover safety steps. Focus on data safety, durability & recovery tradeoffs.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Data Safety, Durability & Recovery emphasizes safe recovery order. Beginning at Verify candidate health and freshness and finishing at Run failback readiness checks before restoration keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The key clue in this question is \"order failover safety steps\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-093",
      "type": "ordering",
      "question": "For data safety, durability & recovery, order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For data safety, durability & recovery, order by increasing overload-protection strength, the correct ordering runs from No admission limits to Priority-aware shedding plus per-domain concurrency bounds. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Start from \"order by increasing overload-protection strength\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-094",
      "type": "ordering",
      "question": "Within data safety, durability & recovery, order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Data Safety, Durability & Recovery should start with Select recovery point by RPO target and end with Promote and re-enable writes with monitoring. Within data safety, durability & recovery, order data recovery execution rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "If you keep \"order data recovery execution\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-095",
      "type": "ordering",
      "question": "In this data safety, durability & recovery context, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Define SLIs tied to user impact must happen before Review incidents and close corrective actions. That ordering matches incident-safe flow in Data Safety, Durability & Recovery.",
      "detailedExplanation": "The core signal here is \"order reliability operations loop\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-096",
      "type": "ordering",
      "question": "Sequence these from minimal to maximal blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Data Safety, Durability & Recovery emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Use \"order by increasing blast radius\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-097",
      "type": "ordering",
      "question": "From a data safety, durability & recovery viewpoint, order retry-policy maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a data safety, durability & recovery viewpoint, order retry-policy maturity, the correct ordering runs from Fixed immediate retries to Jittered backoff with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "This prompt is really about \"order retry-policy maturity\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-098",
      "type": "ordering",
      "question": "Order degradation sophistication. (data safety, durability & recovery lens)",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Data Safety, Durability & Recovery should start with Undocumented ad hoc fallback and end with Automated policy-driven degradation with user semantics. Order degradation sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The decision turns on \"order degradation sophistication\". Order by relative scale and bottleneck effect, then validate neighboring items. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-dr-099",
      "type": "ordering",
      "question": "Order incident command rigor. Use a data safety, durability & recovery perspective.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc responders with no roles must happen before Role-defined operations plus decision log and action tracking. That ordering matches incident-safe flow in Data Safety, Durability & Recovery.",
      "detailedExplanation": "Read this as a scenario about \"order incident command rigor\". Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-dr-100",
      "type": "ordering",
      "question": "Order reliability validation confidence. Focus on data safety, durability & recovery tradeoffs.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Data Safety, Durability & Recovery emphasizes safe recovery order. Beginning at Single success in staging and finishing at Sustained recovery plus recurrence drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The key clue in this question is \"order reliability validation confidence\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    }
  ]
}
