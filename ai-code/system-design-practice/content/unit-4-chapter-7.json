{
  "id": "unit-4-chapter-7",
  "chapterTitle": "Hybrid Architectures",
  "problems": [
    {
      "id": "u4c7-001",
      "type": "multiple-choice",
      "question": "Your e-commerce platform stores product catalog in PostgreSQL and product search in Elasticsearch. What's the primary challenge with this hybrid approach?",
      "options": [
        "Search queries are slower than SQL queries",
        "Data consistency between the two systems",
        "PostgreSQL cannot handle write loads",
        "Elasticsearch cannot scale horizontally"
      ],
      "correct": 1,
      "explanation": "The main challenge is maintaining data consistency when the same data lives in multiple stores. Changes in PostgreSQL must be synced to Elasticsearch, introducing lag and potential inconsistencies.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-002",
      "type": "multiple-choice",
      "question": "You use Redis as a cache in front of PostgreSQL. When should you invalidate the cache?",
      "options": [
        "Only when TTL expires",
        "On every read",
        "On writes to the underlying data",
        "Never—caches are eventually consistent"
      ],
      "correct": 2,
      "explanation": "Cache invalidation should happen on writes to maintain consistency. TTL provides eventual consistency but can serve stale data. Active invalidation on writes ensures readers get current data (cache-aside pattern).",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-003",
      "type": "multiple-choice",
      "question": "Your system uses PostgreSQL for transactions and Cassandra for time-series data. What sync pattern keeps both consistent?",
      "options": [
        "Two-phase commit across both databases",
        "Write to both in application code",
        "Event sourcing with CDC from PostgreSQL",
        "Periodic batch sync jobs"
      ],
      "correct": 2,
      "explanation": "Change Data Capture (CDC) streams changes from PostgreSQL to Cassandra via events. Two-phase commit across different database types is impractical. Application-level dual writes risk partial failures. CDC provides reliable, eventually consistent replication.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-004",
      "type": "multi-select",
      "question": "Which patterns help maintain consistency in polyglot persistence architectures?",
      "options": [
        "Change Data Capture (CDC)",
        "Saga pattern",
        "Outbox pattern",
        "Distributed locks"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All help with consistency: CDC streams changes between stores, Sagas coordinate multi-store transactions with compensations, Outbox ensures atomic event publishing with writes, and distributed locks prevent concurrent modifications across systems.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-005",
      "type": "multiple-choice",
      "question": "You're designing a social media platform. User profiles are in PostgreSQL, friend graphs in Neo4j, and posts in MongoDB. What coordinates a 'delete my account' request?",
      "options": [
        "Three separate DELETE calls from the client",
        "A microservice orchestrating the deletions",
        "Database triggers cascading across stores",
        "A distributed transaction across all three"
      ],
      "correct": 1,
      "explanation": "An orchestration service (or saga) coordinates deletions across the three stores. Triggers can't cascade across different databases. Distributed transactions across heterogeneous stores are impractical. Client coordination is unreliable.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-006",
      "type": "ordering",
      "question": "Order the steps in a CDC-based sync from PostgreSQL to Elasticsearch:",
      "items": [
        "Debezium captures PostgreSQL WAL changes",
        "Kafka consumer processes the events",
        "User updates a row in PostgreSQL",
        "Elasticsearch index is updated"
      ],
      "correctOrder": [2, 0, 1, 3],
      "explanation": "First the user writes to PostgreSQL. Debezium reads the write-ahead log (WAL), produces events to Kafka, a consumer processes those events, and finally updates Elasticsearch.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-007",
      "type": "multiple-choice",
      "question": "Your CQRS system writes to PostgreSQL (command side) and reads from Elasticsearch (query side). A user creates a post and immediately searches for it but doesn't find it. What's happening?",
      "options": [
        "Elasticsearch indexing is disabled",
        "Replication lag between command and query stores",
        "The search query has a typo",
        "PostgreSQL transaction hasn't committed"
      ],
      "correct": 1,
      "explanation": "This is read-after-write inconsistency due to replication lag. The write completed in PostgreSQL but hasn't propagated to Elasticsearch yet. This is expected in eventually consistent CQRS systems.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-008",
      "type": "multiple-choice",
      "question": "How can you solve the read-your-writes problem in a CQRS system?",
      "options": [
        "Always read from the command store",
        "Include a version/timestamp and wait or fall back",
        "Disable caching entirely",
        "Use synchronous replication to query store"
      ],
      "correct": 1,
      "explanation": "Pass the write version to the query. If the query store hasn't reached that version, either wait briefly or fall back to the command store for that request. Always reading from command defeats CQRS benefits.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-009",
      "type": "multiple-choice",
      "question": "You use MongoDB for operational data and S3 + Athena for analytics. How do you move data from MongoDB to S3 efficiently?",
      "options": [
        "Direct MongoDB-to-S3 API calls",
        "Real-time CDC streaming every document",
        "Periodic batch exports to S3 in Parquet format",
        "Query MongoDB from Athena directly"
      ],
      "correct": 2,
      "explanation": "Batch exports to columnar formats like Parquet are efficient for analytics. Real-time CDC for analytics is overkill and expensive. Athena can't query MongoDB directly. Parquet enables efficient analytical queries.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-010",
      "type": "multiple-choice",
      "question": "Your hybrid architecture uses Redis for sessions, PostgreSQL for user data, and S3 for uploads. A user logs in. Which stores are read from?",
      "options": [
        "Redis only",
        "PostgreSQL only",
        "Redis and PostgreSQL",
        "All three stores"
      ],
      "correct": 2,
      "explanation": "Login reads PostgreSQL for credential verification, then creates/updates a session in Redis. S3 isn't involved in authentication—it stores file uploads, not login data.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-011",
      "type": "multiple-choice",
      "question": "What's the main advantage of using a data lake architecture with S3 + Spark over keeping all data in a data warehouse?",
      "options": [
        "Lower query latency",
        "ACID transaction support",
        "Cost-effective storage of diverse data types",
        "Simpler query syntax"
      ],
      "correct": 2,
      "explanation": "Data lakes excel at storing diverse data (structured, semi-structured, unstructured) cost-effectively. Data warehouses typically have higher storage costs and prefer structured data. Lakes trade query latency for flexibility and cost.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-012",
      "type": "multiple-choice",
      "question": "You're implementing a Lambda architecture with Kafka, Spark batch, and Spark Streaming. What's the serving layer's responsibility?",
      "options": [
        "Processing raw events",
        "Running batch computations",
        "Merging batch and speed layer views for queries",
        "Storing raw event streams"
      ],
      "correct": 2,
      "explanation": "The serving layer merges precomputed batch views with real-time speed layer updates to answer queries. Batch processing happens in the batch layer; stream processing in the speed layer.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "u4c7-013",
      "type": "multiple-choice",
      "question": "Compared to Lambda architecture, what does Kappa architecture eliminate?",
      "options": [
        "The serving layer",
        "The batch layer",
        "The speed layer",
        "Event streaming"
      ],
      "correct": 1,
      "explanation": "Kappa architecture eliminates the batch layer, treating all data as streams. Historical reprocessing replays events through the stream processor. This simplifies the architecture by having a single code path.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-014",
      "type": "multi-select",
      "question": "Which are valid reasons to use multiple databases in a single application?",
      "options": [
        "Different access patterns require different optimizations",
        "Team preference for specific technologies",
        "Regulatory requirements for data residency",
        "Scaling different workloads independently"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Different access patterns (OLTP vs OLAP, graph queries vs full-text search), regulatory requirements, and independent scaling are valid reasons. Team preference alone isn't sufficient justification for the complexity of polyglot persistence.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-015",
      "type": "multiple-choice",
      "question": "Your system writes to PostgreSQL with an outbox table. What reads the outbox?",
      "options": [
        "Application code on each request",
        "A CDC connector like Debezium",
        "PostgreSQL triggers",
        "The client application"
      ],
      "correct": 1,
      "explanation": "A CDC connector (like Debezium) tails the outbox table and publishes events to Kafka. This decouples event publishing from the transaction. Polling from application code works but CDC is more efficient and reliable.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-016",
      "type": "multiple-choice",
      "question": "You're choosing between Redis and Memcached as a caching layer in front of your database. What does Redis offer that Memcached doesn't?",
      "options": [
        "Faster get/set operations",
        "Data structures (lists, sets, sorted sets)",
        "Distributed caching",
        "Key-value storage"
      ],
      "correct": 1,
      "explanation": "Redis supports rich data structures (lists, sets, sorted sets, hashes, streams) beyond simple key-value. Both support distributed caching and have similar performance for basic operations. Redis is more feature-rich.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-017",
      "type": "multiple-choice",
      "question": "Your application uses PostgreSQL for transactions and ClickHouse for analytics. A dashboard query joins user metadata with click events. How should you handle this?",
      "options": [
        "Execute a cross-database join",
        "Replicate user metadata to ClickHouse",
        "Run analytics queries in PostgreSQL",
        "Query both and join in application code"
      ],
      "correct": 1,
      "explanation": "Replicate the user metadata to ClickHouse so analytical queries stay in one system. Cross-database joins don't exist. PostgreSQL isn't optimized for analytics. Application-level joins are inefficient for large datasets.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-018",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to maintain a materialized view in Elasticsearch that's derived from PostgreSQL data. What architectural pattern fits?",
          "options": [
            "Direct replication",
            "CDC with event processing",
            "Periodic full sync",
            "Two-phase commit"
          ],
          "correct": 1,
          "explanation": "CDC (Change Data Capture) captures changes from PostgreSQL and processes them into the materialized view in Elasticsearch. This provides near real-time updates without coupling the systems.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "Your CDC pipeline uses Kafka. The Elasticsearch consumer crashes and restarts. How does it know where to resume?",
          "options": [
            "Starts from the beginning",
            "Consumer offset stored in Kafka",
            "Queries Elasticsearch for last document",
            "Administrator manually specifies offset"
          ],
          "correct": 1,
          "explanation": "Kafka tracks consumer offsets—each consumer group's position in each partition. On restart, the consumer resumes from its last committed offset, ensuring no events are missed or reprocessed (assuming at-least-once semantics).",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Storage estimates are most useful when split into raw data, replication factor, metadata/index overhead, and retention horizon.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-019",
      "type": "multiple-choice",
      "question": "What's the purpose of a 'staging' or 'landing' zone in a data lake architecture?",
      "options": [
        "To serve queries to end users",
        "To store raw, unprocessed data before transformation",
        "To cache frequently accessed data",
        "To run real-time analytics"
      ],
      "correct": 1,
      "explanation": "The staging/landing zone holds raw data in its original form before any transformation. This preserves the original data for reprocessing and auditing. Transformed data moves to curated zones.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-020",
      "type": "multiple-choice",
      "question": "You use DynamoDB for user sessions and Aurora PostgreSQL for user profiles. A request needs both session state and profile data. How do you minimize latency?",
      "options": [
        "Sequential calls to each database",
        "Parallel calls to both databases",
        "Cache both in Redis",
        "Replicate profiles to DynamoDB"
      ],
      "correct": 1,
      "explanation": "Issue parallel requests to both stores since they're independent reads. This overlaps latency. Caching adds another layer of complexity. Replication duplicates data across systems. Parallel reads are the simplest optimization.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-021",
      "type": "multiple-choice",
      "question": "Your system uses an event-driven architecture with Kafka. Services write to their own databases and publish events. What ensures a service's database write and event publish are atomic?",
      "options": [
        "Kafka transactions",
        "Database triggers",
        "The outbox pattern",
        "Two-phase commit"
      ],
      "correct": 2,
      "explanation": "The outbox pattern writes the event to an outbox table in the same database transaction as the data change. A separate process (CDC or polling) publishes events from the outbox to Kafka. This ensures atomicity.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-022",
      "type": "multiple-choice",
      "question": "In a microservices architecture with polyglot persistence, Service A uses PostgreSQL and Service B uses MongoDB. Service A needs data from Service B. What's the preferred approach?",
      "options": [
        "Service A queries Service B's MongoDB directly",
        "Service A calls Service B's API",
        "Replicate Service B's data to PostgreSQL",
        "Use a shared database between services"
      ],
      "correct": 1,
      "explanation": "Services should own their data and expose it through APIs. Direct database access couples services and bypasses business logic. API calls maintain service boundaries and encapsulation.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-023",
      "type": "multi-select",
      "question": "Which tools are commonly used for CDC (Change Data Capture) from relational databases?",
      "options": ["Debezium", "AWS DMS", "Kafka Connect", "Redis Streams"],
      "correctIndices": [0, 1, 2],
      "explanation": "Debezium, AWS DMS, and Kafka Connect with appropriate connectors all support CDC from relational databases. Redis Streams is a message queue, not a CDC tool—it doesn't capture database changes.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-024",
      "type": "multiple-choice",
      "question": "You're implementing write-through caching with Redis and PostgreSQL. What happens on a write?",
      "options": [
        "Write to Redis, async sync to PostgreSQL",
        "Write to PostgreSQL, then update Redis",
        "Write to both simultaneously in a transaction",
        "Write to PostgreSQL, invalidate Redis"
      ],
      "correct": 1,
      "explanation": "Write-through caching writes to the primary store (PostgreSQL) first, then updates the cache (Redis). This ensures the cache always reflects the database. Write-behind (option A) risks data loss; invalidation is cache-aside.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-025",
      "type": "multiple-choice",
      "question": "Your analytics system uses a data lakehouse (Delta Lake on S3). What does the lakehouse add over a plain data lake?",
      "options": [
        "Object storage capabilities",
        "ACID transactions and schema enforcement",
        "Lower storage costs",
        "Faster S3 API operations"
      ],
      "correct": 1,
      "explanation": "Lakehouse formats like Delta Lake add ACID transactions, schema enforcement, and time travel on top of object storage. Plain data lakes lack these guarantees, making updates and consistency harder.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-026",
      "type": "multiple-choice",
      "question": "You have a hot tier (SSD-backed) and cold tier (S3) for time-series data. What determines when data moves between tiers?",
      "options": [
        "Manual operator commands",
        "Data age policies",
        "Query frequency analysis",
        "Storage capacity alerts"
      ],
      "correct": 1,
      "explanation": "Time-series systems typically use age-based policies: data older than X days moves to the cold tier. This is predictable and matches access patterns—recent data is queried more often. Query frequency can enhance but age is primary.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-027",
      "type": "ordering",
      "question": "Order the tiers in a typical data warehouse tiered storage from fastest to cheapest:",
      "items": [
        "Archive (Glacier)",
        "Hot (SSD)",
        "Cold (S3 Standard)",
        "Warm (S3 IA)"
      ],
      "correctOrder": [1, 3, 2, 0],
      "explanation": "From fastest to cheapest: Hot (SSD) → Warm (S3 Infrequent Access) → Cold (S3 Standard) → Archive (Glacier). Hot is fastest but most expensive; Glacier is cheapest but has retrieval delays.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-028",
      "type": "multiple-choice",
      "question": "Your system caches database query results in Redis with a 5-minute TTL. A write occurs at minute 2. When does the cache reflect the change?",
      "options": [
        "Immediately",
        "At minute 5",
        "At minute 7",
        "Never, until explicit invalidation"
      ],
      "correct": 2,
      "explanation": "With TTL-based caching and no active invalidation, the stale data remains until the TTL expires. The cache was set at minute 0, expires at minute 5, so the next read after minute 5 fetches fresh data. That's up to minute 7 (5-minute TTL from minute 2 write doesn't apply—TTL was set at cache time).",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-029",
      "type": "multiple-choice",
      "question": "You're building a search-as-you-type feature. The data is in PostgreSQL. What hybrid approach provides sub-100ms autocomplete?",
      "options": [
        "PostgreSQL LIKE queries with indexes",
        "Full-text search in PostgreSQL",
        "Elasticsearch with prefix queries",
        "Redis sorted sets with score-based ranking"
      ],
      "correct": 2,
      "explanation": "Elasticsearch with prefix/edge-ngram queries provides fast autocomplete. PostgreSQL LIKE isn't optimized for prefix matching at scale. Redis sorted sets work for simple cases but lack full-text relevance. Elasticsearch is purpose-built for search.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-030",
      "type": "multi-select",
      "question": "Which are challenges when running both OLTP and OLAP workloads on the same database?",
      "options": [
        "Analytical queries consuming resources needed for transactions",
        "Different indexing strategies for each workload",
        "Lock contention between reads and writes",
        "Schema optimized for one pattern hurts the other"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are challenges: long analytical queries compete with transactions for CPU/memory/IO, OLTP wants B-tree indexes while OLAP prefers columnar storage, locks from analytical scans can block writes, and normalized schemas hurt analytical queries while denormalized schemas hurt transactions.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "u4c7-031",
      "type": "multiple-choice",
      "question": "Your e-commerce site uses PostgreSQL for orders and Redis for inventory counts. A purchase must decrement inventory and create an order atomically. What pattern helps?",
      "options": [
        "Distributed transaction with XA",
        "Saga with compensating transactions",
        "Write to both from the same thread",
        "Eventual consistency with retries"
      ],
      "correct": 1,
      "explanation": "The Saga pattern coordinates across stores: decrement inventory in Redis, create order in PostgreSQL, and if either fails, execute compensating actions (restore inventory, delete order). XA across Redis and PostgreSQL isn't practical.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-032",
      "type": "multiple-choice",
      "question": "A saga for order creation fails at the payment step. What happens to the inventory that was reserved in an earlier step?",
      "options": [
        "It stays reserved permanently",
        "A compensating transaction releases it",
        "It's automatically rolled back",
        "The database handles it"
      ],
      "correct": 1,
      "explanation": "Sagas use compensating transactions to undo previous steps. If payment fails, a compensation releases the reserved inventory. There's no automatic rollback—each step's compensation must be explicitly defined and executed.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-033",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're migrating from a monolithic PostgreSQL database to microservices with separate databases per service. What's this pattern called?",
          "options": [
            "Strangler fig",
            "Database per service",
            "Event sourcing",
            "CQRS"
          ],
          "correct": 1,
          "explanation": "Database per service is the microservices pattern where each service owns its data in a dedicated database. This enables independent scaling, technology choice, and deployment.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "During migration, some queries need data from both the old monolith and new services. What helps?",
          "options": [
            "Distributed joins across databases",
            "API composition or data replication",
            "Rolling back to monolith",
            "Forcing users to wait"
          ],
          "correct": 1,
          "explanation": "API composition aggregates data from multiple services at the API layer. Alternatively, replicate needed data using events. Distributed joins across different databases aren't supported. This is a transitional complexity of migration.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Storage estimates are most useful when split into raw data, replication factor, metadata/index overhead, and retention horizon.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-034",
      "type": "multiple-choice",
      "question": "Your system writes to PostgreSQL and publishes events to Kafka. Without the outbox pattern, what can go wrong?",
      "options": [
        "Events are published but database write fails",
        "Both always succeed or fail together",
        "Kafka becomes a bottleneck",
        "PostgreSQL performance degrades"
      ],
      "correct": 0,
      "explanation": "Without outbox, the database write and Kafka publish are separate operations. The publish might succeed before the commit, and if the commit fails, the event was already sent. Or the write succeeds but publish fails, losing the event.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-035",
      "type": "multiple-choice",
      "question": "You use Elasticsearch for product search and PostgreSQL for product data. Products are updated frequently. What's the risk with periodic bulk sync from PostgreSQL to Elasticsearch?",
      "options": [
        "Higher consistency during sync",
        "Search results show stale data between syncs",
        "PostgreSQL query load decreases",
        "Elasticsearch indexes become smaller"
      ],
      "correct": 1,
      "explanation": "Periodic sync means Elasticsearch lags behind PostgreSQL between sync intervals. Users might search for a product that was just updated but see old data. CDC provides near real-time sync to minimize this window.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-036",
      "type": "multiple-choice",
      "question": "Your mobile app stores data in SQLite locally and syncs to a cloud database. A user makes changes offline, then another device makes conflicting changes. What resolves this?",
      "options": [
        "Last write wins",
        "First write wins",
        "Conflict resolution rules or user choice",
        "Reject the sync"
      ],
      "correct": 2,
      "explanation": "Conflict resolution strategies include: last-write-wins (data loss risk), merge rules (for compatible changes), or prompting the user to choose. The right approach depends on the data semantics. Generic LWW can lose important changes.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-037",
      "type": "multiple-choice",
      "question": "What's the main benefit of using a query engine like Trino (Presto) with a data lake?",
      "options": [
        "It provides ACID transactions on S3",
        "It enables SQL queries across multiple data sources",
        "It automatically optimizes S3 storage costs",
        "It replaces the need for ETL"
      ],
      "correct": 1,
      "explanation": "Trino is a federated query engine—it runs SQL across different data sources (S3, databases, data warehouses) without moving data. It doesn't provide ACID (that's lakehouse formats) or replace ETL for transformation.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-038",
      "type": "multiple-choice",
      "question": "You run PostgreSQL for OLTP and Redshift for OLAP. ETL jobs sync data nightly. A business user wants real-time dashboards. What architectural change helps?",
      "options": [
        "Run dashboards against PostgreSQL",
        "Increase ETL job frequency to hourly",
        "Add streaming ETL via CDC to Redshift",
        "Cache dashboard queries in Redis"
      ],
      "correct": 2,
      "explanation": "CDC-based streaming ETL continuously replicates changes to Redshift, enabling near real-time analytics. Hourly ETL still has lag. PostgreSQL isn't optimized for analytical queries. Caching doesn't solve the data freshness problem.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-039",
      "type": "multi-select",
      "question": "Which strategies help manage data consistency across multiple databases in a microservices architecture?",
      "options": [
        "Eventual consistency with event-driven updates",
        "Saga pattern for distributed transactions",
        "API composition for read operations",
        "Shared database between services"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Eventual consistency, sagas, and API composition all manage consistency while maintaining service autonomy. Shared databases defeat the purpose of database-per-service and create coupling between services.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-040",
      "type": "multiple-choice",
      "question": "Your search service uses Elasticsearch. When the search cluster is down, what fallback provides degraded but functional search?",
      "options": [
        "Return empty results",
        "Queue searches for later",
        "Fall back to database LIKE queries",
        "Cache the last known good results forever"
      ],
      "correct": 2,
      "explanation": "Falling back to database queries (though slower and less feature-rich) provides degraded service. Returning empty results or queueing provides no immediate value. Stale cached results are limited to previously-searched terms.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-041",
      "type": "multiple-choice",
      "question": "You use a write-behind cache. What's the main risk compared to write-through?",
      "options": [
        "Higher latency on writes",
        "Data loss if the cache fails before sync",
        "Cache never gets updated",
        "Database gets overloaded"
      ],
      "correct": 1,
      "explanation": "Write-behind (write-back) caches writes and asynchronously syncs to the database. If the cache fails before syncing, data is lost. Write-through immediately writes to both, avoiding data loss but with higher write latency.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-042",
      "type": "multiple-choice",
      "question": "Your real-time analytics dashboard shows order counts. Data flows: PostgreSQL → Kafka → Flink → Redis → Dashboard. Flink applies what operation?",
      "options": [
        "Store raw events",
        "Aggregate and compute metrics",
        "Query PostgreSQL",
        "Serve the dashboard"
      ],
      "correct": 1,
      "explanation": "Flink is a stream processor that transforms and aggregates events—computing metrics like order counts in real-time windows. PostgreSQL is the source, Kafka transports, Flink processes, Redis serves the computed results.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-043",
      "type": "ordering",
      "question": "Order the data flow for real-time analytics on database changes:",
      "items": [
        "Dashboard reads from serving store",
        "Stream processor aggregates events",
        "CDC captures database changes",
        "Events flow through message queue"
      ],
      "correctOrder": [2, 3, 1, 0],
      "explanation": "CDC captures changes → Message queue (Kafka) transports events → Stream processor (Flink/Spark) aggregates → Dashboard reads from serving store (Redis/Druid). This is the typical streaming analytics pipeline.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-044",
      "type": "multiple-choice",
      "question": "You're implementing multi-master replication between two PostgreSQL clusters in different regions. What's the primary consistency challenge?",
      "options": [
        "Both clusters going offline",
        "Conflicting concurrent writes to the same data",
        "Different schema versions",
        "Network bandwidth between regions"
      ],
      "correct": 1,
      "explanation": "Multi-master allows writes to any cluster, so the same row might be updated in both regions simultaneously. Conflict resolution is needed—last-write-wins, custom merge, or conflict detection with manual resolution.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-045",
      "type": "multiple-choice",
      "question": "Your API gateway caches responses in Redis. The backend uses PostgreSQL. A POST request modifies data. What should the gateway do to the cache?",
      "options": [
        "Nothing—the cache will expire eventually",
        "Invalidate related cached responses",
        "Update the cache with the POST response",
        "Clear the entire cache"
      ],
      "correct": 1,
      "explanation": "The gateway should invalidate cached responses affected by the write. Waiting for TTL expiry serves stale data. Updating cache requires knowing the response format. Clearing everything is overly aggressive. Targeted invalidation balances correctness and performance.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your application uses PostgreSQL for persistence and Redis for caching. You want to ensure cache consistency without significant latency. What pattern should you use?",
          "options": [
            "Write-through caching",
            "Cache-aside with invalidation",
            "Write-behind caching",
            "Read-through caching only"
          ],
          "correct": 1,
          "explanation": "Cache-aside with invalidation: write to the database, invalidate the cache. The next read populates the cache. This avoids the overhead of updating cache on every write while maintaining consistency.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "With cache-aside, a race condition can occur. Read happens between write and invalidation. What's the result?",
          "options": [
            "Read blocks until invalidation completes",
            "Cache might contain stale data until next invalidation or TTL",
            "Read sees the new data",
            "Transaction rolls back"
          ],
          "correct": 1,
          "explanation": "If a read occurs after the database write but before cache invalidation, it might repopulate the cache with old data from a concurrent read. This stale entry persists until TTL or another invalidation. This is a known limitation of cache-aside.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Storage estimates are most useful when split into raw data, replication factor, metadata/index overhead, and retention horizon.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-047",
      "type": "multiple-choice",
      "question": "You use event sourcing where events are in Kafka and materialized views are in PostgreSQL. How do you rebuild a view from scratch?",
      "options": [
        "Export PostgreSQL and reimport",
        "Replay events from Kafka from the beginning",
        "Query the current state and work backwards",
        "Request a snapshot from Kafka"
      ],
      "correct": 1,
      "explanation": "Event sourcing stores all state changes as events. To rebuild a materialized view, replay all events from the beginning (or from a snapshot if available). This is a key benefit of event sourcing—views are derived and reproducible.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-048",
      "type": "multiple-choice",
      "question": "Your company uses PostgreSQL, MongoDB, and Elasticsearch. New engineers struggle to understand which store to use for what. What helps?",
      "options": [
        "Use only one database",
        "Document data ownership and access patterns per store",
        "Let each team choose freely",
        "Migrate everything to a single store"
      ],
      "correct": 1,
      "explanation": "Documentation of which data belongs in which store, why, and how they're synced reduces cognitive load. Consolidating to one store loses the benefits of polyglot persistence. Undocumented freedom leads to inconsistency.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-049",
      "type": "multiple-choice",
      "question": "A data mesh architecture advocates for decentralized data ownership. What does each domain team provide?",
      "options": [
        "Raw database access to other teams",
        "Data as a product with defined interfaces",
        "ETL jobs to the central warehouse",
        "Schemas for a shared database"
      ],
      "correct": 1,
      "explanation": "Data mesh treats data as a product—each domain owns and serves its data through defined APIs or data contracts, rather than dumping to a central warehouse. This decentralizes ownership while maintaining discoverability.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-050",
      "type": "multiple-choice",
      "question": "You need to run complex joins across data in PostgreSQL and Cassandra. What approach works?",
      "options": [
        "Cross-database SQL join",
        "Federation query engine like Trino",
        "Cassandra native joins",
        "Stored procedures spanning databases"
      ],
      "correct": 1,
      "explanation": "Trino (Presto) is a federated query engine that can query multiple data sources and join across them. Cassandra doesn't support joins. Cross-database SQL doesn't exist. Stored procedures can't span different database systems.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-051",
      "type": "multi-select",
      "question": "Which factors should influence your decision to add a specialized database to your architecture?",
      "options": [
        "Current store can't meet latency requirements",
        "Access pattern is fundamentally different",
        "Team wants to learn a new technology",
        "Significant cost savings for specific workload"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Valid reasons: latency requirements current store can't meet, fundamentally different access patterns (graph vs relational), and cost savings. Learning a new technology isn't sufficient justification for operational complexity.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-052",
      "type": "multiple-choice",
      "question": "Your feature store uses Redis for online serving and S3/Parquet for offline training. What ensures both have the same feature values?",
      "options": [
        "Both are readonly",
        "Shared feature computation pipeline",
        "Manual synchronization",
        "Redis replicates to S3"
      ],
      "correct": 1,
      "explanation": "A unified feature computation pipeline writes to both stores, ensuring training and serving use identical feature values. This prevents training-serving skew. Different pipelines for each store can drift.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-053",
      "type": "multiple-choice",
      "question": "Your CDC pipeline from PostgreSQL to Elasticsearch occasionally produces out-of-order updates. A newer update arrives before an older one. What can happen?",
      "options": [
        "Elasticsearch rejects the older update",
        "Older update overwrites newer data",
        "Both updates merge automatically",
        "Pipeline automatically reorders"
      ],
      "correct": 1,
      "explanation": "Without version checking, the older update can overwrite newer data if it arrives later (out of order). Solutions include using version numbers (reject updates with older versions) or timestamps for conflict resolution.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-054",
      "type": "multiple-choice",
      "question": "How do you prevent stale writes from overwriting newer data in a CDC pipeline?",
      "options": [
        "Process events in strict order",
        "Include a version/timestamp and reject older versions",
        "Use exactly-once delivery",
        "Ignore the problem—it's rare"
      ],
      "correct": 1,
      "explanation": "Include a version or timestamp with each event. The consumer checks if the incoming version is newer than the current version before applying. This handles out-of-order delivery gracefully. Strict ordering is often impractical.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-055",
      "type": "multiple-choice",
      "question": "Your GraphQL API aggregates data from PostgreSQL (users), MongoDB (posts), and Redis (sessions). This is an example of what pattern?",
      "options": [
        "Database per service",
        "CQRS",
        "API composition",
        "Event sourcing"
      ],
      "correct": 2,
      "explanation": "API composition aggregates data from multiple backend sources into a unified API response. The GraphQL layer resolves fields from different stores and composes the response. This is distinct from CQRS (separate read/write models).",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-056",
      "type": "multiple-choice",
      "question": "In a GraphQL federation architecture, what's the role of the gateway?",
      "options": [
        "Store all data centrally",
        "Route queries to appropriate subgraphs and merge responses",
        "Cache all database queries",
        "Manage database connections"
      ],
      "correct": 1,
      "explanation": "The federation gateway routes parts of a query to the appropriate subgraph services and merges their responses. Each subgraph owns its portion of the schema and data. The gateway orchestrates composition.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your system stores events in Kafka and maintains a materialized view in PostgreSQL. What happens if the PostgreSQL consumer falls significantly behind?",
          "options": [
            "Events are lost",
            "The view becomes increasingly stale",
            "Kafka automatically catches up",
            "PostgreSQL crashes"
          ],
          "correct": 1,
          "explanation": "Consumer lag means the materialized view falls behind real-time. Events aren't lost (Kafka retains them), but queries against PostgreSQL see stale data. The gap between event time and view reflects the lag.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        },
        {
          "question": "How do you monitor and address consumer lag?",
          "options": [
            "Ignore it—Kafka handles everything",
            "Monitor lag metrics, scale consumers or optimize processing",
            "Restart the consumer periodically",
            "Reduce the number of events"
          ],
          "correct": 1,
          "explanation": "Monitor consumer lag (events behind) via Kafka metrics. Address by scaling consumer instances (parallel processing), optimizing consumer code, or batching writes. Lag indicates processing can't keep up with production.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Storage estimates are most useful when split into raw data, replication factor, metadata/index overhead, and retention horizon.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-058",
      "type": "multiple-choice",
      "question": "You're implementing a shopping cart. Cart data is in Redis for speed. Orders are in PostgreSQL for durability. When does cart data move to PostgreSQL?",
      "options": [
        "Continuously synced",
        "On checkout/order creation",
        "Never—they stay separate",
        "Every 5 minutes"
      ],
      "correct": 1,
      "explanation": "Cart data moves to PostgreSQL as an order on checkout. Before checkout, carts are ephemeral (Redis is fast, eventual loss is acceptable). On checkout, the order must be durably stored in PostgreSQL. This is a common hybrid pattern.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-059",
      "type": "multiple-choice",
      "question": "Your message queue (Kafka) sits between PostgreSQL CDC and Elasticsearch consumers. What happens if Elasticsearch is down for an hour?",
      "options": [
        "Events are lost",
        "Events queue in Kafka until consumer resumes",
        "PostgreSQL stops accepting writes",
        "CDC stops capturing changes"
      ],
      "correct": 1,
      "explanation": "Kafka retains events based on retention policy (typically days). When the Elasticsearch consumer comes back, it resumes from its last committed offset, processing queued events. No data is lost (assuming retention > downtime).",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-060",
      "type": "multiple-choice",
      "question": "You're designing a globally distributed system. User data is in PostgreSQL with multi-region replication. Session data is in Redis. What's the challenge for a user traveling between regions?",
      "options": [
        "PostgreSQL can't replicate",
        "Session might not exist in the new region",
        "User data is lost",
        "The application crashes"
      ],
      "correct": 1,
      "explanation": "If Redis sessions are region-local without cross-region replication, a user routed to a different region won't have their session. Solutions: global session store, session data in cookie, or sticky routing to origin region.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-061",
      "type": "multi-select",
      "question": "Which approaches help with cross-region session consistency?",
      "options": [
        "Global Redis cluster with replication",
        "Session data in signed JWT cookies",
        "Sticky sessions to user's home region",
        "Separate session store per region without sync"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Global Redis replication, JWT cookies (stateless sessions), and sticky routing all ensure session availability across regions. Separate stores without sync means sessions don't follow users—they'd be logged out on region change.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-062",
      "type": "multiple-choice",
      "question": "Your hybrid architecture uses PostgreSQL and Snowflake. To minimize data movement, where should you run a query that joins real-time orders (PostgreSQL) with historical analytics (Snowflake)?",
      "options": [
        "In PostgreSQL by importing Snowflake data",
        "In Snowflake by importing PostgreSQL data",
        "In a federated query engine",
        "Export both to a third location and join there"
      ],
      "correct": 2,
      "explanation": "A federated query engine (Trino) can join across both sources in place, minimizing data movement. Importing into either database duplicates data and adds latency. Federation pushes computation to the data where possible.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-063",
      "type": "multiple-choice",
      "question": "You're migrating a monolithic MySQL database to microservices. What's the strangler fig pattern?",
      "options": [
        "Migrate all data at once",
        "Gradually route traffic to new services while old system runs",
        "Replicate MySQL to multiple copies",
        "Run old and new systems with dual writes"
      ],
      "correct": 1,
      "explanation": "Strangler fig gradually replaces the old system by routing functionality to new services piece by piece. The old system continues running, handling unmigrated parts. Over time, the new services 'strangle' the monolith until it's decommissioned.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-064",
      "type": "multiple-choice",
      "question": "During strangler fig migration, both old MySQL and new PostgreSQL service have user data. How do you keep them in sync?",
      "options": [
        "They don't need to sync",
        "Dual writes from the application",
        "CDC from MySQL to new service, or vice versa",
        "Periodic manual reconciliation"
      ],
      "correct": 2,
      "explanation": "CDC syncs data between old and new systems during migration. The primary can be either system depending on migration phase. Dual writes risk inconsistency. Eventually, the old system is decommissioned and sync stops.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-065",
      "type": "ordering",
      "question": "Order the phases of a typical strangler fig database migration:",
      "items": [
        "Decommission old database",
        "Run both systems with sync",
        "Build new service with its database",
        "Route all traffic to new service"
      ],
      "correctOrder": [2, 1, 3, 0],
      "explanation": "Build new service → Run both with sync (test, validate) → Route traffic to new service (incremental) → Decommission old database. This minimizes risk through incremental cutover.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-066",
      "type": "multiple-choice",
      "question": "Your data warehouse (Redshift) receives data from PostgreSQL (OLTP), MongoDB (product catalog), and S3 (clickstream). What typically orchestrates these ETL jobs?",
      "options": [
        "Manual scheduling",
        "Database triggers",
        "Workflow orchestrator like Airflow",
        "Each source pushes on its own schedule"
      ],
      "correct": 2,
      "explanation": "Airflow (or similar orchestrators like Dagster, Prefect) schedules and coordinates ETL jobs, managing dependencies, retries, and monitoring. Manual scheduling doesn't scale. Database triggers can't coordinate across different sources.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-067",
      "type": "multiple-choice",
      "question": "You have hot data in PostgreSQL and archive data in S3. A query needs to search across both. What pattern helps?",
      "options": [
        "Import S3 data to PostgreSQL for the query",
        "Archive queries aren't supported",
        "Use a federated query engine or partitioned view",
        "Keep all data in PostgreSQL"
      ],
      "correct": 2,
      "explanation": "Federated query engines (Trino) or partitioned views that span both hot (PostgreSQL) and cold (S3) tiers enable seamless querying. PostgreSQL foreign data wrappers can also mount S3 data. Importing defeats the purpose of archiving.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-068",
      "type": "multiple-choice",
      "question": "Your leaderboard uses Redis sorted sets for rankings. The underlying scores come from PostgreSQL. How often should you update Redis?",
      "options": [
        "Never—compute from PostgreSQL on each request",
        "Based on business requirements for freshness",
        "Exactly once per day",
        "Only when users ask"
      ],
      "correct": 1,
      "explanation": "Update frequency depends on business needs. A real-time game needs sub-second updates (event-driven). A weekly leaderboard needs daily updates. Match sync frequency to how stale data affects user experience.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-069",
      "type": "multiple-choice",
      "question": "Your architecture has: PostgreSQL (source of truth) → Kafka (event bus) → Elasticsearch (search). Elasticsearch index gets corrupted. How do you rebuild it?",
      "options": [
        "Restore from Elasticsearch backup",
        "Replay events from Kafka",
        "Query PostgreSQL and reindex",
        "Wait for CDC to naturally rebuild"
      ],
      "correct": 2,
      "explanation": "For full rebuilds, query PostgreSQL directly—it's the source of truth. Kafka has retention limits and replaying all history might be slow or impossible. ES backups work if available. CDC only captures changes, not full state.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-070",
      "type": "multi-select",
      "question": "Which are benefits of separating analytical workloads from operational databases?",
      "options": [
        "Analytical queries don't impact OLTP performance",
        "Can optimize storage format for each workload",
        "Simpler schema management",
        "Can scale analytical compute independently"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Separating workloads: prevents analytical queries from impacting OLTP, allows format optimization (row vs columnar), and enables independent scaling. Schema management actually becomes more complex with multiple systems to sync.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-071",
      "type": "multiple-choice",
      "question": "You use a cache-aside pattern. A cache miss occurs. What's the sequence of operations?",
      "options": [
        "Check cache → Return miss → User retries",
        "Check cache → Miss → Query database → Return → Populate cache",
        "Check database → Populate cache → Return from cache",
        "Check cache → Miss → Return empty"
      ],
      "correct": 1,
      "explanation": "Cache-aside on miss: check cache (miss) → query database → return to caller → populate cache for future reads. The application is responsible for cache population, not the cache system.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-072",
      "type": "multiple-choice",
      "question": "Read-through caching differs from cache-aside how?",
      "options": [
        "Read-through only caches writes",
        "In read-through, the cache fetches from the database on miss",
        "Cache-aside doesn't cache reads",
        "They're the same pattern"
      ],
      "correct": 1,
      "explanation": "In read-through, the cache system itself loads from the database on miss—the application always reads from the cache. In cache-aside, the application manages both cache and database reads. Read-through provides a simpler API.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-073",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're building a notification system. User preferences are in PostgreSQL. Real-time notification state is in Redis. What data goes in each?",
          "options": [
            "All notification data in PostgreSQL",
            "All notification data in Redis",
            "Preferences in PostgreSQL, unread counts/badges in Redis",
            "Preferences in Redis, notification history in PostgreSQL"
          ],
          "correct": 2,
          "explanation": "Persistent preferences (email settings, do-not-disturb) belong in PostgreSQL. Ephemeral, frequently-updated state (unread count, badge numbers) benefits from Redis speed. History should be in PostgreSQL for durability.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "A notification is marked as read. How do both stores get updated?",
          "options": [
            "Update Redis only",
            "Update PostgreSQL, let CDC update Redis",
            "Update PostgreSQL and Redis in the same request",
            "Don't track read status"
          ],
          "correct": 2,
          "explanation": "Update both: PostgreSQL for durable record, Redis for real-time badge count. Dual writes in the same request ensure consistency. CDC adds latency for a real-time feature. Redis-only loses the read state on cache eviction.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Storage estimates are most useful when split into raw data, replication factor, metadata/index overhead, and retention horizon.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-074",
      "type": "multiple-choice",
      "question": "Your billing system writes invoices to PostgreSQL. For reporting, you replicate to BigQuery. What type of replication minimizes impact on the OLTP database?",
      "options": [
        "Synchronous replication",
        "Logical replication from replica, not primary",
        "Full table scans every hour",
        "Triggers that write to BigQuery"
      ],
      "correct": 1,
      "explanation": "Set up logical replication from a PostgreSQL read replica to BigQuery. This offloads the CDC overhead from the primary. Full scans impact performance. Triggers add write latency. Sync replication adds latency to every transaction.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-075",
      "type": "multiple-choice",
      "question": "You want to add full-text search to your PostgreSQL-backed application. What's the simplest hybrid approach?",
      "options": [
        "Add Elasticsearch and sync via CDC",
        "Use PostgreSQL's built-in full-text search",
        "Replace PostgreSQL with Elasticsearch",
        "Build a custom search index"
      ],
      "correct": 1,
      "explanation": "PostgreSQL has built-in full-text search (tsvector, tsquery, GIN indexes). Start here—it avoids the complexity of syncing a separate system. Only add Elasticsearch when PostgreSQL's features are insufficient for your needs.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-076",
      "type": "multiple-choice",
      "question": "When does PostgreSQL full-text search become insufficient, justifying Elasticsearch?",
      "options": [
        "More than 100 documents",
        "Need for complex relevance ranking, faceting, or suggestions",
        "PostgreSQL version is too old",
        "Full-text search is never sufficient"
      ],
      "correct": 1,
      "explanation": "Elasticsearch excels at complex relevance scoring, faceted search, autocomplete, fuzzy matching, and aggregations. If you need these features and PostgreSQL's FTS can't deliver, Elasticsearch is justified. Size alone isn't the factor.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-077",
      "type": "multiple-choice",
      "question": "Your ML feature store computes features in a nightly batch job. Production serving needs real-time features. What's missing?",
      "options": [
        "More frequent batch jobs",
        "A real-time feature computation path",
        "Larger batch job resources",
        "Caching batch results"
      ],
      "correct": 1,
      "explanation": "A streaming/real-time computation path (Flink, Spark Streaming) generates fresh features from live events. Batch provides historical features; streaming provides real-time. Many systems need both paths (feature stores often call this 'online' vs 'offline').",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-078",
      "type": "multiple-choice",
      "question": "Your operational database is PostgreSQL. You want to run machine learning on the data. Where should ML training happen?",
      "options": [
        "Directly on PostgreSQL",
        "On a replicated copy or exported data",
        "ML can't use relational data",
        "Convert PostgreSQL to a document store first"
      ],
      "correct": 1,
      "explanation": "ML training queries can be heavy—run them on a replica or export data to a training environment (S3 + Spark, BigQuery). This prevents ML workloads from impacting production OLTP performance.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-079",
      "type": "multi-select",
      "question": "Which are valid strategies for handling database writes during system maintenance?",
      "options": [
        "Queue writes and replay after maintenance",
        "Reject writes and return errors",
        "Failover to a replica",
        "Buffer in local storage and sync later"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are valid strategies with tradeoffs: queuing accepts writes without completing them, rejecting is explicit about unavailability, failover maintains availability, local buffering works for eventual consistency. Choice depends on requirements.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-080",
      "type": "multiple-choice",
      "question": "Your hybrid architecture includes PostgreSQL, Redis, and S3. An outage takes down the data center. Which store likely recovers user data most easily?",
      "options": [
        "Redis (in-memory with snapshots)",
        "PostgreSQL (WAL and backups)",
        "S3 (multi-region replication)",
        "All recover equally well"
      ],
      "correct": 2,
      "explanation": "S3 with multi-region replication survives data center outages by design—data exists in other regions. PostgreSQL recovery depends on backup location. Redis in-memory data may be lost between snapshots. S3 is the most durable by design.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-081",
      "type": "multiple-choice",
      "question": "You're implementing a distributed transaction across PostgreSQL and MongoDB. What's the recommended approach?",
      "options": [
        "XA transactions",
        "Saga pattern with compensating transactions",
        "Two-phase commit across both",
        "MongoDB's native distributed transactions"
      ],
      "correct": 1,
      "explanation": "Saga pattern is the practical approach for transactions spanning different database technologies. XA/2PC requires support from both databases and is complex and slow. MongoDB's transactions only work within MongoDB.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-082",
      "type": "ordering",
      "question": "Order the saga steps for a travel booking (flight, hotel, payment) where payment fails:",
      "items": [
        "Compensate: cancel hotel reservation",
        "Book flight (success)",
        "Process payment (fails)",
        "Compensate: cancel flight",
        "Book hotel (success)"
      ],
      "correctOrder": [1, 4, 2, 0, 3],
      "explanation": "Book flight → Book hotel → Payment fails → Compensate hotel (reverse order) → Compensate flight. Compensations run in reverse order of the original steps to maintain consistency.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-083",
      "type": "multiple-choice",
      "question": "Your saga orchestrator crashes after booking a flight but before booking the hotel. What happens on restart?",
      "options": [
        "Start from the beginning",
        "Resume from stored saga state",
        "Flight booking is lost",
        "Manual intervention required"
      ],
      "correct": 1,
      "explanation": "Saga orchestrators persist their state. On restart, the orchestrator reads the saga's last state (flight booked, awaiting hotel) and resumes from there. This requires durable storage of saga state—without it, recovery is problematic.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-084",
      "type": "multiple-choice",
      "question": "You use PostgreSQL for orders and DynamoDB for inventory. On checkout, you need to decrease inventory and create an order. What coordinates this?",
      "options": [
        "PostgreSQL transaction",
        "DynamoDB transaction",
        "Application-level saga or orchestration",
        "Direct cross-database join"
      ],
      "correct": 2,
      "explanation": "Application-level coordination (saga) handles cross-database operations. Neither PostgreSQL nor DynamoDB can manage transactions in the other. The application orchestrates: reserve inventory → create order → confirm or compensate.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-085",
      "type": "multiple-choice",
      "question": "Your analytics dashboard needs to combine real-time data (last hour from Kafka) with historical data (from data warehouse). What architecture supports this?",
      "options": [
        "Store everything in Kafka",
        "Lambda architecture with speed and batch layers",
        "Run all queries against the data warehouse",
        "Cache historical data in Kafka"
      ],
      "correct": 1,
      "explanation": "Lambda architecture combines batch layer (historical accuracy from warehouse) with speed layer (real-time from Kafka). Queries merge both views. This handles the trade-off between real-time freshness and historical completeness.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-086",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your company is building a customer 360 view combining data from CRM (Salesforce), transactions (PostgreSQL), and support tickets (Zendesk). Where should this unified view live?",
          "options": [
            "In each source system",
            "In a data warehouse or dedicated customer data platform",
            "In the CRM only",
            "No unified view is needed"
          ],
          "correct": 1,
          "explanation": "A data warehouse or customer data platform (CDP) aggregates data from multiple sources into a unified view. This avoids querying multiple systems in real-time and provides consistent analytics.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "What pattern keeps the customer 360 view up-to-date as source systems change?",
          "options": [
            "Manual exports weekly",
            "CDC or scheduled ETL from each source",
            "Users refresh the view manually",
            "Source systems push directly to the warehouse"
          ],
          "correct": 1,
          "explanation": "CDC or scheduled ETL pipelines keep the view synchronized. CDC provides near real-time updates; scheduled ETL is simpler but has lag. Push from sources requires modifying each source system.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Storage estimates are most useful when split into raw data, replication factor, metadata/index overhead, and retention horizon.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-087",
      "type": "multiple-choice",
      "question": "Your application uses PostgreSQL. You're adding a recommendation engine that needs a graph database for user-product relationships. How do you sync data?",
      "options": [
        "Replace PostgreSQL with the graph database",
        "Dual writes from the application",
        "CDC from PostgreSQL to graph database",
        "Query PostgreSQL from the graph database"
      ],
      "correct": 2,
      "explanation": "CDC streams relevant changes from PostgreSQL to the graph database. This decouples the systems—the application writes to PostgreSQL as before, and changes flow to the graph DB asynchronously. Dual writes risk inconsistency.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-088",
      "type": "multiple-choice",
      "question": "You have hot PostgreSQL data and cold Parquet files in S3. A user queries order history spanning both. What tool provides a unified query experience?",
      "options": [
        "PostgreSQL foreign data wrapper for S3",
        "Amazon Athena with federated queries",
        "S3 Select",
        "Either A or B depending on scale"
      ],
      "correct": 3,
      "explanation": "Both can work. PostgreSQL FDW is simpler for small-scale queries and keeps the PostgreSQL interface. Athena excels at large-scale S3 queries with federated access to other sources. Choose based on query patterns and data volume.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-089",
      "type": "multiple-choice",
      "question": "Your e-commerce site caches product pages in a CDN. The product database is PostgreSQL. When a product price changes, how is the CDN updated?",
      "options": [
        "Wait for CDN TTL to expire",
        "Purge the specific product's CDN cache on update",
        "Don't cache product pages",
        "Store prices only in the CDN"
      ],
      "correct": 1,
      "explanation": "Active cache invalidation purges specific URLs when underlying data changes. Waiting for TTL serves stale prices. Not caching hurts performance. The application triggers CDN purge after database update (via API or event).",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-090",
      "type": "multi-select",
      "question": "Which caching layers might exist in a typical web application architecture?",
      "options": [
        "Browser cache",
        "CDN edge cache",
        "Application-level cache (Redis)",
        "Database query cache"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are common caching layers: browser (HTTP cache headers), CDN (edge servers), application (Redis/Memcached), and database (query/buffer cache). Each layer reduces load on layers behind it.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-091",
      "type": "multiple-choice",
      "question": "Your microservice architecture has 10 services, each with its own database. A compliance audit needs to trace a user's data across all services. What helps?",
      "options": [
        "Query each database individually",
        "Centralized data catalog with lineage tracking",
        "Shared database for all services",
        "Compliance isn't possible with microservices"
      ],
      "correct": 1,
      "explanation": "A data catalog tracks what data exists where (lineage, ownership, classification). For audits, the catalog identifies relevant services and their data. This maintains microservice autonomy while enabling cross-cutting concerns.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-092",
      "type": "multiple-choice",
      "question": "You're implementing soft delete across PostgreSQL and Elasticsearch. A record is marked deleted in PostgreSQL. What should happen in Elasticsearch?",
      "options": [
        "Nothing—they're separate systems",
        "Update the Elasticsearch document with deleted flag",
        "Remove the document from Elasticsearch",
        "Either B or C depending on requirements"
      ],
      "correct": 3,
      "explanation": "Depends on requirements. If deleted items should be searchable (admin view, restore), update with flag. If deleted means 'gone from search', remove the document. The CDC pipeline should propagate whichever action matches your business logic.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-093",
      "type": "multiple-choice",
      "question": "Your search uses Elasticsearch. Users report that recently published content isn't appearing in search. The CDC pipeline is running. What's the most likely issue?",
      "options": [
        "Elasticsearch is down",
        "Elasticsearch refresh interval",
        "Content was never written to PostgreSQL",
        "Users are searching wrong terms"
      ],
      "correct": 1,
      "explanation": "Elasticsearch indexes have a refresh interval (default 1 second) before new documents are searchable. Combined with CDC lag, recently published content may take seconds to appear. Check index refresh settings and CDC consumer lag.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-094",
      "type": "multiple-choice",
      "question": "Your system uses PostgreSQL, Redis, and Elasticsearch. You need to test a feature that touches all three. What testing approach works?",
      "options": [
        "Mock all databases",
        "Integration tests with real instances (Docker)",
        "Only unit test application code",
        "Test in production"
      ],
      "correct": 1,
      "explanation": "Integration tests with real database instances (via Docker/Testcontainers) verify actual behavior across all stores. Mocking hides integration issues. Unit tests don't catch cross-system problems. Production testing is risky.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-095",
      "type": "multiple-choice",
      "question": "Your pipeline writes data to PostgreSQL and then publishes to Kafka. The Kafka publish sometimes fails. What pattern prevents data inconsistency?",
      "options": [
        "Retry Kafka publish indefinitely",
        "Rollback the PostgreSQL write on Kafka failure",
        "Use the transactional outbox pattern",
        "Accept occasional missing events"
      ],
      "correct": 2,
      "explanation": "Transactional outbox writes the event to an outbox table in the same PostgreSQL transaction. A separate process reliably publishes from the outbox to Kafka with retries. This ensures the event is never lost if the DB write succeeded.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-096",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're building a real-time inventory system. Inventory counts need to be accurate (no overselling) and fast. What hybrid architecture helps?",
          "options": [
            "PostgreSQL only with heavy indexes",
            "Redis for real-time counts, PostgreSQL for audit log",
            "Elasticsearch for inventory",
            "Kafka for storing inventory"
          ],
          "correct": 1,
          "explanation": "Redis provides fast, atomic decrement operations for real-time inventory. PostgreSQL stores the durable audit trail of all inventory changes. This combines Redis speed with PostgreSQL durability and queryability.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "Redis shows 5 items in stock. PostgreSQL audit log shows 10 sold, 15 restocked. How do you reconcile if they disagree?",
          "options": [
            "Trust Redis—it's the live system",
            "Trust PostgreSQL—recalculate from audit log",
            "Average the two values",
            "Flag for manual review"
          ],
          "correct": 1,
          "explanation": "PostgreSQL's immutable audit log is the source of truth. Recalculate inventory from the complete event history. Redis may have missed an event or had a bug. The audit log is authoritative for reconciliation.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Storage estimates are most useful when split into raw data, replication factor, metadata/index overhead, and retention horizon.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-097",
      "type": "multiple-choice",
      "question": "Your architecture has grown to include 5 different databases. Engineering velocity is slowing due to complexity. What might help?",
      "options": [
        "Add more databases for specialization",
        "Consolidate where workloads overlap",
        "Remove all caching",
        "Hire more database administrators"
      ],
      "correct": 1,
      "explanation": "Consolidation reduces operational complexity. Modern databases often handle multiple workloads adequately (PostgreSQL for OLTP + search + JSON). Only keep specialized stores where they provide clear value over consolidation.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-098",
      "type": "multiple-choice",
      "question": "You're evaluating whether to add a specialized time-series database or use PostgreSQL with TimescaleDB. What factors favor the specialized database?",
      "options": [
        "Simpler operations with fewer systems",
        "Extreme scale or specific features not in TimescaleDB",
        "Team already knows PostgreSQL",
        "Cost reduction"
      ],
      "correct": 1,
      "explanation": "A specialized TSDB (InfluxDB, TimescaleDB, QuestDB) is warranted when you need extreme ingestion rates, specific features (downsampling, retention), or scale beyond TimescaleDB. Otherwise, sticking with PostgreSQL reduces complexity.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-099",
      "type": "multi-select",
      "question": "What should you monitor in a polyglot persistence architecture?",
      "options": [
        "Sync lag between systems (CDC consumer lag)",
        "Cross-system data consistency",
        "Individual database health metrics",
        "Query patterns that span multiple stores"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are critical: CDC lag indicates sync freshness, consistency checks catch drift, individual health ensures each system works, and cross-store query patterns reveal where composition is complex or slow.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-100",
      "type": "multiple-choice",
      "question": "Your data platform team supports PostgreSQL, MongoDB, Redis, Elasticsearch, and Kafka. A new project team wants to add Cassandra. What question should you ask first?",
      "options": [
        "Can your team maintain another database?",
        "What specific access pattern requires Cassandra?",
        "How much will Cassandra cost?",
        "Is Cassandra the newest technology?"
      ],
      "correct": 1,
      "explanation": "Understand the specific workload requirements. Cassandra excels at high write throughput across partitions. If the workload can be served by existing stores (MongoDB, PostgreSQL), adding Cassandra increases complexity without benefit.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    }
  ]
}
