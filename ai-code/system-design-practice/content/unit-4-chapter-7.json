{
  "id": "unit-4-chapter-7",
  "chapterTitle": "Hybrid Architectures",
  "problems": [
    {
      "id": "u4c7-001",
      "type": "multiple-choice",
      "question": "Your e-commerce platform stores product catalog in PostgreSQL and product search in Elasticsearch. What's the primary challenge with this hybrid approach?",
      "options": [
        "Search queries are slower than SQL queries",
        "Data consistency between the two systems",
        "PostgreSQL cannot handle write loads",
        "Elasticsearch cannot scale horizontally"
      ],
      "correct": 1,
      "explanation": "The main challenge is maintaining data consistency when the same data lives in multiple stores. Changes in PostgreSQL must be synced to Elasticsearch, introducing lag and potential inconsistencies.",
      "detailedExplanation": "Start from \"your e-commerce platform stores product catalog in PostgreSQL and product search in\", then pressure-test the result against the options. Eliminate approaches that hand-wave conflict resolution or quorum behavior. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-002",
      "type": "multiple-choice",
      "question": "You use Redis as a cache in front of PostgreSQL. When should you invalidate the cache?",
      "options": [
        "Only when TTL expires",
        "On every read",
        "On writes to the underlying data",
        "Never—caches are eventually consistent"
      ],
      "correct": 2,
      "explanation": "Cache invalidation should happen on writes to maintain consistency. TTL provides eventual consistency but can serve stale data. Active invalidation on writes ensures readers get current data (cache-aside pattern).",
      "detailedExplanation": "The decision turns on \"you use Redis as a cache in front of PostgreSQL\". Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-003",
      "type": "multiple-choice",
      "question": "Your system uses PostgreSQL for transactions and Cassandra for time-series data. What sync pattern keeps both consistent?",
      "options": [
        "Two-phase commit across both databases",
        "Write to both in application code",
        "Event sourcing with CDC from PostgreSQL",
        "Periodic batch sync jobs"
      ],
      "correct": 2,
      "explanation": "Change Data Capture (CDC) streams changes from PostgreSQL to Cassandra via events. Two-phase commit across different database types is impractical. Application-level dual writes risk partial failures. CDC provides reliable, eventually consistent replication.",
      "detailedExplanation": "Read this as a scenario about \"your system uses PostgreSQL for transactions and Cassandra for time-series data\". Eliminate options that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-004",
      "type": "multi-select",
      "question": "Which patterns help maintain consistency in polyglot persistence architectures?",
      "options": [
        "Change Data Capture (CDC)",
        "Saga pattern",
        "Outbox pattern",
        "Distributed locks"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All help with consistency: CDC streams changes between stores, Sagas coordinate multi-store transactions with compensations, Outbox ensures atomic event publishing with writes, and distributed locks prevent concurrent modifications across systems.",
      "detailedExplanation": "Use \"patterns help maintain consistency in polyglot persistence architectures\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-005",
      "type": "multiple-choice",
      "question": "You're designing a social media platform. User profiles are in PostgreSQL, friend graphs in Neo4j, and posts in MongoDB. What coordinates a 'delete my account' request?",
      "options": [
        "Three separate DELETE calls from the client",
        "A microservice orchestrating the deletions",
        "Database triggers cascading across stores",
        "A distributed transaction across all three"
      ],
      "correct": 1,
      "explanation": "An orchestration service (or saga) coordinates deletions across the three stores. Triggers can't cascade across different databases. Distributed transactions across heterogeneous stores are impractical. Client coordination is unreliable.",
      "detailedExplanation": "This prompt is really about \"you're designing a social media platform\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-006",
      "type": "ordering",
      "question": "Order the steps in a CDC-based sync from PostgreSQL to Elasticsearch:",
      "items": [
        "Debezium captures PostgreSQL WAL changes",
        "Kafka consumer processes the events",
        "User updates a row in PostgreSQL",
        "Elasticsearch index is updated"
      ],
      "correctOrder": [2, 0, 1, 3],
      "explanation": "First the user writes to PostgreSQL. Debezium reads the write-ahead log (WAL), produces events to Kafka, a consumer processes those events, and finally updates Elasticsearch.",
      "detailedExplanation": "If you keep \"order the steps in a CDC-based sync from PostgreSQL to Elasticsearch:\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-007",
      "type": "multiple-choice",
      "question": "Your CQRS system writes to PostgreSQL (command side) and reads from Elasticsearch (query side). A user creates a post and immediately searches for it but doesn't find it. What's happening?",
      "options": [
        "Elasticsearch indexing is disabled",
        "Replication lag between command and query stores",
        "The search query has a typo",
        "PostgreSQL transaction hasn't committed"
      ],
      "correct": 1,
      "explanation": "This is read-after-write inconsistency due to replication lag. The write completed in PostgreSQL but hasn't propagated to Elasticsearch yet. This is expected in eventually consistent CQRS systems.",
      "detailedExplanation": "The core signal here is \"your CQRS system writes to PostgreSQL (command side) and reads from Elasticsearch\". Eliminate options that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-008",
      "type": "multiple-choice",
      "question": "How can you solve the read-your-writes problem in a CQRS system?",
      "options": [
        "Always read from the command store",
        "Include a version/timestamp and wait or fall back",
        "Disable caching entirely",
        "Use synchronous replication to query store"
      ],
      "correct": 1,
      "explanation": "Pass the write version to the query. If the query store hasn't reached that version, either wait briefly or fall back to the command store for that request. Always reading from command defeats CQRS benefits.",
      "detailedExplanation": "The key clue in this question is \"you solve the read-your-writes problem in a CQRS system\". Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-009",
      "type": "multiple-choice",
      "question": "You use MongoDB for operational data and S3 + Athena for analytics. How do you move data from MongoDB to S3 efficiently?",
      "options": [
        "Direct MongoDB-to-S3 API calls",
        "Real-time CDC streaming every document",
        "Periodic batch exports to S3 in Parquet format",
        "Query MongoDB from Athena directly"
      ],
      "correct": 2,
      "explanation": "Batch exports to columnar formats like Parquet are efficient for analytics. Real-time CDC for analytics is overkill and expensive. Athena can't query MongoDB directly. Parquet enables efficient analytical queries.",
      "detailedExplanation": "Start from \"you use MongoDB for operational data and S3 + Athena for analytics\", then pressure-test the result against the options. Discard storage decisions that optimize one dimension while violating another core requirement. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-010",
      "type": "multiple-choice",
      "question": "Your hybrid architecture uses Redis for sessions, PostgreSQL for user data, and S3 for uploads. A user logs in. Which stores are read from?",
      "options": [
        "Redis only",
        "PostgreSQL only",
        "Redis and PostgreSQL",
        "All three stores"
      ],
      "correct": 2,
      "explanation": "Login reads PostgreSQL for credential verification, then creates/updates a session in Redis. S3 isn't involved in authentication—it stores file uploads, not login data.",
      "detailedExplanation": "The core signal here is \"your hybrid architecture uses Redis for sessions, PostgreSQL for user data, and S3 for\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-011",
      "type": "multiple-choice",
      "question": "What's the main advantage of using a data lake architecture with S3 + Spark over keeping all data in a data warehouse?",
      "options": [
        "Lower query latency",
        "ACID transaction support",
        "Cost-effective storage of diverse data types",
        "Simpler query syntax"
      ],
      "correct": 2,
      "explanation": "Data lakes excel at storing diverse data (structured, semi-structured, unstructured) cost-effectively. Data warehouses typically have higher storage costs and prefer structured data. Lakes trade query latency for flexibility and cost.",
      "detailedExplanation": "If you keep \"what's the main advantage of using a data lake architecture with S3 + Spark over\" in view, the correct answer separates faster. Eliminate options that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-012",
      "type": "multiple-choice",
      "question": "You're implementing a Lambda architecture with Kafka, Spark batch, and Spark Streaming. What's the serving layer's responsibility?",
      "options": [
        "Processing raw events",
        "Running batch computations",
        "Merging batch and speed layer views for queries",
        "Storing raw event streams"
      ],
      "correct": 2,
      "explanation": "The serving layer merges precomputed batch views with real-time speed layer updates to answer queries. Batch processing happens in the batch layer; stream processing in the speed layer.",
      "detailedExplanation": "This prompt is really about \"you're implementing a Lambda architecture with Kafka, Spark batch, and Spark Streaming\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "u4c7-013",
      "type": "multiple-choice",
      "question": "Compared to Lambda architecture, what does Kappa architecture eliminate?",
      "options": [
        "The serving layer",
        "The batch layer",
        "The speed layer",
        "Event streaming"
      ],
      "correct": 1,
      "explanation": "Kappa architecture eliminates the batch layer, treating all data as streams. Historical reprocessing replays events through the stream processor. This simplifies the architecture by having a single code path.",
      "detailedExplanation": "Use \"compared to Lambda architecture, what does Kappa architecture eliminate\" as your starting point, then verify tradeoffs carefully. Eliminate options that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-014",
      "type": "multi-select",
      "question": "Which are valid reasons to use multiple databases in a single application?",
      "options": [
        "Different access patterns require different optimizations",
        "Team preference for specific technologies",
        "Regulatory requirements for data residency",
        "Scaling different workloads independently"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Different access patterns (OLTP vs OLAP, graph queries vs full-text search), regulatory requirements, and independent scaling are valid reasons. Team preference alone isn't sufficient justification for the complexity of polyglot persistence.",
      "detailedExplanation": "Read this as a scenario about \"valid reasons to use multiple databases in a single application\". Treat every option as a separate true/false test under the same constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-015",
      "type": "multiple-choice",
      "question": "Your system writes to PostgreSQL with an outbox table. What reads the outbox?",
      "options": [
        "Application code on each request",
        "A CDC connector like Debezium",
        "PostgreSQL triggers",
        "The client application"
      ],
      "correct": 1,
      "explanation": "A CDC connector (like Debezium) tails the outbox table and publishes events to Kafka. This decouples event publishing from the transaction. Polling from application code works but CDC is more efficient and reliable.",
      "detailedExplanation": "The decision turns on \"your system writes to PostgreSQL with an outbox table\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-016",
      "type": "multiple-choice",
      "question": "You're choosing between Redis and Memcached as a caching layer in front of your database. What does Redis offer that Memcached doesn't?",
      "options": [
        "Faster get/set operations",
        "Data structures (lists, sets, sorted sets)",
        "Distributed caching",
        "Key-value storage"
      ],
      "correct": 1,
      "explanation": "Redis supports rich data structures (lists, sets, sorted sets, hashes, streams) beyond simple key-value. Both support distributed caching and have similar performance for basic operations. Redis is more feature-rich.",
      "detailedExplanation": "Start from \"you're choosing between Redis and Memcached as a caching layer in front of your database\", then pressure-test the result against the options. Reject options that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-017",
      "type": "multiple-choice",
      "question": "Your application uses PostgreSQL for transactions and ClickHouse for analytics. A dashboard query joins user metadata with click events. How should you handle this?",
      "options": [
        "Execute a cross-database join",
        "Replicate user metadata to ClickHouse",
        "Run analytics queries in PostgreSQL",
        "Query both and join in application code"
      ],
      "correct": 1,
      "explanation": "Replicate the user metadata to ClickHouse so analytical queries stay in one system. Cross-database joins don't exist. PostgreSQL isn't optimized for analytics. Application-level joins are inefficient for large datasets.",
      "detailedExplanation": "The key clue in this question is \"your application uses PostgreSQL for transactions and ClickHouse for analytics\". Eliminate options that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-018",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to maintain a materialized view in Elasticsearch that's derived from PostgreSQL data. What architectural pattern fits?",
          "options": [
            "Direct replication",
            "CDC with event processing",
            "Periodic full sync",
            "Two-phase commit"
          ],
          "correct": 1,
          "explanation": "CDC (Change Data Capture) captures changes from PostgreSQL and processes them into the materialized view in Elasticsearch. This provides near real-time updates without coupling the systems.",
          "detailedExplanation": "The core signal here is \"you need to maintain a materialized view in Elasticsearch that's derived from\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements."
        },
        {
          "question": "Your CDC pipeline uses Kafka. The Elasticsearch consumer crashes and restarts. How does it know where to resume?",
          "options": [
            "Starts from the beginning",
            "Consumer offset stored in Kafka",
            "Queries Elasticsearch for last document",
            "Administrator manually specifies offset"
          ],
          "correct": 1,
          "explanation": "Kafka tracks consumer offsets—each consumer group's position in each partition. On restart, the consumer resumes from its last committed offset, ensuring no events are missed or reprocessed (assuming at-least-once semantics).",
          "detailedExplanation": "Use \"your CDC pipeline uses Kafka\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "The core signal here is \"hybrid Architectures\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-019",
      "type": "multiple-choice",
      "question": "What's the purpose of a 'staging' or 'landing' zone in a data lake architecture?",
      "options": [
        "To serve queries to end users",
        "To store raw, unprocessed data before transformation",
        "To cache frequently accessed data",
        "To run real-time analytics"
      ],
      "correct": 1,
      "explanation": "The staging/landing zone holds raw data in its original form before any transformation. This preserves the original data for reprocessing and auditing. Transformed data moves to curated zones.",
      "detailedExplanation": "If you keep \"what's the purpose of a 'staging' or 'landing' zone in a data lake architecture\" in view, the correct answer separates faster. Prefer the choice that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-020",
      "type": "multiple-choice",
      "question": "You use DynamoDB for user sessions and Aurora PostgreSQL for user profiles. A request needs both session state and profile data. How do you minimize latency?",
      "options": [
        "Sequential calls to each database",
        "Parallel calls to both databases",
        "Cache both in Redis",
        "Replicate profiles to DynamoDB"
      ],
      "correct": 1,
      "explanation": "Issue parallel requests to both stores since they're independent reads. This overlaps latency. Caching adds another layer of complexity. Replication duplicates data across systems. Parallel reads are the simplest optimization.",
      "detailedExplanation": "If you keep \"you use DynamoDB for user sessions and Aurora PostgreSQL for user profiles\" in view, the correct answer separates faster. Eliminate options that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-021",
      "type": "multiple-choice",
      "question": "Your system uses an event-driven architecture with Kafka. Services write to their own databases and publish events. What ensures a service's database write and event publish are atomic?",
      "options": [
        "Kafka transactions",
        "Database triggers",
        "The outbox pattern",
        "Two-phase commit"
      ],
      "correct": 2,
      "explanation": "The outbox pattern writes the event to an outbox table in the same database transaction as the data change. A separate process (CDC or polling) publishes events from the outbox to Kafka. This ensures atomicity.",
      "detailedExplanation": "The core signal here is \"your system uses an event-driven architecture with Kafka\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-022",
      "type": "multiple-choice",
      "question": "In a microservices architecture with polyglot persistence, Service A uses PostgreSQL and Service B uses MongoDB. Service A needs data from Service B. What's the preferred approach?",
      "options": [
        "Service A queries Service B's MongoDB directly",
        "Service A calls Service B's API",
        "Replicate Service B's data to PostgreSQL",
        "Use a shared database between services"
      ],
      "correct": 1,
      "explanation": "Services should own their data and expose it through APIs. Direct database access couples services and bypasses business logic. API calls maintain service boundaries and encapsulation.",
      "detailedExplanation": "Use \"in a microservices architecture with polyglot persistence, Service A uses PostgreSQL\" as your starting point, then verify tradeoffs carefully. Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-023",
      "type": "multi-select",
      "question": "Which tools are commonly used for CDC (Change Data Capture) from relational databases?",
      "options": ["Debezium", "AWS DMS", "Kafka Connect", "Redis Streams"],
      "correctIndices": [0, 1, 2],
      "explanation": "Debezium, AWS DMS, and Kafka Connect with appropriate connectors all support CDC from relational databases. Redis Streams is a message queue, not a CDC tool—it doesn't capture database changes.",
      "detailedExplanation": "This prompt is really about \"tools are commonly used for CDC (Change Data Capture) from relational databases\". Validate each option independently; do not select statements that are only partially true. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-024",
      "type": "multiple-choice",
      "question": "You're implementing write-through caching with Redis and PostgreSQL. What happens on a write?",
      "options": [
        "Write to Redis, async sync to PostgreSQL",
        "Write to PostgreSQL, then update Redis",
        "Write to both simultaneously in a transaction",
        "Write to PostgreSQL, invalidate Redis"
      ],
      "correct": 1,
      "explanation": "Write-through caching writes to the primary store (PostgreSQL) first, then updates the cache (Redis). This ensures the cache always reflects the database. Write-behind (option A) risks data loss; invalidation is cache-aside.",
      "detailedExplanation": "The decision turns on \"you're implementing write-through caching with Redis and PostgreSQL\". Reject options that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-025",
      "type": "multiple-choice",
      "question": "Your analytics system uses a data lakehouse (Delta Lake on S3). What does the lakehouse add over a plain data lake?",
      "options": [
        "Object storage capabilities",
        "ACID transactions and schema enforcement",
        "Lower storage costs",
        "Faster S3 API operations"
      ],
      "correct": 1,
      "explanation": "Lakehouse formats like Delta Lake add ACID transactions, schema enforcement, and time travel on top of object storage. Plain data lakes lack these guarantees, making updates and consistency harder.",
      "detailedExplanation": "Read this as a scenario about \"your analytics system uses a data lakehouse (Delta Lake on S3)\". Discard choices that violate required invariants during concurrent or failed states. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-026",
      "type": "multiple-choice",
      "question": "You have a hot tier (SSD-backed) and cold tier (S3) for time-series data. What determines when data moves between tiers?",
      "options": [
        "Manual operator commands",
        "Data age policies",
        "Query frequency analysis",
        "Storage capacity alerts"
      ],
      "correct": 1,
      "explanation": "Time-series systems typically use age-based policies: data older than X days moves to the cold tier. This is predictable and matches access patterns—recent data is queried more often. Query frequency can enhance but age is primary.",
      "detailedExplanation": "The key clue in this question is \"you have a hot tier (SSD-backed) and cold tier (S3) for time-series data\". Prefer the choice that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-027",
      "type": "ordering",
      "question": "Order the tiers in a typical data warehouse tiered storage from fastest to cheapest:",
      "items": [
        "Archive (Glacier)",
        "Hot (SSD)",
        "Cold (S3 Standard)",
        "Warm (S3 IA)"
      ],
      "correctOrder": [1, 3, 2, 0],
      "explanation": "From fastest to cheapest: Hot (SSD) → Warm (S3 Infrequent Access) → Cold (S3 Standard) → Archive (Glacier). Hot is fastest but most expensive; Glacier is cheapest but has retrieval delays.",
      "detailedExplanation": "Start from \"order the tiers in a typical data warehouse tiered storage from fastest to cheapest:\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-028",
      "type": "multiple-choice",
      "question": "Your system caches database query results in Redis with a 5-minute TTL. A write occurs at minute 2. When does the cache reflect the change?",
      "options": [
        "Immediately",
        "At minute 5",
        "At minute 7",
        "Never, until explicit invalidation"
      ],
      "correct": 2,
      "explanation": "With TTL-based caching and no active invalidation, the stale data remains until the TTL expires. The cache was set at minute 0, expires at minute 5, so the next read after minute 5 fetches fresh data. That's up to minute 7 (5-minute TTL from minute 2 write doesn't apply—TTL was set at cache time).",
      "detailedExplanation": "If you keep \"your system caches database query results in Redis with a 5-minute TTL\" in view, the correct answer separates faster. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 5 and 2 appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-029",
      "type": "multiple-choice",
      "question": "You're building a search-as-you-type feature. The data is in PostgreSQL. What hybrid approach provides sub-100ms autocomplete?",
      "options": [
        "PostgreSQL LIKE queries with indexes",
        "Full-text search in PostgreSQL",
        "Elasticsearch with prefix queries",
        "Redis sorted sets with score-based ranking"
      ],
      "correct": 2,
      "explanation": "Elasticsearch with prefix/edge-ngram queries provides fast autocomplete. PostgreSQL LIKE isn't optimized for prefix matching at scale. Redis sorted sets work for simple cases but lack full-text relevance. Elasticsearch is purpose-built for search.",
      "detailedExplanation": "The core signal here is \"you're building a search-as-you-type feature\". Reject options that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. If values like 100ms appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-030",
      "type": "multi-select",
      "question": "Which are challenges when running both OLTP and OLAP workloads on the same database?",
      "options": [
        "Analytical queries consuming resources needed for transactions",
        "Different indexing strategies for each workload",
        "Lock contention between reads and writes",
        "Schema optimized for one pattern hurts the other"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are challenges: long analytical queries compete with transactions for CPU/memory/IO, OLTP wants B-tree indexes while OLAP prefers columnar storage, locks from analytical scans can block writes, and normalized schemas hurt analytical queries while denormalized schemas hurt transactions.",
      "detailedExplanation": "This prompt is really about \"challenges when running both OLTP and OLAP workloads on the same database\". Validate each option independently; do not select statements that are only partially true. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "u4c7-031",
      "type": "multiple-choice",
      "question": "Your e-commerce site uses PostgreSQL for orders and Redis for inventory counts. A purchase must decrement inventory and create an order atomically. What pattern helps?",
      "options": [
        "Distributed transaction with XA",
        "Saga with compensating transactions",
        "Write to both from the same thread",
        "Eventual consistency with retries"
      ],
      "correct": 1,
      "explanation": "The Saga pattern coordinates across stores: decrement inventory in Redis, create order in PostgreSQL, and if either fails, execute compensating actions (restore inventory, delete order). XA across Redis and PostgreSQL isn't practical.",
      "detailedExplanation": "Use \"your e-commerce site uses PostgreSQL for orders and Redis for inventory counts\" as your starting point, then verify tradeoffs carefully. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-032",
      "type": "multiple-choice",
      "question": "A saga for order creation fails at the payment step. What happens to the inventory that was reserved in an earlier step?",
      "options": [
        "It stays reserved permanently",
        "A compensating transaction releases it",
        "It's automatically rolled back",
        "The database handles it"
      ],
      "correct": 1,
      "explanation": "Sagas use compensating transactions to undo previous steps. If payment fails, a compensation releases the reserved inventory. There's no automatic rollback—each step's compensation must be explicitly defined and executed.",
      "detailedExplanation": "The core signal here is \"saga for order creation fails at the payment step\". Prefer the choice that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-033",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're migrating from a monolithic PostgreSQL database to microservices with separate databases per service. What's this pattern called?",
          "options": [
            "Strangler fig",
            "Database per service",
            "Event sourcing",
            "CQRS"
          ],
          "correct": 1,
          "explanation": "Database per service is the microservices pattern where each service owns its data in a dedicated database. This enables independent scaling, technology choice, and deployment.",
          "detailedExplanation": "The key clue in this question is \"you're migrating from a monolithic PostgreSQL database to microservices with separate\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "During migration, some queries need data from both the old monolith and new services. What helps?",
          "options": [
            "Distributed joins across databases",
            "API composition or data replication",
            "Rolling back to monolith",
            "Forcing users to wait"
          ],
          "correct": 1,
          "explanation": "API composition aggregates data from multiple services at the API layer. Alternatively, replicate needed data using events. Distributed joins across different databases aren't supported. This is a transitional complexity of migration.",
          "detailedExplanation": "Read this as a scenario about \"during migration, some queries need data from both the old monolith and new services\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "If you keep \"hybrid Architectures\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-034",
      "type": "multiple-choice",
      "question": "Your system writes to PostgreSQL and publishes events to Kafka. Without the outbox pattern, what can go wrong?",
      "options": [
        "Events are published but database write fails",
        "Both always succeed or fail together",
        "Kafka becomes a bottleneck",
        "PostgreSQL performance degrades"
      ],
      "correct": 0,
      "explanation": "Without outbox, the database write and Kafka publish are separate operations. The publish might succeed before the commit, and if the commit fails, the event was already sent. Or the write succeeds but publish fails, losing the event.",
      "detailedExplanation": "Start from \"your system writes to PostgreSQL and publishes events to Kafka\", then pressure-test the result against the options. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-035",
      "type": "multiple-choice",
      "question": "You use Elasticsearch for product search and PostgreSQL for product data. Products are updated frequently. What's the risk with periodic bulk sync from PostgreSQL to Elasticsearch?",
      "options": [
        "Higher consistency during sync",
        "Search results show stale data between syncs",
        "PostgreSQL query load decreases",
        "Elasticsearch indexes become smaller"
      ],
      "correct": 1,
      "explanation": "Periodic sync means Elasticsearch lags behind PostgreSQL between sync intervals. Users might search for a product that was just updated but see old data. CDC provides near real-time sync to minimize this window.",
      "detailedExplanation": "The key clue in this question is \"you use Elasticsearch for product search and PostgreSQL for product data\". Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-036",
      "type": "multiple-choice",
      "question": "Your mobile app stores data in SQLite locally and syncs to a cloud database. A user makes changes offline, then another device makes conflicting changes. What resolves this?",
      "options": [
        "Last write wins",
        "First write wins",
        "Conflict resolution rules or user choice",
        "Reject the sync"
      ],
      "correct": 2,
      "explanation": "Conflict resolution strategies include: last-write-wins (data loss risk), merge rules (for compatible changes), or prompting the user to choose. The right approach depends on the data semantics. Generic LWW can lose important changes.",
      "detailedExplanation": "Read this as a scenario about \"your mobile app stores data in SQLite locally and syncs to a cloud database\". Prefer the choice that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-037",
      "type": "multiple-choice",
      "question": "What's the main benefit of using a query engine like Trino (Presto) with a data lake?",
      "options": [
        "It provides ACID transactions on S3",
        "It enables SQL queries across multiple data sources",
        "It automatically optimizes S3 storage costs",
        "It replaces the need for ETL"
      ],
      "correct": 1,
      "explanation": "Trino is a federated query engine—it runs SQL across different data sources (S3, databases, data warehouses) without moving data. It doesn't provide ACID (that's lakehouse formats) or replace ETL for transformation.",
      "detailedExplanation": "The decision turns on \"what's the main benefit of using a query engine like Trino (Presto) with a data lake\". Eliminate options that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-038",
      "type": "multiple-choice",
      "question": "You run PostgreSQL for OLTP and Redshift for OLAP. ETL jobs sync data nightly. A business user wants real-time dashboards. What architectural change helps?",
      "options": [
        "Run dashboards against PostgreSQL",
        "Increase ETL job frequency to hourly",
        "Add streaming ETL via CDC to Redshift",
        "Cache dashboard queries in Redis"
      ],
      "correct": 2,
      "explanation": "CDC-based streaming ETL continuously replicates changes to Redshift, enabling near real-time analytics. Hourly ETL still has lag. PostgreSQL isn't optimized for analytical queries. Caching doesn't solve the data freshness problem.",
      "detailedExplanation": "This prompt is really about \"you run PostgreSQL for OLTP and Redshift for OLAP\". Eliminate options that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-039",
      "type": "multi-select",
      "question": "Which strategies help manage data consistency across multiple databases in a microservices architecture?",
      "options": [
        "Eventual consistency with event-driven updates",
        "Saga pattern for distributed transactions",
        "API composition for read operations",
        "Shared database between services"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Eventual consistency, sagas, and API composition all manage consistency while maintaining service autonomy. Shared databases defeat the purpose of database-per-service and create coupling between services.",
      "detailedExplanation": "Use \"strategies help manage data consistency across multiple databases in a microservices\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-040",
      "type": "multiple-choice",
      "question": "Your search service uses Elasticsearch. When the search cluster is down, what fallback provides degraded but functional search?",
      "options": [
        "Return empty results",
        "Queue searches for later",
        "Fall back to database LIKE queries",
        "Cache the last known good results forever"
      ],
      "correct": 2,
      "explanation": "Falling back to database queries (though slower and less feature-rich) provides degraded service. Returning empty results or queueing provides no immediate value. Stale cached results are limited to previously-searched terms.",
      "detailedExplanation": "Use \"your search service uses Elasticsearch\" as your starting point, then verify tradeoffs carefully. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-041",
      "type": "multiple-choice",
      "question": "You use a write-behind cache. What's the main risk compared to write-through?",
      "options": [
        "Higher latency on writes",
        "Data loss if the cache fails before sync",
        "Cache never gets updated",
        "Database gets overloaded"
      ],
      "correct": 1,
      "explanation": "Write-behind (write-back) caches writes and asynchronously syncs to the database. If the cache fails before syncing, data is lost. Write-through immediately writes to both, avoiding data loss but with higher write latency.",
      "detailedExplanation": "This prompt is really about \"you use a write-behind cache\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-042",
      "type": "multiple-choice",
      "question": "Your real-time analytics dashboard shows order counts. Data flows: PostgreSQL → Kafka → Flink → Redis → Dashboard. Flink applies what operation?",
      "options": [
        "Store raw events",
        "Aggregate and compute metrics",
        "Query PostgreSQL",
        "Serve the dashboard"
      ],
      "correct": 1,
      "explanation": "Flink is a stream processor that transforms and aggregates events—computing metrics like order counts in real-time windows. PostgreSQL is the source, Kafka transports, Flink processes, Redis serves the computed results.",
      "detailedExplanation": "If you keep \"your real-time analytics dashboard shows order counts\" in view, the correct answer separates faster. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-043",
      "type": "ordering",
      "question": "Order the data flow for real-time analytics on database changes:",
      "items": [
        "Dashboard reads from serving store",
        "Stream processor aggregates events",
        "CDC captures database changes",
        "Events flow through message queue"
      ],
      "correctOrder": [2, 3, 1, 0],
      "explanation": "CDC captures changes → Message queue (Kafka) transports events → Stream processor (Flink/Spark) aggregates → Dashboard reads from serving store (Redis/Druid). This is the typical streaming analytics pipeline.",
      "detailedExplanation": "The core signal here is \"order the data flow for real-time analytics on database changes:\". Order by relative scale and bottleneck effect, then validate neighboring items. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-044",
      "type": "multiple-choice",
      "question": "You're implementing multi-master replication between two PostgreSQL clusters in different regions. What's the primary consistency challenge?",
      "options": [
        "Both clusters going offline",
        "Conflicting concurrent writes to the same data",
        "Different schema versions",
        "Network bandwidth between regions"
      ],
      "correct": 1,
      "explanation": "Multi-master allows writes to any cluster, so the same row might be updated in both regions simultaneously. Conflict resolution is needed—last-write-wins, custom merge, or conflict detection with manual resolution.",
      "detailedExplanation": "The key clue in this question is \"you're implementing multi-master replication between two PostgreSQL clusters in\". Eliminate approaches that hand-wave conflict resolution or quorum behavior. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-045",
      "type": "multiple-choice",
      "question": "Your API gateway caches responses in Redis. The backend uses PostgreSQL. A POST request modifies data. What should the gateway do to the cache?",
      "options": [
        "Nothing—the cache will expire eventually",
        "Invalidate related cached responses",
        "Update the cache with the POST response",
        "Clear the entire cache"
      ],
      "correct": 1,
      "explanation": "The gateway should invalidate cached responses affected by the write. Waiting for TTL expiry serves stale data. Updating cache requires knowing the response format. Clearing everything is overly aggressive. Targeted invalidation balances correctness and performance.",
      "detailedExplanation": "Start from \"your API gateway caches responses in Redis\", then pressure-test the result against the options. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your application uses PostgreSQL for persistence and Redis for caching. You want to ensure cache consistency without significant latency. What pattern should you use?",
          "options": [
            "Write-through caching",
            "Cache-aside with invalidation",
            "Write-behind caching",
            "Read-through caching only"
          ],
          "correct": 1,
          "explanation": "Cache-aside with invalidation: write to the database, invalidate the cache. The next read populates the cache. This avoids the overhead of updating cache on every write while maintaining consistency.",
          "detailedExplanation": "Use \"your application uses PostgreSQL for persistence and Redis for caching\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes."
        },
        {
          "question": "With cache-aside, a race condition can occur. Read happens between write and invalidation. What's the result?",
          "options": [
            "Read blocks until invalidation completes",
            "Cache might contain stale data until next invalidation or TTL",
            "Read sees the new data",
            "Transaction rolls back"
          ],
          "correct": 1,
          "explanation": "If a read occurs after the database write but before cache invalidation, it might repopulate the cache with old data from a concurrent read. This stale entry persists until TTL or another invalidation. This is a known limitation of cache-aside.",
          "detailedExplanation": "The core signal here is \"with cache-aside, a race condition can occur\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load."
        }
      ],
      "detailedExplanation": "The decision turns on \"hybrid Architectures\". Solve this as chained reasoning where stage two must respect stage one assumptions. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-047",
      "type": "multiple-choice",
      "question": "You use event sourcing where events are in Kafka and materialized views are in PostgreSQL. How do you rebuild a view from scratch?",
      "options": [
        "Export PostgreSQL and reimport",
        "Replay events from Kafka from the beginning",
        "Query the current state and work backwards",
        "Request a snapshot from Kafka"
      ],
      "correct": 1,
      "explanation": "Event sourcing stores all state changes as events. To rebuild a materialized view, replay all events from the beginning (or from a snapshot if available). This is a key benefit of event sourcing—views are derived and reproducible.",
      "detailedExplanation": "Read this as a scenario about \"you use event sourcing where events are in Kafka and materialized views are in\". Reject designs that improve throughput while weakening reliability guarantees. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-048",
      "type": "multiple-choice",
      "question": "Your company uses PostgreSQL, MongoDB, and Elasticsearch. New engineers struggle to understand which store to use for what. What helps?",
      "options": [
        "Use only one database",
        "Document data ownership and access patterns per store",
        "Let each team choose freely",
        "Migrate everything to a single store"
      ],
      "correct": 1,
      "explanation": "Documentation of which data belongs in which store, why, and how they're synced reduces cognitive load. Consolidating to one store loses the benefits of polyglot persistence. Undocumented freedom leads to inconsistency.",
      "detailedExplanation": "Use \"your company uses PostgreSQL, MongoDB, and Elasticsearch\" as your starting point, then verify tradeoffs carefully. Prefer the choice that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-049",
      "type": "multiple-choice",
      "question": "A data mesh architecture advocates for decentralized data ownership. What does each domain team provide?",
      "options": [
        "Raw database access to other teams",
        "Data as a product with defined interfaces",
        "ETL jobs to the central warehouse",
        "Schemas for a shared database"
      ],
      "correct": 1,
      "explanation": "Data mesh treats data as a product—each domain owns and serves its data through defined APIs or data contracts, rather than dumping to a central warehouse. This decentralizes ownership while maintaining discoverability.",
      "detailedExplanation": "This prompt is really about \"data mesh architecture advocates for decentralized data ownership\". Eliminate options that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-050",
      "type": "multiple-choice",
      "question": "You need to run complex joins across data in PostgreSQL and Cassandra. What approach works?",
      "options": [
        "Cross-database SQL join",
        "Federation query engine like Trino",
        "Cassandra native joins",
        "Stored procedures spanning databases"
      ],
      "correct": 1,
      "explanation": "Trino (Presto) is a federated query engine that can query multiple data sources and join across them. Cassandra doesn't support joins. Cross-database SQL doesn't exist. Stored procedures can't span different database systems.",
      "detailedExplanation": "Read this as a scenario about \"you need to run complex joins across data in PostgreSQL and Cassandra\". Discard modeling choices that look clean but perform poorly for the target queries. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-051",
      "type": "multi-select",
      "question": "Which factors should influence your decision to add a specialized database to your architecture?",
      "options": [
        "Current store can't meet latency requirements",
        "Access pattern is fundamentally different",
        "Team wants to learn a new technology",
        "Significant cost savings for specific workload"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Valid reasons: latency requirements current store can't meet, fundamentally different access patterns (graph vs relational), and cost savings. Learning a new technology isn't sufficient justification for operational complexity.",
      "detailedExplanation": "The decision turns on \"factors should influence your decision to add a specialized database to your\". Treat every option as a separate true/false test under the same constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-052",
      "type": "multiple-choice",
      "question": "Your feature store uses Redis for online serving and S3/Parquet for offline training. What ensures both have the same feature values?",
      "options": [
        "Both are readonly",
        "Shared feature computation pipeline",
        "Manual synchronization",
        "Redis replicates to S3"
      ],
      "correct": 1,
      "explanation": "A unified feature computation pipeline writes to both stores, ensuring training and serving use identical feature values. This prevents training-serving skew. Different pipelines for each store can drift.",
      "detailedExplanation": "Start from \"your feature store uses Redis for online serving and S3/Parquet for offline training\", then pressure-test the result against the options. Reject options that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-053",
      "type": "multiple-choice",
      "question": "Your CDC pipeline from PostgreSQL to Elasticsearch occasionally produces out-of-order updates. A newer update arrives before an older one. What can happen?",
      "options": [
        "Elasticsearch rejects the older update",
        "Older update overwrites newer data",
        "Both updates merge automatically",
        "Pipeline automatically reorders"
      ],
      "correct": 1,
      "explanation": "Without version checking, the older update can overwrite newer data if it arrives later (out of order). Solutions include using version numbers (reject updates with older versions) or timestamps for conflict resolution.",
      "detailedExplanation": "The key clue in this question is \"your CDC pipeline from PostgreSQL to Elasticsearch occasionally produces out-of-order\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-054",
      "type": "multiple-choice",
      "question": "How do you prevent stale writes from overwriting newer data in a CDC pipeline?",
      "options": [
        "Process events in strict order",
        "Include a version/timestamp and reject older versions",
        "Use exactly-once delivery",
        "Ignore the problem—it's rare"
      ],
      "correct": 1,
      "explanation": "Include a version or timestamp with each event. The consumer checks if the incoming version is newer than the current version before applying. This handles out-of-order delivery gracefully. Strict ordering is often impractical.",
      "detailedExplanation": "The core signal here is \"you prevent stale writes from overwriting newer data in a CDC pipeline\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-055",
      "type": "multiple-choice",
      "question": "Your GraphQL API aggregates data from PostgreSQL (users), MongoDB (posts), and Redis (sessions). This is an example of what pattern?",
      "options": [
        "Database per service",
        "CQRS",
        "API composition",
        "Event sourcing"
      ],
      "correct": 2,
      "explanation": "API composition aggregates data from multiple backend sources into a unified API response. The GraphQL layer resolves fields from different stores and composes the response. This is distinct from CQRS (separate read/write models).",
      "detailedExplanation": "If you keep \"your GraphQL API aggregates data from PostgreSQL (users), MongoDB (posts), and Redis\" in view, the correct answer separates faster. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-056",
      "type": "multiple-choice",
      "question": "In a GraphQL federation architecture, what's the role of the gateway?",
      "options": [
        "Store all data centrally",
        "Route queries to appropriate subgraphs and merge responses",
        "Cache all database queries",
        "Manage database connections"
      ],
      "correct": 1,
      "explanation": "The federation gateway routes parts of a query to the appropriate subgraph services and merges their responses. Each subgraph owns its portion of the schema and data. The gateway orchestrates composition.",
      "detailedExplanation": "This prompt is really about \"in a GraphQL federation architecture, what's the role of the gateway\". Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your system stores events in Kafka and maintains a materialized view in PostgreSQL. What happens if the PostgreSQL consumer falls significantly behind?",
          "options": [
            "Events are lost",
            "The view becomes increasingly stale",
            "Kafka automatically catches up",
            "PostgreSQL crashes"
          ],
          "correct": 1,
          "explanation": "Consumer lag means the materialized view falls behind real-time. Events aren't lost (Kafka retains them), but queries against PostgreSQL see stale data. The gap between event time and view reflects the lag.",
          "detailedExplanation": "The decision turns on \"your system stores events in Kafka and maintains a materialized view in PostgreSQL\". Solve this as chained reasoning where stage two must respect stage one assumptions. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates."
        },
        {
          "question": "How do you monitor and address consumer lag?",
          "options": [
            "Ignore it—Kafka handles everything",
            "Monitor lag metrics, scale consumers or optimize processing",
            "Restart the consumer periodically",
            "Reduce the number of events"
          ],
          "correct": 1,
          "explanation": "Monitor consumer lag (events behind) via Kafka metrics. Address by scaling consumer instances (parallel processing), optimizing consumer code, or batching writes. Lag indicates processing can't keep up with production.",
          "detailedExplanation": "Start from \"you monitor and address consumer lag\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Use \"hybrid Architectures\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-058",
      "type": "multiple-choice",
      "question": "You're implementing a shopping cart. Cart data is in Redis for speed. Orders are in PostgreSQL for durability. When does cart data move to PostgreSQL?",
      "options": [
        "Continuously synced",
        "On checkout/order creation",
        "Never—they stay separate",
        "Every 5 minutes"
      ],
      "correct": 1,
      "explanation": "Cart data moves to PostgreSQL as an order on checkout. Before checkout, carts are ephemeral (Redis is fast, eventual loss is acceptable). On checkout, the order must be durably stored in PostgreSQL. This is a common hybrid pattern.",
      "detailedExplanation": "Read this as a scenario about \"you're implementing a shopping cart\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-059",
      "type": "multiple-choice",
      "question": "Your message queue (Kafka) sits between PostgreSQL CDC and Elasticsearch consumers. What happens if Elasticsearch is down for an hour?",
      "options": [
        "Events are lost",
        "Events queue in Kafka until consumer resumes",
        "PostgreSQL stops accepting writes",
        "CDC stops capturing changes"
      ],
      "correct": 1,
      "explanation": "Kafka retains events based on retention policy (typically days). When the Elasticsearch consumer comes back, it resumes from its last committed offset, processing queued events. No data is lost (assuming retention > downtime).",
      "detailedExplanation": "The decision turns on \"your message queue (Kafka) sits between PostgreSQL CDC and Elasticsearch consumers\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-060",
      "type": "multiple-choice",
      "question": "You're designing a globally distributed system. User data is in PostgreSQL with multi-region replication. Session data is in Redis. What's the challenge for a user traveling between regions?",
      "options": [
        "PostgreSQL can't replicate",
        "Session might not exist in the new region",
        "User data is lost",
        "The application crashes"
      ],
      "correct": 1,
      "explanation": "If Redis sessions are region-local without cross-region replication, a user routed to a different region won't have their session. Solutions: global session store, session data in cookie, or sticky routing to origin region.",
      "detailedExplanation": "The decision turns on \"you're designing a globally distributed system\". Reject options that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-061",
      "type": "multi-select",
      "question": "Which approaches help with cross-region session consistency?",
      "options": [
        "Global Redis cluster with replication",
        "Session data in signed JWT cookies",
        "Sticky sessions to user's home region",
        "Separate session store per region without sync"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Global Redis replication, JWT cookies (stateless sessions), and sticky routing all ensure session availability across regions. Separate stores without sync means sessions don't follow users—they'd be logged out on region change.",
      "detailedExplanation": "Read this as a scenario about \"approaches help with cross-region session consistency\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-062",
      "type": "multiple-choice",
      "question": "Your hybrid architecture uses PostgreSQL and Snowflake. To minimize data movement, where should you run a query that joins real-time orders (PostgreSQL) with historical analytics (Snowflake)?",
      "options": [
        "In PostgreSQL by importing Snowflake data",
        "In Snowflake by importing PostgreSQL data",
        "In a federated query engine",
        "Export both to a third location and join there"
      ],
      "correct": 2,
      "explanation": "A federated query engine (Trino) can join across both sources in place, minimizing data movement. Importing into either database duplicates data and adds latency. Federation pushes computation to the data where possible.",
      "detailedExplanation": "The key clue in this question is \"your hybrid architecture uses PostgreSQL and Snowflake\". Discard modeling choices that look clean but perform poorly for the target queries. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-063",
      "type": "multiple-choice",
      "question": "You're migrating a monolithic MySQL database to microservices. What's the strangler fig pattern?",
      "options": [
        "Migrate all data at once",
        "Gradually route traffic to new services while old system runs",
        "Replicate MySQL to multiple copies",
        "Run old and new systems with dual writes"
      ],
      "correct": 1,
      "explanation": "Strangler fig gradually replaces the old system by routing functionality to new services piece by piece. The old system continues running, handling unmigrated parts. Over time, the new services 'strangle' the monolith until it's decommissioned.",
      "detailedExplanation": "Start from \"you're migrating a monolithic MySQL database to microservices\", then pressure-test the result against the options. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-064",
      "type": "multiple-choice",
      "question": "During strangler fig migration, both old MySQL and new PostgreSQL service have user data. How do you keep them in sync?",
      "options": [
        "They don't need to sync",
        "Dual writes from the application",
        "CDC from MySQL to new service, or vice versa",
        "Periodic manual reconciliation"
      ],
      "correct": 2,
      "explanation": "CDC syncs data between old and new systems during migration. The primary can be either system depending on migration phase. Dual writes risk inconsistency. Eventually, the old system is decommissioned and sync stops.",
      "detailedExplanation": "If you keep \"during strangler fig migration, both old MySQL and new PostgreSQL service have user data\" in view, the correct answer separates faster. Prefer the choice that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-065",
      "type": "ordering",
      "question": "Order the phases of a typical strangler fig database migration:",
      "items": [
        "Decommission old database",
        "Run both systems with sync",
        "Build new service with its database",
        "Route all traffic to new service"
      ],
      "correctOrder": [2, 1, 3, 0],
      "explanation": "Build new service → Run both with sync (test, validate) → Route traffic to new service (incremental) → Decommission old database. This minimizes risk through incremental cutover.",
      "detailedExplanation": "The core signal here is \"order the phases of a typical strangler fig database migration:\". Build the rank from biggest differences first, then refine with adjacent checks. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-066",
      "type": "multiple-choice",
      "question": "Your data warehouse (Redshift) receives data from PostgreSQL (OLTP), MongoDB (product catalog), and S3 (clickstream). What typically orchestrates these ETL jobs?",
      "options": [
        "Manual scheduling",
        "Database triggers",
        "Workflow orchestrator like Airflow",
        "Each source pushes on its own schedule"
      ],
      "correct": 2,
      "explanation": "Airflow (or similar orchestrators like Dagster, Prefect) schedules and coordinates ETL jobs, managing dependencies, retries, and monitoring. Manual scheduling doesn't scale. Database triggers can't coordinate across different sources.",
      "detailedExplanation": "Use \"your data warehouse (Redshift) receives data from PostgreSQL (OLTP), MongoDB (product\" as your starting point, then verify tradeoffs carefully. Prefer the choice that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-067",
      "type": "multiple-choice",
      "question": "You have hot data in PostgreSQL and archive data in S3. A query needs to search across both. What pattern helps?",
      "options": [
        "Import S3 data to PostgreSQL for the query",
        "Archive queries aren't supported",
        "Use a federated query engine or partitioned view",
        "Keep all data in PostgreSQL"
      ],
      "correct": 2,
      "explanation": "Federated query engines (Trino) or partitioned views that span both hot (PostgreSQL) and cold (S3) tiers enable seamless querying. PostgreSQL foreign data wrappers can also mount S3 data. Importing defeats the purpose of archiving.",
      "detailedExplanation": "This prompt is really about \"you have hot data in PostgreSQL and archive data in S3\". Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-068",
      "type": "multiple-choice",
      "question": "Your leaderboard uses Redis sorted sets for rankings. The underlying scores come from PostgreSQL. How often should you update Redis?",
      "options": [
        "Never—compute from PostgreSQL on each request",
        "Based on business requirements for freshness",
        "Exactly once per day",
        "Only when users ask"
      ],
      "correct": 1,
      "explanation": "Update frequency depends on business needs. A real-time game needs sub-second updates (event-driven). A weekly leaderboard needs daily updates. Match sync frequency to how stale data affects user experience.",
      "detailedExplanation": "The decision turns on \"your leaderboard uses Redis sorted sets for rankings\". Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "u4c7-069",
      "type": "multiple-choice",
      "question": "Your architecture has: PostgreSQL (source of truth) → Kafka (event bus) → Elasticsearch (search). Elasticsearch index gets corrupted. How do you rebuild it?",
      "options": [
        "Restore from Elasticsearch backup",
        "Replay events from Kafka",
        "Query PostgreSQL and reindex",
        "Wait for CDC to naturally rebuild"
      ],
      "correct": 2,
      "explanation": "For full rebuilds, query PostgreSQL directly—it's the source of truth. Kafka has retention limits and replaying all history might be slow or impossible. ES backups work if available. CDC only captures changes, not full state.",
      "detailedExplanation": "Read this as a scenario about \"your architecture has: PostgreSQL (source of truth) → Kafka (event bus) → Elasticsearch\". Eliminate options that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-070",
      "type": "multi-select",
      "question": "Which are benefits of separating analytical workloads from operational databases?",
      "options": [
        "Analytical queries don't impact OLTP performance",
        "Can optimize storage format for each workload",
        "Simpler schema management",
        "Can scale analytical compute independently"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Separating workloads: prevents analytical queries from impacting OLTP, allows format optimization (row vs columnar), and enables independent scaling. Schema management actually becomes more complex with multiple systems to sync.",
      "detailedExplanation": "Start from \"benefits of separating analytical workloads from operational databases\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-071",
      "type": "multiple-choice",
      "question": "You use a cache-aside pattern. A cache miss occurs. What's the sequence of operations?",
      "options": [
        "Check cache → Return miss → User retries",
        "Check cache → Miss → Query database → Return → Populate cache",
        "Check database → Populate cache → Return from cache",
        "Check cache → Miss → Return empty"
      ],
      "correct": 1,
      "explanation": "Cache-aside on miss: check cache (miss) → query database → return to caller → populate cache for future reads. The application is responsible for cache population, not the cache system.",
      "detailedExplanation": "The key clue in this question is \"you use a cache-aside pattern\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-072",
      "type": "multiple-choice",
      "question": "Read-through caching differs from cache-aside how?",
      "options": [
        "Read-through only caches writes",
        "In read-through, the cache fetches from the database on miss",
        "Cache-aside doesn't cache reads",
        "They're the same pattern"
      ],
      "correct": 1,
      "explanation": "In read-through, the cache system itself loads from the database on miss—the application always reads from the cache. In cache-aside, the application manages both cache and database reads. Read-through provides a simpler API.",
      "detailedExplanation": "Read this as a scenario about \"read-through caching differs from cache-aside how\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-073",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're building a notification system. User preferences are in PostgreSQL. Real-time notification state is in Redis. What data goes in each?",
          "options": [
            "All notification data in PostgreSQL",
            "All notification data in Redis",
            "Preferences in PostgreSQL, unread counts/badges in Redis",
            "Preferences in Redis, notification history in PostgreSQL"
          ],
          "correct": 2,
          "explanation": "Persistent preferences (email settings, do-not-disturb) belong in PostgreSQL. Ephemeral, frequently-updated state (unread count, badge numbers) benefits from Redis speed. History should be in PostgreSQL for durability.",
          "detailedExplanation": "Use \"you're building a notification system\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates."
        },
        {
          "question": "A notification is marked as read. How do both stores get updated?",
          "options": [
            "Update Redis only",
            "Update PostgreSQL, let CDC update Redis",
            "Update PostgreSQL and Redis in the same request",
            "Don't track read status"
          ],
          "correct": 2,
          "explanation": "Update both: PostgreSQL for durable record, Redis for real-time badge count. Dual writes in the same request ensure consistency. CDC adds latency for a real-time feature. Redis-only loses the read state on cache eviction.",
          "detailedExplanation": "The core signal here is \"notification is marked as read\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load."
        }
      ],
      "detailedExplanation": "The decision turns on \"hybrid Architectures\". Do not reset assumptions between stages; carry forward prior constraints directly. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-074",
      "type": "multiple-choice",
      "question": "Your billing system writes invoices to PostgreSQL. For reporting, you replicate to BigQuery. What type of replication minimizes impact on the OLTP database?",
      "options": [
        "Synchronous replication",
        "Logical replication from replica, not primary",
        "Full table scans every hour",
        "Triggers that write to BigQuery"
      ],
      "correct": 1,
      "explanation": "Set up logical replication from a PostgreSQL read replica to BigQuery. This offloads the CDC overhead from the primary. Full scans impact performance. Triggers add write latency. Sync replication adds latency to every transaction.",
      "detailedExplanation": "This prompt is really about \"your billing system writes invoices to PostgreSQL\". Discard choices that violate required invariants during concurrent or failed states. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-075",
      "type": "multiple-choice",
      "question": "You want to add full-text search to your PostgreSQL-backed application. What's the simplest hybrid approach?",
      "options": [
        "Add Elasticsearch and sync via CDC",
        "Use PostgreSQL's built-in full-text search",
        "Replace PostgreSQL with Elasticsearch",
        "Build a custom search index"
      ],
      "correct": 1,
      "explanation": "PostgreSQL has built-in full-text search (tsvector, tsquery, GIN indexes). Start here—it avoids the complexity of syncing a separate system. Only add Elasticsearch when PostgreSQL's features are insufficient for your needs.",
      "detailedExplanation": "Use \"you want to add full-text search to your PostgreSQL-backed application\" as your starting point, then verify tradeoffs carefully. Prefer the choice that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-076",
      "type": "multiple-choice",
      "question": "When does PostgreSQL full-text search become insufficient, justifying Elasticsearch?",
      "options": [
        "More than 100 documents",
        "Need for complex relevance ranking, faceting, or suggestions",
        "PostgreSQL version is too old",
        "Full-text search is never sufficient"
      ],
      "correct": 1,
      "explanation": "Elasticsearch excels at complex relevance scoring, faceted search, autocomplete, fuzzy matching, and aggregations. If you need these features and PostgreSQL's FTS can't deliver, Elasticsearch is justified. Size alone isn't the factor.",
      "detailedExplanation": "The core signal here is \"postgreSQL full-text search become insufficient, justifying Elasticsearch\". Prefer the choice that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-077",
      "type": "multiple-choice",
      "question": "Your ML feature store computes features in a nightly batch job. Production serving needs real-time features. What's missing?",
      "options": [
        "More frequent batch jobs",
        "A real-time feature computation path",
        "Larger batch job resources",
        "Caching batch results"
      ],
      "correct": 1,
      "explanation": "A streaming/real-time computation path (Flink, Spark Streaming) generates fresh features from live events. Batch provides historical features; streaming provides real-time. Many systems need both paths (feature stores often call this 'online' vs 'offline').",
      "detailedExplanation": "If you keep \"your ML feature store computes features in a nightly batch job\" in view, the correct answer separates faster. Eliminate options that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-078",
      "type": "multiple-choice",
      "question": "Your operational database is PostgreSQL. You want to run machine learning on the data. Where should ML training happen?",
      "options": [
        "Directly on PostgreSQL",
        "On a replicated copy or exported data",
        "ML can't use relational data",
        "Convert PostgreSQL to a document store first"
      ],
      "correct": 1,
      "explanation": "ML training queries can be heavy—run them on a replica or export data to a training environment (S3 + Spark, BigQuery). This prevents ML workloads from impacting production OLTP performance.",
      "detailedExplanation": "Start from \"your operational database is PostgreSQL\", then pressure-test the result against the options. Discard choices that violate required invariants during concurrent or failed states. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-079",
      "type": "multi-select",
      "question": "Which are valid strategies for handling database writes during system maintenance?",
      "options": [
        "Queue writes and replay after maintenance",
        "Reject writes and return errors",
        "Failover to a replica",
        "Buffer in local storage and sync later"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are valid strategies with tradeoffs: queuing accepts writes without completing them, rejecting is explicit about unavailability, failover maintains availability, local buffering works for eventual consistency. Choice depends on requirements.",
      "detailedExplanation": "The key clue in this question is \"valid strategies for handling database writes during system maintenance\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "u4c7-080",
      "type": "multiple-choice",
      "question": "Your hybrid architecture includes PostgreSQL, Redis, and S3. An outage takes down the data center. Which store likely recovers user data most easily?",
      "options": [
        "Redis (in-memory with snapshots)",
        "PostgreSQL (WAL and backups)",
        "S3 (multi-region replication)",
        "All recover equally well"
      ],
      "correct": 2,
      "explanation": "S3 with multi-region replication survives data center outages by design—data exists in other regions. PostgreSQL recovery depends on backup location. Redis in-memory data may be lost between snapshots. S3 is the most durable by design.",
      "detailedExplanation": "The key clue in this question is \"your hybrid architecture includes PostgreSQL, Redis, and S3\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "u4c7-081",
      "type": "multiple-choice",
      "question": "You're implementing a distributed transaction across PostgreSQL and MongoDB. What's the recommended approach?",
      "options": [
        "XA transactions",
        "Saga pattern with compensating transactions",
        "Two-phase commit across both",
        "MongoDB's native distributed transactions"
      ],
      "correct": 1,
      "explanation": "Saga pattern is the practical approach for transactions spanning different database technologies. XA/2PC requires support from both databases and is complex and slow. MongoDB's transactions only work within MongoDB.",
      "detailedExplanation": "Start from \"you're implementing a distributed transaction across PostgreSQL and MongoDB\", then pressure-test the result against the options. Eliminate options that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-082",
      "type": "ordering",
      "question": "Order the saga steps for a travel booking (flight, hotel, payment) where payment fails:",
      "items": [
        "Compensate: cancel hotel reservation",
        "Book flight (success)",
        "Process payment (fails)",
        "Compensate: cancel flight",
        "Book hotel (success)"
      ],
      "correctOrder": [1, 4, 2, 0, 3],
      "explanation": "Book flight → Book hotel → Payment fails → Compensate hotel (reverse order) → Compensate flight. Compensations run in reverse order of the original steps to maintain consistency.",
      "detailedExplanation": "The decision turns on \"order the saga steps for a travel booking (flight, hotel, payment) where payment fails:\". Build the rank from biggest differences first, then refine with adjacent checks. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-083",
      "type": "multiple-choice",
      "question": "Your saga orchestrator crashes after booking a flight but before booking the hotel. What happens on restart?",
      "options": [
        "Start from the beginning",
        "Resume from stored saga state",
        "Flight booking is lost",
        "Manual intervention required"
      ],
      "correct": 1,
      "explanation": "Saga orchestrators persist their state. On restart, the orchestrator reads the saga's last state (flight booked, awaiting hotel) and resumes from there. This requires durable storage of saga state—without it, recovery is problematic.",
      "detailedExplanation": "Read this as a scenario about \"your saga orchestrator crashes after booking a flight but before booking the hotel\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-084",
      "type": "multiple-choice",
      "question": "You use PostgreSQL for orders and DynamoDB for inventory. On checkout, you need to decrease inventory and create an order. What coordinates this?",
      "options": [
        "PostgreSQL transaction",
        "DynamoDB transaction",
        "Application-level saga or orchestration",
        "Direct cross-database join"
      ],
      "correct": 2,
      "explanation": "Application-level coordination (saga) handles cross-database operations. Neither PostgreSQL nor DynamoDB can manage transactions in the other. The application orchestrates: reserve inventory → create order → confirm or compensate.",
      "detailedExplanation": "Use \"you use PostgreSQL for orders and DynamoDB for inventory\" as your starting point, then verify tradeoffs carefully. Prefer the option that preserves correctness guarantees for the stated consistency boundary. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-085",
      "type": "multiple-choice",
      "question": "Your analytics dashboard needs to combine real-time data (last hour from Kafka) with historical data (from data warehouse). What architecture supports this?",
      "options": [
        "Store everything in Kafka",
        "Lambda architecture with speed and batch layers",
        "Run all queries against the data warehouse",
        "Cache historical data in Kafka"
      ],
      "correct": 1,
      "explanation": "Lambda architecture combines batch layer (historical accuracy from warehouse) with speed layer (real-time from Kafka). Queries merge both views. This handles the trade-off between real-time freshness and historical completeness.",
      "detailedExplanation": "This prompt is really about \"your analytics dashboard needs to combine real-time data (last hour from Kafka) with\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-086",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your company is building a customer 360 view combining data from CRM (Salesforce), transactions (PostgreSQL), and support tickets (Zendesk). Where should this unified view live?",
          "options": [
            "In each source system",
            "In a data warehouse or dedicated customer data platform",
            "In the CRM only",
            "No unified view is needed"
          ],
          "correct": 1,
          "explanation": "A data warehouse or customer data platform (CDP) aggregates data from multiple sources into a unified view. This avoids querying multiple systems in real-time and provides consistent analytics.",
          "detailedExplanation": "The key clue in this question is \"your company is building a customer 360 view combining data from CRM (Salesforce),\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 360 in aligned units before selecting an answer. Common pitfall: ignoring durability/recovery requirements."
        },
        {
          "question": "What pattern keeps the customer 360 view up-to-date as source systems change?",
          "options": [
            "Manual exports weekly",
            "CDC or scheduled ETL from each source",
            "Users refresh the view manually",
            "Source systems push directly to the warehouse"
          ],
          "correct": 1,
          "explanation": "CDC or scheduled ETL pipelines keep the view synchronized. CDC provides near real-time updates; scheduled ETL is simpler but has lag. Push from sources requires modifying each source system.",
          "detailedExplanation": "Read this as a scenario about \"pattern keeps the customer 360 view up-to-date as source systems change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 360 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead."
        }
      ],
      "detailedExplanation": "If you keep \"hybrid Architectures\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-087",
      "type": "multiple-choice",
      "question": "Your application uses PostgreSQL. You're adding a recommendation engine that needs a graph database for user-product relationships. How do you sync data?",
      "options": [
        "Replace PostgreSQL with the graph database",
        "Dual writes from the application",
        "CDC from PostgreSQL to graph database",
        "Query PostgreSQL from the graph database"
      ],
      "correct": 2,
      "explanation": "CDC streams relevant changes from PostgreSQL to the graph database. This decouples the systems—the application writes to PostgreSQL as before, and changes flow to the graph DB asynchronously. Dual writes risk inconsistency.",
      "detailedExplanation": "The core signal here is \"your application uses PostgreSQL\". Prefer the choice that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-088",
      "type": "multiple-choice",
      "question": "You have hot PostgreSQL data and cold Parquet files in S3. A user queries order history spanning both. What tool provides a unified query experience?",
      "options": [
        "PostgreSQL foreign data wrapper for S3",
        "Amazon Athena with federated queries",
        "S3 Select",
        "Either A or B depending on scale"
      ],
      "correct": 3,
      "explanation": "Both can work. PostgreSQL FDW is simpler for small-scale queries and keeps the PostgreSQL interface. Athena excels at large-scale S3 queries with federated access to other sources. Choose based on query patterns and data volume.",
      "detailedExplanation": "The key clue in this question is \"you have hot PostgreSQL data and cold Parquet files in S3\". Eliminate options that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-089",
      "type": "multiple-choice",
      "question": "Your e-commerce site caches product pages in a CDN. The product database is PostgreSQL. When a product price changes, how is the CDN updated?",
      "options": [
        "Wait for CDN TTL to expire",
        "Purge the specific product's CDN cache on update",
        "Don't cache product pages",
        "Store prices only in the CDN"
      ],
      "correct": 1,
      "explanation": "Active cache invalidation purges specific URLs when underlying data changes. Waiting for TTL serves stale prices. Not caching hurts performance. The application triggers CDN purge after database update (via API or event).",
      "detailedExplanation": "Start from \"your e-commerce site caches product pages in a CDN\", then pressure-test the result against the options. Prefer the choice that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-090",
      "type": "multi-select",
      "question": "Which caching layers might exist in a typical web application architecture?",
      "options": [
        "Browser cache",
        "CDN edge cache",
        "Application-level cache (Redis)",
        "Database query cache"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are common caching layers: browser (HTTP cache headers), CDN (edge servers), application (Redis/Memcached), and database (query/buffer cache). Each layer reduces load on layers behind it.",
      "detailedExplanation": "The core signal here is \"caching layers might exist in a typical web application architecture\". Validate each option independently; do not select statements that are only partially true. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "u4c7-091",
      "type": "multiple-choice",
      "question": "Your microservice architecture has 10 services, each with its own database. A compliance audit needs to trace a user's data across all services. What helps?",
      "options": [
        "Query each database individually",
        "Centralized data catalog with lineage tracking",
        "Shared database for all services",
        "Compliance isn't possible with microservices"
      ],
      "correct": 1,
      "explanation": "A data catalog tracks what data exists where (lineage, ownership, classification). For audits, the catalog identifies relevant services and their data. This maintains microservice autonomy while enabling cross-cutting concerns.",
      "detailedExplanation": "If you keep \"your microservice architecture has 10 services, each with its own database\" in view, the correct answer separates faster. Prefer the choice that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Keep quantities like 10 in aligned units before selecting an answer. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-092",
      "type": "multiple-choice",
      "question": "You're implementing soft delete across PostgreSQL and Elasticsearch. A record is marked deleted in PostgreSQL. What should happen in Elasticsearch?",
      "options": [
        "Nothing—they're separate systems",
        "Update the Elasticsearch document with deleted flag",
        "Remove the document from Elasticsearch",
        "Either B or C depending on requirements"
      ],
      "correct": 3,
      "explanation": "Depends on requirements. If deleted items should be searchable (admin view, restore), update with flag. If deleted means 'gone from search', remove the document. The CDC pipeline should propagate whichever action matches your business logic.",
      "detailedExplanation": "This prompt is really about \"you're implementing soft delete across PostgreSQL and Elasticsearch\". Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-093",
      "type": "multiple-choice",
      "question": "Your search uses Elasticsearch. Users report that recently published content isn't appearing in search. The CDC pipeline is running. What's the most likely issue?",
      "options": [
        "Elasticsearch is down",
        "Elasticsearch refresh interval",
        "Content was never written to PostgreSQL",
        "Users are searching wrong terms"
      ],
      "correct": 1,
      "explanation": "Elasticsearch indexes have a refresh interval (default 1 second) before new documents are searchable. Combined with CDC lag, recently published content may take seconds to appear. Check index refresh settings and CDC consumer lag.",
      "detailedExplanation": "Use \"your search uses Elasticsearch\" as your starting point, then verify tradeoffs carefully. Reject designs that improve throughput while weakening reliability guarantees. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 1 second appear, convert them into one unit basis before comparison. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-094",
      "type": "multiple-choice",
      "question": "Your system uses PostgreSQL, Redis, and Elasticsearch. You need to test a feature that touches all three. What testing approach works?",
      "options": [
        "Mock all databases",
        "Integration tests with real instances (Docker)",
        "Only unit test application code",
        "Test in production"
      ],
      "correct": 1,
      "explanation": "Integration tests with real database instances (via Docker/Testcontainers) verify actual behavior across all stores. Mocking hides integration issues. Unit tests don't catch cross-system problems. Production testing is risky.",
      "detailedExplanation": "Read this as a scenario about \"your system uses PostgreSQL, Redis, and Elasticsearch\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-095",
      "type": "multiple-choice",
      "question": "Your pipeline writes data to PostgreSQL and then publishes to Kafka. The Kafka publish sometimes fails. What pattern prevents data inconsistency?",
      "options": [
        "Retry Kafka publish indefinitely",
        "Rollback the PostgreSQL write on Kafka failure",
        "Use the transactional outbox pattern",
        "Accept occasional missing events"
      ],
      "correct": 2,
      "explanation": "Transactional outbox writes the event to an outbox table in the same PostgreSQL transaction. A separate process reliably publishes from the outbox to Kafka with retries. This ensures the event is never lost if the DB write succeeded.",
      "detailedExplanation": "The decision turns on \"your pipeline writes data to PostgreSQL and then publishes to Kafka\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-096",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're building a real-time inventory system. Inventory counts need to be accurate (no overselling) and fast. What hybrid architecture helps?",
          "options": [
            "PostgreSQL only with heavy indexes",
            "Redis for real-time counts, PostgreSQL for audit log",
            "Elasticsearch for inventory",
            "Kafka for storing inventory"
          ],
          "correct": 1,
          "explanation": "Redis provides fast, atomic decrement operations for real-time inventory. PostgreSQL stores the durable audit trail of all inventory changes. This combines Redis speed with PostgreSQL durability and queryability.",
          "detailedExplanation": "This prompt is really about \"you're building a real-time inventory system\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "Redis shows 5 items in stock. PostgreSQL audit log shows 10 sold, 15 restocked. How do you reconcile if they disagree?",
          "options": [
            "Trust Redis—it's the live system",
            "Trust PostgreSQL—recalculate from audit log",
            "Average the two values",
            "Flag for manual review"
          ],
          "correct": 1,
          "explanation": "PostgreSQL's immutable audit log is the source of truth. Recalculate inventory from the complete event history. Redis may have missed an event or had a bug. The audit log is authoritative for reconciliation.",
          "detailedExplanation": "If you keep \"redis shows 5 items in stock\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 5 and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "Start from \"hybrid Architectures\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-097",
      "type": "multiple-choice",
      "question": "Your architecture has grown to include 5 different databases. Engineering velocity is slowing due to complexity. What might help?",
      "options": [
        "Add more databases for specialization",
        "Consolidate where workloads overlap",
        "Remove all caching",
        "Hire more database administrators"
      ],
      "correct": 1,
      "explanation": "Consolidation reduces operational complexity. Modern databases often handle multiple workloads adequately (PostgreSQL for OLTP + search + JSON). Only keep specialized stores where they provide clear value over consolidation.",
      "detailedExplanation": "The key clue in this question is \"your architecture has grown to include 5 different databases\". Eliminate options that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. If values like 5 appear, convert them into one unit basis before comparison. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-098",
      "type": "multiple-choice",
      "question": "You're evaluating whether to add a specialized time-series database or use PostgreSQL with TimescaleDB. What factors favor the specialized database?",
      "options": [
        "Simpler operations with fewer systems",
        "Extreme scale or specific features not in TimescaleDB",
        "Team already knows PostgreSQL",
        "Cost reduction"
      ],
      "correct": 1,
      "explanation": "A specialized TSDB (InfluxDB, TimescaleDB, QuestDB) is warranted when you need extreme ingestion rates, specific features (downsampling, retention), or scale beyond TimescaleDB. Otherwise, sticking with PostgreSQL reduces complexity.",
      "detailedExplanation": "The core signal here is \"you're evaluating whether to add a specialized time-series database or use PostgreSQL\". Prefer the choice that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-099",
      "type": "multi-select",
      "question": "What should you monitor in a polyglot persistence architecture?",
      "options": [
        "Sync lag between systems (CDC consumer lag)",
        "Cross-system data consistency",
        "Individual database health metrics",
        "Query patterns that span multiple stores"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are critical: CDC lag indicates sync freshness, consistency checks catch drift, individual health ensures each system works, and cross-store query patterns reveal where composition is complex or slow.",
      "detailedExplanation": "If you keep \"you monitor in a polyglot persistence architecture\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "u4c7-100",
      "type": "multiple-choice",
      "question": "Your data platform team supports PostgreSQL, MongoDB, Redis, Elasticsearch, and Kafka. A new project team wants to add Cassandra. What question should you ask first?",
      "options": [
        "Can your team maintain another database?",
        "What specific access pattern requires Cassandra?",
        "How much will Cassandra cost?",
        "Is Cassandra the newest technology?"
      ],
      "correct": 1,
      "explanation": "Understand the specific workload requirements. Cassandra excels at high write throughput across partitions. If the workload can be served by existing stores (MongoDB, PostgreSQL), adding Cassandra increases complexity without benefit.",
      "detailedExplanation": "Start from \"your data platform team supports PostgreSQL, MongoDB, Redis, Elasticsearch, and Kafka\", then pressure-test the result against the options. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    }
  ]
}
