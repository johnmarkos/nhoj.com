{
  "chapterTitle": "Eviction Policies",
  "problems": [
    {
      "id": "evict-001",
      "type": "multiple-choice",
      "question": "Which eviction policy removes the item that hasn't been accessed for the longest time?",
      "options": ["FIFO", "LRU", "LFU", "Random"],
      "correct": 1,
      "explanation": "LRU (Least Recently Used) evicts the item with the oldest access time. It assumes recently accessed items are more likely to be accessed again.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-002",
      "type": "multiple-choice",
      "question": "Which eviction policy removes the item with the fewest accesses?",
      "options": ["LRU", "LFU", "FIFO", "MRU"],
      "correct": 1,
      "explanation": "LFU (Least Frequently Used) tracks access counts and evicts the item accessed the fewest times. It favors items with sustained popularity over recently accessed items.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-003",
      "type": "multiple-choice",
      "question": "A cache uses FIFO eviction. Items were added in order: A, B, C, D. The cache is full. Which item is evicted when E is added?",
      "options": ["A", "B", "D", "Random selection"],
      "correct": 0,
      "explanation": "FIFO (First In First Out) evicts the oldest item by insertion time. A was inserted first, so A is evicted regardless of access patterns.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-004",
      "type": "multiple-choice",
      "question": "What is the main advantage of random eviction over LRU?",
      "options": [
        "Better hit rate",
        "O(1) time complexity with no bookkeeping",
        "Adapts to access patterns",
        "Prevents cache pollution"
      ],
      "correct": 1,
      "explanation": "Random eviction requires no tracking of access times or frequencies—just pick a random item. This gives O(1) eviction with zero bookkeeping overhead, though hit rates are typically worse than LRU.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Keep quantities like 1 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-005",
      "type": "multiple-choice",
      "question": "Which data structure combination is typically used to implement LRU with O(1) operations?",
      "options": [
        "Array + binary search",
        "Hash map + doubly linked list",
        "Min-heap + hash map",
        "BST + array"
      ],
      "correct": 1,
      "explanation": "LRU uses a hash map for O(1) lookups and a doubly linked list to track access order. On access, move the node to the head; on eviction, remove from tail. Both operations are O(1).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 1 appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-006",
      "type": "multi-select",
      "question": "Which eviction policies require tracking per-item metadata beyond the key-value pair? (Select all that apply)",
      "options": ["FIFO", "LRU", "LFU", "Random"],
      "correctIndices": [0, 1, 2],
      "explanation": "FIFO needs insertion timestamps/order. LRU needs last access time/order. LFU needs access counts. Random requires no per-item metadata—it just picks randomly.",
      "detailedExplanation": "Generalize from eviction policies require tracking per-item metadata beyond the key-value pair? (Select to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-007",
      "type": "multiple-choice",
      "question": "A news site caches articles. Traffic spikes briefly when articles are published, then drops. Which eviction policy would cause the most cache pollution?",
      "options": ["LRU", "LFU", "FIFO", "Random"],
      "correct": 1,
      "explanation": "LFU would keep old articles with high historical counts even after they're no longer relevant. New articles can't accumulate enough accesses to compete, causing cache pollution with stale content.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-008",
      "type": "multiple-choice",
      "question": "What problem does LFU have that LRU doesn't?",
      "options": [
        "Higher memory overhead",
        "Items can become 'stuck' with high counts even when no longer relevant",
        "Cannot handle concurrent access",
        "Requires sorting on every eviction"
      ],
      "correct": 1,
      "explanation": "LFU accumulates counts over time. An item accessed heavily in the past but not recently keeps its high count and won't be evicted, even if it's no longer useful. This is called 'cache pollution' or 'frequency aging' problem.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-009",
      "type": "multiple-choice",
      "question": "How does LFU with aging (or decaying LFU) address the 'stuck items' problem?",
      "options": [
        "Resets all counts periodically",
        "Decreases counts over time or on each eviction cycle",
        "Removes items that haven't been accessed in the last hour",
        "Converts to LRU after a threshold"
      ],
      "correct": 1,
      "explanation": "Decaying LFU gradually reduces access counts over time (e.g., halving counts periodically or on each eviction). This allows old popular items to eventually be evicted if they stop being accessed.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-010",
      "type": "ordering",
      "question": "Rank these eviction policies by typical memory overhead per item (lowest to highest):",
      "items": ["Random", "FIFO", "LRU", "LFU"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Random: no per-item overhead. FIFO: just position/timestamp. LRU: access time + list pointers. LFU: access count + often additional structures for efficient eviction of min-count items.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Build the rank from biggest differences first, then refine with adjacent checks. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-011",
      "type": "multiple-choice",
      "question": "Redis supports multiple eviction policies. Which is the default when maxmemory is set?",
      "options": [
        "noeviction",
        "allkeys-lru",
        "volatile-lru",
        "allkeys-random"
      ],
      "correct": 0,
      "explanation": "Redis defaults to 'noeviction'—it returns errors when memory is full rather than evicting. You must explicitly configure an eviction policy like allkeys-lru or volatile-lru.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-012",
      "type": "multiple-choice",
      "question": "In Redis, what does 'volatile-lru' evict?",
      "options": [
        "All keys using LRU",
        "Only keys with TTL set, using LRU",
        "Random keys with TTL",
        "Keys without TTL using LRU"
      ],
      "correct": 1,
      "explanation": "'volatile-lru' only considers keys that have an expiration (TTL) set. It applies LRU eviction to that subset. Keys without TTL are never evicted under this policy.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-013",
      "type": "multiple-choice",
      "question": "What is the Clock algorithm (also called Second Chance)?",
      "options": [
        "Evicts based on wall-clock time",
        "An approximation of LRU using a circular buffer and reference bits",
        "Evicts items after a fixed time interval",
        "Rotates between LRU and LFU"
      ],
      "correct": 1,
      "explanation": "Clock is an efficient LRU approximation. Items are arranged in a circle with reference bits. On eviction, the 'hand' sweeps, clearing bits until finding an item with bit=0. Accessed items get their bit set to 1.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Numbers such as 0 and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-014",
      "type": "multiple-choice",
      "question": "Why might a system choose the Clock algorithm over true LRU?",
      "options": [
        "Better hit rate",
        "Lower CPU and memory overhead",
        "Handles concurrent access better",
        "Adapts to changing workloads"
      ],
      "correct": 1,
      "explanation": "True LRU requires updating a linked list on every access (even cache hits). Clock only flips a bit on access, which is much cheaper. The approximation works well in practice with lower overhead.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-015",
      "type": "multiple-choice",
      "question": "Memcached uses a segmented LRU. What does 'segmented' mean in this context?",
      "options": [
        "Cache is split by key hash ranges",
        "Items are grouped into slabs by size class",
        "Multiple LRU queues for different access frequencies",
        "LRU is applied per-node in a cluster"
      ],
      "correct": 1,
      "explanation": "Memcached's slab allocator groups items by size class (e.g., 96 bytes, 120 bytes, etc.). Each slab class has its own LRU. Eviction happens within a slab class, which can cause imbalanced eviction if sizes are unevenly distributed.",
      "detailedExplanation": "Generalize from memcached uses a segmented LRU to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 96 and 120 in aligned units before deciding on an implementation approach. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-016",
      "type": "two-stage",
      "stages": [
        {
          "question": "A cache has capacity for 4 items. Using LRU, items A, B, C, D are accessed in that order (A first, D most recent). Which item is at the head (most recently used)?",
          "options": ["A", "B", "C", "D"],
          "correct": 3,
          "explanation": "D was accessed most recently, so it's at the head of the LRU list.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 4 in aligned units before deciding on an implementation approach. Common pitfall: invalidation races under concurrent writes."
        },
        {
          "question": "Now item B is accessed again. The order before was A→B→C→D (A=oldest). What's the new LRU order from oldest to newest?",
          "options": ["A→C→D→B", "B→A→C→D", "A→B→C→D", "C→D→A→B"],
          "correct": 0,
          "explanation": "Accessing B moves it to the most recent position. A stays oldest (wasn't accessed), then C and D maintain their relative order, then B becomes newest: A→C→D→B.",
          "detailedExplanation": "Generalize from now item B is accessed again to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-017",
      "type": "multiple-choice",
      "question": "What is 'cache pollution'?",
      "options": [
        "When cached data becomes stale",
        "When low-value items displace high-value items",
        "When cache memory leaks",
        "When cache keys contain invalid characters"
      ],
      "correct": 1,
      "explanation": "Cache pollution occurs when items unlikely to be accessed again take up space, displacing items that would provide more cache hits. It's a key concern with sequential scans and one-time access patterns.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-018",
      "type": "multiple-choice",
      "question": "A batch job scans through millions of records sequentially, each accessed once. Using LRU, what happens to the cache?",
      "options": [
        "Hit rate improves as more data is cached",
        "Cache becomes polluted with items that won't be accessed again",
        "LRU automatically detects and avoids caching scan data",
        "Memory usage decreases"
      ],
      "correct": 1,
      "explanation": "Sequential scans are pathological for LRU—each new item evicts the previous one, and none are accessed again. The cache fills with useless data, evicting potentially valuable items. This is called 'scan resistance' failure.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-019",
      "type": "multiple-choice",
      "question": "Which eviction policy is specifically designed to be 'scan resistant'?",
      "options": ["LRU", "FIFO", "2Q (Two Queue)", "Random"],
      "correct": 2,
      "explanation": "2Q uses two queues: items enter a FIFO queue first, and only move to the main LRU queue if accessed again. Single-access items from scans never pollute the main cache.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-020",
      "type": "multiple-choice",
      "question": "How does the 2Q algorithm work?",
      "options": [
        "Alternates between two different eviction policies",
        "New items go to a probationary FIFO queue; re-accessed items promote to a protected LRU queue",
        "Splits the cache into two equal halves with different eviction",
        "Uses two access counts instead of one"
      ],
      "correct": 1,
      "explanation": "2Q has two queues: A1 (FIFO, for new/unproven items) and Am (LRU, for items accessed at least twice). Items enter A1; if accessed again while in A1, they promote to Am. This filters out one-time accesses.",
      "detailedExplanation": "Generalize from the 2Q algorithm work to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-021",
      "type": "multiple-choice",
      "question": "What is ARC (Adaptive Replacement Cache)?",
      "options": [
        "A hardware cache architecture",
        "A self-tuning algorithm that balances between recency and frequency",
        "An asynchronous replication cache",
        "A distributed cache protocol"
      ],
      "correct": 1,
      "explanation": "ARC dynamically balances between LRU (recency) and LFU (frequency) based on workload. It maintains ghost lists to track recently evicted items and adjusts the balance based on which would have been hits.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-022",
      "type": "multiple-choice",
      "question": "ARC maintains 'ghost' entries. What are these?",
      "options": [
        "Entries that failed to write",
        "Metadata for recently evicted items (without values) to inform adaptation",
        "Replicated entries for fault tolerance",
        "Entries pending deletion"
      ],
      "correct": 1,
      "explanation": "Ghost entries store only keys of recently evicted items (no values, so low overhead). If a ghost entry is requested, ARC knows it would have been a hit and adjusts its recency/frequency balance accordingly.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-023",
      "type": "multiple-choice",
      "question": "Why isn't ARC more widely used despite its excellent hit rates?",
      "options": [
        "Too complex to implement",
        "Patent restrictions (until recently) and higher memory/CPU overhead",
        "Only works with small caches",
        "Requires special hardware"
      ],
      "correct": 1,
      "explanation": "ARC was patented by IBM until recently, limiting adoption. It also has higher overhead than simple LRU—maintaining 4 lists plus ghost entries. Many systems use simpler approximations instead.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. If values like 4 appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-024",
      "type": "multiple-choice",
      "question": "Caffeine (Java caching library) uses W-TinyLFU. What does the 'W' stand for?",
      "options": ["Weighted", "Window", "Wide", "Write"],
      "correct": 1,
      "explanation": "W-TinyLFU uses a small 'Window' LRU for admission. New items enter the window; when evicted from window, they compete with main cache items based on frequency. This provides scan resistance.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-025",
      "type": "multiple-choice",
      "question": "TinyLFU uses a data structure for approximate frequency counting with low memory. What is it?",
      "options": ["Hash map", "Count-Min Sketch", "Bloom filter", "Skip list"],
      "correct": 1,
      "explanation": "Count-Min Sketch provides approximate frequency counts with fixed memory regardless of item count. It may overcount (never undercount), but the approximation is good enough for eviction decisions.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-026",
      "type": "multi-select",
      "question": "Which are advantages of W-TinyLFU over pure LRU? (Select all that apply)",
      "options": [
        "Scan resistant",
        "Balances recency and frequency",
        "Lower memory overhead",
        "Simpler implementation"
      ],
      "correctIndices": [0, 1],
      "explanation": "W-TinyLFU is scan resistant (window filters one-time accesses) and balances recency/frequency. However, it has higher memory overhead (Count-Min Sketch) and more complex implementation than basic LRU.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-027",
      "type": "multiple-choice",
      "question": "What is MRU (Most Recently Used) eviction, and when might it be useful?",
      "options": [
        "Same as LRU—it's an alternate name",
        "Evicts most recently used item; useful for stack-like access patterns",
        "Evicts items accessed most often",
        "Evicts the largest items first"
      ],
      "correct": 1,
      "explanation": "MRU evicts the most recently accessed item—the opposite of LRU. It's useful when older items are more likely to be reaccessed (e.g., looping through a dataset larger than cache, or stack-like patterns).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-028",
      "type": "multiple-choice",
      "question": "A cache repeatedly scans through a dataset of size N+1 where cache size is N. What's the hit rate with LRU?",
      "options": ["~100%", "~50%", "~1/(N+1)", "0%"],
      "correct": 3,
      "explanation": "With LRU, each access evicts the item that will be needed next in the scan. Every access is a miss. This is the worst case for LRU—MRU would actually perform better here.",
      "detailedExplanation": "Generalize from cache repeatedly scans through a dataset of size N+1 where cache size is N to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. If values like 1 appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-029",
      "type": "multiple-choice",
      "question": "What is LIRS (Low Inter-reference Recency Set)?",
      "options": [
        "A distributed cache protocol",
        "An algorithm that uses reuse distance instead of recency for eviction decisions",
        "A write-behind caching strategy",
        "A cache compression technique"
      ],
      "correct": 1,
      "explanation": "LIRS tracks 'inter-reference recency' (IRR)—the number of distinct items accessed between two consecutive accesses to the same item. Items with high IRR are evicted first, as they indicate less temporal locality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-030",
      "type": "ordering",
      "question": "Rank these eviction algorithms by typical hit rate on mixed workloads (lowest to highest):",
      "items": ["Random", "FIFO", "LRU", "ARC/W-TinyLFU"],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Random < FIFO (slightly better, respects insertion order) < LRU (captures recency) < ARC/W-TinyLFU (adapts to workload, balances recency/frequency). The gap between LRU and adaptive algorithms varies by workload.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-031",
      "type": "multiple-choice",
      "question": "Browser caches typically use which eviction approach?",
      "options": [
        "Pure LRU",
        "LRU with size weighting and origin quotas",
        "FIFO only",
        "Random eviction"
      ],
      "correct": 1,
      "explanation": "Browsers use sophisticated eviction considering: item size (larger items evicted sooner per-byte), per-origin quotas (one site can't monopolize cache), and LRU for recency. HTTP Cache-Control headers also influence caching.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-032",
      "type": "multiple-choice",
      "question": "CPU caches typically use which eviction policy?",
      "options": [
        "Pure LRU",
        "Pseudo-LRU or Tree-PLRU approximations",
        "LFU",
        "Random"
      ],
      "correct": 1,
      "explanation": "True LRU is too expensive in hardware (N! states for N-way cache). CPU caches use approximations like Pseudo-LRU (tree-based) or random within a set. These achieve near-LRU performance with simpler circuits.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-033",
      "type": "multiple-choice",
      "question": "What is 'cost-aware' or 'weighted' eviction?",
      "options": [
        "Charging users for cache usage",
        "Considering the cost to regenerate items when making eviction decisions",
        "Evicting based on financial cost of storage",
        "Prioritizing paid vs free tier users"
      ],
      "correct": 1,
      "explanation": "Cost-aware eviction factors in how expensive an item is to recompute/refetch. An item that took 5 seconds to generate should be kept longer than one that took 5ms, even with similar access patterns.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Numbers such as 5 seconds and 5ms should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-034",
      "type": "multiple-choice",
      "question": "The GreedyDual algorithm combines which factors for eviction?",
      "options": [
        "Only recency",
        "Only frequency",
        "Recency, size, and cost",
        "Frequency and size only"
      ],
      "correct": 2,
      "explanation": "GreedyDual considers: recency (when last accessed), size (larger items are 'cheaper' to evict per-byte), and cost (expensive-to-regenerate items kept longer). It generalizes LRU to account for heterogeneous items.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-035",
      "type": "two-stage",
      "stages": [
        {
          "question": "A cache holds items with different sizes. Item A is 1KB (accessed 10s ago), Item B is 100KB (accessed 5s ago). Using LRU, which is evicted first?",
          "options": [
            "A (older access)",
            "B (larger size)",
            "Neither—need more info"
          ],
          "correct": 0,
          "explanation": "Pure LRU only considers recency. A was accessed longer ago, so A is evicted first regardless of size.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 1KB and 10s appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "Now using size-aware LRU (evict to free at least 100KB). Item A is 1KB, Item B is 100KB. Which gets evicted?",
          "options": ["A only", "B only", "Both A and B"],
          "correct": 1,
          "explanation": "To free 100KB, evicting A (1KB) isn't enough. Evicting B (100KB) satisfies the requirement in one eviction. Size-aware policies prefer evicting fewer large items over many small ones.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 100KB and 1KB in aligned units before deciding on an implementation approach. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-036",
      "type": "multiple-choice",
      "question": "Redis uses 'approximated LRU' by default. How does it approximate?",
      "options": [
        "Uses wall-clock time instead of logical time",
        "Samples a random subset of keys and evicts the LRU among the sample",
        "Only tracks the 100 most recently used keys",
        "Uses Clock algorithm"
      ],
      "correct": 1,
      "explanation": "Redis samples N random keys (default 5, configurable via maxmemory-samples) and evicts the least recently used among them. This is O(1) regardless of key count, trading accuracy for efficiency.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 5 and 1 appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-037",
      "type": "numeric-input",
      "question": "Redis samples 5 keys by default for approximated LRU. If you increase maxmemory-samples to 10, by approximately what factor does eviction accuracy improve?",
      "answer": 2,
      "tolerance": 0.3,
      "unit": "x",
      "explanation": "More samples = closer to true LRU. Doubling samples roughly doubles accuracy (approaches true LRU behavior). Redis documentation shows diminishing returns past ~10 samples.",
      "detailedExplanation": "Generalize from redis samples 5 keys by default for approximated LRU to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 5 and 10 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-038",
      "type": "multiple-choice",
      "question": "What is 'lazy eviction' or 'passive expiration'?",
      "options": [
        "Evicting items only when space is needed",
        "Checking TTL only when an item is accessed, not proactively",
        "Delaying eviction until off-peak hours",
        "Evicting in background threads"
      ],
      "correct": 1,
      "explanation": "Lazy/passive expiration means expired items remain in cache until accessed (then the stale item is found and removed). This avoids active scanning for expired items but may waste memory on expired data.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-039",
      "type": "multiple-choice",
      "question": "Redis uses both passive and active expiration. What does active expiration do?",
      "options": [
        "Deletes items immediately when TTL expires",
        "Periodically samples keys with TTL and deletes expired ones",
        "Sends notifications when items expire",
        "Prevents items from being accessed after expiration"
      ],
      "correct": 1,
      "explanation": "Redis periodically (10 times/sec) samples 20 keys with TTL, deletes expired ones, and repeats if >25% were expired. This probabilistically cleans up expired keys without scanning all keys.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 10 and 20 should be normalized first so downstream reasoning stays consistent. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-040",
      "type": "multi-select",
      "question": "Which factors might you consider when choosing an eviction policy? (Select all that apply)",
      "options": [
        "Memory overhead per item",
        "CPU cost per access/eviction",
        "Workload access patterns",
        "Total dataset size on disk"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Key factors: Memory overhead (LFU > LRU > FIFO > Random). CPU cost (true LRU updates on every hit). Access patterns (scans favor 2Q/ARC). Total dataset size on disk doesn't directly influence eviction policy choice — what matters is the working set relative to cache size, not raw dataset size.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-041",
      "type": "multiple-choice",
      "question": "A cache serves requests with Zipf distribution (few items are very popular). Which policy performs best?",
      "options": ["FIFO", "LRU", "LFU", "Random"],
      "correct": 2,
      "explanation": "Zipf distribution means a small number of items get most accesses. LFU excels here—it keeps the popular items regardless of when they were last accessed. LRU also works well; FIFO and Random are worse.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-042",
      "type": "multiple-choice",
      "question": "A cache serves requests with uniform random distribution (all items equally likely). Which policy performs best?",
      "options": ["FIFO", "LRU", "LFU", "All perform similarly"],
      "correct": 3,
      "explanation": "With uniform access, there's no pattern to exploit. LRU, LFU, and FIFO all achieve similar hit rates (determined by cache size / working set size). No policy can predict future accesses better than another.",
      "detailedExplanation": "Generalize from cache serves requests with uniform random distribution (all items equally likely) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-043",
      "type": "multiple-choice",
      "question": "What is 'admission control' in caching?",
      "options": [
        "Rate limiting cache writes",
        "Deciding whether to cache an item at all, not just what to evict",
        "Authenticating cache access",
        "Controlling network admission to cache servers"
      ],
      "correct": 1,
      "explanation": "Admission control decides if a new item should enter the cache, separate from eviction. TinyLFU uses this: a new item only enters if its frequency is higher than the item it would evict. This prevents pollution.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-044",
      "type": "multiple-choice",
      "question": "TinyLFU admission control compares a new item against what?",
      "options": [
        "A random existing item",
        "The item that would be evicted (victim)",
        "The most popular item",
        "All existing items"
      ],
      "correct": 1,
      "explanation": "TinyLFU compares the new item's estimated frequency against the eviction victim's frequency. The new item only enters if it has higher frequency. This prevents low-frequency items from evicting high-frequency ones.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-045",
      "type": "multiple-choice",
      "question": "A Count-Min Sketch with width W and depth D can count up to 2^32 items. How much memory does it use?",
      "options": ["W bytes", "W × D × 4 bytes", "2^32 bytes", "D bytes"],
      "correct": 1,
      "explanation": "Count-Min Sketch uses a 2D array of W × D counters. If each counter is 4 bytes (32-bit int), total memory is W × D × 4 bytes. Typical configurations use W=~cache_size, D=4, giving compact frequency tracking.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Numbers such as 2 and 32 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a Count-Min Sketch, what happens when you increment item X?",
          "options": [
            "Increment one counter at hash(X)",
            "Increment D counters, one in each row at hash_d(X)",
            "Increment all counters",
            "Increment the minimum counter"
          ],
          "correct": 1,
          "explanation": "Each row has its own hash function. For item X, increment counter at position hash_d(X) in each of D rows.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "To estimate item X's count, what do you read?",
          "options": [
            "Sum of all D counters",
            "Average of all D counters",
            "Minimum of all D counters",
            "Maximum of all D counters"
          ],
          "correct": 2,
          "explanation": "Take the minimum across all D counters. Due to hash collisions, some counters may be inflated by other items. The minimum is the most accurate estimate (always >= true count).",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-047",
      "type": "multiple-choice",
      "question": "In TinyLFU, how is frequency count aging handled?",
      "options": [
        "Counts never decrease",
        "Counts reset to zero periodically",
        "All counts are halved (divided by 2) when the sample window is full",
        "Individual counts decay exponentially"
      ],
      "correct": 2,
      "explanation": "TinyLFU periodically halves all counts (when the sample counter reaches a threshold). This decays old frequency data, allowing the cache to adapt to changing access patterns over time.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-048",
      "type": "multiple-choice",
      "question": "Which statement about FIFO eviction is true?",
      "options": [
        "FIFO always has lower hit rate than LRU",
        "FIFO has lower hit rate than LRU for most workloads but not all",
        "FIFO and LRU have identical hit rates",
        "FIFO always has higher hit rate than LRU"
      ],
      "correct": 1,
      "explanation": "FIFO usually underperforms LRU because it ignores access patterns. However, for some workloads (like repeated scans slightly larger than cache), FIFO can match or beat LRU. No policy wins universally.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-049",
      "type": "multiple-choice",
      "question": "What is the primary advantage of FIFO over LRU?",
      "options": [
        "Better hit rate",
        "No update needed on cache hits (access is a no-op)",
        "Lower memory usage",
        "Better scan resistance"
      ],
      "correct": 1,
      "explanation": "FIFO only tracks insertion order—accessing an existing item doesn't require any update. LRU must move accessed items to the head on every hit. FIFO's simpler bookkeeping means lower CPU overhead.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-050",
      "type": "multi-select",
      "question": "Which statements about priority-based eviction are true? (Select all that apply)",
      "options": [
        "Items are assigned explicit priority levels",
        "Lower priority items are evicted before higher priority items",
        "Useful when some data is more valuable regardless of access patterns",
        "Common in operating system page replacement"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Priority-based eviction uses explicit priorities—evict low priority first. It's useful when business logic determines value (e.g., paid users > free users). OS page replacement typically uses LRU variants, not explicit priorities.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-051",
      "type": "multiple-choice",
      "question": "In a multi-tier cache (L1 + L2), what's a common eviction flow?",
      "options": [
        "Evicted from L1 and L2 simultaneously",
        "Evicted from L1, promoted to L2 (inclusive)",
        "Evicted from L1, discarded if not in L2",
        "Random selection between L1 and L2"
      ],
      "correct": 1,
      "explanation": "In inclusive caches, items evicted from L1 move to L2 (larger, slower). L2 serves as a 'victim cache' for L1 evictions. The item has a second chance to prove its worth before full eviction.",
      "detailedExplanation": "Generalize from in a multi-tier cache (L1 + L2), what's a common eviction flow to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-052",
      "type": "multiple-choice",
      "question": "An exclusive cache hierarchy means what?",
      "options": [
        "Cache access requires authentication",
        "An item exists in only one level at a time (L1 or L2, not both)",
        "Each level uses a different eviction policy",
        "Only certain data types can be cached"
      ],
      "correct": 1,
      "explanation": "Exclusive caches don't duplicate items across levels. An item is in L1 XOR L2. On L1 miss/L2 hit, the item swaps: moves to L1, and L1's evicted item moves to L2. This maximizes total effective capacity.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-053",
      "type": "multiple-choice",
      "question": "What is 'ghost caching' (also called 'shadow caching')?",
      "options": [
        "Caching deletion markers for recently deleted items",
        "Caching metadata for recently evicted items to inform future decisions",
        "Caching error responses",
        "Running cache in the background"
      ],
      "correct": 1,
      "explanation": "Ghost caching keeps metadata (keys only, no values) for recently evicted items. If a ghost entry is requested, the system knows it would have been a hit. ARC uses this to adapt its recency/frequency balance.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-054",
      "type": "multiple-choice",
      "question": "What happens if eviction can't keep up with write rate?",
      "options": [
        "Cache automatically expands",
        "Writes block or fail (back-pressure)",
        "Oldest items are instantly deleted",
        "System switches to write-through mode"
      ],
      "correct": 1,
      "explanation": "If eviction is slower than writes, the cache fills faster than it can free space. This causes write operations to block or fail until space is available. Monitoring eviction rate vs write rate is important.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-055",
      "type": "numeric-input",
      "question": "A cache has 1M items and runs LRU approximation by sampling 10 items per eviction. Approximately what fraction of items are considered for eviction each time?",
      "answer": 0.001,
      "tolerance": 0.1,
      "unit": "%",
      "explanation": "10 samples out of 1,000,000 items = 10/1,000,000 = 0.00001 = 0.001%. This tiny fraction is why sampled LRU is an approximation—it might not find the globally least-recently-used item.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 1M and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-056",
      "type": "multiple-choice",
      "question": "What is 'eviction churn'?",
      "options": [
        "Rapid repeated eviction and re-caching of the same items",
        "Slow eviction rate",
        "Evicting items that should be kept",
        "Network delays during eviction"
      ],
      "correct": 0,
      "explanation": "Eviction churn occurs when items are evicted and immediately re-requested, only to be evicted again. It indicates working set > cache size or poor eviction policy. High churn wastes bandwidth and CPU.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-057",
      "type": "multiple-choice",
      "question": "How can you reduce eviction churn?",
      "options": [
        "Increase cache size to fit working set",
        "Use faster eviction algorithm",
        "Decrease TTL",
        "Use FIFO instead of LRU"
      ],
      "correct": 0,
      "explanation": "Churn means the cache can't hold the working set. The primary solution is increasing cache size. Alternatively, identify what's causing the large working set (scans? attack?) and address that.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "A cache serves 1000 requests/sec with 80% hit rate. Cache size is 10GB. How many evictions per second occur approximately (assuming each miss adds a new item and items are similar size)?",
          "options": ["200/sec", "800/sec", "1000/sec", "Cannot determine"],
          "correct": 0,
          "explanation": "20% miss rate = 200 misses/sec. If each miss results in caching a new item (and cache is full), that's ~200 evictions/sec to make room.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 1000 and 80 should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "If cache size doubles to 20GB and hit rate improves to 90%, how do evictions change?",
          "options": [
            "Increase to 400/sec",
            "Decrease to 100/sec",
            "Stay at 200/sec",
            "Decrease to 50/sec"
          ],
          "correct": 1,
          "explanation": "10% miss rate = 100 misses/sec = ~100 evictions/sec. Larger cache means higher hit rate and fewer evictions needed. Evictions are halved.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 20GB and 90 should be normalized first so downstream reasoning stays consistent. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-059",
      "type": "multiple-choice",
      "question": "What metric indicates your eviction policy might be wrong for your workload?",
      "options": [
        "High memory usage",
        "Low hit rate despite sufficient cache size",
        "Fast response times",
        "Low eviction rate"
      ],
      "correct": 1,
      "explanation": "If cache is large enough to hold working set but hit rate is still low, the eviction policy may be evicting useful items. This suggests a mismatch between policy and access patterns (e.g., LRU with scan-heavy workload).",
      "detailedExplanation": "Generalize from metric indicates your eviction policy might be wrong for your workload to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-060",
      "type": "multiple-choice",
      "question": "What is 'min' eviction (Bélády's algorithm)?",
      "options": [
        "Evict the smallest item",
        "Evict the item that won't be used for the longest time in the future",
        "Evict the minimum frequency item",
        "Evict after minimum TTL"
      ],
      "correct": 1,
      "explanation": "Bélády's algorithm (OPT/MIN) evicts the item whose next access is furthest in the future. It's optimal but requires knowledge of future accesses—impossible in practice, but useful as a theoretical upper bound for comparison.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-061",
      "type": "multiple-choice",
      "question": "Why is Bélády's algorithm useful even though it can't be implemented?",
      "options": [
        "It's actually implementable with ML",
        "It provides an upper bound to measure how close other policies get to optimal",
        "It's used in hardware caches",
        "It's used for cache warming"
      ],
      "correct": 1,
      "explanation": "Bélády's gives the maximum possible hit rate for a workload. By comparing LRU/LFU hit rates against OPT, you can see how much room for improvement exists. If LRU is 90% and OPT is 92%, LRU is near-optimal.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 90 and 92 should be normalized first so downstream reasoning stays consistent. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-062",
      "type": "multiple-choice",
      "question": "What does it mean when LRU hit rate is far below OPT (Bélády's) hit rate?",
      "options": [
        "LRU is buggy",
        "Workload has patterns that LRU can't exploit well",
        "Cache is too large",
        "Items are too small"
      ],
      "correct": 1,
      "explanation": "A large gap between LRU and OPT indicates the workload has structure that LRU fails to capture. This might be loops, scans, or frequency-dominated patterns. Consider trying ARC, 2Q, or LFU.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-063",
      "type": "multi-select",
      "question": "Which are valid Redis eviction policies? (Select all that apply)",
      "options": [
        "volatile-lru",
        "allkeys-lfu",
        "volatile-random",
        "allkeys-arc"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Redis supports: volatile-lru, volatile-lfu, volatile-random, volatile-ttl, allkeys-lru, allkeys-lfu, allkeys-random, and noeviction. ARC is not supported in Redis.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-064",
      "type": "multiple-choice",
      "question": "Redis volatile-ttl eviction policy does what?",
      "options": [
        "Evicts items with the longest TTL",
        "Evicts items with the shortest remaining TTL (soonest to expire)",
        "Evicts items that have been cached longest",
        "Evicts items only after TTL expires"
      ],
      "correct": 1,
      "explanation": "volatile-ttl evicts keys with TTL set, prioritizing those with shortest remaining TTL. The logic: if it's about to expire anyway, evict it now. Only considers keys with explicit expiration.",
      "detailedExplanation": "Generalize from redis volatile-ttl eviction policy does what to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-065",
      "type": "multiple-choice",
      "question": "When would you choose volatile-ttl over volatile-lru in Redis?",
      "options": [
        "When access patterns are uniform",
        "When you want items with longer TTL (more important) to survive eviction",
        "When you have no TTLs set",
        "When you want newest items evicted first"
      ],
      "correct": 1,
      "explanation": "If you've set TTLs to reflect item importance (longer TTL = more important), volatile-ttl respects that intent. Items set to expire soon are evicted before items you intended to keep longer.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-066",
      "type": "multiple-choice",
      "question": "What is the downside of using Redis allkeys-random?",
      "options": [
        "Higher memory usage",
        "Completely ignores access patterns, may evict hot items",
        "Slower eviction",
        "Requires more configuration"
      ],
      "correct": 1,
      "explanation": "Random eviction has no intelligence—a hot item with 1000 accesses/sec has the same eviction probability as a cold item accessed once. For workloads with skewed popularity (most real workloads), this hurts hit rate significantly.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 1000 should be normalized first so downstream reasoning stays consistent. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-067",
      "type": "multiple-choice",
      "question": "Memcached slab allocator can cause 'slab calcification'. What is this?",
      "options": [
        "Slabs becoming read-only",
        "Memory stuck in slab classes that no longer match current item sizes",
        "Slabs filling with expired items",
        "Slab metadata corruption"
      ],
      "correct": 1,
      "explanation": "If item size distribution changes, memory may be stuck in slab classes for sizes no longer used. E.g., if you used to have many 100-byte items but now have 500-byte items, the 100-byte slabs are wasted.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 100 and 500 should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-068",
      "type": "multiple-choice",
      "question": "How does Memcached's 'slab automove' address calcification?",
      "options": [
        "Moves items between slabs",
        "Automatically rebalances memory between slab classes based on eviction rates",
        "Compacts slabs periodically",
        "Prevents new slab creation"
      ],
      "correct": 1,
      "explanation": "Slab automove monitors eviction rates per slab class. If one class is evicting frequently while another is idle, it can move memory pages between them. This adapts to changing item size distributions.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-069",
      "type": "two-stage",
      "stages": [
        {
          "question": "A distributed cache has 10 nodes with independent LRU. Hot item X is cached on all 10 nodes. If 1 node evicts X, what happens?",
          "options": [
            "X is evicted from all nodes",
            "Only that node loses X; other 9 still have it",
            "X is replicated back immediately",
            "Request fails"
          ],
          "correct": 1,
          "explanation": "With independent per-node caches, eviction is local. Only the evicting node loses X. Requests to that node will miss; requests to other nodes still hit.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Keep quantities like 10 and 1 in aligned units before deciding on an implementation approach. Common pitfall: invalidation races under concurrent writes."
        },
        {
          "question": "Requests are load-balanced across the 10 nodes. What's the approximate hit rate for X after one node evicts it?",
          "options": ["0%", "50%", "90%", "100%"],
          "correct": 2,
          "explanation": "9 of 10 nodes still have X. With random load balancing, 90% of requests for X hit a node that has it. The 10% going to the evicting node miss (and may re-cache X on that node).",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 10 and 9 appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-070",
      "type": "multiple-choice",
      "question": "What is 'coordinated eviction' in distributed caches?",
      "options": [
        "Evicting the same key from all nodes simultaneously",
        "Coordinating eviction timing across nodes",
        "Having a leader decide all evictions",
        "Evicting based on global (not local) access patterns"
      ],
      "correct": 3,
      "explanation": "Coordinated eviction makes decisions based on cluster-wide access patterns rather than per-node. A rarely-accessed item on one node might be hot cluster-wide. This is complex but can improve overall hit rate.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-071",
      "type": "multiple-choice",
      "question": "Why might coordinated eviction not be worth the complexity?",
      "options": [
        "It never improves hit rates",
        "Coordination overhead may exceed benefits; local LRU is often good enough",
        "It's not possible with existing technology",
        "It only works with FIFO"
      ],
      "correct": 1,
      "explanation": "Coordinating requires messaging between nodes, tracking global state, and consensus—significant overhead. For many workloads, independent per-node LRU with consistent hashing achieves good hit rates with zero coordination.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-072",
      "type": "multiple-choice",
      "question": "In a CDN with edge and origin caches, where should eviction be more aggressive?",
      "options": [
        "Edge (closer to users)",
        "Origin (centralized)",
        "Same at both",
        "Depends on content type"
      ],
      "correct": 0,
      "explanation": "Edge caches are smaller and serve regional traffic—more aggressive eviction keeps them focused on locally popular content. Origin cache is larger and serves edge misses—it can retain more content longer.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-073",
      "type": "multiple-choice",
      "question": "What is 'cache tiering' in the context of eviction?",
      "options": [
        "Using different pricing tiers",
        "Having multiple cache levels where evictions from one tier flow to the next",
        "Tiered TTLs for different content types",
        "Access control tiers"
      ],
      "correct": 1,
      "explanation": "Cache tiering (e.g., memory → SSD → disk) lets evicted items from fast tiers move to slower tiers rather than being fully evicted. Items still in slower tiers can be promoted back if accessed.",
      "detailedExplanation": "Generalize from 'cache tiering' in the context of eviction to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-074",
      "type": "multi-select",
      "question": "Which are advantages of using SSD as a cache tier behind memory? (Select all that apply)",
      "options": [
        "Larger capacity for evicted items",
        "Faster than origin/database access",
        "Lower cost per GB than memory",
        "Zero latency"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "SSD is larger and cheaper than RAM, while faster than disk/network origin. It's a middle tier: items evicted from memory go to SSD (not discarded), giving them a second chance before full eviction.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-075",
      "type": "multiple-choice",
      "question": "What is 'write amplification' in SSD cache tiering?",
      "options": [
        "Writing to multiple replicas",
        "Excessive writes due to eviction/promotion churn wearing out the SSD",
        "Amplified write throughput",
        "Writing larger chunks than necessary"
      ],
      "correct": 1,
      "explanation": "SSD has limited write endurance. If cache churn constantly evicts to SSD and promotes back, excessive writes wear out the SSD. This must be managed with admission control or write coalescing.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-076",
      "type": "multiple-choice",
      "question": "Facebook's TAO uses 'expiry buckets'. What are these?",
      "options": [
        "Groups of servers by region",
        "Time-based buckets that batch items with similar expiration for efficient scanning",
        "Storage partitions by object type",
        "Rate limiting buckets"
      ],
      "correct": 1,
      "explanation": "Instead of per-item TTL tracking, items are placed in time buckets (e.g., expire in 1-2 hours). When a bucket's time arrives, scan the whole bucket. This amortizes expiration overhead across many items.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 1 and 2 hours appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-077",
      "type": "multiple-choice",
      "question": "What is the 'working set' concept in cache sizing?",
      "options": [
        "All items ever accessed",
        "Items that should be configured",
        "The set of items actively being accessed in a time window",
        "Items with highest priority"
      ],
      "correct": 2,
      "explanation": "Working set is the subset of data actively accessed during a time period. If cache size ≥ working set, hit rates approach 100%. If cache < working set, some items must be evicted and later re-fetched.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. If values like 100 appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-078",
      "type": "numeric-input",
      "question": "Working set analysis shows 95% of accesses hit 10% of items. Your full dataset is 1TB. What's the minimum cache size to achieve ~95% hit rate?",
      "answer": 100,
      "tolerance": 0.1,
      "unit": "GB",
      "explanation": "If 10% of data (hot set) serves 95% of requests, caching that 10% gives ~95% hit rate. 10% of 1TB = 100GB. This is the 'hot data cache' approach.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Normalize units before computing so conversion mistakes do not propagate. Treat freshness policy and invalidation paths as first-class constraints. If values like 95 and 10 appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-079",
      "type": "multiple-choice",
      "question": "What is 'cache warming' and how does it relate to eviction?",
      "options": [
        "Increasing cache temperature for better performance",
        "Pre-populating cache after restart to avoid cold-start misses and eviction churn",
        "Gradually increasing eviction rate",
        "Heating servers to optimal temperature"
      ],
      "correct": 1,
      "explanation": "After restart, cache is empty (cold). Cache warming pre-loads expected hot items to avoid the cold-start period where everything misses. Without warming, the initial flood of misses may cause churn.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-080",
      "type": "two-stage",
      "stages": [
        {
          "question": "A cache uses LFU with no aging. After running for 1 year, item X has 1 billion accesses. New item Y is accessed 10 times/second. X is still being accessed occasionally. How long until Y has enough count to avoid eviction against X?",
          "options": [
            "~1 day",
            "~1 month",
            "~3 years",
            "Never (without aging)"
          ],
          "correct": 3,
          "explanation": "At 10/sec, Y accumulates ~315M accesses/year. To reach X's 1B would take ~3.2 years even if X stopped entirely. But X continues accumulating too. Without aging, Y may never catch up — historically popular items are practically permanent.",
          "detailedExplanation": "Generalize from cache uses LFU with no aging to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 1 and 10 appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "What if X is now completely inactive (zero accesses) and Y sustains 10/sec?",
          "options": ["~28 hours", "~3 years", "~10 years", "Still never"],
          "correct": 1,
          "explanation": "If X stops being accessed, Y catches up at 10/sec. 1B / (10 × 3600 × 24 × 365) ≈ 3.2 years. Even with X completely inactive, it takes years for Y to overtake. This shows why LFU without aging keeps stale popular items practically forever.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 10 and 1B in aligned units before deciding on an implementation approach. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-081",
      "type": "multiple-choice",
      "question": "What is the 'one-hit-wonder' problem in caching?",
      "options": [
        "Items that cause exactly one cache hit",
        "Items accessed once that take cache space from items that would be accessed multiple times",
        "Popular items that stop being accessed",
        "Cache entries that hit only one server"
      ],
      "correct": 1,
      "explanation": "One-hit-wonders are items accessed once and never again. In pure LRU, they can evict items that would have had multiple future accesses. Scan resistance policies (2Q, W-TinyLFU) address this.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-082",
      "type": "multiple-choice",
      "question": "How does the 'probationary period' concept help with one-hit-wonders?",
      "options": [
        "Items must wait before being cached",
        "New items enter a smaller/separate pool and must prove value to enter main cache",
        "Items are tested for correctness",
        "Items must pass validation before eviction"
      ],
      "correct": 1,
      "explanation": "Probationary designs (2Q, W-TinyLFU) put new items in a trial area. Only if re-accessed during probation do they enter the protected main cache. One-hit-wonders are evicted from probation without polluting main cache.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-083",
      "type": "multi-select",
      "question": "Which eviction policies have built-in scan resistance? (Select all that apply)",
      "options": ["Pure LRU", "2Q", "ARC", "W-TinyLFU"],
      "correctIndices": [1, 2, 3],
      "explanation": "2Q, ARC, and W-TinyLFU all filter one-time accesses: 2Q via FIFO probation, ARC via ghost lists and adaptation, W-TinyLFU via window admission. Pure LRU has no scan resistance—scans pollute the cache.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-084",
      "type": "multiple-choice",
      "question": "What does 'frequency + recency' mean in combined eviction policies?",
      "options": [
        "Running both LRU and LFU separately",
        "Using both how often and how recently an item was accessed in eviction decisions",
        "Counting accesses per time period",
        "Alternating between frequency and recency"
      ],
      "correct": 1,
      "explanation": "Combined policies score items by both frequency (total accesses) and recency (time since last access). ARC, LIRS, and W-TinyLFU combine these factors to outperform pure LRU or pure LFU alone.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-085",
      "type": "multiple-choice",
      "question": "What is 'segmented LRU' (SLRU)?",
      "options": [
        "LRU per memory segment",
        "Multiple LRU queues where items promote/demote between segments based on access",
        "Geographic segments with independent LRU",
        "LRU with size-based segmentation"
      ],
      "correct": 1,
      "explanation": "SLRU has segments like 'probationary' and 'protected'. New items enter probationary. Accessed items promote to protected. Protected items demote to probationary on access to others. This provides scan resistance.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-086",
      "type": "multiple-choice",
      "question": "In SLRU, what happens when the protected segment is full and an item needs to promote?",
      "options": [
        "Promotion is denied",
        "An item is demoted from protected to probationary to make room",
        "The protected segment expands",
        "The item skips protected"
      ],
      "correct": 1,
      "explanation": "When protected is full, the LRU item in protected demotes to probationary segment. This makes room for the promoting item. Demoted items can promote again if re-accessed.",
      "detailedExplanation": "Generalize from in SLRU, what happens when the protected segment is full and an item needs to promote to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-087",
      "type": "multiple-choice",
      "question": "What is the main trade-off in using more sophisticated eviction policies?",
      "options": [
        "Lower hit rate",
        "Higher memory and CPU overhead for tracking",
        "Incompatibility with distributed caches",
        "Shorter TTLs"
      ],
      "correct": 1,
      "explanation": "Sophisticated policies (ARC, W-TinyLFU) track more state per item and require more computation. For some workloads, simple LRU with larger cache achieves similar hit rates more cheaply.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-088",
      "type": "numeric-input",
      "question": "ARC maintains 4 lists plus 2 ghost lists. If each list entry needs 16 bytes for pointers, what's the per-item overhead compared to simple LRU (1 list, 16 bytes)?",
      "answer": 3,
      "tolerance": 0.2,
      "unit": "x",
      "explanation": "Items may exist in T1 or T2 (16 bytes), plus ghost entries in B1 or B2 (16 bytes each). Maximum overhead is ~48 bytes vs 16 bytes = 3x. Actual overhead depends on configuration.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep every transformation in one unit system and check order of magnitude at the end. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 4 and 2 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-089",
      "type": "multiple-choice",
      "question": "When might you prefer simple LRU over W-TinyLFU despite lower hit rates?",
      "options": [
        "Never—W-TinyLFU is always better",
        "When cache is so large that working set fits easily",
        "When items are very large",
        "When using distributed cache"
      ],
      "correct": 1,
      "explanation": "If cache is much larger than working set, hit rates approach 100% regardless of policy. The sophistication of W-TinyLFU doesn't help when nothing needs evicting. Simple LRU has lower overhead.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. If values like 100 appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-090",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're evaluating eviction policies for a new cache. What's the best data to analyze?",
          "options": [
            "Random sample requests",
            "Production access logs (trace replay)",
            "Synthetic benchmarks",
            "Theoretical analysis"
          ],
          "correct": 1,
          "explanation": "Real access traces from production show your actual workload patterns. Replaying these traces against different policies gives accurate predictions of hit rates for your specific use case.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes."
        },
        {
          "question": "Your trace shows: LRU=85% hit rate, ARC=88% hit rate, OPT=89% hit rate. What should you conclude?",
          "options": [
            "ARC is obviously better, switch now",
            "LRU is close to optimal; ARC's 3% gain may not justify complexity",
            "Need more data",
            "OPT proves your cache is undersized"
          ],
          "correct": 1,
          "explanation": "OPT is the upper bound (89%). LRU (85%) is already within 4% of optimal. ARC (88%) gains only 3% over LRU while adding significant complexity. Whether that 3% justifies the complexity depends on your scale.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 85 and 88 in aligned units before deciding on an implementation approach. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-091",
      "type": "multiple-choice",
      "question": "What is 'admission policy' vs 'eviction policy'?",
      "options": [
        "Same thing, different names",
        "Admission decides what enters; eviction decides what leaves",
        "Admission is for writes; eviction is for reads",
        "Admission controls rate; eviction controls capacity"
      ],
      "correct": 1,
      "explanation": "They're separate decisions: Admission policy (should this item enter the cache at all?). Eviction policy (which item should leave when cache is full?). Both work together for optimal hit rates.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-092",
      "type": "multiple-choice",
      "question": "A 'bypass' or 'never cache' admission rule might be used for what type of items?",
      "options": [
        "Very popular items",
        "Items known to be accessed only once (scan data, sequential reads)",
        "Small items",
        "High-priority items"
      ],
      "correct": 1,
      "explanation": "If you know an item won't be reaccessed (sequential scan, backup reads), don't cache it. Bypassing saves the cost of admission and eviction while preventing pollution. Some storage systems detect sequential patterns automatically.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-093",
      "type": "multi-select",
      "question": "Which are effective ways to monitor if your eviction policy is working well? (Select all that apply)",
      "options": [
        "Track hit rate over time",
        "Monitor eviction rate and churn",
        "Compare against OPT using trace replay",
        "Count total items ever cached"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Useful metrics: Hit rate shows overall effectiveness. Eviction rate/churn indicates pressure and thrashing. OPT comparison shows room for improvement. Total items ever cached is a cumulative counter that doesn't reveal whether the current policy is working — it grows monotonically regardless of policy quality.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-094",
      "type": "multiple-choice",
      "question": "What might indicate your cache is too small regardless of eviction policy?",
      "options": [
        "Hit rate is stable at 99%",
        "Eviction rate tracks closely with request rate (constant churn)",
        "Latency is low",
        "Memory usage is low"
      ],
      "correct": 1,
      "explanation": "If evictions closely track requests, everything that enters immediately gets evicted. This indicates working set > cache size. No policy can help—you need more capacity or to reduce working set.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-095",
      "type": "multiple-choice",
      "question": "What is 'hit rate cliff'?",
      "options": [
        "Sudden drop in hit rate when cache size decreases below working set",
        "Maximum possible hit rate",
        "Hit rate plateau",
        "Hit rate at cold start"
      ],
      "correct": 0,
      "explanation": "Hit rate often drops sharply (cliff) when cache size falls below working set. Above the threshold, hit rate is high. Below it, hit rate plummets because the cache can't hold active data. This non-linear behavior is important for capacity planning.",
      "detailedExplanation": "Generalize from 'hit rate cliff' to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-096",
      "type": "multiple-choice",
      "question": "How do you find the optimal cache size for cost efficiency?",
      "options": [
        "Always use the largest cache possible",
        "Plot hit rate vs cache size; find the knee where gains flatten",
        "Use the smallest cache with any hits",
        "Match cache size to database size"
      ],
      "correct": 1,
      "explanation": "Plot hit rate vs cache size. Initially, adding cache gives large hit rate gains. Eventually, gains flatten (working set is cached). The 'knee' of this curve is often the cost-efficient point—beyond it, you pay more for diminishing returns.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-097",
      "type": "two-stage",
      "stages": [
        {
          "question": "Cache hit rate data: 10GB cache=70%, 20GB=85%, 40GB=95%, 80GB=97%, 160GB=98%. Where's the approximate knee?",
          "options": ["20GB", "40GB", "80GB", "160GB"],
          "correct": 1,
          "explanation": "Gains: 10→20GB gives +15%, 20→40GB gives +10%, 40→80GB gives +2%, 80→160GB gives +1%. The steep drop in marginal gain is between 40GB and 80GB. 40GB is approximately the knee.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 10GB and 70 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "If 40GB cache costs $400/mo and 80GB costs $700/mo, which is more cost-effective for a busy service where misses cost latency?",
          "options": [
            "40GB—diminishing returns make 80GB wasteful",
            "80GB—the 2% gain likely justifies $300/mo for a busy service",
            "Cannot determine without request volume",
            "Both are equivalent"
          ],
          "correct": 2,
          "explanation": "Need to know request volume to compute cost-per-miss-avoided. At high volume, 2% of millions of requests is significant. At low volume, the $300/mo may not pay off. Cost-effectiveness depends on scale.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 40GB and 400 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-098",
      "type": "multiple-choice",
      "question": "What is 'thrashing' in the context of caching?",
      "options": [
        "Cache server crashing",
        "Continuous eviction and reload of the same items, resulting in nearly 0% hit rate",
        "High memory usage",
        "Fast cache growth"
      ],
      "correct": 1,
      "explanation": "Thrashing occurs when working set far exceeds cache size. Items are evicted immediately after caching, and every request needs to reload from origin. The cache provides no benefit—it's just overhead.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-099",
      "type": "multi-select",
      "question": "Which are symptoms of cache thrashing? (Select all that apply)",
      "options": [
        "Near-0% hit rate",
        "Eviction rate ≈ request rate",
        "High origin load despite having cache",
        "Low cache memory usage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Thrashing symptoms: ~0% hits (everything misses), eviction rate matches request rate (everything evicted immediately), origin overwhelmed (cache not helping). Memory is actually full—that's the problem.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Treat freshness policy and invalidation paths as first-class constraints. If values like 0 appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    },
    {
      "id": "evict-100",
      "type": "multiple-choice",
      "question": "When evaluating eviction policies for your system, what's the best first step?",
      "options": [
        "Implement ARC since it's most sophisticated",
        "Analyze your workload's access patterns with production traces",
        "Use random eviction for simplicity",
        "Ask vendors for recommendations"
      ],
      "correct": 1,
      "explanation": "Start with workload analysis. Understand: Is access skewed or uniform? Are there scans? What's the working set size? Then simulate different policies against your traces. The best policy depends entirely on your specific patterns.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "eviction-policies"],
      "difficulty": "senior"
    }
  ]
}
