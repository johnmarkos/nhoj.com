{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 8,
  "chapterTitle": "Notification System Scale & Scenarios",
  "chapterDescription": "Integrated notification incidents combining spikes, provider failures, retries, dedupe, and SLA-priority trade-offs.",
  "problems": [
    {
      "id": "cd-ns-001",
      "type": "multiple-choice",
      "question": "Case Alpha: global campaign launch incident. Dominant risk is priority inversion between transactional and bulk queues. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Stabilize with priority queue isolation before expanding throughput.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For global campaign launch incident, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Stabilize with priority queue isolation before expanding throughput\" outperforms the alternatives because it targets priority inversion between transactional and bulk queues and preserves safe recovery behavior. It is also the most compatible with The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "The core signal here is \"global campaign launch incident\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-002",
      "type": "multiple-choice",
      "question": "Case Beta: critical security alert dispatch. Dominant risk is cross-provider dedupe failure during failover. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply provider-aware failover with cross-channel dedupe protection.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat critical security alert dispatch as a reliability-control decision, not an averages-only optimization. \"Apply provider-aware failover with cross-channel dedupe protection\" is correct since it mitigates cross-provider dedupe failure during failover while keeping containment local. The decision remains valid given: Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "Use \"critical security alert dispatch\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-003",
      "type": "multiple-choice",
      "question": "Case Gamma: provider outage failover workflow. Dominant risk is retry storm breaching provider quotas. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Gate retries with budgets and jitter so recovery does not cause quota collapse.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Provider outage failover workflow should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Gate retries with budgets and jitter so recovery does not cause quota collapse\" is strongest because it directly addresses retry storm breaching provider quotas and improves repeatability under stress. This aligns with the extra condition (Leadership asked for a fix that reduces recurrence, not just MTTR).",
      "detailedExplanation": "This prompt is really about \"provider outage failover workflow\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-004",
      "type": "multiple-choice",
      "question": "Case Delta: multi-channel retry storm event. Dominant risk is scheduler lag violating delivery SLA windows. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Protect SLA-critical alerts with reserved capacity and separate admission policy."
      ],
      "correct": 3,
      "explanation": "In Notification System Scale & Scenarios, multi-channel retry storm event fails mainly through scheduler lag violating delivery SLA windows. The best choice is \"Protect SLA-critical alerts with reserved capacity and separate admission policy\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A previous rollback fixed averages but not tail impact.",
      "detailedExplanation": "The decision turns on \"multi-channel retry storm event\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: holiday-traffic queue surge. Dominant risk is fallback routing loops across adapters. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Use explicit fallback graphs to prevent adapter routing loops.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For holiday-traffic queue surge, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Use explicit fallback graphs to prevent adapter routing loops\" outperforms the alternatives because it targets fallback routing loops across adapters and preserves safe recovery behavior. It is also the most compatible with User trust risk is highest on this path.",
      "detailedExplanation": "Read this as a scenario about \"holiday-traffic queue surge\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-006",
      "type": "multiple-choice",
      "question": "Case Zeta: preference-sync inconsistency incident. Dominant risk is audit gaps obscuring delivery truth. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Reconcile audit truth via idempotent event lineage across providers.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat preference-sync inconsistency incident as a reliability-control decision, not an averages-only optimization. \"Reconcile audit truth via idempotent event lineage across providers\" is correct since it mitigates audit gaps obscuring delivery truth while keeping containment local. The decision remains valid given: A shared dependency has uncertain health right now.",
      "detailedExplanation": "The key clue in this question is \"preference-sync inconsistency incident\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-007",
      "type": "multiple-choice",
      "question": "Case Eta: bulk notification throttling event. Dominant risk is campaign controls disabled during incident response. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Throttle campaign traffic automatically when transactional latency degrades.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Bulk notification throttling event should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Throttle campaign traffic automatically when transactional latency degrades\" is strongest because it directly addresses campaign controls disabled during incident response and improves repeatability under stress. This aligns with the extra condition (The change must preserve cost discipline during peak).",
      "detailedExplanation": "Start from \"bulk notification throttling event\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-008",
      "type": "multiple-choice",
      "question": "Case Theta: transactional alert delay bridge. Dominant risk is preference mismatch causing over-notification. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Sequence recovery by user-impact class, then backfill deferred low-priority sends."
      ],
      "correct": 3,
      "explanation": "In Notification System Scale & Scenarios, transactional alert delay bridge fails mainly through preference mismatch causing over-notification. The best choice is \"Sequence recovery by user-impact class, then backfill deferred low-priority sends\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry shows risk concentrated in one partition class.",
      "detailedExplanation": "If you keep \"transactional alert delay bridge\" in view, the correct answer separates faster. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-009",
      "type": "multiple-choice",
      "question": "Case Iota: regional dispatch degradation case. Dominant risk is regional outage amplifying backlog. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Validate preference snapshots during incident mitigation before replaying backlog.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For regional dispatch degradation case, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Validate preference snapshots during incident mitigation before replaying backlog\" outperforms the alternatives because it targets regional outage amplifying backlog and preserves safe recovery behavior. It is also the most compatible with The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "The core signal here is \"regional dispatch degradation case\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-010",
      "type": "multiple-choice",
      "question": "Case Kappa: post-incident notification hardening review. Dominant risk is incomplete incident sequencing creating second outage. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Close incidents with drill-backed guardrails to prevent repeat cascades.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Post-incident notification hardening review should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Close incidents with drill-backed guardrails to prevent repeat cascades\" is strongest because it directly addresses incomplete incident sequencing creating second outage and improves repeatability under stress. This aligns with the extra condition (Current runbooks are missing explicit ownership for this boundary).",
      "detailedExplanation": "This prompt is really about \"post-incident notification hardening review\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-011",
      "type": "multiple-choice",
      "question": "Case Lambda: global campaign launch incident. Dominant risk is priority inversion between transactional and bulk queues. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Stabilize with priority queue isolation before expanding throughput.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Notification System Scale & Scenarios, global campaign launch incident fails mainly through priority inversion between transactional and bulk queues. The best choice is \"Stabilize with priority queue isolation before expanding throughput\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A cross-region path recently changed behavior after migration.",
      "detailedExplanation": "Use \"global campaign launch incident\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-012",
      "type": "multiple-choice",
      "question": "Case Mu: critical security alert dispatch. Dominant risk is cross-provider dedupe failure during failover. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply provider-aware failover with cross-channel dedupe protection."
      ],
      "correct": 3,
      "explanation": "For critical security alert dispatch, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Apply provider-aware failover with cross-channel dedupe protection\" outperforms the alternatives because it targets cross-provider dedupe failure during failover and preserves safe recovery behavior. It is also the most compatible with Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "The core signal here is \"critical security alert dispatch\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-013",
      "type": "multiple-choice",
      "question": "Case Nu: provider outage failover workflow. Dominant risk is retry storm breaching provider quotas. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Gate retries with budgets and jitter so recovery does not cause quota collapse.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat provider outage failover workflow as a reliability-control decision, not an averages-only optimization. \"Gate retries with budgets and jitter so recovery does not cause quota collapse\" is correct since it mitigates retry storm breaching provider quotas while keeping containment local. The decision remains valid given: Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "If you keep \"provider outage failover workflow\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-014",
      "type": "multiple-choice",
      "question": "Case Xi: multi-channel retry storm event. Dominant risk is scheduler lag violating delivery SLA windows. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Protect SLA-critical alerts with reserved capacity and separate admission policy.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Multi-channel retry storm event should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Protect SLA-critical alerts with reserved capacity and separate admission policy\" is strongest because it directly addresses scheduler lag violating delivery SLA windows and improves repeatability under stress. This aligns with the extra condition (A partial failure is masking itself as success in metrics).",
      "detailedExplanation": "Start from \"multi-channel retry storm event\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-015",
      "type": "multiple-choice",
      "question": "Case Omicron: holiday-traffic queue surge. Dominant risk is fallback routing loops across adapters. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use explicit fallback graphs to prevent adapter routing loops.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Notification System Scale & Scenarios, holiday-traffic queue surge fails mainly through fallback routing loops across adapters. The best choice is \"Use explicit fallback graphs to prevent adapter routing loops\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This fix must hold under celebrity or campaign spike conditions.",
      "detailedExplanation": "The key clue in this question is \"holiday-traffic queue surge\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-016",
      "type": "multiple-choice",
      "question": "Case Pi: preference-sync inconsistency incident. Dominant risk is audit gaps obscuring delivery truth. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Reconcile audit truth via idempotent event lineage across providers."
      ],
      "correct": 3,
      "explanation": "For preference-sync inconsistency incident, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Reconcile audit truth via idempotent event lineage across providers\" outperforms the alternatives because it targets audit gaps obscuring delivery truth and preserves safe recovery behavior. It is also the most compatible with SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "Read this as a scenario about \"preference-sync inconsistency incident\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-017",
      "type": "multiple-choice",
      "question": "Case Rho: bulk notification throttling event. Dominant risk is campaign controls disabled during incident response. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Throttle campaign traffic automatically when transactional latency degrades.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat bulk notification throttling event as a reliability-control decision, not an averages-only optimization. \"Throttle campaign traffic automatically when transactional latency degrades\" is correct since it mitigates campaign controls disabled during incident response while keeping containment local. The decision remains valid given: On-call requested a reversible operational first step.",
      "detailedExplanation": "The decision turns on \"bulk notification throttling event\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-018",
      "type": "multiple-choice",
      "question": "Case Sigma: transactional alert delay bridge. Dominant risk is preference mismatch causing over-notification. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Sequence recovery by user-impact class, then backfill deferred low-priority sends.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Transactional alert delay bridge should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Sequence recovery by user-impact class, then backfill deferred low-priority sends\" is strongest because it directly addresses preference mismatch causing over-notification and improves repeatability under stress. This aligns with the extra condition (The system mixes strict and eventual paths with unclear contracts).",
      "detailedExplanation": "This prompt is really about \"transactional alert delay bridge\". Discard choices that violate required invariants during concurrent or failed states. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-019",
      "type": "multiple-choice",
      "question": "Case Tau: regional dispatch degradation case. Dominant risk is regional outage amplifying backlog. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Validate preference snapshots during incident mitigation before replaying backlog.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Notification System Scale & Scenarios, regional dispatch degradation case fails mainly through regional outage amplifying backlog. The best choice is \"Validate preference snapshots during incident mitigation before replaying backlog\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A hot-key pattern is likely from real traffic skew.",
      "detailedExplanation": "Use \"regional dispatch degradation case\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: post-incident notification hardening review. Dominant risk is incomplete incident sequencing creating second outage. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Close incidents with drill-backed guardrails to prevent repeat cascades."
      ],
      "correct": 3,
      "explanation": "Treat post-incident notification hardening review as a reliability-control decision, not an averages-only optimization. \"Close incidents with drill-backed guardrails to prevent repeat cascades\" is correct since it mitigates incomplete incident sequencing creating second outage while keeping containment local. The decision remains valid given: The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "The key clue in this question is \"post-incident notification hardening review\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-021",
      "type": "multiple-choice",
      "question": "Case Phi: global campaign launch incident. Dominant risk is priority inversion between transactional and bulk queues. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Stabilize with priority queue isolation before expanding throughput.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Global campaign launch incident should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Stabilize with priority queue isolation before expanding throughput\" is strongest because it directly addresses priority inversion between transactional and bulk queues and improves repeatability under stress. This aligns with the extra condition (Compliance requires explicit behavior for edge-case failures).",
      "detailedExplanation": "Start from \"global campaign launch incident\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-022",
      "type": "multiple-choice",
      "question": "Case Chi: critical security alert dispatch. Dominant risk is cross-provider dedupe failure during failover. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply provider-aware failover with cross-channel dedupe protection.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In Notification System Scale & Scenarios, critical security alert dispatch fails mainly through cross-provider dedupe failure during failover. The best choice is \"Apply provider-aware failover with cross-channel dedupe protection\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This boundary has failed during the last two game days.",
      "detailedExplanation": "The decision turns on \"critical security alert dispatch\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-023",
      "type": "multiple-choice",
      "question": "Case Psi: provider outage failover workflow. Dominant risk is retry storm breaching provider quotas. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Gate retries with budgets and jitter so recovery does not cause quota collapse.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For provider outage failover workflow, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Gate retries with budgets and jitter so recovery does not cause quota collapse\" outperforms the alternatives because it targets retry storm breaching provider quotas and preserves safe recovery behavior. It is also the most compatible with A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "Read this as a scenario about \"provider outage failover workflow\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-024",
      "type": "multiple-choice",
      "question": "Case Omega: multi-channel retry storm event. Dominant risk is scheduler lag violating delivery SLA windows. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Protect SLA-critical alerts with reserved capacity and separate admission policy."
      ],
      "correct": 3,
      "explanation": "Treat multi-channel retry storm event as a reliability-control decision, not an averages-only optimization. \"Protect SLA-critical alerts with reserved capacity and separate admission policy\" is correct since it mitigates scheduler lag violating delivery SLA windows while keeping containment local. The decision remains valid given: Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "Use \"multi-channel retry storm event\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-025",
      "type": "multiple-choice",
      "question": "Case Atlas: holiday-traffic queue surge. Dominant risk is fallback routing loops across adapters. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Use explicit fallback graphs to prevent adapter routing loops.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Holiday-traffic queue surge should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Use explicit fallback graphs to prevent adapter routing loops\" is strongest because it directly addresses fallback routing loops across adapters and improves repeatability under stress. This aligns with the extra condition (The fix should avoid broad architectural rewrites this quarter).",
      "detailedExplanation": "This prompt is really about \"holiday-traffic queue surge\". Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-026",
      "type": "multiple-choice",
      "question": "Case Nova: preference-sync inconsistency incident. Dominant risk is audit gaps obscuring delivery truth. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Reconcile audit truth via idempotent event lineage across providers.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In Notification System Scale & Scenarios, preference-sync inconsistency incident fails mainly through audit gaps obscuring delivery truth. The best choice is \"Reconcile audit truth via idempotent event lineage across providers\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Current metrics hide per-tenant variance that matters.",
      "detailedExplanation": "If you keep \"preference-sync inconsistency incident\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-027",
      "type": "multiple-choice",
      "question": "Case Orion: bulk notification throttling event. Dominant risk is campaign controls disabled during incident response. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Throttle campaign traffic automatically when transactional latency degrades.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For bulk notification throttling event, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Throttle campaign traffic automatically when transactional latency degrades\" outperforms the alternatives because it targets campaign controls disabled during incident response and preserves safe recovery behavior. It is also the most compatible with The fallback path is under-tested in production-like load.",
      "detailedExplanation": "The core signal here is \"bulk notification throttling event\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-028",
      "type": "multiple-choice",
      "question": "Case Vega: transactional alert delay bridge. Dominant risk is preference mismatch causing over-notification. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Sequence recovery by user-impact class, then backfill deferred low-priority sends."
      ],
      "correct": 3,
      "explanation": "Treat transactional alert delay bridge as a reliability-control decision, not an averages-only optimization. \"Sequence recovery by user-impact class, then backfill deferred low-priority sends\" is correct since it mitigates preference mismatch causing over-notification while keeping containment local. The decision remains valid given: A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "The key clue in this question is \"transactional alert delay bridge\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-029",
      "type": "multiple-choice",
      "question": "Case Helios: regional dispatch degradation case. Dominant risk is regional outage amplifying backlog. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Validate preference snapshots during incident mitigation before replaying backlog.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Regional dispatch degradation case should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Validate preference snapshots during incident mitigation before replaying backlog\" is strongest because it directly addresses regional outage amplifying backlog and improves repeatability under stress. This aligns with the extra condition (The system must preserve critical events over bulk traffic).",
      "detailedExplanation": "Start from \"regional dispatch degradation case\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-030",
      "type": "multiple-choice",
      "question": "Case Aurora: post-incident notification hardening review. Dominant risk is incomplete incident sequencing creating second outage. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Close incidents with drill-backed guardrails to prevent repeat cascades.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For post-incident notification hardening review, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Close incidents with drill-backed guardrails to prevent repeat cascades\" outperforms the alternatives because it targets incomplete incident sequencing creating second outage and preserves safe recovery behavior. It is also the most compatible with Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "The core signal here is \"post-incident notification hardening review\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: global campaign launch incident. Dominant risk is priority inversion between transactional and bulk queues. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Stabilize with priority queue isolation before expanding throughput.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat global campaign launch incident as a reliability-control decision, not an averages-only optimization. \"Stabilize with priority queue isolation before expanding throughput\" is correct since it mitigates priority inversion between transactional and bulk queues while keeping containment local. The decision remains valid given: The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "If you keep \"global campaign launch incident\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-032",
      "type": "multiple-choice",
      "question": "Case Pulse: critical security alert dispatch. Dominant risk is cross-provider dedupe failure during failover. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply provider-aware failover with cross-channel dedupe protection."
      ],
      "correct": 3,
      "explanation": "Critical security alert dispatch should be solved at the failure boundary named in Notification System Scale & Scenarios. \"Apply provider-aware failover with cross-channel dedupe protection\" is strongest because it directly addresses cross-provider dedupe failure during failover and improves repeatability under stress. This aligns with the extra condition (Operational complexity is rising faster than team onboarding).",
      "detailedExplanation": "This prompt is really about \"critical security alert dispatch\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-033",
      "type": "multiple-choice",
      "question": "Case Forge: provider outage failover workflow. Dominant risk is retry storm breaching provider quotas. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Gate retries with budgets and jitter so recovery does not cause quota collapse.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In Notification System Scale & Scenarios, provider outage failover workflow fails mainly through retry storm breaching provider quotas. The best choice is \"Gate retries with budgets and jitter so recovery does not cause quota collapse\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Stakeholders need clear trade-off rationale in the postmortem.",
      "detailedExplanation": "Use \"provider outage failover workflow\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-034",
      "type": "multiple-choice",
      "question": "Case Harbor: multi-channel retry storm event. Dominant risk is scheduler lag violating delivery SLA windows. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Protect SLA-critical alerts with reserved capacity and separate admission policy.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For multi-channel retry storm event, prefer the option that prevents reoccurrence in Notification System Scale & Scenarios. \"Protect SLA-critical alerts with reserved capacity and separate admission policy\" outperforms the alternatives because it targets scheduler lag violating delivery SLA windows and preserves safe recovery behavior. It is also the most compatible with A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "Read this as a scenario about \"multi-channel retry storm event\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-035",
      "type": "multiple-choice",
      "question": "Case Vector: holiday-traffic queue surge. Dominant risk is fallback routing loops across adapters. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use explicit fallback graphs to prevent adapter routing loops.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat holiday-traffic queue surge as a reliability-control decision, not an averages-only optimization. \"Use explicit fallback graphs to prevent adapter routing loops\" is correct since it mitigates fallback routing loops across adapters while keeping containment local. The decision remains valid given: Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "The decision turns on \"holiday-traffic queue surge\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for global campaign launch incident: signal points to scheduler lag violating delivery SLA windows. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around global campaign launch incident mismatches scheduler lag violating delivery SLA windows, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Notification System Scale & Scenarios, the best answer is \"The current decomposition around global campaign launch incident mismatches scheduler lag violating delivery SLA windows, creating repeated failures\". It is the option most directly aligned to scheduler lag violating delivery SLA windows while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident review for global campaign launch incident: signal points to scheduler lag\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for global campaign launch incident:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Reconcile audit truth via idempotent event lineage across providers.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Notification System Scale & Scenarios: for With diagnosis confirmed in \"incident review for global campaign launch incident:\", what is the highest-leverage change to make now, \"Reconcile audit truth via idempotent event lineage across providers\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Start from \"notification System Scale & Scenarios\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for critical security alert dispatch: signal points to fallback routing loops across adapters. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around critical security alert dispatch mismatches fallback routing loops across adapters, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Incident review for critical security alert dispatch: signal points to fallback routing loops across adapters is a two-step reliability decision. At stage 1, \"The current decomposition around critical security alert dispatch mismatches fallback routing loops across adapters, creating repeated failures\" wins because it balances immediate containment with long-term prevention around fallback routing loops across adapters.",
          "detailedExplanation": "If you keep \"incident review for critical security alert dispatch: signal points to fallback routing\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for critical security alert dispatch:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Throttle campaign traffic automatically when transactional latency degrades.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Throttle campaign traffic automatically when transactional latency degrades\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Scale & Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for provider outage failover workflow: signal points to audit gaps obscuring delivery truth. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures\" best matches Incident review for provider outage failover workflow: signal points to audit gaps obscuring delivery truth by targeting audit gaps obscuring delivery truth and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident review for provider outage failover workflow: signal points to audit gaps\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for provider outage failover workflow:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Sequence recovery by user-impact class, then backfill deferred low-priority sends."
          ],
          "correct": 3,
          "explanation": "Now that \"incident review for provider outage failover workflow:\" is diagnosed, what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Sequence recovery by user-impact class, then backfill deferred low-priority sends\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"notification System Scale & Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-channel retry storm event: signal points to campaign controls disabled during incident response. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around multi-channel retry storm event mismatches campaign controls disabled during incident response, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Notification System Scale & Scenarios: for Incident review for multi-channel retry storm event: signal points to campaign controls disabled during incident response, \"The current decomposition around multi-channel retry storm event mismatches campaign controls disabled during incident response, creating repeated failures\" is correct because it addresses campaign controls disabled during incident response and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident review for multi-channel retry storm event: signal points to campaign controls\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for multi-channel retry storm event:\", what first move gives the best reliability impact?",
          "options": [
            "Validate preference snapshots during incident mitigation before replaying backlog.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Validate preference snapshots during incident mitigation before replaying backlog\" best matches Using the diagnosis from \"incident review for multi-channel retry storm event:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"notification System Scale & Scenarios\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for holiday-traffic queue surge: signal points to preference mismatch causing over-notification. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around holiday-traffic queue surge mismatches preference mismatch causing over-notification, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for holiday-traffic queue surge: signal points to preference mismatch causing over-notification is a two-step reliability decision. At stage 1, \"The current decomposition around holiday-traffic queue surge mismatches preference mismatch causing over-notification, creating repeated failures\" wins because it balances immediate containment with long-term prevention around preference mismatch causing over-notification.",
          "detailedExplanation": "Use \"incident review for holiday-traffic queue surge: signal points to preference mismatch\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for holiday-traffic queue surge: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Close incidents with drill-backed guardrails to prevent repeat cascades.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Close incidents with drill-backed guardrails to prevent repeat cascades\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"notification System Scale & Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for preference-sync inconsistency incident: signal points to regional outage amplifying backlog. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around preference-sync inconsistency incident mismatches regional outage amplifying backlog, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around preference-sync inconsistency incident mismatches regional outage amplifying backlog, creating repeated failures\" best matches Incident review for preference-sync inconsistency incident: signal points to regional outage amplifying backlog by targeting regional outage amplifying backlog and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"incident review for preference-sync inconsistency incident: signal points to regional\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for preference-sync inconsistency\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Stabilize with priority queue isolation before expanding throughput.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "For \"incident review for preference-sync inconsistency\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Stabilize with priority queue isolation before expanding throughput\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for bulk notification throttling event: signal points to incomplete incident sequencing creating second outage. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around bulk notification throttling event mismatches incomplete incident sequencing creating second outage, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Notification System Scale & Scenarios: for Incident review for bulk notification throttling event: signal points to incomplete incident sequencing creating second outage, \"The current decomposition around bulk notification throttling event mismatches incomplete incident sequencing creating second outage, creating repeated failures\" is correct because it addresses incomplete incident sequencing creating second outage and improves controllability.",
          "detailedExplanation": "If you keep \"incident review for bulk notification throttling event: signal points to incomplete\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for bulk notification throttling event:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Apply provider-aware failover with cross-channel dedupe protection."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Apply provider-aware failover with cross-channel dedupe protection\" best matches Given the diagnosis in \"incident review for bulk notification throttling event:\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Scale & Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for transactional alert delay bridge: signal points to priority inversion between transactional and bulk queues. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around transactional alert delay bridge mismatches priority inversion between transactional and bulk queues, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Notification System Scale & Scenarios, the best answer is \"The current decomposition around transactional alert delay bridge mismatches priority inversion between transactional and bulk queues, creating repeated failures\". It is the option most directly aligned to priority inversion between transactional and bulk queues while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident review for transactional alert delay bridge: signal points to priority\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for transactional alert delay bridge:\", what should change first before wider rollout?",
          "options": [
            "Gate retries with budgets and jitter so recovery does not cause quota collapse.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Notification System Scale & Scenarios: for With diagnosis confirmed in \"incident review for transactional alert delay bridge:\", what should change first before wider rollout, \"Gate retries with budgets and jitter so recovery does not cause quota collapse\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"notification System Scale & Scenarios\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional dispatch degradation case: signal points to cross-provider dedupe failure during failover. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around regional dispatch degradation case mismatches cross-provider dedupe failure during failover, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for regional dispatch degradation case: signal points to cross-provider dedupe failure during failover is a two-step reliability decision. At stage 1, \"The current decomposition around regional dispatch degradation case mismatches cross-provider dedupe failure during failover, creating repeated failures\" wins because it balances immediate containment with long-term prevention around cross-provider dedupe failure during failover.",
          "detailedExplanation": "The key clue in this question is \"incident review for regional dispatch degradation case: signal points to cross-provider\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for regional dispatch degradation case:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Protect SLA-critical alerts with reserved capacity and separate admission policy.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Protect SLA-critical alerts with reserved capacity and separate admission policy\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"notification System Scale & Scenarios\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for post-incident notification hardening review: signal points to retry storm breaching provider quotas. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around post-incident notification hardening review mismatches retry storm breaching provider quotas, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around post-incident notification hardening review mismatches retry storm breaching provider quotas, creating repeated failures\" best matches Incident review for post-incident notification hardening review: signal points to retry storm breaching provider quotas by targeting retry storm breaching provider quotas and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident review for post-incident notification hardening review: signal points to retry\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for post-incident notification\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Use explicit fallback graphs to prevent adapter routing loops.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident review for post-incident notification\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Use explicit fallback graphs to prevent adapter routing loops\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for global campaign launch incident: signal points to scheduler lag violating delivery SLA windows. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around global campaign launch incident mismatches scheduler lag violating delivery SLA windows, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Notification System Scale & Scenarios: for Incident review for global campaign launch incident: signal points to scheduler lag violating delivery SLA windows, \"The current decomposition around global campaign launch incident mismatches scheduler lag violating delivery SLA windows, creating repeated failures\" is correct because it addresses scheduler lag violating delivery SLA windows and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident review for global campaign launch incident: signal points to scheduler lag\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for global campaign launch incident:\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Reconcile audit truth via idempotent event lineage across providers."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Reconcile audit truth via idempotent event lineage across providers\" best matches Now that \"incident review for global campaign launch incident:\" is diagnosed, what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Use \"notification System Scale & Scenarios\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for critical security alert dispatch: signal points to fallback routing loops across adapters. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around critical security alert dispatch mismatches fallback routing loops across adapters, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Notification System Scale & Scenarios, the best answer is \"The current decomposition around critical security alert dispatch mismatches fallback routing loops across adapters, creating repeated failures\". It is the option most directly aligned to fallback routing loops across adapters while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"incident review for critical security alert dispatch: signal points to fallback routing\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for critical security alert dispatch:\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Throttle campaign traffic automatically when transactional latency degrades.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Notification System Scale & Scenarios: for In the \"incident review for critical security alert dispatch:\" scenario, what is the highest-leverage change to make now, \"Throttle campaign traffic automatically when transactional latency degrades\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "This prompt is really about \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for provider outage failover workflow: signal points to audit gaps obscuring delivery truth. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for provider outage failover workflow: signal points to audit gaps obscuring delivery truth is a two-step reliability decision. At stage 1, \"The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures\" wins because it balances immediate containment with long-term prevention around audit gaps obscuring delivery truth.",
          "detailedExplanation": "Use \"incident review for provider outage failover workflow: signal points to audit gaps\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident review for provider outage failover workflow:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Sequence recovery by user-impact class, then backfill deferred low-priority sends.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Sequence recovery by user-impact class, then backfill deferred low-priority sends\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"notification System Scale & Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-channel retry storm event: signal points to campaign controls disabled during incident response. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around multi-channel retry storm event mismatches campaign controls disabled during incident response, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around multi-channel retry storm event mismatches campaign controls disabled during incident response, creating repeated failures\" best matches Incident review for multi-channel retry storm event: signal points to campaign controls disabled during incident response by targeting campaign controls disabled during incident response and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"incident review for multi-channel retry storm event: signal points to campaign controls\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident review for multi-channel retry storm event:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Validate preference snapshots during incident mitigation before replaying backlog.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "With root cause identified for \"incident review for multi-channel retry storm event:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Validate preference snapshots during incident mitigation before replaying backlog\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"notification System Scale & Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for holiday-traffic queue surge: signal points to preference mismatch causing over-notification. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around holiday-traffic queue surge mismatches preference mismatch causing over-notification, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Notification System Scale & Scenarios, the best answer is \"The current decomposition around holiday-traffic queue surge mismatches preference mismatch causing over-notification, creating repeated failures\". It is the option most directly aligned to preference mismatch causing over-notification while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident review for holiday-traffic queue surge: signal points to preference mismatch\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for holiday-traffic queue surge: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Close incidents with drill-backed guardrails to prevent repeat cascades."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Notification System Scale & Scenarios: for Given the diagnosis in \"incident review for holiday-traffic queue surge: signal\", which next change should be prioritized first, \"Close incidents with drill-backed guardrails to prevent repeat cascades\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"notification System Scale & Scenarios\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for preference-sync inconsistency incident: signal points to regional outage amplifying backlog. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around preference-sync inconsistency incident mismatches regional outage amplifying backlog, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for preference-sync inconsistency incident: signal points to regional outage amplifying backlog is a two-step reliability decision. At stage 1, \"The current decomposition around preference-sync inconsistency incident mismatches regional outage amplifying backlog, creating repeated failures\" wins because it balances immediate containment with long-term prevention around regional outage amplifying backlog.",
          "detailedExplanation": "If you keep \"incident review for preference-sync inconsistency incident: signal points to regional\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for preference-sync inconsistency\", which immediate adjustment best addresses the risk?",
          "options": [
            "Stabilize with priority queue isolation before expanding throughput.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Stabilize with priority queue isolation before expanding throughput\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for bulk notification throttling event: signal points to incomplete incident sequencing creating second outage. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around bulk notification throttling event mismatches incomplete incident sequencing creating second outage, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around bulk notification throttling event mismatches incomplete incident sequencing creating second outage, creating repeated failures\" best matches Incident review for bulk notification throttling event: signal points to incomplete incident sequencing creating second outage by targeting incomplete incident sequencing creating second outage and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"incident review for bulk notification throttling event: signal points to incomplete\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for bulk notification throttling event:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Apply provider-aware failover with cross-channel dedupe protection.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "With root cause identified for \"incident review for bulk notification throttling event:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Apply provider-aware failover with cross-channel dedupe protection\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for transactional alert delay bridge: signal points to priority inversion between transactional and bulk queues. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around transactional alert delay bridge mismatches priority inversion between transactional and bulk queues, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Notification System Scale & Scenarios: for Incident review for transactional alert delay bridge: signal points to priority inversion between transactional and bulk queues, \"The current decomposition around transactional alert delay bridge mismatches priority inversion between transactional and bulk queues, creating repeated failures\" is correct because it addresses priority inversion between transactional and bulk queues and improves controllability.",
          "detailedExplanation": "Use \"incident review for transactional alert delay bridge: signal points to priority\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for transactional alert delay bridge:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Gate retries with budgets and jitter so recovery does not cause quota collapse.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Gate retries with budgets and jitter so recovery does not cause quota collapse\" best matches After diagnosing \"incident review for transactional alert delay bridge:\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional dispatch degradation case: signal points to cross-provider dedupe failure during failover. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around regional dispatch degradation case mismatches cross-provider dedupe failure during failover, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Notification System Scale & Scenarios, the best answer is \"The current decomposition around regional dispatch degradation case mismatches cross-provider dedupe failure during failover, creating repeated failures\". It is the option most directly aligned to cross-provider dedupe failure during failover while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"incident review for regional dispatch degradation case: signal points to cross-provider\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for regional dispatch degradation case:\" scenario, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Protect SLA-critical alerts with reserved capacity and separate admission policy."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Notification System Scale & Scenarios: for In the \"incident review for regional dispatch degradation case:\" scenario, what should change first before wider rollout, \"Protect SLA-critical alerts with reserved capacity and separate admission policy\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "This prompt is really about \"notification System Scale & Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for post-incident notification hardening review: signal points to retry storm breaching provider quotas. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around post-incident notification hardening review mismatches retry storm breaching provider quotas, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for post-incident notification hardening review: signal points to retry storm breaching provider quotas is a two-step reliability decision. At stage 1, \"The current decomposition around post-incident notification hardening review mismatches retry storm breaching provider quotas, creating repeated failures\" wins because it balances immediate containment with long-term prevention around retry storm breaching provider quotas.",
          "detailedExplanation": "The decision turns on \"incident review for post-incident notification hardening review: signal points to retry\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for post-incident notification\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Use explicit fallback graphs to prevent adapter routing loops.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Use explicit fallback graphs to prevent adapter routing loops\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Use \"notification System Scale & Scenarios\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for global campaign launch incident: signal points to scheduler lag violating delivery SLA windows. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around global campaign launch incident mismatches scheduler lag violating delivery SLA windows, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around global campaign launch incident mismatches scheduler lag violating delivery SLA windows, creating repeated failures\" best matches Incident review for global campaign launch incident: signal points to scheduler lag violating delivery SLA windows by targeting scheduler lag violating delivery SLA windows and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident review for global campaign launch incident: signal points to scheduler lag\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for global campaign launch incident:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Reconcile audit truth via idempotent event lineage across providers.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Using the diagnosis from \"incident review for global campaign launch incident:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Reconcile audit truth via idempotent event lineage across providers\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"notification System Scale & Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for critical security alert dispatch: signal points to fallback routing loops across adapters. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around critical security alert dispatch mismatches fallback routing loops across adapters, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Notification System Scale & Scenarios: for Incident review for critical security alert dispatch: signal points to fallback routing loops across adapters, \"The current decomposition around critical security alert dispatch mismatches fallback routing loops across adapters, creating repeated failures\" is correct because it addresses fallback routing loops across adapters and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident review for critical security alert dispatch: signal points to fallback routing\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for critical security alert dispatch:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Throttle campaign traffic automatically when transactional latency degrades.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Throttle campaign traffic automatically when transactional latency degrades\" best matches With diagnosis confirmed in \"incident review for critical security alert dispatch:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"notification System Scale & Scenarios\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for provider outage failover workflow: signal points to audit gaps obscuring delivery truth. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Notification System Scale & Scenarios, the best answer is \"The current decomposition around provider outage failover workflow mismatches audit gaps obscuring delivery truth, creating repeated failures\". It is the option most directly aligned to audit gaps obscuring delivery truth while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident review for provider outage failover workflow: signal points to audit gaps\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident review for provider outage failover workflow:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Sequence recovery by user-impact class, then backfill deferred low-priority sends."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Notification System Scale & Scenarios: for Given the diagnosis in \"incident review for provider outage failover workflow:\", what is the highest-leverage change to make now, \"Sequence recovery by user-impact class, then backfill deferred low-priority sends\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"notification System Scale & Scenarios\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-channel retry storm event: signal points to campaign controls disabled during incident response. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around multi-channel retry storm event mismatches campaign controls disabled during incident response, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for multi-channel retry storm event: signal points to campaign controls disabled during incident response is a two-step reliability decision. At stage 1, \"The current decomposition around multi-channel retry storm event mismatches campaign controls disabled during incident response, creating repeated failures\" wins because it balances immediate containment with long-term prevention around campaign controls disabled during incident response.",
          "detailedExplanation": "If you keep \"incident review for multi-channel retry storm event: signal points to campaign controls\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for multi-channel retry storm event:\", what should change first before wider rollout?",
          "options": [
            "Validate preference snapshots during incident mitigation before replaying backlog.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Notification System Scale & Scenarios, the best answer is \"Validate preference snapshots during incident mitigation before replaying backlog\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Scale & Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for holiday-traffic queue surge: signal points to preference mismatch causing over-notification. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around holiday-traffic queue surge mismatches preference mismatch causing over-notification, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Notification System Scale & Scenarios: for Incident review for holiday-traffic queue surge: signal points to preference mismatch causing over-notification, \"The current decomposition around holiday-traffic queue surge mismatches preference mismatch causing over-notification, creating repeated failures\" is correct because it addresses preference mismatch causing over-notification and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident review for holiday-traffic queue surge: signal points to preference mismatch\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for holiday-traffic queue surge: signal\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Close incidents with drill-backed guardrails to prevent repeat cascades.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Close incidents with drill-backed guardrails to prevent repeat cascades\" best matches Now that \"incident review for holiday-traffic queue surge: signal\" is diagnosed, which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Use \"notification System Scale & Scenarios\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-061",
      "type": "multi-select",
      "question": "Identify every correct choice: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Identify every correct choice: which signals best identify decomposition boundary mistakes is intentionally multi-dimensional in Notification System Scale & Scenarios. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "This prompt is really about \"signals best identify decomposition boundary mistakes? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-062",
      "type": "multi-select",
      "question": "Identify every correct choice: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Notification System Scale & Scenarios, Identify every correct choice: which controls improve safety on critical write paths needs layered controls, not one silver bullet. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "If you keep \"controls improve safety on critical write paths? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-063",
      "type": "multi-select",
      "question": "Identify every correct choice: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Identify every correct choice: which practices reduce hot-key or hot-partition impact, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The core signal here is \"practices reduce hot-key or hot-partition impact? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-064",
      "type": "multi-select",
      "question": "Identify every correct choice: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Notification System Scale & Scenarios: The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The key clue in this question is \"improves reliability when mixing sync and async paths? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-065",
      "type": "multi-select",
      "question": "Identify every correct choice: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Identify every correct choice: which choices usually lower operational risk at scale is intentionally multi-dimensional in Notification System Scale & Scenarios. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Start from \"choices usually lower operational risk at scale? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-066",
      "type": "multi-select",
      "question": "Identify every correct choice: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Notification System Scale & Scenarios, Identify every correct choice: what should be explicit in API/service contracts for this design needs layered controls, not one silver bullet. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The decision turns on \"be explicit in API/service contracts for this design? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-067",
      "type": "multi-select",
      "question": "Identify every correct choice: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Identify every correct choice: which anti-patterns often cause incident recurrence, the highest-signal answer is a bundle of controls. The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Read this as a scenario about \"anti-patterns often cause incident recurrence? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-068",
      "type": "multi-select",
      "question": "Identify every correct choice: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Notification System Scale & Scenarios: The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Use \"increases confidence before broad traffic rollout? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-069",
      "type": "multi-select",
      "question": "Identify every correct choice: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Identify every correct choice: which controls protect high-priority traffic during spikes is intentionally multi-dimensional in Notification System Scale & Scenarios. The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "This prompt is really about \"controls protect high-priority traffic during spikes? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-070",
      "type": "multi-select",
      "question": "Identify every correct choice: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Identify every correct choice: which telemetry dimensions are most actionable for design triage, the highest-signal answer is a bundle of controls. The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Read this as a scenario about \"telemetry dimensions are most actionable for design triage? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-071",
      "type": "multi-select",
      "question": "Identify every correct choice: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Notification System Scale & Scenarios: The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The decision turns on \"governance actions improve cross-team reliability ownership? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-072",
      "type": "multi-select",
      "question": "Identify every correct choice: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Identify every correct choice: what helps prevent retry amplification cascades is intentionally multi-dimensional in Notification System Scale & Scenarios. The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Start from \"helps prevent retry amplification cascades? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-073",
      "type": "multi-select",
      "question": "Identify every correct choice: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Notification System Scale & Scenarios, Identify every correct choice: which fallback strategies are strong when dependencies degrade needs layered controls, not one silver bullet. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The key clue in this question is \"fallback strategies are strong when dependencies degrade? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-074",
      "type": "multi-select",
      "question": "Identify every correct choice: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Identify every correct choice: what reduces data-quality regressions in eventual pipelines, the highest-signal answer is a bundle of controls. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The core signal here is \"reduces data-quality regressions in eventual pipelines? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-075",
      "type": "multi-select",
      "question": "Identify every correct choice: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Notification System Scale & Scenarios: The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "If you keep \"runbook components improve incident execution quality? (Select all that apply)\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-076",
      "type": "multi-select",
      "question": "Identify every correct choice: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Identify every correct choice: which architecture choices improve blast-radius containment is intentionally multi-dimensional in Notification System Scale & Scenarios. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "This prompt is really about \"architecture choices improve blast-radius containment? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-077",
      "type": "multi-select",
      "question": "Identify every correct choice: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Notification System Scale & Scenarios, Identify every correct choice: what evidence demonstrates a fix worked beyond short-term recovery needs layered controls, not one silver bullet. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Use \"evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-078",
      "type": "numeric-input",
      "question": "From a critical path handles 5,400,000 requests/day and 0.18% fail SLO, failures/day?",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "Use first-pass reliability arithmetic for From a critical path handles 5,400,000 requests/day and 0: 9720 requests. Answers within +/-3% show correct directional reasoning for Notification System Scale & Scenarios.",
      "detailedExplanation": "Read this as a scenario about \"critical path handles 5,400,000 requests/day and 0\". Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 5,400 and 000 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-079",
      "type": "numeric-input",
      "question": "From queue ingest is 2,200 events/min and drain is 2,530 events/min, net drain rate?",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "The operational math for From queue ingest is 2,200 events/min and drain is 2,530 events/min, net drain rate gives 330 events/min. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "The decision turns on \"queue ingest is 2,200 events/min and drain is 2,530 events/min\". Normalize units before computing so conversion mistakes do not propagate. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 2,200 and 2,530 appear, convert them into one unit basis before comparison. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-080",
      "type": "numeric-input",
      "question": "From retries add 0.28 extra attempts at 75,000 req/sec, effective attempts/sec?",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "For From retries add 0, the computed target in Notification System Scale & Scenarios is 96000 attempts/sec. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "If you keep \"retries add 0\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 0.28 and 75,000 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-081",
      "type": "numeric-input",
      "question": "From failover takes 16s and occurs 24 times/day, total failover seconds/day?",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for From failover takes 16s and occurs 24 times/day, total failover seconds/day: 384 seconds. Answers within +/-0% show correct directional reasoning for Notification System Scale & Scenarios.",
      "detailedExplanation": "The core signal here is \"failover takes 16s and occurs 24 times/day\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 16s and 24 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-082",
      "type": "numeric-input",
      "question": "From target p99 is 650ms; observed p99 is 845ms, percent over target?",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for From target p99 is 650ms; observed p99 is 845ms, percent over target gives 30 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "Use \"target p99 is 650ms\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 650ms and 845ms should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-083",
      "type": "numeric-input",
      "question": "How would you answer this: if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "Notification System Scale & Scenarios expects quick quantitative triage: How would you answer this: if 34% of 130,000 req/min are high-priority, how many high-priority req/min evaluates to 44200 requests/min. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "This prompt is really about \"if 34% of 130,000 req/min are high-priority, how many high-priority req/min\". Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 34 and 130,000 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-084",
      "type": "numeric-input",
      "question": "From error rate drops from 1.0% to 0.22%, percent reduction?",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For From error rate drops from 1, the computed target in Notification System Scale & Scenarios is 78 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "The decision turns on \"error rate drops from 1\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 1.0 and 0.22 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-085",
      "type": "numeric-input",
      "question": "From a 9-node quorum cluster requires majority writes, minimum acknowledgements?",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for From a 9-node quorum cluster requires majority writes, minimum acknowledgements: 5 acks. Answers within +/-0% show correct directional reasoning for Notification System Scale & Scenarios.",
      "detailedExplanation": "Read this as a scenario about \"9-node quorum cluster requires majority writes\". Keep every transformation in one unit system and check order of magnitude at the end. Consistency decisions should be explicit about which conflicts are acceptable and why. Keep quantities like 9 and 5 in aligned units before selecting an answer. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-086",
      "type": "numeric-input",
      "question": "From backlog is 56,000 tasks with net drain 350 tasks/min, minutes to clear?",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "The operational math for From backlog is 56,000 tasks with net drain 350 tasks/min, minutes to clear gives 160 minutes. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "The key clue in this question is \"backlog is 56,000 tasks with net drain 350 tasks/min\". Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 56,000 and 350 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-087",
      "type": "numeric-input",
      "question": "From a fleet has 18 zones and 3 are unavailable, percent remaining available?",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Notification System Scale & Scenarios expects quick quantitative triage: From a fleet has 18 zones and 3 are unavailable, percent remaining available evaluates to 83.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "Start from \"fleet has 18 zones and 3 are unavailable\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 18 and 3 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-088",
      "type": "numeric-input",
      "question": "From mTTR improved from 52 min to 34 min, percent reduction?",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For From mTTR improved from 52 min to 34 min, percent reduction, the computed target in Notification System Scale & Scenarios is 34.62 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "If you keep \"mTTR improved from 52 min to 34 min\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 52 min and 34 min appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-089",
      "type": "numeric-input",
      "question": "How would you answer this: if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for How would you answer this: if 11% of 2,800,000 daily ops need manual checks, checks/day: 308000 operations. Answers within +/-2% show correct directional reasoning for Notification System Scale & Scenarios.",
      "detailedExplanation": "The core signal here is \"if 11% of 2,800,000 daily ops need manual checks, checks/day\". Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 11 and 2,800 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-090",
      "type": "ordering",
      "question": "In this notification system scale & scenarios context, order a classic-design decomposition workflow.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Identify critical user journey and invariants must happen before Validate with load/failure drills and refine. That ordering matches incident-safe flow in Notification System Scale & Scenarios.",
      "detailedExplanation": "This prompt is really about \"order a classic-design decomposition workflow\". Place obvious extremes first, then sort the middle by pairwise comparison. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: propagating an early bad assumption through all steps.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-091",
      "type": "ordering",
      "question": "Within notification system scale & scenarios, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Notification System Scale & Scenarios should start with Explicit boundaries with contracts and end with Implicit coupling with no ownership. Within notification system scale & scenarios, order by increasing design risk rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Use \"order by increasing design risk\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-092",
      "type": "ordering",
      "question": "For notification system scale & scenarios, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For notification system scale & scenarios, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The core signal here is \"order safe incident mitigation steps\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-093",
      "type": "ordering",
      "question": "Order by increasing retry-control maturity. Focus on notification system scale & scenarios tradeoffs.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Notification System Scale & Scenarios emphasizes safe recovery order. Beginning at Fixed immediate retries and finishing at Jittered retries with retry budgets and telemetry keeps blast radius controlled while restoring service.",
      "detailedExplanation": "If you keep \"order by increasing retry-control maturity\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-094",
      "type": "ordering",
      "question": "Order fallback sophistication. Use a notification system scale & scenarios perspective.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Implicit fallback behavior must happen before Policy-driven automated fallback with tests. That ordering matches incident-safe flow in Notification System Scale & Scenarios.",
      "detailedExplanation": "Start from \"order fallback sophistication\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-095",
      "type": "ordering",
      "question": "Order failover validation rigor. (notification system scale & scenarios lens)",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Notification System Scale & Scenarios should start with Host health check only and end with Staged shift plus failback rehearsal and rollback gates. Order failover validation rigor rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The key clue in this question is \"order failover validation rigor\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-096",
      "type": "ordering",
      "question": "Order by growing blast radius impact.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order by growing blast radius impact, the correct ordering runs from Single process failure to Cross-region control-plane failure. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing blast radius\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-097",
      "type": "ordering",
      "question": "Considering notification system scale & scenarios, order data-path durability confidence.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Notification System Scale & Scenarios emphasizes safe recovery order. Beginning at In-memory only acknowledgment and finishing at Replicated durable write plus replay/integrity verification keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The decision turns on \"order data-path durability confidence\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-098",
      "type": "ordering",
      "question": "In this notification system scale & scenarios context, order by increasing operational discipline.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc incident response must happen before Role-based response plus action closure tracking. That ordering matches incident-safe flow in Notification System Scale & Scenarios.",
      "detailedExplanation": "This prompt is really about \"order by increasing operational discipline\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-099",
      "type": "ordering",
      "question": "Within notification system scale & scenarios, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Notification System Scale & Scenarios should start with Canary small cohort and end with Finalize runbook and ownership updates. Within notification system scale & scenarios, order rollout safety for major design changes rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Use \"order rollout safety for major design changes\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "cd-ns-100",
      "type": "ordering",
      "question": "For notification system scale & scenarios, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For notification system scale & scenarios, order evidence strength for fix success, the correct ordering runs from Single successful test run to Sustained recovery plus failure-drill pass. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The core signal here is \"order evidence strength for fix success\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-scale-and-scenarios"],
      "difficulty": "principal"
    }
  ]
}
