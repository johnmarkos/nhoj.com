{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 1,
  "chapterTitle": "Failure Modes & Fault Domains",
  "chapterDescription": "Identify realistic failure classes and map blast radius across service, node, zone, and region boundaries.",
  "problems": [
    {
      "id": "rel-fd-001",
      "type": "multiple-choice",
      "question": "Case Alpha: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat checkout control plane as a reliability-control decision, not an averages-only optimization. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" is correct since it mitigates partial failure masked as success while keeping containment local. The decision remains valid given: A rollback window is still available for the next 15 minutes.",
      "detailedExplanation": "Read this as a scenario about \"checkout control plane\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 15 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-002",
      "type": "multiple-choice",
      "question": "Case Beta: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For identity graph service, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" outperforms the alternatives because it targets correlated AZ dependency failure and preserves safe recovery behavior. It is also the most compatible with Leadership asked for an action that lowers recurrence, not just symptoms.",
      "detailedExplanation": "The key clue in this question is \"identity graph service\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-003",
      "type": "multiple-choice",
      "question": "Case Gamma: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Failure Modes & Fault Domains, recommendation stream processor fails mainly through retry-amplified saturation after brownout. The best choice is \"Separate shared dependencies by zone and enforce per-domain circuit limits\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "Start from \"recommendation stream processor\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-004",
      "type": "multiple-choice",
      "question": "Case Delta: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Create a dependency failure taxonomy and map runbook actions to each class."
      ],
      "correct": 3,
      "explanation": "Ledger write path should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Create a dependency failure taxonomy and map runbook actions to each class\" is strongest because it directly addresses stale config propagation during regional drift and improves repeatability under stress. This aligns with the extra condition (Recent game-day results showed hidden cross-zone coupling).",
      "detailedExplanation": "If you keep \"ledger write path\" in view, the correct answer separates faster. Reject options that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat search indexing pipeline as a reliability-control decision, not an averages-only optimization. \"Design for correlated-failure mode by reducing hidden single points in control planes\" is correct since it mitigates shared control-plane credential expiry while keeping containment local. The decision remains valid given: Customer impact is concentrated on invariant-critical transactions.",
      "detailedExplanation": "The core signal here is \"search indexing pipeline\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-006",
      "type": "multiple-choice",
      "question": "Case Zeta: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For notification fanout service, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" outperforms the alternatives because it targets degraded dependency returning slow 200s and preserves safe recovery behavior. It is also the most compatible with The previous mitigation improved averages but not tail behavior.",
      "detailedExplanation": "Use \"notification fanout service\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 200s appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-007",
      "type": "multiple-choice",
      "question": "Case Eta: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Failure Modes & Fault Domains, session gateway fails mainly through silent packet loss in one network segment. The best choice is \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "This prompt is really about \"session gateway\". Prefer the answer that correctly handles unit conversion and link-capacity limits. Treat network capacity as a steady-state constraint, then test against peak windows. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-008",
      "type": "multiple-choice",
      "question": "Case Theta: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
      ],
      "correct": 3,
      "explanation": "Catalog ingestion job should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" is strongest because it directly addresses stateful worker hot-node exhaustion and improves repeatability under stress. This aligns with the extra condition (Operations wants a reversible step before broader architecture changes).",
      "detailedExplanation": "The decision turns on \"catalog ingestion job\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-009",
      "type": "multiple-choice",
      "question": "Case Iota: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat quota evaluator as a reliability-control decision, not an averages-only optimization. \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" is correct since it mitigates cross-region DNS control lag while keeping containment local. The decision remains valid given: SLO burn rate accelerated after a config rollout this morning.",
      "detailedExplanation": "Read this as a scenario about \"quota evaluator\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-010",
      "type": "multiple-choice",
      "question": "Case Kappa: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Failure Modes & Fault Domains, fraud decision service fails mainly through upstream queue visibility timeout mismatch. The best choice is \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "Start from \"fraud decision service\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-011",
      "type": "multiple-choice",
      "question": "Case Lambda: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Checkout control plane should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" is strongest because it directly addresses partial failure masked as success and improves repeatability under stress. This aligns with the extra condition (The incident review highlighted missing boundary ownership).",
      "detailedExplanation": "The key clue in this question is \"checkout control plane\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-012",
      "type": "multiple-choice",
      "question": "Case Mu: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
      ],
      "correct": 3,
      "explanation": "Treat identity graph service as a reliability-control decision, not an averages-only optimization. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" is correct since it mitigates correlated AZ dependency failure while keeping containment local. The decision remains valid given: Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "detailedExplanation": "Read this as a scenario about \"identity graph service\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-013",
      "type": "multiple-choice",
      "question": "Case Nu: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For recommendation stream processor, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" outperforms the alternatives because it targets retry-amplified saturation after brownout and preserves safe recovery behavior. It is also the most compatible with A canary can be deployed immediately if the strategy is clear.",
      "detailedExplanation": "The decision turns on \"recommendation stream processor\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-014",
      "type": "multiple-choice",
      "question": "Case Xi: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Create a dependency failure taxonomy and map runbook actions to each class.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Failure Modes & Fault Domains, ledger write path fails mainly through stale config propagation during regional drift. The best choice is \"Create a dependency failure taxonomy and map runbook actions to each class\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "This prompt is really about \"ledger write path\". Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-015",
      "type": "multiple-choice",
      "question": "Case Omicron: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Search indexing pipeline should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Design for correlated-failure mode by reducing hidden single points in control planes\" is strongest because it directly addresses shared control-plane credential expiry and improves repeatability under stress. This aligns with the extra condition (Client retries are already elevated and could amplify mistakes).",
      "detailedExplanation": "Use \"search indexing pipeline\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-016",
      "type": "multiple-choice",
      "question": "Case Pi: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages."
      ],
      "correct": 3,
      "explanation": "Treat notification fanout service as a reliability-control decision, not an averages-only optimization. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" is correct since it mitigates degraded dependency returning slow 200s while keeping containment local. The decision remains valid given: The team must preserve core write correctness under mitigation.",
      "detailedExplanation": "The core signal here is \"notification fanout service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 200s should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-017",
      "type": "multiple-choice",
      "question": "Case Rho: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For session gateway, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" outperforms the alternatives because it targets silent packet loss in one network segment and preserves safe recovery behavior. It is also the most compatible with Recent staffing changes require simpler operational controls.",
      "detailedExplanation": "If you keep \"session gateway\" in view, the correct answer separates faster. Discard bandwidth plans that omit overhead and burst behavior. Treat network capacity as a steady-state constraint, then test against peak windows. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-018",
      "type": "multiple-choice",
      "question": "Case Sigma: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Failure Modes & Fault Domains, catalog ingestion job fails mainly through stateful worker hot-node exhaustion. The best choice is \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Cross-region latency variance increased during the event.",
      "detailedExplanation": "Start from \"catalog ingestion job\", then pressure-test the result against the options. Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-019",
      "type": "multiple-choice",
      "question": "Case Tau: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Quota evaluator should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" is strongest because it directly addresses cross-region DNS control lag and improves repeatability under stress. This aligns with the extra condition (This path mixes latency-sensitive and correctness-sensitive requests).",
      "detailedExplanation": "The key clue in this question is \"quota evaluator\". Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic."
      ],
      "correct": 3,
      "explanation": "For fraud decision service, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" outperforms the alternatives because it targets upstream queue visibility timeout mismatch and preserves safe recovery behavior. It is also the most compatible with The service has one hidden shared component with no backup path.",
      "detailedExplanation": "Use \"fraud decision service\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-021",
      "type": "multiple-choice",
      "question": "Case Phi: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Failure Modes & Fault Domains, checkout control plane fails mainly through partial failure masked as success. The best choice is \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "This prompt is really about \"checkout control plane\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-022",
      "type": "multiple-choice",
      "question": "Case Chi: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Identity graph service should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" is strongest because it directly addresses correlated AZ dependency failure and improves repeatability under stress. This aligns with the extra condition (Change approval favors narrowly scoped policies over global flips).",
      "detailedExplanation": "If you keep \"identity graph service\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-023",
      "type": "multiple-choice",
      "question": "Case Psi: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery. (Failure Modes & Fault Domains context)",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat recommendation stream processor as a reliability-control decision, not an averages-only optimization. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is correct since it mitigates retry-amplified saturation after brownout while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "The core signal here is \"recommendation stream processor\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-024",
      "type": "multiple-choice",
      "question": "Case Omega: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Create a dependency failure taxonomy and map runbook actions to each class."
      ],
      "correct": 3,
      "explanation": "For ledger write path, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Create a dependency failure taxonomy and map runbook actions to each class\" outperforms the alternatives because it targets stale config propagation during regional drift and preserves safe recovery behavior. It is also the most compatible with On-call needs mitigation that is observable by explicit metrics.",
      "detailedExplanation": "The key clue in this question is \"ledger write path\". Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-025",
      "type": "multiple-choice",
      "question": "Case Atlas: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Failure Modes & Fault Domains, search indexing pipeline fails mainly through shared control-plane credential expiry. The best choice is \"Design for correlated-failure mode by reducing hidden single points in control planes\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "Start from \"search indexing pipeline\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-026",
      "type": "multiple-choice",
      "question": "Case Nova: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Notification fanout service should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" is strongest because it directly addresses degraded dependency returning slow 200s and improves repeatability under stress. This aligns with the extra condition (Business impact is highest in the top 5% of critical flows).",
      "detailedExplanation": "The decision turns on \"notification fanout service\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 200s and 5 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-027",
      "type": "multiple-choice",
      "question": "Case Orion: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat session gateway as a reliability-control decision, not an averages-only optimization. \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" is correct since it mitigates silent packet loss in one network segment while keeping containment local. The decision remains valid given: Regional failover is possible but expensive if used prematurely.",
      "detailedExplanation": "Read this as a scenario about \"session gateway\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-028",
      "type": "multiple-choice",
      "question": "Case Vega: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
      ],
      "correct": 3,
      "explanation": "For catalog ingestion job, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" outperforms the alternatives because it targets stateful worker hot-node exhaustion and preserves safe recovery behavior. It is also the most compatible with A hot tenant currently consumes disproportionate worker capacity.",
      "detailedExplanation": "Use \"catalog ingestion job\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-029",
      "type": "multiple-choice",
      "question": "Case Helios: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Failure Modes & Fault Domains, quota evaluator fails mainly through cross-region DNS control lag. The best choice is \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "This prompt is really about \"quota evaluator\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-030",
      "type": "multiple-choice",
      "question": "Case Aurora: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat fraud decision service as a reliability-control decision, not an averages-only optimization. \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" is correct since it mitigates upstream queue visibility timeout mismatch while keeping containment local. The decision remains valid given: Queue age is rising even though average CPU appears normal.",
      "detailedExplanation": "Read this as a scenario about \"fraud decision service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For checkout control plane, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" outperforms the alternatives because it targets partial failure masked as success and preserves safe recovery behavior. It is also the most compatible with A control-plane API is healthy but data-plane errors are increasing.",
      "detailedExplanation": "The decision turns on \"checkout control plane\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-032",
      "type": "multiple-choice",
      "question": "Case Pulse: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
      ],
      "correct": 3,
      "explanation": "In Failure Modes & Fault Domains, identity graph service fails mainly through correlated AZ dependency failure. The best choice is \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "Start from \"identity graph service\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-033",
      "type": "multiple-choice",
      "question": "Case Forge: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode. (Failure Modes & Fault Domains context)",
      "options": [
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Recommendation stream processor should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is strongest because it directly addresses retry-amplified saturation after brownout and improves repeatability under stress. This aligns with the extra condition (Legal/compliance constraints require explicit behavior in degraded mode).",
      "detailedExplanation": "The key clue in this question is \"recommendation stream processor\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-034",
      "type": "multiple-choice",
      "question": "Case Harbor: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Create a dependency failure taxonomy and map runbook actions to each class.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat ledger write path as a reliability-control decision, not an averages-only optimization. \"Create a dependency failure taxonomy and map runbook actions to each class\" is correct since it mitigates stale config propagation during regional drift while keeping containment local. The decision remains valid given: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "The core signal here is \"ledger write path\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-035",
      "type": "multiple-choice",
      "question": "Case Vector: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For search indexing pipeline, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Design for correlated-failure mode by reducing hidden single points in control planes\" outperforms the alternatives because it targets shared control-plane credential expiry and preserves safe recovery behavior. It is also the most compatible with User trust impact is tied to visible inconsistency, not only downtime.",
      "detailedExplanation": "If you keep \"search indexing pipeline\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Checkout control plane is a two-step reliability decision. At stage 1, \"The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around stale config propagation during regional drift.",
          "detailedExplanation": "Start from \"incident diagnosis for checkout control plane: signal points to stale config\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident diagnosis for checkout control plane: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Add failure-mode specific observability keyed by fault domain instead of global averages\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"failure Modes & Fault Domains\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents\" best matches identity graph service by targeting shared control-plane credential expiry and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for identity graph service: signal points to shared control-plane\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for identity graph service: signal\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Now that \"incident diagnosis for identity graph service: signal\" is diagnosed, what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"failure Modes & Fault Domains\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for recommendation stream processor, \"The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents\" is correct because it addresses degraded dependency returning slow 200s and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for recommendation stream processor: signal points to degraded\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 200s should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for recommendation stream processor:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" best matches With root cause identified for \"incident diagnosis for recommendation stream processor:\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"failure Modes & Fault Domains\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents\". It is the option most directly aligned to silent packet loss in one network segment while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"incident diagnosis for ledger write path: signal points to silent packet loss in one\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for ledger write path: signal points\", which next change should be prioritized first?",
          "options": [
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for After diagnosing \"incident diagnosis for ledger write path: signal points\", which next change should be prioritized first, \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"failure Modes & Fault Domains\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents\" best matches search indexing pipeline by targeting stateful worker hot-node exhaustion and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for search indexing pipeline: signal points to stateful worker\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for search indexing pipeline: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for search indexing pipeline: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"failure Modes & Fault Domains\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification fanout service: signal points to cross-region DNS control lag. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for notification fanout service, \"The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents\" is correct because it addresses cross-region DNS control lag and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for notification fanout service: signal points to cross-region DNS\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for notification fanout service:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" best matches Using the diagnosis from \"incident diagnosis for notification fanout service:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"failure Modes & Fault Domains\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session gateway: signal points to upstream queue visibility timeout mismatch. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents\". It is the option most directly aligned to upstream queue visibility timeout mismatch while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for session gateway: signal points to upstream queue visibility\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for session gateway: signal points\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for Now that \"incident diagnosis for session gateway: signal points\" is diagnosed, what is the highest-leverage change to make now, \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"failure Modes & Fault Domains\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog ingestion job: signal points to partial failure masked as success. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Catalog ingestion job is a two-step reliability decision. At stage 1, \"The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around partial failure masked as success.",
          "detailedExplanation": "Start from \"incident diagnosis for catalog ingestion job: signal points to partial failure masked\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident diagnosis for catalog ingestion job: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Separate shared dependencies by zone and enforce per-domain circuit limits.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Separate shared dependencies by zone and enforce per-domain circuit limits\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "This prompt is really about \"failure Modes & Fault Domains\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for quota evaluator: signal points to correlated AZ dependency failure. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents\" best matches quota evaluator by targeting correlated AZ dependency failure and lowering repeat risk.",
          "detailedExplanation": "Use \"incident diagnosis for quota evaluator: signal points to correlated AZ dependency\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for quota evaluator: signal points\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Create a dependency failure taxonomy and map runbook actions to each class.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "After diagnosing \"incident diagnosis for quota evaluator: signal points\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Create a dependency failure taxonomy and map runbook actions to each class\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The decision turns on \"failure Modes & Fault Domains\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud decision service: signal points to retry-amplified saturation after brownout. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for fraud decision service, \"The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents\" is correct because it addresses retry-amplified saturation after brownout and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for fraud decision service: signal points to retry-amplified\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for fraud decision service: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Design for correlated-failure mode by reducing hidden single points in control planes.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Design for correlated-failure mode by reducing hidden single points in control planes\" best matches With root cause identified for \"incident diagnosis for fraud decision service: signal\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"failure Modes & Fault Domains\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents\". It is the option most directly aligned to stale config propagation during regional drift while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"incident diagnosis for checkout control plane: signal points to stale config\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for checkout control plane: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for \"incident diagnosis for checkout control plane: signal\", which immediate adjustment best addresses the risk, \"Add failure-mode specific observability keyed by fault domain instead of global averages\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"failure Modes & Fault Domains\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Identity graph service is a two-step reliability decision. At stage 1, \"The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around shared control-plane credential expiry.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for identity graph service: signal points to shared control-plane\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for identity graph service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Start from \"failure Modes & Fault Domains\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents\" best matches recommendation stream processor by targeting degraded dependency returning slow 200s and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for recommendation stream processor: signal points to degraded\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 200s in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for recommendation stream processor:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for recommendation stream processor:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"failure Modes & Fault Domains\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for ledger write path, \"The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents\" is correct because it addresses silent packet loss in one network segment and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for ledger write path: signal points to silent packet loss in one\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for ledger write path: signal points\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" best matches Using the diagnosis from \"incident diagnosis for ledger write path: signal points\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"failure Modes & Fault Domains\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Search indexing pipeline is a two-step reliability decision. At stage 1, \"The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around stateful worker hot-node exhaustion.",
          "detailedExplanation": "Start from \"incident diagnosis for search indexing pipeline: signal points to stateful worker\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident diagnosis for search indexing pipeline: signal\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Define fault-domain aware admission policies that preserve invariant-critical traffic\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"failure Modes & Fault Domains\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification fanout service: signal points to cross-region DNS control lag. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents\" best matches notification fanout service by targeting cross-region DNS control lag and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for notification fanout service: signal points to cross-region DNS\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident diagnosis for notification fanout service:\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Now that \"incident diagnosis for notification fanout service:\" is diagnosed, what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Use \"failure Modes & Fault Domains\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session gateway: signal points to upstream queue visibility timeout mismatch. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for session gateway, \"The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents\" is correct because it addresses upstream queue visibility timeout mismatch and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for session gateway: signal points to upstream queue visibility\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for session gateway: signal points\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" best matches Using the diagnosis from \"incident diagnosis for session gateway: signal points\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The core signal here is \"failure Modes & Fault Domains\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog ingestion job: signal points to partial failure masked as success. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents\". It is the option most directly aligned to partial failure masked as success while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for catalog ingestion job: signal points to partial failure masked\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for catalog ingestion job: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Separate shared dependencies by zone and enforce per-domain circuit limits.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for With diagnosis confirmed in \"incident diagnosis for catalog ingestion job: signal\", which next step is strongest under current constraints, \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"failure Modes & Fault Domains\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for quota evaluator: signal points to correlated AZ dependency failure. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Quota evaluator is a two-step reliability decision. At stage 1, \"The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around correlated AZ dependency failure.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for quota evaluator: signal points to correlated AZ dependency\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for quota evaluator: signal points\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Create a dependency failure taxonomy and map runbook actions to each class."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Create a dependency failure taxonomy and map runbook actions to each class\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"failure Modes & Fault Domains\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud decision service: signal points to retry-amplified saturation after brownout. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents\" best matches fraud decision service by targeting retry-amplified saturation after brownout and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident diagnosis for fraud decision service: signal points to retry-amplified\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for fraud decision service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Design for correlated-failure mode by reducing hidden single points in control planes.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For \"incident diagnosis for fraud decision service: signal\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Design for correlated-failure mode by reducing hidden single points in control planes\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"failure Modes & Fault Domains\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for checkout control plane, \"The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents\" is correct because it addresses stale config propagation during regional drift and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for checkout control plane: signal points to stale config\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for checkout control plane: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" best matches With root cause identified for \"incident diagnosis for checkout control plane: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"failure Modes & Fault Domains\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents\". It is the option most directly aligned to shared control-plane credential expiry while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"incident diagnosis for identity graph service: signal points to shared control-plane\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for identity graph service: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for After diagnosing \"incident diagnosis for identity graph service: signal\", what is the highest-leverage change to make now, \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"failure Modes & Fault Domains\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Recommendation stream processor is a two-step reliability decision. At stage 1, \"The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around degraded dependency returning slow 200s.",
          "detailedExplanation": "Start from \"incident diagnosis for recommendation stream processor: signal points to degraded\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 200s appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for recommendation stream processor:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"failure Modes & Fault Domains\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents\" best matches ledger write path by targeting silent packet loss in one network segment and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for ledger write path: signal points to silent packet loss in one\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for ledger write path: signal points\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Now that \"incident diagnosis for ledger write path: signal points\" is diagnosed, which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"failure Modes & Fault Domains\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents\". It is the option most directly aligned to stateful worker hot-node exhaustion while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"incident diagnosis for search indexing pipeline: signal points to stateful worker\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for search indexing pipeline: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for \"incident diagnosis for search indexing pipeline: signal\", which next change should be prioritized first, \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"failure Modes & Fault Domains\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-061",
      "type": "multi-select",
      "question": "Which indicators most directly reveal cross-domain blast radius? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Failure Modes & Fault Domains, Which indicators most directly reveal cross-domain blast radius needs layered controls, not one silver bullet. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"indicators most directly reveal cross-domain blast radius? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-062",
      "type": "multi-select",
      "question": "Which controls reduce hidden single points of failure? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Which controls reduce hidden single points of failure is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The decision turns on \"controls reduce hidden single points of failure? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-063",
      "type": "multi-select",
      "question": "During partial failures, which practices improve diagnosis quality? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Read this as a scenario about \"during partial failures, which practices improve diagnosis quality? (Select all that\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-064",
      "type": "multi-select",
      "question": "What belongs in a useful dependency failure taxonomy? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For What belongs in a useful dependency failure taxonomy, the highest-signal answer is a bundle of controls. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Use \"belongs in a useful dependency failure taxonomy? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-065",
      "type": "multi-select",
      "question": "Which patterns limit correlated failures across zones? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Failure Modes & Fault Domains, Which patterns limit correlated failures across zones needs layered controls, not one silver bullet. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "This prompt is really about \"patterns limit correlated failures across zones? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-066",
      "type": "multi-select",
      "question": "Which runbook elements increase incident execution reliability? (Select all that apply)",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Which runbook elements increase incident execution reliability is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "If you keep \"runbook elements increase incident execution reliability? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-067",
      "type": "multi-select",
      "question": "Which signals should trigger graceful isolation first? (Select all that apply)",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The core signal here is \"signals should trigger graceful isolation first? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-068",
      "type": "multi-select",
      "question": "Which architectural choices help contain tenant-induced overload? (Select all that apply)",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Which architectural choices help contain tenant-induced overload, the highest-signal answer is a bundle of controls. The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The key clue in this question is \"architectural choices help contain tenant-induced overload? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-069",
      "type": "multi-select",
      "question": "For reliability policies, which items should be explicit per endpoint? (Select all that apply)",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Failure Modes & Fault Domains, For reliability policies, which items should be explicit per endpoint needs layered controls, not one silver bullet. The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"for reliability policies, which items should be explicit per endpoint? (Select all that\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-070",
      "type": "multi-select",
      "question": "Which anti-patterns commonly enlarge outage blast radius? (Select all that apply)",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The core signal here is \"anti-patterns commonly enlarge outage blast radius? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-071",
      "type": "multi-select",
      "question": "What improves confidence in failover assumptions? (Select all that apply)",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For What improves confidence in failover assumptions, the highest-signal answer is a bundle of controls. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "If you keep \"improves confidence in failover assumptions? (Select all that apply)\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-072",
      "type": "multi-select",
      "question": "Which data is essential when classifying partial vs fail-stop incidents? (Select all that apply)",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Failure Modes & Fault Domains, Which data is essential when classifying partial vs fail-stop incidents needs layered controls, not one silver bullet. The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "This prompt is really about \"data is essential when classifying partial vs fail-stop incidents? (Select all that\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-073",
      "type": "multi-select",
      "question": "Which controls improve safety when control-plane health is uncertain? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Which controls improve safety when control-plane health is uncertain is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Use \"controls improve safety when control-plane health is uncertain? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-074",
      "type": "multi-select",
      "question": "For critical writes, which guardrails reduce corruption risk under faults? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Read this as a scenario about \"for critical writes, which guardrails reduce corruption risk under faults? (Select all\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-075",
      "type": "multi-select",
      "question": "Which recurring reviews keep reliability boundaries accurate over time? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Which recurring reviews keep reliability boundaries accurate over time, the highest-signal answer is a bundle of controls. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The decision turns on \"recurring reviews keep reliability boundaries accurate over time? (Select all that\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-076",
      "type": "multi-select",
      "question": "Which decisions help teams align on reliability trade-offs during incidents? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Failure Modes & Fault Domains, Which decisions help teams align on reliability trade-offs during incidents needs layered controls, not one silver bullet. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"decisions help teams align on reliability trade-offs during incidents? (Select all that\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-077",
      "type": "multi-select",
      "question": "What evidence best shows a mitigation reduced recurrence risk? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "What evidence best shows a mitigation reduced recurrence risk is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The key clue in this question is \"evidence best shows a mitigation reduced recurrence risk? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-078",
      "type": "numeric-input",
      "question": "A service processes 4,200,000 requests/day and 0.22% violate reliability SLO. How many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for A service processes 4,200,000 requests/day and 0 gives 9240 requests. In interview pacing, hitting this value within +/-3% is the pass condition. For A service processes 4,200,000 requests/day and 0, this is the strongest fit in Failure Modes & Fault Domains.",
      "detailedExplanation": "The core signal here is \"service processes 4,200,000 requests/day and 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 4,200 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-079",
      "type": "numeric-input",
      "question": "Incident queue receives 1,800 items/min and drains 2,050 items/min. Net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Incident queue receives 1,800 items/min and drains 2,050 items/min: 250 items/min. Answers within +/-0% show correct directional reasoning for Failure Modes & Fault Domains.",
      "detailedExplanation": "If you keep \"incident queue receives 1,800 items/min and drains 2,050 items/min\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 1,800 and 2,050 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-080",
      "type": "numeric-input",
      "question": "Retry policy adds 0.35 extra attempts per request at 60,000 req/sec. Effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Failure Modes & Fault Domains expects quick quantitative triage: Retry policy adds 0 evaluates to 81000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "The decision turns on \"retry policy adds 0\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 0.35 and 60,000 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-081",
      "type": "numeric-input",
      "question": "Failover takes 18 seconds and happens 21 times/day. Total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for Failover takes 18 seconds and happens 21 times/day gives 378 seconds. In interview pacing, hitting this value within +/-0% is the pass condition. For Failover takes 18 seconds and happens 21 times/day, this is the strongest fit in Failure Modes & Fault Domains.",
      "detailedExplanation": "Read this as a scenario about \"failover takes 18 seconds and happens 21 times/day\". Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 18 seconds and 21 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-082",
      "type": "numeric-input",
      "question": "Target p99 latency is 700ms; observed p99 is 980ms. Percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Target p99 latency is 700ms; observed p99 is 980ms: 40 %. Answers within +/-30% show correct directional reasoning for Failure Modes & Fault Domains.",
      "detailedExplanation": "The key clue in this question is \"target p99 latency is 700ms\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 700ms and 980ms should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-083",
      "type": "numeric-input",
      "question": "If 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For If 31% of 120,000 requests/min are critical-path, how many critical requests/min, the computed target in Failure Modes & Fault Domains is 37200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Start from \"if 31% of 120,000 requests/min are critical-path, how many critical requests/min\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 31 and 120,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.2% to 0.3%. Percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Failure Modes & Fault Domains expects quick quantitative triage: Error rate drops from 1 evaluates to 75 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "If you keep \"error rate drops from 1\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 1.2 and 0.3 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-085",
      "type": "numeric-input",
      "question": "A 7-node quorum system requires majority writes. Minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for A 7-node quorum system requires majority writes gives 4 acks. In interview pacing, hitting this value within +/-0% is the pass condition. For A 7-node quorum system requires majority writes, this is the strongest fit in Failure Modes & Fault Domains.",
      "detailedExplanation": "The core signal here is \"7-node quorum system requires majority writes\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Numbers such as 7 and 4 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-086",
      "type": "numeric-input",
      "question": "Backlog is 48,000 tasks and net drain is 320 tasks/min. Minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Backlog is 48,000 tasks and net drain is 320 tasks/min: 150 minutes. Answers within +/-0% show correct directional reasoning for Failure Modes & Fault Domains.",
      "detailedExplanation": "Use \"backlog is 48,000 tasks and net drain is 320 tasks/min\" as your starting point, then verify tradeoffs carefully. Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 48,000 and 320 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-087",
      "type": "numeric-input",
      "question": "A system with 14 zones has 2 unavailable. What percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For A system with 14 zones has 2 unavailable, the computed target in Failure Modes & Fault Domains is 85.71 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "This prompt is really about \"system with 14 zones has 2 unavailable\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 14 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-088",
      "type": "numeric-input",
      "question": "MTTR improved from 45 min to 30 min. Percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Failure Modes & Fault Domains expects quick quantitative triage: MTTR improved from 45 min to 30 min evaluates to 33.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "The decision turns on \"mTTR improved from 45 min to 30 min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 45 min and 30 min should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-089",
      "type": "numeric-input",
      "question": "If 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for If 9% of 2,500,000 daily operations need manual recovery checks, checks/day gives 225000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "Read this as a scenario about \"if 9% of 2,500,000 daily operations need manual recovery checks, checks/day\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 9 and 2,500 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-090",
      "type": "ordering",
      "question": "Within failure modes & fault domains, order a reliability response lifecycle.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Failure Modes & Fault Domains should start with Detect and scope affected fault domains and end with Validate recovery and harden recurrence defenses. Within failure modes & fault domains, order a reliability response lifecycle rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Start from \"order a reliability response lifecycle\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-091",
      "type": "ordering",
      "question": "For failure modes & fault domains, order from lowest to highest reliability risk.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Isolated dependency with fallback and budget must happen before Implicit dependency with no failure policy. That ordering matches incident-safe flow in Failure Modes & Fault Domains.",
      "detailedExplanation": "The key clue in this question is \"order from lowest to highest reliability risk\". Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-092",
      "type": "ordering",
      "question": "Order failover safety steps. Focus on failure modes & fault domains tradeoffs.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failure Modes & Fault Domains emphasizes safe recovery order. Beginning at Verify candidate health and freshness and finishing at Run failback readiness checks before restoration keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Read this as a scenario about \"order failover safety steps\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-093",
      "type": "ordering",
      "question": "Order by increasing overload-protection strength. Use a failure modes & fault domains perspective.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order by increasing overload-protection strength, the correct ordering runs from No admission limits to Priority-aware shedding plus per-domain concurrency bounds. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The decision turns on \"order by increasing overload-protection strength\". Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-094",
      "type": "ordering",
      "question": "Order data recovery execution. (failure modes & fault domains lens)",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Failure Modes & Fault Domains should start with Select recovery point by RPO target and end with Promote and re-enable writes with monitoring. Order data recovery execution rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "This prompt is really about \"order data recovery execution\". Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-095",
      "type": "ordering",
      "question": "From a failure modes & fault domains viewpoint, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Define SLIs tied to user impact must happen before Review incidents and close corrective actions. That ordering matches incident-safe flow in Failure Modes & Fault Domains.",
      "detailedExplanation": "Use \"order reliability operations loop\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-096",
      "type": "ordering",
      "question": "Rank these from smallest to largest blast radius. (Failure Modes & Fault Domains context)",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failure Modes & Fault Domains emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The core signal here is \"order by increasing blast radius\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-097",
      "type": "ordering",
      "question": "In this failure modes & fault domains context, order retry-policy maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For In this failure modes & fault domains context, order retry-policy maturity, the correct ordering runs from Fixed immediate retries to Jittered backoff with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "If you keep \"order retry-policy maturity\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-098",
      "type": "ordering",
      "question": "Within failure modes & fault domains, order degradation sophistication.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Failure Modes & Fault Domains should start with Undocumented ad hoc fallback and end with Automated policy-driven degradation with user semantics. Within failure modes & fault domains, order degradation sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Start from \"order degradation sophistication\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-099",
      "type": "ordering",
      "question": "For failure modes & fault domains, order incident command rigor.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc responders with no roles must happen before Role-defined operations plus decision log and action tracking. That ordering matches incident-safe flow in Failure Modes & Fault Domains.",
      "detailedExplanation": "The key clue in this question is \"order incident command rigor\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-100",
      "type": "ordering",
      "question": "Order reliability validation confidence. Focus on failure modes & fault domains tradeoffs.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failure Modes & Fault Domains emphasizes safe recovery order. Beginning at Single success in staging and finishing at Sustained recovery plus recurrence drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Read this as a scenario about \"order reliability validation confidence\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    }
  ]
}
