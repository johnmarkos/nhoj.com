{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 1,
  "chapterTitle": "Failure Modes & Fault Domains",
  "chapterDescription": "Identify realistic failure classes and map blast radius across service, node, zone, and region boundaries.",
  "problems": [
    {
      "id": "rel-fd-001",
      "type": "multiple-choice",
      "question": "Case Alpha: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-002",
      "type": "multiple-choice",
      "question": "Case Beta: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-003",
      "type": "multiple-choice",
      "question": "Case Gamma: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-004",
      "type": "multiple-choice",
      "question": "Case Delta: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Create a dependency failure taxonomy and map runbook actions to each class."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-006",
      "type": "multiple-choice",
      "question": "Case Zeta: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-007",
      "type": "multiple-choice",
      "question": "Case Eta: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-008",
      "type": "multiple-choice",
      "question": "Case Theta: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-009",
      "type": "multiple-choice",
      "question": "Case Iota: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-010",
      "type": "multiple-choice",
      "question": "Case Kappa: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-011",
      "type": "multiple-choice",
      "question": "Case Lambda: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-012",
      "type": "multiple-choice",
      "question": "Case Mu: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-013",
      "type": "multiple-choice",
      "question": "Case Nu: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-014",
      "type": "multiple-choice",
      "question": "Case Xi: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Create a dependency failure taxonomy and map runbook actions to each class.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-015",
      "type": "multiple-choice",
      "question": "Case Omicron: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-016",
      "type": "multiple-choice",
      "question": "Case Pi: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-017",
      "type": "multiple-choice",
      "question": "Case Rho: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-018",
      "type": "multiple-choice",
      "question": "Case Sigma: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-019",
      "type": "multiple-choice",
      "question": "Case Tau: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-021",
      "type": "multiple-choice",
      "question": "Case Phi: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-022",
      "type": "multiple-choice",
      "question": "Case Chi: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-023",
      "type": "multiple-choice",
      "question": "Case Psi: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-024",
      "type": "multiple-choice",
      "question": "Case Omega: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Create a dependency failure taxonomy and map runbook actions to each class."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-025",
      "type": "multiple-choice",
      "question": "Case Atlas: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-026",
      "type": "multiple-choice",
      "question": "Case Nova: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-027",
      "type": "multiple-choice",
      "question": "Case Orion: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-028",
      "type": "multiple-choice",
      "question": "Case Vega: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-029",
      "type": "multiple-choice",
      "question": "Case Helios: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-030",
      "type": "multiple-choice",
      "question": "Case Aurora: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-032",
      "type": "multiple-choice",
      "question": "Case Pulse: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-033",
      "type": "multiple-choice",
      "question": "Case Forge: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-034",
      "type": "multiple-choice",
      "question": "Case Harbor: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Create a dependency failure taxonomy and map runbook actions to each class.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-035",
      "type": "multiple-choice",
      "question": "Case Vector: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-fd-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification fanout service: signal points to cross-region DNS control lag. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session gateway: signal points to upstream queue visibility timeout mismatch. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog ingestion job: signal points to partial failure masked as success. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Separate shared dependencies by zone and enforce per-domain circuit limits.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for quota evaluator: signal points to correlated AZ dependency failure. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Create a dependency failure taxonomy and map runbook actions to each class.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud decision service: signal points to retry-amplified saturation after brownout. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Design for correlated-failure mode by reducing hidden single points in control planes.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification fanout service: signal points to cross-region DNS control lag. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session gateway: signal points to upstream queue visibility timeout mismatch. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog ingestion job: signal points to partial failure masked as success. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Separate shared dependencies by zone and enforce per-domain circuit limits.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for quota evaluator: signal points to correlated AZ dependency failure. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Create a dependency failure taxonomy and map runbook actions to each class."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud decision service: signal points to retry-amplified saturation after brownout. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Design for correlated-failure mode by reducing hidden single points in control planes.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-fd-061",
      "type": "multi-select",
      "question": "Which indicators most directly reveal cross-domain blast radius? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-062",
      "type": "multi-select",
      "question": "Which controls reduce hidden single points of failure? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-063",
      "type": "multi-select",
      "question": "During partial failures, which practices improve diagnosis quality? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-064",
      "type": "multi-select",
      "question": "What belongs in a useful dependency failure taxonomy? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-065",
      "type": "multi-select",
      "question": "Which patterns limit correlated failures across zones? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-066",
      "type": "multi-select",
      "question": "Which runbook elements increase incident execution reliability? (Select all that apply)",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-067",
      "type": "multi-select",
      "question": "Which signals should trigger graceful isolation first? (Select all that apply)",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-068",
      "type": "multi-select",
      "question": "Which architectural choices help contain tenant-induced overload? (Select all that apply)",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-069",
      "type": "multi-select",
      "question": "For reliability policies, which items should be explicit per endpoint? (Select all that apply)",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-070",
      "type": "multi-select",
      "question": "Which anti-patterns commonly enlarge outage blast radius? (Select all that apply)",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-071",
      "type": "multi-select",
      "question": "What improves confidence in failover assumptions? (Select all that apply)",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-072",
      "type": "multi-select",
      "question": "Which data is essential when classifying partial vs fail-stop incidents? (Select all that apply)",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-073",
      "type": "multi-select",
      "question": "Which controls improve safety when control-plane health is uncertain? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-074",
      "type": "multi-select",
      "question": "For critical writes, which guardrails reduce corruption risk under faults? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-075",
      "type": "multi-select",
      "question": "Which recurring reviews keep reliability boundaries accurate over time? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-076",
      "type": "multi-select",
      "question": "Which decisions help teams align on reliability trade-offs during incidents? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-077",
      "type": "multi-select",
      "question": "What evidence best shows a mitigation reduced recurrence risk? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-fd-078",
      "type": "numeric-input",
      "question": "A service processes 4,200,000 requests/day and 0.22% violate reliability SLO. How many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "0.0022 * 4,200,000 = 9,240."
    },
    {
      "id": "rel-fd-079",
      "type": "numeric-input",
      "question": "Incident queue receives 1,800 items/min and drains 2,050 items/min. Net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "2,050 - 1,800 = 250."
    },
    {
      "id": "rel-fd-080",
      "type": "numeric-input",
      "question": "Retry policy adds 0.35 extra attempts per request at 60,000 req/sec. Effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "60,000 * 1.35 = 81,000."
    },
    {
      "id": "rel-fd-081",
      "type": "numeric-input",
      "question": "Failover takes 18 seconds and happens 21 times/day. Total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "18 * 21 = 378."
    },
    {
      "id": "rel-fd-082",
      "type": "numeric-input",
      "question": "Target p99 latency is 700ms; observed p99 is 980ms. Percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(980 - 700) / 700 = 40%."
    },
    {
      "id": "rel-fd-083",
      "type": "numeric-input",
      "question": "If 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "0.31 * 120,000 = 37,200."
    },
    {
      "id": "rel-fd-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.2% to 0.3%. Percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1.2 - 0.3) / 1.2 = 75%."
    },
    {
      "id": "rel-fd-085",
      "type": "numeric-input",
      "question": "A 7-node quorum system requires majority writes. Minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Majority of 7 is 4."
    },
    {
      "id": "rel-fd-086",
      "type": "numeric-input",
      "question": "Backlog is 48,000 tasks and net drain is 320 tasks/min. Minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "48,000 / 320 = 150."
    },
    {
      "id": "rel-fd-087",
      "type": "numeric-input",
      "question": "A system with 14 zones has 2 unavailable. What percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "12 / 14 = 85.71%."
    },
    {
      "id": "rel-fd-088",
      "type": "numeric-input",
      "question": "MTTR improved from 45 min to 30 min. Percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(45 - 30) / 45 = 33.33%."
    },
    {
      "id": "rel-fd-089",
      "type": "numeric-input",
      "question": "If 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "0.09 * 2,500,000 = 225,000."
    },
    {
      "id": "rel-fd-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope, contain, fix, then harden."
    },
    {
      "id": "rel-fd-091",
      "type": "ordering",
      "question": "Order from lowest to highest reliability risk.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Risk grows as boundaries and controls are removed."
    },
    {
      "id": "rel-fd-092",
      "type": "ordering",
      "question": "Order failover safety steps.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safety comes from validation, fencing, gradual shift, and planned restoration."
    },
    {
      "id": "rel-fd-093",
      "type": "ordering",
      "question": "Order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Protection strengthens with class-aware and domain-aware controls."
    },
    {
      "id": "rel-fd-094",
      "type": "ordering",
      "question": "Order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Reliable recovery is staged and verified before write promotion."
    },
    {
      "id": "rel-fd-095",
      "type": "ordering",
      "question": "Order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The operations loop ties objective targets to execution and learning."
    },
    {
      "id": "rel-fd-096",
      "type": "ordering",
      "question": "Order by increasing blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Blast radius expands from local process to regional control failure."
    },
    {
      "id": "rel-fd-097",
      "type": "ordering",
      "question": "Order retry-policy maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Maturity grows with safeguards and measurable control."
    },
    {
      "id": "rel-fd-098",
      "type": "ordering",
      "question": "Order degradation sophistication.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sophistication increases with explicit, automated, user-visible policy."
    },
    {
      "id": "rel-fd-099",
      "type": "ordering",
      "question": "Order incident command rigor.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rigor improves with role clarity, timeline, and accountability."
    },
    {
      "id": "rel-fd-100",
      "type": "ordering",
      "question": "Order reliability validation confidence.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Confidence rises with sustained production behavior and recurrence testing."
    }
  ]
}
