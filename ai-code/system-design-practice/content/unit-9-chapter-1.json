{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 1,
  "chapterTitle": "Failure Modes & Fault Domains",
  "chapterDescription": "Identify realistic failure classes and map blast radius across service, node, zone, and region boundaries.",
  "problems": [
    {
      "id": "rel-fd-001",
      "type": "multiple-choice",
      "question": "Case Alpha: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat checkout control plane as a reliability-control decision, not an averages-only optimization. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" is correct since it mitigates partial failure masked as success while keeping containment local. The decision remains valid given: A rollback window is still available for the next 15 minutes.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 15 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-002",
      "type": "multiple-choice",
      "question": "Case Beta: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "For identity graph service, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" outperforms the alternatives because it targets correlated AZ dependency failure and preserves safe recovery behavior. It is also the most compatible with Leadership asked for an action that lowers recurrence, not just symptoms.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-003",
      "type": "multiple-choice",
      "question": "Case Gamma: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "In Failure Modes & Fault Domains, recommendation stream processor fails mainly through retry-amplified saturation after brownout. The best choice is \"Separate shared dependencies by zone and enforce per-domain circuit limits\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-004",
      "type": "multiple-choice",
      "question": "Case Delta: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Create a dependency failure taxonomy and map runbook actions to each class."
      ],
      "correct": 3,
      "explanation": "Ledger write path should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Create a dependency failure taxonomy and map runbook actions to each class\" is strongest because it directly addresses stale config propagation during regional drift and improves repeatability under stress. This aligns with the extra condition (Recent game-day results showed hidden cross-zone coupling).",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat search indexing pipeline as a reliability-control decision, not an averages-only optimization. \"Design for correlated-failure mode by reducing hidden single points in control planes\" is correct since it mitigates shared control-plane credential expiry while keeping containment local. The decision remains valid given: Customer impact is concentrated on invariant-critical transactions.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-006",
      "type": "multiple-choice",
      "question": "Case Zeta: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "For notification fanout service, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" outperforms the alternatives because it targets degraded dependency returning slow 200s and preserves safe recovery behavior. It is also the most compatible with The previous mitigation improved averages but not tail behavior.",
      "detailedExplanation": "Generalize from notification fanout service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 200s appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-007",
      "type": "multiple-choice",
      "question": "Case Eta: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "In Failure Modes & Fault Domains, session gateway fails mainly through silent packet loss in one network segment. The best choice is \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the answer that correctly handles unit conversion and link-capacity limits. Treat network capacity as a steady-state constraint, then test against peak windows. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-008",
      "type": "multiple-choice",
      "question": "Case Theta: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
      ],
      "correct": 3,
      "explanation": "Catalog ingestion job should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" is strongest because it directly addresses stateful worker hot-node exhaustion and improves repeatability under stress. This aligns with the extra condition (Operations wants a reversible step before broader architecture changes).",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-009",
      "type": "multiple-choice",
      "question": "Case Iota: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat quota evaluator as a reliability-control decision, not an averages-only optimization. \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" is correct since it mitigates cross-region DNS control lag while keeping containment local. The decision remains valid given: SLO burn rate accelerated after a config rollout this morning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-010",
      "type": "multiple-choice",
      "question": "Case Kappa: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Failure Modes & Fault Domains, fraud decision service fails mainly through upstream queue visibility timeout mismatch. The best choice is \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-011",
      "type": "multiple-choice",
      "question": "Case Lambda: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Checkout control plane should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" is strongest because it directly addresses partial failure masked as success and improves repeatability under stress. This aligns with the extra condition (The incident review highlighted missing boundary ownership).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-012",
      "type": "multiple-choice",
      "question": "Case Mu: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
      ],
      "correct": 3,
      "explanation": "Treat identity graph service as a reliability-control decision, not an averages-only optimization. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" is correct since it mitigates correlated AZ dependency failure while keeping containment local. The decision remains valid given: Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-013",
      "type": "multiple-choice",
      "question": "Case Nu: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "For recommendation stream processor, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" outperforms the alternatives because it targets retry-amplified saturation after brownout and preserves safe recovery behavior. It is also the most compatible with A canary can be deployed immediately if the strategy is clear.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-014",
      "type": "multiple-choice",
      "question": "Case Xi: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Create a dependency failure taxonomy and map runbook actions to each class.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Failure Modes & Fault Domains, ledger write path fails mainly through stale config propagation during regional drift. The best choice is \"Create a dependency failure taxonomy and map runbook actions to each class\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-015",
      "type": "multiple-choice",
      "question": "Case Omicron: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Search indexing pipeline should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Design for correlated-failure mode by reducing hidden single points in control planes\" is strongest because it directly addresses shared control-plane credential expiry and improves repeatability under stress. This aligns with the extra condition (Client retries are already elevated and could amplify mistakes).",
      "detailedExplanation": "Generalize from search indexing pipeline to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-016",
      "type": "multiple-choice",
      "question": "Case Pi: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages."
      ],
      "correct": 3,
      "explanation": "Treat notification fanout service as a reliability-control decision, not an averages-only optimization. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" is correct since it mitigates degraded dependency returning slow 200s while keeping containment local. The decision remains valid given: The team must preserve core write correctness under mitigation.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 200s should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-017",
      "type": "multiple-choice",
      "question": "Case Rho: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "For session gateway, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" outperforms the alternatives because it targets silent packet loss in one network segment and preserves safe recovery behavior. It is also the most compatible with Recent staffing changes require simpler operational controls.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard bandwidth plans that omit overhead and burst behavior. Treat network capacity as a steady-state constraint, then test against peak windows. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-018",
      "type": "multiple-choice",
      "question": "Case Sigma: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Failure Modes & Fault Domains, catalog ingestion job fails mainly through stateful worker hot-node exhaustion. The best choice is \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Cross-region latency variance increased during the event.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-019",
      "type": "multiple-choice",
      "question": "Case Tau: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Quota evaluator should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" is strongest because it directly addresses cross-region DNS control lag and improves repeatability under stress. This aligns with the extra condition (This path mixes latency-sensitive and correctness-sensitive requests).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic."
      ],
      "correct": 3,
      "explanation": "For fraud decision service, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" outperforms the alternatives because it targets upstream queue visibility timeout mismatch and preserves safe recovery behavior. It is also the most compatible with The service has one hidden shared component with no backup path.",
      "detailedExplanation": "Generalize from fraud decision service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-021",
      "type": "multiple-choice",
      "question": "Case Phi: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "In Failure Modes & Fault Domains, checkout control plane fails mainly through partial failure masked as success. The best choice is \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-022",
      "type": "multiple-choice",
      "question": "Case Chi: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Identity graph service should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" is strongest because it directly addresses correlated AZ dependency failure and improves repeatability under stress. This aligns with the extra condition (Change approval favors narrowly scoped policies over global flips).",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-023",
      "type": "multiple-choice",
      "question": "Case Psi: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery. (Failure Modes & Fault Domains context)",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Treat recommendation stream processor as a reliability-control decision, not an averages-only optimization. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is correct since it mitigates retry-amplified saturation after brownout while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-024",
      "type": "multiple-choice",
      "question": "Case Omega: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Create a dependency failure taxonomy and map runbook actions to each class."
      ],
      "correct": 3,
      "explanation": "For ledger write path, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Create a dependency failure taxonomy and map runbook actions to each class\" outperforms the alternatives because it targets stale config propagation during regional drift and preserves safe recovery behavior. It is also the most compatible with On-call needs mitigation that is observable by explicit metrics.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-025",
      "type": "multiple-choice",
      "question": "Case Atlas: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "In Failure Modes & Fault Domains, search indexing pipeline fails mainly through shared control-plane credential expiry. The best choice is \"Design for correlated-failure mode by reducing hidden single points in control planes\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-026",
      "type": "multiple-choice",
      "question": "Case Nova: notification fanout service. Primary reliability risk is degraded dependency returning slow 200s. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Add failure-mode specific observability keyed by fault domain instead of global averages.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Notification fanout service should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" is strongest because it directly addresses degraded dependency returning slow 200s and improves repeatability under stress. This aligns with the extra condition (Business impact is highest in the top 5% of critical flows).",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 200s and 5 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-027",
      "type": "multiple-choice",
      "question": "Case Orion: session gateway. Primary reliability risk is silent packet loss in one network segment. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Treat session gateway as a reliability-control decision, not an averages-only optimization. \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" is correct since it mitigates silent packet loss in one network segment while keeping containment local. The decision remains valid given: Regional failover is possible but expensive if used prematurely.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-028",
      "type": "multiple-choice",
      "question": "Case Vega: catalog ingestion job. Primary reliability risk is stateful worker hot-node exhaustion. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
      ],
      "correct": 3,
      "explanation": "For catalog ingestion job, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" outperforms the alternatives because it targets stateful worker hot-node exhaustion and preserves safe recovery behavior. It is also the most compatible with A hot tenant currently consumes disproportionate worker capacity.",
      "detailedExplanation": "Generalize from catalog ingestion job to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-029",
      "type": "multiple-choice",
      "question": "Case Helios: quota evaluator. Primary reliability risk is cross-region DNS control lag. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "In Failure Modes & Fault Domains, quota evaluator fails mainly through cross-region DNS control lag. The best choice is \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-030",
      "type": "multiple-choice",
      "question": "Case Aurora: fraud decision service. Primary reliability risk is upstream queue visibility timeout mismatch. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Treat fraud decision service as a reliability-control decision, not an averages-only optimization. \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" is correct since it mitigates upstream queue visibility timeout mismatch while keeping containment local. The decision remains valid given: Queue age is rising even though average CPU appears normal.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: checkout control plane. Primary reliability risk is partial failure masked as success. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "For checkout control plane, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" outperforms the alternatives because it targets partial failure masked as success and preserves safe recovery behavior. It is also the most compatible with A control-plane API is healthy but data-plane errors are increasing.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-032",
      "type": "multiple-choice",
      "question": "Case Pulse: identity graph service. Primary reliability risk is correlated AZ dependency failure. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
      ],
      "correct": 3,
      "explanation": "In Failure Modes & Fault Domains, identity graph service fails mainly through correlated AZ dependency failure. The best choice is \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-033",
      "type": "multiple-choice",
      "question": "Case Forge: recommendation stream processor. Primary reliability risk is retry-amplified saturation after brownout. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode. (Failure Modes & Fault Domains context)",
      "options": [
        "Separate shared dependencies by zone and enforce per-domain circuit limits.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Recommendation stream processor should be solved at the failure boundary named in Failure Modes & Fault Domains. \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is strongest because it directly addresses retry-amplified saturation after brownout and improves repeatability under stress. This aligns with the extra condition (Legal/compliance constraints require explicit behavior in degraded mode).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-034",
      "type": "multiple-choice",
      "question": "Case Harbor: ledger write path. Primary reliability risk is stale config propagation during regional drift. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Create a dependency failure taxonomy and map runbook actions to each class.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Treat ledger write path as a reliability-control decision, not an averages-only optimization. \"Create a dependency failure taxonomy and map runbook actions to each class\" is correct since it mitigates stale config propagation during regional drift while keeping containment local. The decision remains valid given: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-035",
      "type": "multiple-choice",
      "question": "Case Vector: search indexing pipeline. Primary reliability risk is shared control-plane credential expiry. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Design for correlated-failure mode by reducing hidden single points in control planes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "For search indexing pipeline, prefer the option that prevents reoccurrence in Failure Modes & Fault Domains. \"Design for correlated-failure mode by reducing hidden single points in control planes\" outperforms the alternatives because it targets shared control-plane credential expiry and preserves safe recovery behavior. It is also the most compatible with User trust impact is tied to visible inconsistency, not only downtime.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Checkout control plane is a two-step reliability decision. At stage 1, \"The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around stale config propagation during regional drift.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident diagnosis for checkout control plane: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Add failure-mode specific observability keyed by fault domain instead of global averages\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents\" best matches identity graph service by targeting shared control-plane credential expiry and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for identity graph service: signal\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "Now that \"incident diagnosis for identity graph service: signal\" is diagnosed, what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from failure Modes & Fault Domains to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for recommendation stream processor, \"The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents\" is correct because it addresses degraded dependency returning slow 200s and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 200s should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for recommendation stream processor:\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" best matches With root cause identified for \"incident diagnosis for recommendation stream processor:\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents\". It is the option most directly aligned to silent packet loss in one network segment while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident diagnosis for ledger write path: signal points to silent packet loss in one to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for ledger write path: signal points\", which next change should be prioritized first?",
          "options": [
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for After diagnosing \"incident diagnosis for ledger write path: signal points\", which next change should be prioritized first, \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents\" best matches search indexing pipeline by targeting stateful worker hot-node exhaustion and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for search indexing pipeline: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for search indexing pipeline: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification fanout service: signal points to cross-region DNS control lag. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for notification fanout service, \"The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents\" is correct because it addresses cross-region DNS control lag and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for notification fanout service:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" best matches Using the diagnosis from \"incident diagnosis for notification fanout service:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session gateway: signal points to upstream queue visibility timeout mismatch. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents\". It is the option most directly aligned to upstream queue visibility timeout mismatch while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for session gateway: signal points\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Treat partial failures as first-class by adding explicit success criteria and timeout fencing."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for Now that \"incident diagnosis for session gateway: signal points\" is diagnosed, what is the highest-leverage change to make now, \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from failure Modes & Fault Domains to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog ingestion job: signal points to partial failure masked as success. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Catalog ingestion job is a two-step reliability decision. At stage 1, \"The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around partial failure masked as success.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident diagnosis for catalog ingestion job: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Separate shared dependencies by zone and enforce per-domain circuit limits.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Separate shared dependencies by zone and enforce per-domain circuit limits\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for quota evaluator: signal points to correlated AZ dependency failure. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents\" best matches quota evaluator by targeting correlated AZ dependency failure and lowering repeat risk.",
          "detailedExplanation": "Generalize from incident diagnosis for quota evaluator: signal points to correlated AZ dependency to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for quota evaluator: signal points\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Create a dependency failure taxonomy and map runbook actions to each class.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "After diagnosing \"incident diagnosis for quota evaluator: signal points\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Create a dependency failure taxonomy and map runbook actions to each class\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud decision service: signal points to retry-amplified saturation after brownout. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for fraud decision service, \"The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents\" is correct because it addresses retry-amplified saturation after brownout and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for fraud decision service: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Design for correlated-failure mode by reducing hidden single points in control planes.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Design for correlated-failure mode by reducing hidden single points in control planes\" best matches With root cause identified for \"incident diagnosis for fraud decision service: signal\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents\". It is the option most directly aligned to stale config propagation during regional drift while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for checkout control plane: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for \"incident diagnosis for checkout control plane: signal\", which immediate adjustment best addresses the risk, \"Add failure-mode specific observability keyed by fault domain instead of global averages\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Identity graph service is a two-step reliability decision. At stage 1, \"The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around shared control-plane credential expiry.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for identity graph service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents\" best matches recommendation stream processor by targeting degraded dependency returning slow 200s and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 200s in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for recommendation stream processor:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for recommendation stream processor:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for ledger write path, \"The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents\" is correct because it addresses silent packet loss in one network segment and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for ledger write path: signal points\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" best matches Using the diagnosis from \"incident diagnosis for ledger write path: signal points\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Search indexing pipeline is a two-step reliability decision. At stage 1, \"The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around stateful worker hot-node exhaustion.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident diagnosis for search indexing pipeline: signal\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Define fault-domain aware admission policies that preserve invariant-critical traffic\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification fanout service: signal points to cross-region DNS control lag. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for notification fanout service is mismatched to cross-region DNS control lag, creating repeat reliability incidents\" best matches notification fanout service by targeting cross-region DNS control lag and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident diagnosis for notification fanout service:\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Model blast radius by fault-domain layer and isolate critical paths with dependency budgets.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Now that \"incident diagnosis for notification fanout service:\" is diagnosed, what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Model blast radius by fault-domain layer and isolate critical paths with dependency budgets\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize from failure Modes & Fault Domains to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session gateway: signal points to upstream queue visibility timeout mismatch. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for session gateway, \"The design for session gateway is mismatched to upstream queue visibility timeout mismatch, creating repeat reliability incidents\" is correct because it addresses upstream queue visibility timeout mismatch and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for session gateway: signal points\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Treat partial failures as first-class by adding explicit success criteria and timeout fencing.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Treat partial failures as first-class by adding explicit success criteria and timeout fencing\" best matches Using the diagnosis from \"incident diagnosis for session gateway: signal points\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog ingestion job: signal points to partial failure masked as success. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for catalog ingestion job is mismatched to partial failure masked as success, creating repeat reliability incidents\". It is the option most directly aligned to partial failure masked as success while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for catalog ingestion job: signal\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Separate shared dependencies by zone and enforce per-domain circuit limits.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for With diagnosis confirmed in \"incident diagnosis for catalog ingestion job: signal\", which next step is strongest under current constraints, \"Separate shared dependencies by zone and enforce per-domain circuit limits\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for quota evaluator: signal points to correlated AZ dependency failure. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Quota evaluator is a two-step reliability decision. At stage 1, \"The design for quota evaluator is mismatched to correlated AZ dependency failure, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around correlated AZ dependency failure.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for quota evaluator: signal points\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Create a dependency failure taxonomy and map runbook actions to each class."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Create a dependency failure taxonomy and map runbook actions to each class\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud decision service: signal points to retry-amplified saturation after brownout. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for fraud decision service is mismatched to retry-amplified saturation after brownout, creating repeat reliability incidents\" best matches fraud decision service by targeting retry-amplified saturation after brownout and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for fraud decision service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Design for correlated-failure mode by reducing hidden single points in control planes.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "For \"incident diagnosis for fraud decision service: signal\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Design for correlated-failure mode by reducing hidden single points in control planes\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout control plane: signal points to stale config propagation during regional drift. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Failure Modes & Fault Domains: for checkout control plane, \"The design for checkout control plane is mismatched to stale config propagation during regional drift, creating repeat reliability incidents\" is correct because it addresses stale config propagation during regional drift and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for checkout control plane: signal\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Add failure-mode specific observability keyed by fault domain instead of global averages.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Add failure-mode specific observability keyed by fault domain instead of global averages\" best matches With root cause identified for \"incident diagnosis for checkout control plane: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity graph service: signal points to shared control-plane credential expiry. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for identity graph service is mismatched to shared control-plane credential expiry, creating repeat reliability incidents\". It is the option most directly aligned to shared control-plane credential expiry while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident diagnosis for identity graph service: signal points to shared control-plane to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for identity graph service: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Constrain critical writes to known-healthy domains while serving degraded reads explicitly.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for After diagnosing \"incident diagnosis for identity graph service: signal\", what is the highest-leverage change to make now, \"Constrain critical writes to known-healthy domains while serving degraded reads explicitly\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation stream processor: signal points to degraded dependency returning slow 200s. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Recommendation stream processor is a two-step reliability decision. At stage 1, \"The design for recommendation stream processor is mismatched to degraded dependency returning slow 200s, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around degraded dependency returning slow 200s.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 200s appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for recommendation stream processor:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Failure Modes & Fault Domains, the best answer is \"Introduce isolation boundaries so one saturated tenant cannot consume whole-domain capacity\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for ledger write path: signal points to silent packet loss in one network segment. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for ledger write path is mismatched to silent packet loss in one network segment, creating repeat reliability incidents\" best matches ledger write path by targeting silent packet loss in one network segment and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for ledger write path: signal points\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Stage failover drills by node, zone, and region to validate blast-radius assumptions.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Now that \"incident diagnosis for ledger write path: signal points\" is diagnosed, which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Stage failover drills by node, zone, and region to validate blast-radius assumptions\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from failure Modes & Fault Domains to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search indexing pipeline: signal points to stateful worker hot-node exhaustion. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Failure Modes & Fault Domains, the best answer is \"The design for search indexing pipeline is mismatched to stateful worker hot-node exhaustion, creating repeat reliability incidents\". It is the option most directly aligned to stateful worker hot-node exhaustion while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for search indexing pipeline: signal\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Define fault-domain aware admission policies that preserve invariant-critical traffic.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Failure Modes & Fault Domains: for \"incident diagnosis for search indexing pipeline: signal\", which next change should be prioritized first, \"Define fault-domain aware admission policies that preserve invariant-critical traffic\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-061",
      "type": "multi-select",
      "question": "Which indicators most directly reveal cross-domain blast radius? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Failure Modes & Fault Domains, Which indicators most directly reveal cross-domain blast radius needs layered controls, not one silver bullet. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-062",
      "type": "multi-select",
      "question": "Which controls reduce hidden single points of failure? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Which controls reduce hidden single points of failure is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-063",
      "type": "multi-select",
      "question": "During partial failures, which practices improve diagnosis quality? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-064",
      "type": "multi-select",
      "question": "What belongs in a useful dependency failure taxonomy? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For What belongs in a useful dependency failure taxonomy, the highest-signal answer is a bundle of controls. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Generalize from belongs in a useful dependency failure taxonomy? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-065",
      "type": "multi-select",
      "question": "Which patterns limit correlated failures across zones? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Failure Modes & Fault Domains, Which patterns limit correlated failures across zones needs layered controls, not one silver bullet. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-066",
      "type": "multi-select",
      "question": "Which runbook elements increase incident execution reliability? (Select all that apply)",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Which runbook elements increase incident execution reliability is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-067",
      "type": "multi-select",
      "question": "Which signals should trigger graceful isolation first? (Select all that apply)",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-068",
      "type": "multi-select",
      "question": "Which architectural choices help contain tenant-induced overload? (Select all that apply)",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Which architectural choices help contain tenant-induced overload, the highest-signal answer is a bundle of controls. The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-069",
      "type": "multi-select",
      "question": "For reliability policies, which items should be explicit per endpoint? (Select all that apply)",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Failure Modes & Fault Domains, For reliability policies, which items should be explicit per endpoint needs layered controls, not one silver bullet. The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-070",
      "type": "multi-select",
      "question": "Which anti-patterns commonly enlarge outage blast radius? (Select all that apply)",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-071",
      "type": "multi-select",
      "question": "What improves confidence in failover assumptions? (Select all that apply)",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For What improves confidence in failover assumptions, the highest-signal answer is a bundle of controls. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-072",
      "type": "multi-select",
      "question": "Which data is essential when classifying partial vs fail-stop incidents? (Select all that apply)",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Failure Modes & Fault Domains, Which data is essential when classifying partial vs fail-stop incidents needs layered controls, not one silver bullet. The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-073",
      "type": "multi-select",
      "question": "Which controls improve safety when control-plane health is uncertain? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Which controls improve safety when control-plane health is uncertain is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from controls improve safety when control-plane health is uncertain? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-074",
      "type": "multi-select",
      "question": "For critical writes, which guardrails reduce corruption risk under faults? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Failure Modes & Fault Domains: The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-075",
      "type": "multi-select",
      "question": "Which recurring reviews keep reliability boundaries accurate over time? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Which recurring reviews keep reliability boundaries accurate over time, the highest-signal answer is a bundle of controls. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-076",
      "type": "multi-select",
      "question": "Which decisions help teams align on reliability trade-offs during incidents? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Failure Modes & Fault Domains, Which decisions help teams align on reliability trade-offs during incidents needs layered controls, not one silver bullet. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-077",
      "type": "multi-select",
      "question": "What evidence best shows a mitigation reduced recurrence risk? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "What evidence best shows a mitigation reduced recurrence risk is intentionally multi-dimensional in Failure Modes & Fault Domains. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-078",
      "type": "numeric-input",
      "question": "A service processes 4,200,000 requests/day and 0.22% violate reliability SLO. How many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for A service processes 4,200,000 requests/day and 0 gives 9240 requests. In interview pacing, hitting this value within +/-3% is the pass condition. For A service processes 4,200,000 requests/day and 0, this is the strongest fit in Failure Modes & Fault Domains.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 4,200 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-079",
      "type": "numeric-input",
      "question": "Incident queue receives 1,800 items/min and drains 2,050 items/min. Net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Incident queue receives 1,800 items/min and drains 2,050 items/min: 250 items/min. Answers within +/-0% show correct directional reasoning for Failure Modes & Fault Domains.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 1,800 and 2,050 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-080",
      "type": "numeric-input",
      "question": "Retry policy adds 0.35 extra attempts per request at 60,000 req/sec. Effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Failure Modes & Fault Domains expects quick quantitative triage: Retry policy adds 0 evaluates to 81000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 0.35 and 60,000 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-081",
      "type": "numeric-input",
      "question": "Failover takes 18 seconds and happens 21 times/day. Total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for Failover takes 18 seconds and happens 21 times/day gives 378 seconds. In interview pacing, hitting this value within +/-0% is the pass condition. For Failover takes 18 seconds and happens 21 times/day, this is the strongest fit in Failure Modes & Fault Domains.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Normalize units before computing so conversion mistakes do not propagate. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. If values like 18 seconds and 21 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-082",
      "type": "numeric-input",
      "question": "Target p99 latency is 700ms; observed p99 is 980ms. Percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Target p99 latency is 700ms; observed p99 is 980ms: 40 %. Answers within +/-30% show correct directional reasoning for Failure Modes & Fault Domains.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 700ms and 980ms should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-083",
      "type": "numeric-input",
      "question": "If 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For If 31% of 120,000 requests/min are critical-path, how many critical requests/min, the computed target in Failure Modes & Fault Domains is 37200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 31 and 120,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.2% to 0.3%. Percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Failure Modes & Fault Domains expects quick quantitative triage: Error rate drops from 1 evaluates to 75 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 1.2 and 0.3 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-085",
      "type": "numeric-input",
      "question": "A 7-node quorum system requires majority writes. Minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for A 7-node quorum system requires majority writes gives 4 acks. In interview pacing, hitting this value within +/-0% is the pass condition. For A 7-node quorum system requires majority writes, this is the strongest fit in Failure Modes & Fault Domains.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Numbers such as 7 and 4 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-086",
      "type": "numeric-input",
      "question": "Backlog is 48,000 tasks and net drain is 320 tasks/min. Minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Backlog is 48,000 tasks and net drain is 320 tasks/min: 150 minutes. Answers within +/-0% show correct directional reasoning for Failure Modes & Fault Domains.",
      "detailedExplanation": "Generalize from backlog is 48,000 tasks and net drain is 320 tasks/min to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 48,000 and 320 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-087",
      "type": "numeric-input",
      "question": "A system with 14 zones has 2 unavailable. What percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For A system with 14 zones has 2 unavailable, the computed target in Failure Modes & Fault Domains is 85.71 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 14 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-088",
      "type": "numeric-input",
      "question": "MTTR improved from 45 min to 30 min. Percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Failure Modes & Fault Domains expects quick quantitative triage: MTTR improved from 45 min to 30 min evaluates to 33.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 45 min and 30 min should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-089",
      "type": "numeric-input",
      "question": "If 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for If 9% of 2,500,000 daily operations need manual recovery checks, checks/day gives 225000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 9 and 2,500 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-090",
      "type": "ordering",
      "question": "Within failure modes & fault domains, order a reliability response lifecycle.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Failure Modes & Fault Domains should start with Detect and scope affected fault domains and end with Validate recovery and harden recurrence defenses. Within failure modes & fault domains, order a reliability response lifecycle rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-091",
      "type": "ordering",
      "question": "For failure modes & fault domains, order from lowest to highest reliability risk.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Isolated dependency with fallback and budget must happen before Implicit dependency with no failure policy. That ordering matches incident-safe flow in Failure Modes & Fault Domains.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-092",
      "type": "ordering",
      "question": "Order failover safety steps. Focus on failure modes & fault domains tradeoffs.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failure Modes & Fault Domains emphasizes safe recovery order. Beginning at Verify candidate health and freshness and finishing at Run failback readiness checks before restoration keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-093",
      "type": "ordering",
      "question": "Order by increasing overload-protection strength. Use a failure modes & fault domains perspective.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order by increasing overload-protection strength, the correct ordering runs from No admission limits to Priority-aware shedding plus per-domain concurrency bounds. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Place obvious extremes first, then sort the middle by pairwise comparison. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-094",
      "type": "ordering",
      "question": "Order data recovery execution. (failure modes & fault domains lens)",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Failure Modes & Fault Domains should start with Select recovery point by RPO target and end with Promote and re-enable writes with monitoring. Order data recovery execution rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-095",
      "type": "ordering",
      "question": "From a failure modes & fault domains viewpoint, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Define SLIs tied to user impact must happen before Review incidents and close corrective actions. That ordering matches incident-safe flow in Failure Modes & Fault Domains.",
      "detailedExplanation": "Generalize from order reliability operations loop to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-096",
      "type": "ordering",
      "question": "Rank these from smallest to largest blast radius. (Failure Modes & Fault Domains context)",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failure Modes & Fault Domains emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-097",
      "type": "ordering",
      "question": "In this failure modes & fault domains context, order retry-policy maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For In this failure modes & fault domains context, order retry-policy maturity, the correct ordering runs from Fixed immediate retries to Jittered backoff with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-098",
      "type": "ordering",
      "question": "Within failure modes & fault domains, order degradation sophistication.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Failure Modes & Fault Domains should start with Undocumented ad hoc fallback and end with Automated policy-driven degradation with user semantics. Within failure modes & fault domains, order degradation sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-099",
      "type": "ordering",
      "question": "For failure modes & fault domains, order incident command rigor.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc responders with no roles must happen before Role-defined operations plus decision log and action tracking. That ordering matches incident-safe flow in Failure Modes & Fault Domains.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-fd-100",
      "type": "ordering",
      "question": "Order reliability validation confidence. Focus on failure modes & fault domains tradeoffs.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failure Modes & Fault Domains emphasizes safe recovery order. Beginning at Single success in staging and finishing at Sustained recovery plus recurrence drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "failure-modes-and-fault-domains"],
      "difficulty": "staff-level"
    }
  ]
}
