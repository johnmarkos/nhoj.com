{
  "unit": 1,
  "unitTitle": "Estimation",
  "chapter": 8,
  "chapterTitle": "Compound Scenarios",
  "chapterDescription": "Multi-step estimation problems combining time, storage, bandwidth, QPS, and growth projections",
  "problems": [
    {
      "id": "compound-001",
      "type": "multiple-choice",
      "question": "Video platform: 10M DAU, each watches 30 min/day of 1080p video (5 Mbps). Peak concurrent viewers are 10% of DAU. Peak bandwidth needed?",
      "options": ["~500 Gbps", "~5 Tbps", "~50 Tbps", "~500 Tbps"],
      "correct": 1,
      "explanation": "Peak concurrent: 10M × 10% = 1M viewers. Bandwidth: 1M × 5 Mbps = 5,000,000 Mbps = 5 Tbps.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "compound-002",
      "type": "multiple-choice",
      "question": "Chat app: 100M users, 50 messages/day average, 200 bytes/message. 90-day retention. Total storage?",
      "options": ["~9 TB", "~90 TB", "~900 TB", "~9 PB"],
      "correct": 1,
      "explanation": "Daily: 100M × 50 × 200 bytes = 1 TB/day. 90 days retention: 90 TB.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-003",
      "type": "multiple-choice",
      "question": "E-commerce: 1M orders/day, each order triggers 5 downstream events, each event is 1 KB. Event processing must complete in 24 hours. Minimum throughput needed?",
      "options": [
        "~58 events/sec",
        "~580 events/sec",
        "~5,800 events/sec",
        "~58,000 events/sec"
      ],
      "correct": 0,
      "explanation": "Total events: 1M × 5 = 5M/day. Throughput: 5M ÷ 86,400 sec = 58 events/sec minimum. (Build in headroom for bursts.)",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-004",
      "type": "multiple-choice",
      "question": "Photo sharing: 50M DAU upload 2 photos/day at 2 MB each. CDN serves each photo 100 times on average. Daily CDN egress?",
      "options": ["~2 PB", "~20 PB", "~200 PB", "~2 EB"],
      "correct": 1,
      "explanation": "Daily uploads: 50M × 2 = 100M photos × 2 MB = 200 TB. Total serves: 100M × 100 × 2 MB = 20 PB egress.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-005",
      "type": "multiple-choice",
      "question": "Ride-sharing: 10M rides/day, each ride has 100 GPS pings (50 bytes each). Keep 1 year of data. Annual storage?",
      "options": ["~18 TB", "~180 TB", "~1.8 PB", "~18 PB"],
      "correct": 0,
      "explanation": "Daily: 10M × 100 × 50 bytes = 50 GB/day. Annual: 50 GB × 365 = 18.25 TB ≈ 18 TB.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-006",
      "type": "multiple-choice",
      "question": "Notification service: 100M users, 80% opt-in, send 3 notifications/day per user. Each notification = 500 bytes + 1 KB push payload. Daily bandwidth for push?",
      "options": ["~120 GB", "~360 GB", "~1.2 TB", "~3.6 TB"],
      "correct": 1,
      "explanation": "Recipients: 100M × 80% = 80M. Daily notifications: 80M × 3 = 240M. Payload: 240M × 1.5 KB = 360 GB.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-007",
      "type": "multiple-choice",
      "question": "Search engine: 1B queries/day, each query searches 10B documents in 200ms. Index is 1% of document size. If documents total 100 PB, index size?",
      "options": ["~100 TB", "~1 PB", "~10 PB", "~100 PB"],
      "correct": 1,
      "explanation": "Index = 1% of 100 PB = 1 PB. (Query rate and latency don't affect index size.)",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-008",
      "type": "multiple-choice",
      "question": "Gaming platform: 5M concurrent players, each sends 20 packets/sec at 100 bytes. Server processes and broadcasts to 50 nearby players. Server egress bandwidth?",
      "options": ["~500 Gbps", "~4 Tbps", "~40 Tbps", "~400 Tbps"],
      "correct": 1,
      "explanation": "Inbound: 5M × 20 × 100 bytes = 10 GB/sec. Egress (50x fan-out): 10 GB/s × 50 = 500 GB/s = 4 Tbps.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-009",
      "type": "multiple-choice",
      "question": "IoT platform: 10M devices reporting 1 KB every 10 seconds. Data retained 30 days. Writes/sec and storage?",
      "options": [
        "1M writes/sec, 2.6 PB storage",
        "100K writes/sec, 260 TB storage",
        "1M writes/sec, 260 TB storage",
        "100K writes/sec, 2.6 PB storage"
      ],
      "correct": 0,
      "explanation": "Writes: 10M ÷ 10 sec = 1M writes/sec. Daily: 1M writes/sec × 86,400 sec × 1 KB = 86.4 TB/day. 30 days: 86.4 × 30 = 2.6 PB.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-010",
      "type": "multiple-choice",
      "question": "Ad platform: 10B ad impressions/day, each impression logged at 500 bytes. Logs compressed 5x. 7-day retention. Storage needed?",
      "options": ["~7 TB", "~70 TB", "~700 TB", "~7 PB"],
      "correct": 0,
      "explanation": "Daily raw: 10B × 500 bytes = 5 TB. Compressed: 5 TB ÷ 5 = 1 TB/day. 7 days: 7 TB.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-011",
      "type": "multiple-choice",
      "question": "Social network: 500M users, average 200 friends each. Store friend relationships. Each edge = 16 bytes (two 8-byte IDs). Total storage for friend graph?",
      "options": ["~800 GB", "~1.6 TB", "~8 TB", "~16 TB"],
      "correct": 0,
      "explanation": "Edges: 500M × 200 ÷ 2 (bidirectional) = 50B edges. Storage: 50B × 16 bytes = 800 GB.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-012",
      "type": "multiple-choice",
      "question": "Streaming music: 50M subscribers, each streams 2 hours/day at 320 kbps. 80% cache hit at CDN. Daily origin bandwidth?",
      "options": ["~1.4 PB", "~2.9 PB", "~11.5 PB", "~14.4 PB"],
      "correct": 1,
      "explanation": "Total streaming: 50M × 2 hr × 320 kbps = 50M × 288 MB = 14.4 PB/day. Cache miss (20%): 14.4 × 0.2 = 2.88 PB to origin.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-013",
      "type": "multiple-choice",
      "question": "URL shortener: 100M new URLs/month, each URL accessed 10 times/month average on month of creation, then rarely. Peak read QPS (assume uniform distribution)?",
      "options": ["~40 QPS", "~400 QPS", "~4,000 QPS", "~40,000 QPS"],
      "correct": 2,
      "explanation": "Monthly reads: 100M × 10 = 1B. QPS: 1B ÷ (30 × 86,400) = 1B ÷ 2.6M = ~385 QPS average. Peak is typically 3-10x average, so ~4,000 QPS.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-014",
      "type": "multiple-choice",
      "question": "Email service: 1B emails/day, average 50 KB each, 30% have 1 MB attachment average. Total daily ingest?",
      "options": ["~50 TB", "~350 TB", "~500 TB", "~850 TB"],
      "correct": 1,
      "explanation": "Body: 1B × 50 KB = 50 TB. Attachments: 1B × 30% × 1 MB = 300 TB. Total: 350 TB/day.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-015",
      "type": "multiple-choice",
      "question": "Video conferencing: 10M concurrent calls, each has 4 participants sending 2 Mbps video. Server receives and re-broadcasts to 3 others per participant. Server throughput?",
      "options": [
        "~80 Tbps in, ~240 Tbps out",
        "~80 Tbps in, ~80 Tbps out",
        "~20 Tbps in, ~60 Tbps out",
        "~240 Tbps total (in + out)"
      ],
      "correct": 0,
      "explanation": "Participants: 10M × 4 = 40M. Inbound: 40M × 2 Mbps = 80 Tbps. Outbound per participant: 2 Mbps × 3 = 6 Mbps. Total out: 40M × 6 Mbps = 240 Tbps.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "compound-016",
      "type": "multiple-choice",
      "question": "Log aggregation: 1000 servers, each produces 100 MB logs/hour. Ship to central store, compress 10x, retain 90 days. Storage needed?",
      "options": ["~2.2 TB", "~22 TB", "~216 TB", "~2.16 PB"],
      "correct": 1,
      "explanation": "Daily raw: 1000 × 100 MB × 24 = 2.4 TB. Compressed (÷10): 240 GB/day. 90 days: 21.6 TB ≈ 22 TB.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-017",
      "type": "multiple-choice",
      "question": "Food delivery: 5M orders/day. Each order requires: 1 user lookup, 5 restaurant menu reads, 3 driver location updates. Database read capacity needed?",
      "options": [
        "~52 reads/sec",
        "~520 reads/sec",
        "~5,200 reads/sec",
        "~52,000 reads/sec"
      ],
      "correct": 1,
      "explanation": "Reads per order: 1 + 5 + 3 = 9. Daily reads: 5M × 9 = 45M. QPS: 45M ÷ 86,400 = 520 reads/sec.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-018",
      "type": "multiple-choice",
      "question": "Analytics pipeline: 100M events/hour, each 1 KB. Process in Spark with 3x data expansion during joins. Peak memory across cluster?",
      "options": ["~100 GB", "~300 GB", "~1 TB", "~3 TB"],
      "correct": 1,
      "explanation": "Hourly data: 100M × 1 KB = 100 GB. With 3x expansion: 300 GB minimum memory needed to hold working set.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-019",
      "type": "multiple-choice",
      "question": "News feed: 100M DAU, each sees 100 posts/day, each post averages 10 KB (text + thumbnail). Feed is personalized (no caching). Daily bandwidth to users?",
      "options": ["~10 TB", "~100 TB", "~1 PB", "~10 PB"],
      "correct": 1,
      "explanation": "Feed data: 100M × 100 × 10 KB = 100 TB/day to users.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-020",
      "type": "multiple-choice",
      "question": "Payment processor: 10,000 TPS, each transaction = 2 DB writes (debit + credit). Database can handle 50,000 writes/sec. Utilization?",
      "options": ["~20%", "~40%", "~80%", "~200% (need to scale)"],
      "correct": 1,
      "explanation": "DB writes: 10,000 × 2 = 20,000 writes/sec. Utilization: 20,000 ÷ 50,000 = 40%.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-021",
      "type": "multiple-choice",
      "question": "Recommendation engine: 50M users, precompute top 100 recs per user daily. Each rec = 8 bytes (item ID). Storage for all recommendations?",
      "options": ["~4 GB", "~40 GB", "~400 GB", "~4 TB"],
      "correct": 1,
      "explanation": "Total: 50M × 100 × 8 bytes = 40B bytes = 40 GB.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-022",
      "type": "multiple-choice",
      "question": "Map tiles: World at zoom 18 = 69B tiles. Each tile 20 KB. With 5x versioning for updates. Total storage?",
      "options": ["~1.4 PB", "~6.9 PB", "~34.5 PB", "~69 PB"],
      "correct": 1,
      "explanation": "Base: 69B × 20 KB = 1.38 PB. With 5 versions: 1.38 × 5 = 6.9 PB.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-023",
      "type": "multiple-choice",
      "question": "CDN design: 1 PB content library. 80% of requests hit 1% of content. Size hot tier to serve 80% of requests from memory?",
      "options": ["~1 TB", "~10 TB", "~100 TB", "~500 TB"],
      "correct": 1,
      "explanation": "Hot content: 1% of 1 PB = 10 TB. This serves 80% of requests. (Pareto principle in action.)",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-024",
      "type": "multiple-choice",
      "question": "Taxi dispatch: City has 50,000 taxis. Each updates location every 5 seconds (100 bytes). Match requests at 1000 rides/min. Location write throughput?",
      "options": [
        "~1,000 writes/sec",
        "~10,000 writes/sec",
        "~100,000 writes/sec",
        "~1,000,000 writes/sec"
      ],
      "correct": 1,
      "explanation": "Location writes: 50,000 taxis ÷ 5 sec = 10,000 writes/sec.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "compound-025",
      "type": "multiple-choice",
      "question": "ML training: 1TB dataset, train for 100 epochs. Each epoch reads full dataset. Disk throughput at 500 MB/s. Minimum read time?",
      "options": ["~2 hours", "~6 hours", "~56 hours", "~200 hours"],
      "correct": 2,
      "explanation": "Total reads: 1 TB × 100 = 100 TB. Time: 100 TB ÷ 500 MB/s = 200,000 sec = 55.6 hours.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-026",
      "type": "multiple-choice",
      "question": "Web crawler: Crawl 1B pages/day, each page averages 100 KB. Parse and extract 5 KB of text per page. Raw vs processed storage ratio?",
      "options": [
        "100 TB raw, 5 TB processed (20:1)",
        "100 TB raw, 500 GB processed (200:1)",
        "10 PB raw, 500 TB processed (20:1)",
        "1 PB raw, 50 TB processed (20:1)"
      ],
      "correct": 0,
      "explanation": "Raw: 1B × 100 KB = 100 TB. Processed: 1B × 5 KB = 5 TB. Ratio: 20:1.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-027",
      "type": "multiple-choice",
      "question": "Stock exchange: 1M trades/day, each creates 5 events (order, match, confirm, settle, report). Events processed by 10 downstream systems. Daily message count?",
      "options": [
        "~5M messages",
        "~50M messages",
        "~500M messages",
        "~5B messages"
      ],
      "correct": 1,
      "explanation": "Events: 1M × 5 = 5M. Fan-out to 10 systems: 5M × 10 = 50M messages.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-028",
      "type": "multiple-choice",
      "question": "Healthcare records: 100M patients, average 500 KB record, updated 5 times/year. Keep all versions forever. After 10 years, storage?",
      "options": ["~50 TB", "~250 TB", "~2.5 PB", "~25 PB"],
      "correct": 2,
      "explanation": "Base records: 100M × 500 KB = 50 TB. Updates: 100M × 5 × 500 KB × 10 years = 2.5 PB (assuming full record stored per version). Total: ~2.5 PB.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-029",
      "type": "multiple-choice",
      "question": "API gateway: 100K RPM, average response 10 KB. Log full request/response for debugging. 24-hour log retention. Storage?",
      "options": ["~14 GB", "~144 GB", "~1.4 TB", "~14 TB"],
      "correct": 2,
      "explanation": "Daily requests: 100K × 60 × 24 = 144M. Storage: 144M × 10 KB = 1.44 TB.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-030",
      "type": "multiple-choice",
      "question": "Backup system: 100 TB database, daily incremental 1%, weekly full. Monthly storage for backups?",
      "options": ["~430 TB", "~500 TB", "~700 TB", "~1 PB"],
      "correct": 0,
      "explanation": "Weekly fulls: 4 × 100 TB = 400 TB. Daily incrementals: ~28 × 1 TB = 28 TB. Total: ~428 TB.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-031",
      "type": "multiple-choice",
      "question": "Real-time bidding: 1M ad requests/sec, each request sent to 50 bidders, 10ms timeout. Bidder response at 1 KB. Aggregate inbound bandwidth from bidders?",
      "options": ["~50 GB/s", "~500 GB/s", "~5 TB/s", "~50 TB/s"],
      "correct": 0,
      "explanation": "Responses: 1M × 50 = 50M responses/sec. Bandwidth: 50M × 1 KB = 50 GB/s inbound.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-032",
      "type": "multiple-choice",
      "question": "Code repository: 10M repos, average 100 MB each. Each repo cloned 10 times/day on average. Daily egress?",
      "options": ["~1 PB", "~10 PB", "~100 PB", "~1 EB"],
      "correct": 1,
      "explanation": "Daily clones: 10M × 10 = 100M. Egress: 100M × 100 MB = 10 PB.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-033",
      "type": "multiple-choice",
      "question": "Sensor network: 1M sensors, 10 readings/min, 100 bytes each. Keep raw data for 24 hours. Raw storage needed?",
      "options": ["~144 GB", "~1.44 TB", "~14.4 TB", "~144 TB"],
      "correct": 1,
      "explanation": "Per hour: 1M × 10 × 60 × 100 bytes = 60 GB. 24 hours: 60 × 24 = 1.44 TB raw storage.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-034",
      "type": "multiple-choice",
      "question": "E-commerce search: 10M products, 1 KB metadata each. Index takes 3x raw size. Update 10% of products daily. Daily index rebuild time at 100 MB/s write?",
      "options": ["~5 minutes", "~50 minutes", "~5 hours", "~50 hours"],
      "correct": 0,
      "explanation": "Full index: 10M × 1 KB × 3 = 30 GB. Write time: 30 GB ÷ 100 MB/s = 300 sec = 5 minutes. (Incremental updates would be faster.)",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-035",
      "type": "multiple-choice",
      "question": "Social media: Post goes viral - 10M views in 1 hour. Video is 50 MB. With 90% CDN cache hit, origin serves?",
      "options": [
        "~50 TB from origin",
        "~5 TB from origin",
        "~500 GB from origin",
        "~50 GB from origin"
      ],
      "correct": 0,
      "explanation": "Cache miss: 10% of 10M = 1M requests to origin. Origin egress: 1M × 50 MB = 50 TB. (This is why CDN cache hit rate matters!)",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-036",
      "type": "multiple-choice",
      "question": "Auth service: 100M users, each generates 10 sessions/month, session token = 64 bytes. Sessions expire in 30 days. Max concurrent session storage?",
      "options": ["~64 GB", "~640 GB", "~6.4 TB", "~64 TB"],
      "correct": 0,
      "explanation": "Sessions/month: 100M × 10 = 1B. With 30-day expiry ≈ 1 month of sessions active: 1B × 64 bytes = 64 GB.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-037",
      "type": "multiple-choice",
      "question": "Monitoring system: 10,000 hosts, 100 metrics each, 15-sec scrape interval. Time-series DB at 8 bytes/point. Daily storage?",
      "options": ["~46 GB", "~460 GB", "~4.6 TB", "~46 TB"],
      "correct": 0,
      "explanation": "Points/day: 10K × 100 metrics × 5,760 points/day = 5.76B points. Storage: 5.76B × 8 bytes = 46 GB.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-038",
      "type": "multiple-choice",
      "question": "Document search: 100M documents, average 10 KB. Build inverted index with 5000 unique terms per doc, 12 bytes per posting. Index size?",
      "options": ["~60 TB", "~6 TB", "~600 GB", "~60 GB"],
      "correct": 1,
      "explanation": "Postings: 100M × 5000 = 500B. Size: 500B × 12 bytes = 6 TB.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-039",
      "type": "multiple-choice",
      "question": "Fleet management: 100K vehicles, GPS ping every 30 sec. Calculate nearest 10 vehicles to rider in 100ms. With 1M ride requests/day, daily distance calculations?",
      "options": [
        "~100M calculations",
        "~1B calculations",
        "~10B calculations",
        "~100B calculations"
      ],
      "correct": 3,
      "explanation": "Each request compares against 100K vehicles = 100K distance calcs. Daily: 1M × 100K = 100B calculations. (This is why spatial indexing matters.)",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-040",
      "type": "multiple-choice",
      "question": "Video upload: User uploads 1 hour 4K video (50 GB). Transcode to 5 resolutions. Each resolution takes 2x real-time on 1 CPU. Total CPU-hours?",
      "options": [
        "~5 CPU-hours",
        "~10 CPU-hours",
        "~50 CPU-hours",
        "~100 CPU-hours"
      ],
      "correct": 1,
      "explanation": "Per resolution: 1 hour × 2 = 2 CPU-hours. 5 resolutions: 10 CPU-hours total.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-041",
      "type": "multiple-choice",
      "question": "Multiplayer game: 100 players in a match, 60 tick rate, 50 bytes game state per player. Server broadcasts full state to all players. Bandwidth per match?",
      "options": ["~30 MB/s", "~300 MB/s", "~3 GB/s", "~30 GB/s"],
      "correct": 0,
      "explanation": "State size: 100 × 50 = 5 KB. Broadcasts/sec: 60. To 100 players: 60 × 5 KB × 100 = 30 MB/s per match.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-042",
      "type": "multiple-choice",
      "question": "Data warehouse: 10 TB daily ingest. Queries scan 30% of data on average. 1000 queries/day. Each query gets dedicated throughput at 1 GB/s. Daily query time?",
      "options": ["~50 minutes", "~8 hours", "~83 hours", "~833 hours"],
      "correct": 2,
      "explanation": "Scan per query: 10 TB × 30% = 3 TB. Time per query: 3 TB ÷ 1 GB/s = 3000 sec = 50 min. Daily: 1000 × 50 min = 833 hours. But queries can run in parallel...",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "compound-043",
      "type": "multiple-choice",
      "question": "Image recognition API: 10M images/day, each 500 KB. Pre-process (resize) to 100 KB, then inference at 50ms/image on GPU. Daily GPU-hours?",
      "options": [
        "~14 GPU-hours",
        "~139 GPU-hours",
        "~1,390 GPU-hours",
        "~13,900 GPU-hours"
      ],
      "correct": 1,
      "explanation": "Inference time: 10M × 50ms = 500M ms = 500,000 sec = 139 GPU-hours.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-044",
      "type": "multiple-choice",
      "question": "Content moderation: 50M posts/day, 10% flagged for review, each review takes ML model 200ms. GPU capacity needed to process same-day?",
      "options": ["~12 GPUs", "~29 GPUs", "~116 GPUs", "~290 GPUs"],
      "correct": 0,
      "explanation": "Reviews: 50M × 10% = 5M. Total GPU time: 5M × 200ms = 1M seconds. GPUs needed: 1M sec ÷ 86,400 sec/day = 11.6 ≈ 12 GPUs.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-045",
      "type": "multiple-choice",
      "question": "Blockchain node: 1 MB block every 10 min, store full history (700K blocks). Each block distributed to 10K nodes. Monthly bandwidth for new blocks globally?",
      "options": ["~4 TB", "~40 TB", "~400 TB", "~4 PB"],
      "correct": 1,
      "explanation": "Blocks/month: 6/hour × 24 × 30 = 4,320 blocks = 4.32 GB. Distributed to 10K nodes: 4.32 GB × 10K = 43.2 TB ≈ 40 TB global bandwidth.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-046",
      "type": "multiple-choice",
      "question": "Job scheduler: 1M jobs/day, average 5 minutes runtime. Jobs distributed across 1000-server cluster. Average cluster utilization?",
      "options": ["~35%", "~70%", "~140%", "~350%"],
      "correct": 3,
      "explanation": "Work needed: 1M × 5 min = 5M server-minutes/day. Capacity: 1000 servers × 1440 min/day = 1.44M server-minutes. Utilization: 5M ÷ 1.44M = 347% - cluster is overloaded and needs more servers!",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-047",
      "type": "multiple-choice",
      "question": "DNS service: 1M domains, each has 5 records averaging 100 bytes. 10B queries/day, 99% cache hit at resolver. Authoritative server query load?",
      "options": ["~1,200 QPS", "~12,000 QPS", "~120,000 QPS", "~1.2M QPS"],
      "correct": 0,
      "explanation": "Cache miss: 1% of 10B = 100M queries/day to authoritative. QPS: 100M ÷ 86,400 = 1,157 ≈ 1,200 QPS.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "compound-048",
      "type": "multiple-choice",
      "question": "Feature store: 10M users × 500 features each, 8 bytes per feature. Refresh features every hour. Hourly write throughput?",
      "options": ["~40 GB/hour", "~400 GB/hour", "~4 TB/hour", "~40 TB/hour"],
      "correct": 0,
      "explanation": "Data: 10M × 500 × 8 = 40 GB. Hourly refresh: 40 GB/hour write throughput.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "compound-049",
      "type": "multiple-choice",
      "question": "Live streaming: 10K concurrent streamers at 5 Mbps each. Average 1000 viewers per stream. Total egress bandwidth?",
      "options": ["~50 Gbps", "~500 Gbps", "~5 Tbps", "~50 Tbps"],
      "correct": 3,
      "explanation": "Total viewers: 10K × 1000 = 10M. Egress: 10M × 5 Mbps = 50 Tbps.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "compound-050",
      "type": "multiple-choice",
      "question": "Fraud detection: 1M transactions/hour. ML model runs 5ms per transaction. Flag 0.1% as suspicious for human review. If reviewer handles 100 cases/day, reviewers needed?",
      "options": [
        "~10 reviewers",
        "~24 reviewers",
        "~100 reviewers",
        "~240 reviewers"
      ],
      "correct": 3,
      "explanation": "Daily transactions: 24M. Suspicious: 24M × 0.1% = 24,000 cases. Reviewers: 24,000 ÷ 100 = 240.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-051",
      "type": "multiple-choice",
      "question": "Geospatial database: 1B points of interest, each 200 bytes + 16 bytes coordinates. Spatial index adds 50% overhead. Total storage?",
      "options": ["~216 GB", "~324 GB", "~432 GB", "~648 GB"],
      "correct": 1,
      "explanation": "Raw: 1B × (200 + 16) = 216 GB. With index (+50%): 216 × 1.5 = 324 GB.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-052",
      "type": "multiple-choice",
      "question": "Video transcoding farm: 10,000 hours of video uploaded/day. Each hour transcoded to 6 formats, each takes 30 min on one core. Servers needed (32-core each)?",
      "options": [
        "~40 servers",
        "~125 servers",
        "~400 servers",
        "~1,250 servers"
      ],
      "correct": 1,
      "explanation": "Work: 10K hours × 6 × 0.5 hours = 30K core-hours/day. Daily capacity per server: 32 × 24 = 768 core-hours. Servers: 30K ÷ 768 = 39 servers minimum. With headroom: ~40-125.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "compound-053",
      "type": "multiple-choice",
      "question": "Chat history search: 1B messages, average 100 bytes. Full-text index is 2x message size. Search query scans 0.1% of index. Single-threaded query throughput at 1 GB/s disk?",
      "options": [
        "~5 queries/sec",
        "~50 queries/sec",
        "~500 queries/sec",
        "~5,000 queries/sec"
      ],
      "correct": 0,
      "explanation": "Messages: 1B × 100 bytes = 100 GB. Index: 200 GB. Query scan: 200 GB × 0.1% = 200 MB per query. At 1 GB/s: 1000 MB ÷ 200 MB = 5 queries/sec.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "compound-054",
      "type": "multiple-choice",
      "question": "Smart home: 50M homes, 10 devices each, device sends 1 event/minute, 200 bytes per event. Daily events and storage?",
      "options": [
        "~720B events, ~144 TB",
        "~72B events, ~14.4 TB",
        "~7.2B events, ~1.44 TB",
        "~720M events, ~144 GB"
      ],
      "correct": 0,
      "explanation": "Devices: 50M × 10 = 500M. Daily events: 500M × 1440 = 720B. Storage: 720B × 200 bytes = 144 TB.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-055",
      "type": "multiple-choice",
      "question": "A/B testing: 100M users, 20 concurrent experiments, each user in 5 experiments. Record assignment (32 bytes) and 10 events (50 bytes each) per user per experiment. Daily storage?",
      "options": ["~2.9 GB", "~29 GB", "~290 GB", "~2.9 TB"],
      "correct": 2,
      "explanation": "Per user: 5 experiments × (32 + 10 × 50) = 5 × 532 = 2,660 bytes. Total: 100M × 2,660 = 266 GB ≈ 290 GB.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-056",
      "type": "multiple-choice",
      "question": "Email delivery: 1B emails/day. Each email attempt: DNS lookup (1ms), connection (50ms), send (100ms average). Sequential processing throughput per server?",
      "options": [
        "~6.6 emails/sec",
        "~66 emails/sec",
        "~660 emails/sec",
        "~6,600 emails/sec"
      ],
      "correct": 0,
      "explanation": "Time per email: 1 + 50 + 100 = 151ms. Per server: 1000 ÷ 151 = 6.6 emails/sec. (This is why connection pooling and pipelining matter.)",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ]
    },
    {
      "id": "compound-057",
      "type": "multiple-choice",
      "question": "Database migration: 10 TB source, replicate at 100 MB/s, while handling 1000 writes/sec (1 KB each). Time to initial sync?",
      "options": ["~28 hours", "~56 hours", "~112 hours", "~224 hours"],
      "correct": 0,
      "explanation": "Sync time: 10 TB ÷ 100 MB/s = 100,000 sec = 27.8 hours. New writes during sync: 1000 × 1 KB × 100,000 = 100 GB (negligible vs 10 TB). Total: ~28 hours.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-058",
      "type": "multiple-choice",
      "question": "Rate limiter: 10M users, each gets 100 requests/minute limit. Store counter per user (16 bytes). With 1-minute sliding window (60 buckets), memory needed?",
      "options": ["~160 MB", "~1.6 GB", "~9.6 GB", "~96 GB"],
      "correct": 2,
      "explanation": "Per user: 60 buckets × 16 bytes = 960 bytes. Total: 10M × 960 = 9.6 GB.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        }
      ]
    },
    {
      "id": "compound-059",
      "type": "multiple-choice",
      "question": "Global load balancer: 100 regions, health check each region every 5 sec. Update DNS with 30-sec TTL. If region fails, max time until traffic rerouted?",
      "options": ["~5 seconds", "~35 seconds", "~65 seconds", "~95 seconds"],
      "correct": 1,
      "explanation": "Worst case: health check just passed, failure happens immediately after, next check in 5 sec detects failure, DNS updated, clients wait up to 30 sec TTL. Total: 5 + 30 = 35 seconds max.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ]
    },
    {
      "id": "compound-060",
      "type": "multiple-choice",
      "question": "Object storage: 10B objects, 500 KB average. Lifecycle rule moves objects older than 30 days to cold storage. Cold storage size (ignoring new objects)?",
      "options": ["~500 TB", "~5 PB", "~50 PB", "~500 PB"],
      "correct": 1,
      "explanation": "10B objects × 500 KB = 5 PB total. Most objects (older than 30 days) are in cold storage ≈ 5 PB.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-061",
      "type": "multiple-choice",
      "question": "Social graph: Query 'friends of friends' for user with 500 friends, each having 500 friends. Unique users in result (assuming 50% overlap)?",
      "options": [
        "~125,000 users",
        "~250,000 users",
        "~500,000 users",
        "~1,000,000 users"
      ],
      "correct": 0,
      "explanation": "Friends of friends: 500 × 500 = 250,000. With 50% overlap: 250,000 × 0.5 = 125,000 unique users.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-062",
      "type": "multiple-choice",
      "question": "Event sourcing: 10M events/day, average 500 bytes. Snapshots every 1000 events at 10 KB each. Keep all events + snapshots. Daily storage?",
      "options": [
        "~5 GB events + ~100 MB snapshots",
        "~5 GB events + ~1 GB snapshots",
        "~50 GB events + ~100 MB snapshots",
        "~50 GB events + ~1 GB snapshots"
      ],
      "correct": 0,
      "explanation": "Events: 10M × 500 bytes = 5 GB. Snapshots: 10M ÷ 1000 × 10 KB = 100 MB.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-063",
      "type": "multiple-choice",
      "question": "Caching layer: 100K QPS, 10% cache miss, each miss costs $0.0001 (DB query). Monthly cost of cache misses?",
      "options": ["~$2,600", "~$26,000", "~$260,000", "~$2.6M"],
      "correct": 3,
      "explanation": "Misses/sec: 100K × 10% = 10K. Monthly misses: 10K × 2.6M sec = 26B. Cost: 26B × $0.0001 = $2.6M.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-064",
      "type": "multiple-choice",
      "question": "Log shipping: 1000 servers, 1 GB logs/day each. Ship compressed (10x) every hour. Network bandwidth per server during ship?",
      "options": [
        "~2.3 KB/s average",
        "~23 KB/s average",
        "~230 KB/s average",
        "~2.3 MB/s average"
      ],
      "correct": 0,
      "explanation": "Daily compressed: 1 GB ÷ 10 = 100 MB. Hourly: 100 MB ÷ 24 = 4.2 MB. Bandwidth: 4.2 MB ÷ 3600 sec ≈ 1.2 KB/s average. Closest is 2.3 KB/s.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-065",
      "type": "multiple-choice",
      "question": "Microservices: 50 services, each calls average 3 downstream services per request. 10K external requests/sec. Internal service calls/sec?",
      "options": [
        "~30K calls/sec",
        "~150K calls/sec",
        "~1.5M calls/sec",
        "~15M calls/sec"
      ],
      "correct": 0,
      "explanation": "Each external request generates: 1 + 3 = 4 hops initially, but if each downstream also calls 3... First level: 10K. Each generates 3 more: 30K internal calls. (Assuming 1 level depth shown.)",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-066",
      "type": "multiple-choice",
      "question": "Time-series forecasting: 1M metrics, model generates 24-hour forecast in 10ms per metric. Total forecast compute time?",
      "options": ["~2.8 hours", "~28 hours", "~280 hours", "~2,800 hours"],
      "correct": 0,
      "explanation": "Total: 1M metrics × 10ms = 10,000 sec = 2.78 hours of compute time.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Growth planning should show today vs horizon requirements side by side because compounding makes small percentages material quickly.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-067",
      "type": "multiple-choice",
      "question": "CI/CD: 500 repos, each builds 5 times/day, build takes 10 minutes, uses 4 CPU cores. Total daily compute?",
      "options": [
        "~167 core-hours",
        "~417 core-hours",
        "~1,667 core-hours",
        "~4,167 core-hours"
      ],
      "correct": 2,
      "explanation": "Builds: 500 × 5 = 2,500/day. Core-minutes: 2,500 × 10 × 4 = 100,000. Core-hours: 100,000 ÷ 60 = 1,667.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-068",
      "type": "multiple-choice",
      "question": "Graph database: 100M nodes (100 bytes each), 1B edges (50 bytes each). Index on node properties adds 20% overhead. Total storage?",
      "options": ["~12 GB", "~62 GB", "~120 GB", "~620 GB"],
      "correct": 1,
      "explanation": "Nodes: 100M × 100 = 10 GB. Edges: 1B × 50 = 50 GB. Index: 10 GB × 0.2 = 2 GB. Total: 62 GB.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-069",
      "type": "multiple-choice",
      "question": "Image pipeline: 1M images/day uploaded. Generate 5 thumbnail sizes (2 KB, 10 KB, 50 KB, 200 KB, 500 KB). Total thumbnail storage after 1 year?",
      "options": ["~28 TB", "~278 TB", "~2.78 PB", "~27.8 PB"],
      "correct": 1,
      "explanation": "Per image: 2 + 10 + 50 + 200 + 500 = 762 KB. Daily: 1M × 762 KB = 762 GB. Annual: 762 × 365 = 278 TB.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-070",
      "type": "multiple-choice",
      "question": "Webhook delivery: 10M events/day, each triggers webhook to 3 endpoints average. 5% failure rate, retry 3 times with exponential backoff. Total daily HTTP requests?",
      "options": [
        "~30M requests",
        "~31.5M requests",
        "~34.5M requests",
        "~45M requests"
      ],
      "correct": 2,
      "explanation": "Initial: 10M × 3 = 30M. Failures: 30M × 5% = 1.5M. Retries (up to 3): ~1.5M × 3 = 4.5M extra. But some retries succeed early... Estimate: 30M + ~4.5M = 34.5M.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-071",
      "type": "multiple-choice",
      "question": "Audit log: 50M API calls/day. Each log entry: 512 bytes. Compliance requires 7-year retention with immutable storage. Storage after 7 years?",
      "options": ["~64 TB", "~640 TB", "~6.4 PB", "~64 PB"],
      "correct": 0,
      "explanation": "Daily: 50M × 512 = 25.6 GB. 7 years: 25.6 × 365 × 7 = 65.4 TB ≈ 64 TB.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-072",
      "type": "multiple-choice",
      "question": "Real-time analytics: 1B events/day ingested. Each event joins with 3 dimension tables (1M rows each). Query 95th percentile latency target: 100ms. Events/sec that meet SLA with proper indexing?",
      "options": [
        "Depends on query pattern and hardware",
        "~1,000 events/sec always achievable",
        "~10,000 events/sec with sharding",
        "~100,000 events/sec with columnar storage"
      ],
      "correct": 0,
      "explanation": "100ms p95 depends heavily on: index design, join strategy, data distribution, hardware specs, query complexity. Can't determine from given info alone. This tests recognizing when more context is needed.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-073",
      "type": "multiple-choice",
      "question": "Distributed cache: 50 nodes, each 64 GB RAM. Keys are 100 bytes, values average 1 KB. Replication factor 2. Max unique keys?",
      "options": ["~1.5B keys", "~3B keys", "~6B keys", "~12B keys"],
      "correct": 0,
      "explanation": "Total RAM: 50 × 64 GB = 3.2 TB. With replication 2: effective 1.6 TB. Per key: 100 + 1000 = 1.1 KB. Keys: 1.6 TB ÷ 1.1 KB = 1.45B keys.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-074",
      "type": "multiple-choice",
      "question": "Search ranking: 100M documents, rank top 1000 for each query. Scoring takes 1 μs per document. 10K QPS. CPU cores needed just for ranking?",
      "options": [
        "~100 cores",
        "~1,000 cores",
        "~10,000 cores",
        "~100,000 cores"
      ],
      "correct": 1,
      "explanation": "Per query: 100M × 1μs = 100 sec? No, that's if scoring all docs. With index, score top candidates. But question says 'rank top 1000' implying all docs considered... 100M × 1μs = 100 sec per query isn't feasible. Assuming index narrows to 100K candidates: 100K × 1μs = 0.1 sec. At 10K QPS: 1000 cores.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-075",
      "type": "multiple-choice",
      "question": "Billing system: 10M customers, usage aggregated hourly. Each aggregation reads 1000 usage records (100 bytes each). Monthly compute for aggregation?",
      "options": [
        "~7.2 TB data processed",
        "~72 TB data processed",
        "~720 TB data processed",
        "~7.2 PB data processed"
      ],
      "correct": 2,
      "explanation": "Per customer per hour: 1000 × 100 = 100 KB. Hourly total: 10M × 100 KB = 1 TB. Monthly: 1 TB × 24 × 30 = 720 TB processed.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Growth planning should show today vs horizon requirements side by side because compounding makes small percentages material quickly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-076",
      "type": "multiple-choice",
      "question": "Content delivery: Video on demand, 10M titles, each 2 GB average. 90-10 rule (10% of content gets 90% of views). Hot storage for 90% of views?",
      "options": ["~200 TB", "~2 PB", "~18 PB", "~20 PB"],
      "correct": 1,
      "explanation": "Hot content: 10% of 10M = 1M titles × 2 GB = 2 PB serves 90% of views.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-077",
      "type": "multiple-choice",
      "question": "Message queue: 1M messages/sec, 10 KB each. Consumers process at 100K messages/sec total. Queue depth after 1 hour if not scaled?",
      "options": [
        "~3.2B messages, ~32 TB",
        "~32M messages, ~320 GB",
        "~3.2M messages, ~32 GB",
        "Queue would be empty"
      ],
      "correct": 0,
      "explanation": "Net accumulation: (1M - 100K) × 3600 = 3.24B messages. Size: 3.24B × 10 KB = 32.4 TB.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "compound-078",
      "type": "multiple-choice",
      "question": "KV store: 99.9% read, 0.1% write. Total 100K ops/sec. Each read 1 KB, each write 10 KB. Network bandwidth?",
      "options": ["~100 MB/s", "~110 MB/s", "~200 MB/s", "~1 GB/s"],
      "correct": 0,
      "explanation": "Reads: 100K × 99.9% = 99.9K × 1 KB = 99.9 MB/s. Writes: 100K × 0.1% = 100 × 10 KB = 1 MB/s. Total: ~101 MB/s ≈ 100 MB/s.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-079",
      "type": "multiple-choice",
      "question": "AB test analysis: 100M users, 1000 metrics per user, analyze daily. Each metric comparison is 1 KB output. Daily report storage?",
      "options": ["~100 GB", "~1 TB", "~10 TB", "~100 TB"],
      "correct": 3,
      "explanation": "Comparisons: 100M × 1000 = 100B. Storage: 100B × 1 KB = 100 TB. (This is why you sample or pre-aggregate!)",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-080",
      "type": "multiple-choice",
      "question": "Service mesh: 100 services, each has 10 replicas (1000 total). Each replica connects to 5 downstream services (all 10 replicas each). Total outbound connections per replica?",
      "options": [
        "~5 connections",
        "~50 connections",
        "~500 connections",
        "~5,000 connections"
      ],
      "correct": 1,
      "explanation": "Each replica connects to: 5 downstream services × 10 replicas each = 50 outbound connections per replica.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-081",
      "type": "multiple-choice",
      "question": "Data replication: Primary in US, replica in EU. 10 GB/hour writes. RTT US-EU: 100ms. Synchronous replication latency overhead?",
      "options": [
        "~100ms per write",
        "~200ms per write (round trip)",
        "Depends on batch size",
        "Negligible with pipelining"
      ],
      "correct": 0,
      "explanation": "Synchronous replication waits for acknowledgment: 1 RTT = 100ms added to each write commit. (Async would be near-zero overhead but risks data loss.)",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "compound-082",
      "type": "multiple-choice",
      "question": "Ticket system: Concert with 50K seats. Sales open, 1M users ready. Each page load = 5 API calls. If each user refreshes 10 times in first minute, initial QPS?",
      "options": ["~83K QPS", "~417K QPS", "~833K QPS", "~8.3M QPS"],
      "correct": 2,
      "explanation": "Requests in first minute: 1M users × 10 refreshes × 5 calls = 50M. QPS: 50M ÷ 60 = 833K QPS.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-083",
      "type": "multiple-choice",
      "question": "Genomics: 1M patient genomes, 100 GB each. Query 1% of genome (1 GB) per analysis. 10K analyses/day. Daily data scanned?",
      "options": ["~1 TB", "~10 TB", "~100 TB", "~1 PB"],
      "correct": 1,
      "explanation": "Per analysis: 1 GB scanned. Daily: 10K × 1 GB = 10 TB.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-084",
      "type": "multiple-choice",
      "question": "Autocomplete: 1B queries/day. Each keystroke triggers suggestion (average 10 keystrokes per query). Suggestion service QPS?",
      "options": ["~11K QPS", "~116K QPS", "~1.16M QPS", "~11.6M QPS"],
      "correct": 1,
      "explanation": "Daily suggestions: 1B × 10 = 10B. QPS: 10B ÷ 86,400 = 115,740 ≈ 116K QPS.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-085",
      "type": "multiple-choice",
      "question": "Election system: 100M voters over 12-hour voting window. Each vote = 1 KB transaction, must be durably stored. Minimum write throughput?",
      "options": [
        "~2.3K writes/sec",
        "~23K writes/sec",
        "~230K writes/sec",
        "~2.3M writes/sec"
      ],
      "correct": 0,
      "explanation": "Votes: 100M ÷ (12 × 3600) = 2,315 writes/sec. (But need significant headroom for peak hours.)",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-086",
      "type": "multiple-choice",
      "question": "Network telemetry: 10K switches, each sends 1000 flow records/sec at 100 bytes each. Central collector ingests all. Ingestion rate?",
      "options": ["~1 GB/s", "~10 GB/s", "~100 GB/s", "~1 TB/s"],
      "correct": 0,
      "explanation": "Records/sec: 10K × 1000 = 10M. Bandwidth: 10M × 100 bytes = 1 GB/s.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-087",
      "type": "multiple-choice",
      "question": "Inventory system: 1M products across 1000 warehouses. Each product-warehouse combo (1B combinations) has quantity (8 bytes). Inventory snapshot size?",
      "options": ["~8 GB", "~80 GB", "~800 GB", "~8 TB"],
      "correct": 0,
      "explanation": "Combinations: 1M × 1000 = 1B. Storage: 1B × 8 bytes = 8 GB.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-088",
      "type": "multiple-choice",
      "question": "PDF generation: 10K reports/hour, each report 50 pages, 100 KB/page. Generation takes 500ms/page. Servers needed (sequential page generation)?",
      "options": [
        "~7 servers",
        "~70 servers",
        "~700 servers",
        "~7,000 servers"
      ],
      "correct": 1,
      "explanation": "Pages/hour: 10K × 50 = 500K. Time/hour: 500K × 0.5s = 250K sec = 69.4 hours of work/hour. Servers: 70.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-089",
      "type": "multiple-choice",
      "question": "Shopping cart: 50M active sessions, average 5 items per cart at 500 bytes per item. Session data with 30-min timeout. Memory for cart service?",
      "options": ["~125 GB", "~250 GB", "~500 GB", "~1 TB"],
      "correct": 0,
      "explanation": "Per session: 5 × 500 = 2,500 bytes. Total: 50M × 2.5 KB = 125 GB.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-090",
      "type": "multiple-choice",
      "question": "Notification fanout: Celebrity posts to 10M followers. Each notification 1 KB. Fanout via pub/sub with 3 regional replicas. Total data written?",
      "options": ["~10 GB", "~30 GB", "~100 GB", "~300 GB"],
      "correct": 1,
      "explanation": "Notifications: 10M × 1 KB = 10 GB. With 3 replicas: 30 GB total written.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-091",
      "type": "multiple-choice",
      "question": "Error budget: 99.9% SLO (43.8 min downtime/month allowed). Burn rate dashboard shows 2x budget burn. Time until SLO breach at this rate?",
      "options": [
        "~22 minutes downtime remaining",
        "~11 days until breach",
        "~15 days until breach",
        "Already breached"
      ],
      "correct": 2,
      "explanation": "At 2x burn rate, budget depletes in half the time: 30 days ÷ 2 = 15 days until budget exhausted.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-092",
      "type": "multiple-choice",
      "question": "Mobile sync: 100M users, sync 10 MB data each every 4 hours. 20% delta sync (2 MB). Server bandwidth for sync?",
      "options": ["~14 GB/s", "~140 GB/s", "~1.4 TB/s", "~14 TB/s"],
      "correct": 0,
      "explanation": "Syncs/sec: 100M ÷ (4 × 3600) = 6,944/sec. Bandwidth: 6,944 × 2 MB = 13.9 GB/s.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-093",
      "type": "multiple-choice",
      "question": "Leaderboard: 100M players, update score every game (average 10 games/day). Leaderboard shows top 100, recalculated on each query at 10K QPS. Storage for scores?",
      "options": ["~800 MB (user ID + score)", "~8 GB", "~80 GB", "~800 GB"],
      "correct": 0,
      "explanation": "Storage: 100M × 8 bytes (ID + score) = 800 MB. (Sorted set or heap structure.)",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "compound-094",
      "type": "multiple-choice",
      "question": "Data pipeline SLA: 99% of records processed within 5 minutes. 1M records/minute. At 99%, how many records can exceed 5 min per day?",
      "options": [
        "~14,400 records",
        "~144,000 records",
        "~1.44M records",
        "~14.4M records"
      ],
      "correct": 3,
      "explanation": "Daily records: 1M × 1440 = 1.44B. 1% can be slow: 1.44B × 1% = 14.4M records/day.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-095",
      "type": "multiple-choice",
      "question": "Container registry: Pull 500 images/sec, average 200 MB each. 70% layer cache hit. Registry egress bandwidth?",
      "options": ["~10 GB/s", "~30 GB/s", "~70 GB/s", "~100 GB/s"],
      "correct": 1,
      "explanation": "Cache miss: 30% of 200 MB = 60 MB per pull. Bandwidth: 500 × 60 MB = 30 GB/s.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "compound-096",
      "type": "multiple-choice",
      "question": "Compliance archival: 10 TB/day, keep 10 years. Storage cost: hot $0.02/GB/month, cold after 30 days at $0.004/GB/month. Year 1 storage cost?",
      "options": ["~$75K", "~$750K", "~$1.8M", "~$7.5M"],
      "correct": 2,
      "explanation": "Year 1 accumulation: 365 × 10 TB = 3.65 PB. Hot (30 days): 300 TB × $0.02 × 12 = $72K. Cold (growing): average ~1.8 PB × $0.004 × 12 = $86K? This is complex. Rough estimate: ~$1-2M range.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-097",
      "type": "multiple-choice",
      "question": "Distributed lock: 10K services acquire locks at 1K locks/sec total. Lock held 100ms average. Contention (locks held simultaneously)?",
      "options": [
        "~10 locks held",
        "~100 locks held",
        "~1,000 locks held",
        "~10,000 locks held"
      ],
      "correct": 1,
      "explanation": "Locks/sec × hold time = concurrent locks. 1000 locks/sec × 0.1 sec = 100 locks held simultaneously on average.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "compound-098",
      "type": "multiple-choice",
      "question": "Batch ETL: Source has 1 TB, transform reads each row 3 times (parse, validate, enrich), writes 1.5 TB output. I/O amplification factor?",
      "options": ["~3x", "~4.5x", "~5.5x", "~6x"],
      "correct": 1,
      "explanation": "Reads: 1 TB × 3 = 3 TB. Writes: 1.5 TB. Total I/O: 4.5 TB for 1 TB source = 4.5x amplification.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-099",
      "type": "multiple-choice",
      "question": "Multi-region active-active: 3 regions, each handles 100K writes/sec locally. Cross-region replication at 150ms RTT. Conflict resolution rate if 0.01% of writes conflict?",
      "options": [
        "~30 conflicts/sec",
        "~300 conflicts/sec",
        "~3,000 conflicts/sec",
        "~30,000 conflicts/sec"
      ],
      "correct": 0,
      "explanation": "Total writes: 300K/sec across 3 regions. Conflicts: 300K × 0.01% = 30 conflicts/sec.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "compound-100",
      "type": "multiple-choice",
      "question": "Full system: 10M DAU, each generates 100 events/day (1 KB each), makes 50 API calls (10 KB response avg), uploads 1 photo/week (2 MB). Daily storage growth and bandwidth?",
      "options": [
        "~1 TB storage/day, ~5 TB bandwidth/day",
        "~4 TB storage/day, ~8 TB bandwidth/day",
        "~1 TB storage/day, ~500 GB bandwidth/day",
        "~3 TB storage/day, ~5 TB bandwidth/day"
      ],
      "correct": 0,
      "explanation": "Events: 10M × 100 × 1 KB = 1 TB/day storage. API bandwidth: 10M × 50 × 10 KB = 5 TB/day egress. Photos: 10M ÷ 7 × 2 MB = 2.9 TB/day storage. Total storage: ~4 TB/day, bandwidth: ~5 TB/day.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-n001",
      "type": "numeric-input",
      "question": "E-commerce: 5M DAU, 50 page views/user/day, 20 KB/page. Daily bandwidth in TB?",
      "answer": 5,
      "unit": "TB",
      "tolerance": 0.1,
      "explanation": "5M × 50 × 20 KB = 5 billion KB = 5 TB.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-n002",
      "type": "numeric-input",
      "question": "Chat app: 10M users, 100 messages/user/day, 500 bytes/message. Daily storage in GB?",
      "answer": 500,
      "unit": "GB",
      "tolerance": 0.1,
      "explanation": "10M × 100 × 500 bytes = 500 billion bytes = 500 GB.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-n003",
      "type": "numeric-input",
      "question": "Video platform: 1M daily uploads, 100 MB average, 3 quality versions. Daily storage growth in TB?",
      "answer": 300,
      "unit": "TB",
      "tolerance": 0.1,
      "explanation": "1M × 100 MB × 3 = 300 million MB = 300 TB.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-n004",
      "type": "numeric-input",
      "question": "Social feed: 100M users, 10% active, check feed 20×/day, 50 posts/load. Daily post reads in billions?",
      "answer": 10,
      "unit": "billion",
      "tolerance": 0.1,
      "explanation": "100M × 0.1 × 20 × 50 = 10 billion post reads.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-n005",
      "type": "numeric-input",
      "question": "Ride-sharing: 1M rides/day, driver location updates every 5 seconds for 20-minute average ride. Daily location updates in billions?",
      "answer": 0.24,
      "unit": "billion",
      "tolerance": 0.15,
      "explanation": "1M rides × (20 min × 60 sec / 5 sec) = 1M × 240 = 240 million.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-n006",
      "type": "numeric-input",
      "question": "IoT platform: 10M devices, heartbeat every 30 seconds, 100 bytes each. Daily data in GB?",
      "answer": 288,
      "unit": "GB",
      "tolerance": 0.1,
      "explanation": "10M × (86400/30) × 100 bytes = 10M × 2880 × 100 = 2.88 trillion bytes = 288 GB.",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-n007",
      "type": "numeric-input",
      "question": "Search engine: 50K QPS, each query fans out to 100 shards. Total shard QPS in millions?",
      "answer": 5,
      "unit": "million",
      "tolerance": 0.1,
      "explanation": "50K × 100 = 5 million shard queries per second.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-n008",
      "type": "numeric-input",
      "question": "Email service: 100M users, 50 emails/user/day, 10 KB average, 90-day retention. Storage in PB?",
      "answer": 4.5,
      "unit": "PB",
      "tolerance": 0.15,
      "explanation": "100M × 50 × 10 KB × 90 = 4.5 quadrillion bytes = 4.5 PB.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-n009",
      "type": "numeric-input",
      "question": "Gaming: 1M concurrent players, 50 updates/second/player, 200 bytes each. Bandwidth in Gbps?",
      "answer": 80,
      "unit": "Gbps",
      "tolerance": 0.1,
      "explanation": "1M × 50 × 200 = 10 billion bytes/sec = 80 Gbps.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-n010",
      "type": "numeric-input",
      "question": "Analytics: 1B events/day, 500 bytes/event, 10× for indexes/aggregates. Total daily storage in TB?",
      "answer": 5,
      "unit": "TB",
      "tolerance": 0.15,
      "explanation": "1B × 500 × 10 = 5 trillion bytes = 5 TB.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-n011",
      "type": "numeric-input",
      "question": "CDN: 10M unique assets, 95% cache hit rate, 1M requests/second. Origin QPS?",
      "answer": 50000,
      "tolerance": 0.1,
      "explanation": "1M × 0.05 = 50,000 QPS to origin.",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "comp-n012",
      "type": "numeric-input",
      "question": "Payment system: 1000 TPS, each transaction writes to 3 databases. Database writes per second?",
      "answer": 3000,
      "tolerance": 0.1,
      "explanation": "1000 × 3 = 3000 database writes per second.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-n013",
      "type": "numeric-input",
      "question": "Photo sharing: 500M photos, 3 thumbnail sizes (10KB, 50KB, 200KB), plus 2MB original. Total storage in PB?",
      "answer": 1.13,
      "unit": "PB",
      "tolerance": 0.15,
      "explanation": "500M × (10KB + 50KB + 200KB + 2MB) = 500M × 2.26MB = 1.13 PB.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-n014",
      "type": "numeric-input",
      "question": "Notification service: 100M users, 5 notifications/day, 10% enabled push. Daily pushes in millions?",
      "answer": 50,
      "unit": "million",
      "tolerance": 0.1,
      "explanation": "100M × 5 × 0.1 = 50 million daily pushes.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-n015",
      "type": "numeric-input",
      "question": "Streaming: 1M concurrent viewers, 5 Mbps each, 40% peak:average ratio. Peak bandwidth in Tbps?",
      "answer": 5,
      "unit": "Tbps",
      "tolerance": 0.1,
      "explanation": "1M × 5 Mbps = 5 Tbps at this concurrency level.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "comp-n016",
      "type": "numeric-input",
      "question": "Logging: 1000 servers, 10 GB logs/server/day, 30-day retention, 50% compression. Storage in TB?",
      "answer": 150,
      "unit": "TB",
      "tolerance": 0.1,
      "explanation": "1000 × 10 GB × 30 × 0.5 = 150 TB.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-n017",
      "type": "numeric-input",
      "question": "API gateway: 100K QPS, 5KB request, 20KB response. Total bandwidth in Gbps?",
      "answer": 20,
      "unit": "Gbps",
      "tolerance": 0.1,
      "explanation": "100K × (5KB + 20KB) = 2.5 GB/s = 20 Gbps.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-n018",
      "type": "numeric-input",
      "question": "ML inference: 10K QPS, 100ms model latency, 50 concurrent models. GPU servers needed (each handles 500 QPS)?",
      "answer": 20,
      "tolerance": 0.1,
      "explanation": "10K QPS ÷ 500 QPS/server = 20 servers.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "comp-o001",
      "type": "ordering",
      "question": "Rank by daily storage growth (lowest to highest).",
      "items": [
        "Chat: 10M users, 100 msg/day, 500B each",
        "Photos: 1M uploads/day, 2MB each",
        "Video: 100K uploads/day, 500MB each",
        "Logs: 1000 servers, 10GB/day each"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Chat: 500GB < Photos: 2TB < Logs: 10TB < Video: 50TB.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-o002",
      "type": "ordering",
      "question": "Rank by QPS generated (lowest to highest).",
      "items": [
        "100K DAU × 10 req/day",
        "1M concurrent WebSockets × 1 msg/10s",
        "10M DAU × 5 req/day",
        "1K TPS payment system"
      ],
      "correctOrder": [0, 3, 2, 1],
      "explanation": "100K×10/86400=12 QPS < 1K TPS < 10M×5/86400=579 QPS < 1M/10=100K QPS.",
      "detailedExplanation": "Compare relative scale first, then confirm neighboring items pairwise to lock in the order. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-o003",
      "type": "ordering",
      "question": "Rank by bandwidth requirement (lowest to highest).",
      "items": [
        "Chat: 10K QPS × 1KB",
        "API: 10K QPS × 10KB",
        "Images: 1K QPS × 1MB",
        "Video: 100 streams × 5Mbps"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Chat: 10MB/s=80Mbps < API: 100MB/s=800Mbps < Video: 500Mbps < Images: 1GB/s=8Gbps.",
      "detailedExplanation": "Order by relative impact rather than exact values, then verify the sequence one boundary at a time. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "comp-o004",
      "type": "ordering",
      "question": "Rank these systems by infrastructure complexity (lowest to highest).",
      "items": [
        "Static website",
        "CRUD API",
        "Real-time multiplayer game",
        "Global social network"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Static (CDN) < CRUD (DB+API) < Real-time (low latency, state sync) < Global social (all of above at scale).",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "comp-o005",
      "type": "ordering",
      "question": "Rank by cost driver (lowest to highest cost impact for a video platform).",
      "items": [
        "Metadata storage",
        "Compute for encoding",
        "Video storage",
        "CDN bandwidth"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Metadata (tiny) < Encoding (one-time) < Storage (grows) < CDN bandwidth (continuous, high volume).",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-o006",
      "type": "ordering",
      "question": "Rank by number of servers typically needed (fewest to most) for 10M DAU.",
      "items": [
        "Cache (Redis)",
        "Search (Elasticsearch)",
        "Database (PostgreSQL)",
        "Web servers"
      ],
      "correctOrder": [0, 2, 3, 1],
      "explanation": "Redis: 2-5. PostgreSQL: 3-10. Web: 10-50. Elasticsearch: 10-100 (many shards).",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-o007",
      "type": "ordering",
      "question": "Rank by data model complexity (simplest to most complex).",
      "items": [
        "Key-value cache",
        "User profiles",
        "Social graph",
        "Activity feed"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "KV (key→value) < Profiles (structured doc) < Graph (relationships) < Feed (graph + time + ranking).",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-o008",
      "type": "ordering",
      "question": "Rank by latency sensitivity (least to most sensitive).",
      "items": [
        "Batch analytics",
        "Email delivery",
        "Web page load",
        "Real-time trading"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Batch (hours ok) < Email (minutes) < Web (seconds) < Trading (microseconds).",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-m001",
      "type": "multi-select",
      "question": "For a 10M DAU social app, which are reasonable resource estimates?",
      "options": [
        "10 TB database",
        "100 QPS average",
        "1 PB total storage (with media)",
        "20 web servers"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "10 TB DB, 1 PB media storage, 20 servers reasonable. 100 QPS too low (should be ~1K-10K).",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "comp-m002",
      "type": "multi-select",
      "question": "Which significantly increase infrastructure costs?",
      "options": [
        "User-uploaded video",
        "Global low-latency requirement",
        "Real-time features",
        "Text-only content"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Video (storage + CDN), global (multi-region), real-time (more servers) all expensive. Text is cheap.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "comp-m003",
      "type": "multi-select",
      "question": "A messaging app has 50M MAU, 1M concurrent, 100 msg/user/day. Which are true?",
      "options": [
        "~60K messages/second average",
        "~500GB daily storage (at 100B/msg)",
        "~10Gbps bandwidth for delivery",
        "~100 servers for handling connections"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "50M×100/86400≈58K msg/s ✓. 5B×100B=500GB ✓. 58K×100B≈6MB/s=48Mbps ✗. 1M/10K per server≈100 ✓.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-m004",
      "type": "multi-select",
      "question": "For ride-sharing with 100K daily rides, which estimates are reasonable?",
      "options": [
        "~1 GPS update/second during rides",
        "~100GB daily location data",
        "~1K QPS for matching",
        "~10 servers for the platform"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "1 GPS/sec reasonable. 100K×1200updates×100B=12GB not 100GB. 1K QPS for peaks. 10 servers for small scale.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-m005",
      "type": "multi-select",
      "question": "E-commerce flash sale: normal 1K QPS, sale peaks at 100×. Which preparations help?",
      "options": [
        "Pre-warm caches",
        "Auto-scaling policies",
        "Queue for checkout",
        "Increase database replicas"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All help: pre-warm avoids cold start, auto-scale handles surge, queues smooth spikes, replicas handle reads.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-m006",
      "type": "multi-select",
      "question": "Video platform: 1M videos, 1B views/month. Which require significant resources?",
      "options": [
        "Thumbnail storage",
        "Video transcoding",
        "CDN bandwidth",
        "View count updates"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Thumbnails small. Transcoding is CPU-heavy. CDN bandwidth is huge cost. 1B writes/month = 400 QPS needs handling.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "comp-m007",
      "type": "multi-select",
      "question": "Analytics pipeline: 10B events/day. Which are true?",
      "options": [
        "~115K events/second",
        "~5TB raw data (at 500B/event)",
        "Need distributed processing",
        "Can use single PostgreSQL"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "10B/86400=115K/s ✓. 10B×500B=5TB ✓. Distributed needed ✓. Single PG can't handle this.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-m008",
      "type": "multi-select",
      "question": "Global chat with 100ms latency requirement. Which are necessary?",
      "options": [
        "Multi-region deployment",
        "Edge message routing",
        "Single central database",
        "WebSocket connections"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Multi-region and edge routing needed for latency. WebSockets for real-time. Central DB creates latency bottleneck.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-t001",
      "type": "two-stage",
      "stages": [
        {
          "question": "Photo app: 10M DAU, 5 photos/day uploaded, 2MB each. Daily upload volume?",
          "options": ["~10 TB", "~100 TB", "~1 PB", "~10 PB"],
          "correct": 1,
          "explanation": "10M × 5 × 2MB = 100 TB daily uploads.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        },
        {
          "question": "With 3 copies for redundancy and 90-day retention, total storage needed?",
          "options": ["~9 PB", "~27 PB", "~90 PB", "~270 PB"],
          "correct": 1,
          "explanation": "100 TB × 90 days × 3 copies = 27 PB.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "explanation": "Photo storage at scale requires accounting for retention and redundancy.",
      "detailedExplanation": "Think of this as staged reasoning: outcome one becomes input constraint for outcome two. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "comp-t002",
      "type": "two-stage",
      "stages": [
        {
          "question": "Streaming service: 10M subscribers, 2 hours/day average watch time, 5 Mbps bitrate. Peak concurrent viewers (10% watching)?",
          "options": ["~100K", "~1M", "~10M", "~100M"],
          "correct": 1,
          "explanation": "10M × 10% = 1M concurrent viewers at peak.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        },
        {
          "question": "Bandwidth needed for peak concurrency?",
          "options": ["~500 Gbps", "~5 Tbps", "~50 Tbps", "~500 Tbps"],
          "correct": 1,
          "explanation": "1M × 5 Mbps = 5,000 Gbps = 5 Tbps.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "explanation": "Streaming bandwidth scales with concurrent viewers.",
      "detailedExplanation": "Solve stage one, freeze its implications, then use them as hard bounds for stage two. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "comp-t003",
      "type": "two-stage",
      "stages": [
        {
          "question": "E-commerce: 1M orders/day, each order queries inventory (1 read), reserves stock (1 write), processes payment (1 write). Database writes/second?",
          "options": [
            "~12 writes/sec",
            "~23 writes/sec",
            "~115 writes/sec",
            "~230 writes/sec"
          ],
          "correct": 1,
          "explanation": "1M orders × 2 writes / 86400 sec = 23 writes/sec average.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        },
        {
          "question": "If peak is 10× average during flash sales, and DB handles 500 writes/sec, how many DB instances for peak?",
          "options": [
            "1 instance",
            "2 instances",
            "5 instances",
            "10 instances"
          ],
          "correct": 0,
          "explanation": "Peak = 230 writes/sec. 500 > 230, so 1 instance handles peak.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "explanation": "Write scaling requires understanding peak loads.",
      "detailedExplanation": "Handle this as a sequential constraint problem where each stage narrows the feasible design space. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "comp-t004",
      "type": "two-stage",
      "stages": [
        {
          "question": "Search engine: 10K QPS, each query searches 50 shards, each shard returns 100 results (1KB each) to be merged. Data transferred per query?",
          "options": ["~50 KB", "~500 KB", "~5 MB", "~50 MB"],
          "correct": 2,
          "explanation": "50 shards × 100 results × 1KB = 5 MB per query.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        },
        {
          "question": "Internal bandwidth needed for query fan-out/gather?",
          "options": ["~500 MB/s", "~5 GB/s", "~50 GB/s", "~500 GB/s"],
          "correct": 2,
          "explanation": "10K QPS × 5 MB = 50 GB/s internal bandwidth.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "explanation": "Search fan-out creates significant internal traffic.",
      "detailedExplanation": "This format tests handoff quality between steps: carry forward results and avoid context drift. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    }
  ]
}
