{
  "unit": 1,
  "unitTitle": "Estimation",
  "chapter": 8,
  "chapterTitle": "Compound Scenarios",
  "chapterDescription": "Multi-step estimation problems combining time, storage, bandwidth, QPS, and growth projections",
  "problems": [
    {
      "id": "compound-001",
      "type": "multiple-choice",
      "question": "Video platform: 10M DAU, each watches 30 min/day of 1080p video (5 Mbps). Peak concurrent viewers are 10% of DAU. Peak bandwidth needed?",
      "options": ["~500 Gbps", "~5 Tbps", "~50 Tbps", "~500 Tbps"],
      "correct": 1,
      "explanation": "Peak concurrent: 10M × 10% = 1M viewers. Bandwidth: 1M × 5 Mbps = 5,000,000 Mbps = 5 Tbps.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard bandwidth plans that omit overhead and burst behavior. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. If values like 10M and 30 min appear, convert them into one unit basis before comparison. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-002",
      "type": "multiple-choice",
      "question": "Chat app: 100M users, 50 messages/day average, 200 bytes/message. 90-day retention. Total storage?",
      "options": ["~9 TB", "~90 TB", "~900 TB", "~9 PB"],
      "correct": 1,
      "explanation": "Daily: 100M × 50 × 200 bytes = 1 TB/day. 90 days retention: 90 TB.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 100M and 50 in aligned units before deciding on an implementation approach. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-003",
      "type": "multiple-choice",
      "question": "E-commerce: 1M orders/day, each order triggers 5 downstream events, each event is 1 KB. Event processing must complete in 24 hours. Minimum throughput needed?",
      "options": [
        "~58 events/sec",
        "~580 events/sec",
        "~5,800 events/sec",
        "~58,000 events/sec"
      ],
      "correct": 0,
      "explanation": "Total events: 1M × 5 = 5M/day. Throughput: 5M ÷ 86,400 sec = 58 events/sec minimum. (Build in headroom for bursts.)",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that ignore delivery semantics or backpressure behavior. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 1M and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-004",
      "type": "multiple-choice",
      "question": "Photo sharing: 50M DAU upload 2 photos/day at 2 MB each. CDN serves each photo 100 times on average. Daily CDN egress?",
      "options": ["~2 PB", "~20 PB", "~200 PB", "~2 EB"],
      "correct": 1,
      "explanation": "Daily uploads: 50M × 2 = 100M photos × 2 MB = 200 TB. Total serves: 100M × 100 × 2 MB = 20 PB egress.",
      "detailedExplanation": "Generalize from photo sharing: 50M DAU upload 2 photos/day at 2 MB each to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the answer that correctly handles unit conversion and link-capacity limits. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. If values like 50M and 2 appear, convert them into one unit basis before comparison. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-005",
      "type": "multiple-choice",
      "question": "Ride-sharing: 10M rides/day, each ride has 100 GPS pings (50 bytes each). Keep 1 year of data. Annual storage?",
      "options": ["~18 TB", "~180 TB", "~1.8 PB", "~18 PB"],
      "correct": 0,
      "explanation": "Daily: 10M × 100 × 50 bytes = 50 GB/day. Annual: 50 GB × 365 = 18.25 TB ≈ 18 TB.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 10M and 100 in aligned units before deciding on an implementation approach. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-006",
      "type": "multiple-choice",
      "question": "Notification service: 100M users, 80% opt-in, send 3 notifications/day per user. Each notification = 500 bytes + 1 KB push payload. Daily bandwidth for push?",
      "options": ["~120 GB", "~360 GB", "~1.2 TB", "~3.6 TB"],
      "correct": 1,
      "explanation": "Recipients: 100M × 80% = 80M. Daily notifications: 80M × 3 = 240M. Payload: 240M × 1.5 KB = 360 GB.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. If values like 100M and 80 appear, convert them into one unit basis before comparison. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-007",
      "type": "multiple-choice",
      "question": "Search engine: 1B queries/day, each query searches 10B documents in 200ms. Index is 1% of document size. If documents total 100 PB, index size?",
      "options": ["~100 TB", "~1 PB", "~10 PB", "~100 PB"],
      "correct": 1,
      "explanation": "Index = 1% of 100 PB = 1 PB. (Query rate and latency don't affect index size.)",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard modeling choices that look clean but perform poorly for the target queries. Schema and index choices should follow access patterns and write/read amplification constraints. Keep quantities like 1B and 10B in aligned units before deciding on an implementation approach. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-008",
      "type": "multiple-choice",
      "question": "Gaming platform: 5M concurrent players, each sends 20 packets/sec at 100 bytes. Server processes and broadcasts to 50 nearby players. Server egress bandwidth?",
      "options": ["~500 Gbps", "~4 Tbps", "~40 Tbps", "~400 Tbps"],
      "correct": 1,
      "explanation": "Inbound: 5M × 20 × 100 bytes = 10 GB/sec. Egress (50x fan-out): 10 GB/s × 50 = 500 GB/s = 4 Tbps.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 5M and 20 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-009",
      "type": "multiple-choice",
      "question": "IoT platform: 10M devices reporting 1 KB every 10 seconds. Data retained 30 days. Writes/sec and storage?",
      "options": [
        "1M writes/sec, 2.6 PB storage",
        "100K writes/sec, 260 TB storage",
        "1M writes/sec, 260 TB storage",
        "100K writes/sec, 2.6 PB storage"
      ],
      "correct": 0,
      "explanation": "Writes: 10M ÷ 10 sec = 1M writes/sec. Daily: 1M writes/sec × 86,400 sec × 1 KB = 86.4 TB/day. 30 days: 86.4 × 30 = 2.6 PB.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 10M and 1 KB appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-010",
      "type": "multiple-choice",
      "question": "Ad platform: 10B ad impressions/day, each impression logged at 500 bytes. Logs compressed 5x. 7-day retention. Storage needed?",
      "options": ["~7 TB", "~70 TB", "~700 TB", "~7 PB"],
      "correct": 0,
      "explanation": "Daily raw: 10B × 500 bytes = 5 TB. Compressed: 5 TB ÷ 5 = 1 TB/day. 7 days: 7 TB.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 10B and 500 should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-011",
      "type": "multiple-choice",
      "question": "Social network: 500M users, average 200 friends each. Store friend relationships. Each edge = 16 bytes (two 8-byte IDs). Total storage for friend graph?",
      "options": ["~800 GB", "~1.6 TB", "~8 TB", "~16 TB"],
      "correct": 0,
      "explanation": "Edges: 500M × 200 ÷ 2 (bidirectional) = 50B edges. Storage: 50B × 16 bytes = 800 GB.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. If values like 500M and 200 appear, convert them into one unit basis before comparison. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-012",
      "type": "multiple-choice",
      "question": "Streaming music: 50M subscribers, each streams 2 hours/day at 320 kbps. 80% cache hit at CDN. Daily origin bandwidth?",
      "options": ["~1.4 PB", "~2.9 PB", "~11.5 PB", "~14.4 PB"],
      "correct": 1,
      "explanation": "Total streaming: 50M × 2 hr × 320 kbps = 50M × 288 MB = 14.4 PB/day. Cache miss (20%): 14.4 × 0.2 = 2.88 PB to origin.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 50M and 2 hours appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-013",
      "type": "multiple-choice",
      "question": "URL shortener: 100M new URLs/month, each URL accessed 10 times/month average on month of creation, then rarely. Peak read QPS (assume uniform distribution)?",
      "options": ["~40 QPS", "~400 QPS", "~4,000 QPS", "~40,000 QPS"],
      "correct": 2,
      "explanation": "Monthly reads: 100M × 10 = 1B. QPS: 1B ÷ (30 × 86,400) = 1B ÷ 2.6M = ~385 QPS average. Peak is typically 3-10x average, so ~4,000 QPS.",
      "detailedExplanation": "Generalize from uRL shortener: 100M new URLs/month, each URL accessed 10 times/month average on month to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 100M and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-014",
      "type": "multiple-choice",
      "question": "Email service: 1B emails/day, average 50 KB each, 30% have 1 MB attachment average. Total daily ingest?",
      "options": ["~50 TB", "~350 TB", "~500 TB", "~850 TB"],
      "correct": 1,
      "explanation": "Body: 1B × 50 KB = 50 TB. Attachments: 1B × 30% × 1 MB = 300 TB. Total: 350 TB/day.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 1B and 50 KB in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-015",
      "type": "multiple-choice",
      "question": "Video conferencing: 10M concurrent calls, each has 4 participants sending 2 Mbps video. Server receives and re-broadcasts to 3 others per participant. Server throughput?",
      "options": [
        "~80 Tbps in, ~240 Tbps out",
        "~80 Tbps in, ~80 Tbps out",
        "~20 Tbps in, ~60 Tbps out",
        "~240 Tbps total (in + out)"
      ],
      "correct": 0,
      "explanation": "Participants: 10M × 4 = 40M. Inbound: 40M × 2 Mbps = 80 Tbps. Outbound per participant: 2 Mbps × 3 = 6 Mbps. Total out: 40M × 6 Mbps = 240 Tbps.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 10M and 4 should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-016",
      "type": "multiple-choice",
      "question": "Log aggregation: 1000 servers, each produces 100 MB logs/hour. Ship to central store, compress 10x, retain 90 days. Storage needed?",
      "options": ["~2.2 TB", "~22 TB", "~216 TB", "~2.16 PB"],
      "correct": 1,
      "explanation": "Daily raw: 1000 × 100 MB × 24 = 2.4 TB. Compressed (÷10): 240 GB/day. 90 days: 21.6 TB ≈ 22 TB.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 1000 and 100 MB should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-017",
      "type": "multiple-choice",
      "question": "Food delivery: 5M orders/day. Each order requires: 1 user lookup, 5 restaurant menu reads, 3 driver location updates. Database read capacity needed?",
      "options": [
        "~52 reads/sec",
        "~520 reads/sec",
        "~5,200 reads/sec",
        "~52,000 reads/sec"
      ],
      "correct": 1,
      "explanation": "Reads per order: 1 + 5 + 3 = 9. Daily reads: 5M × 9 = 45M. QPS: 45M ÷ 86,400 = 520 reads/sec.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 5M and 1 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-018",
      "type": "multiple-choice",
      "question": "Analytics pipeline: 100M events/hour, each 1 KB. Process in Spark with 3x data expansion during joins. Peak memory across cluster?",
      "options": ["~100 GB", "~300 GB", "~1 TB", "~3 TB"],
      "correct": 1,
      "explanation": "Hourly data: 100M × 1 KB = 100 GB. With 3x expansion: 300 GB minimum memory needed to hold working set.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 100M and 1 KB should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-019",
      "type": "multiple-choice",
      "question": "News feed: 100M DAU, each sees 100 posts/day, each post averages 10 KB (text + thumbnail). Feed is personalized (no caching). Daily bandwidth to users?",
      "options": ["~10 TB", "~100 TB", "~1 PB", "~10 PB"],
      "correct": 1,
      "explanation": "Feed data: 100M × 100 × 10 KB = 100 TB/day to users.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that ignore sustained peak transfer constraints. Treat network capacity as a steady-state constraint, then test against peak windows. Numbers such as 100M and 100 should be normalized first so downstream reasoning stays consistent. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-020",
      "type": "multiple-choice",
      "question": "Payment processor: 10,000 TPS, each transaction = 2 DB writes (debit + credit). Database can handle 50,000 writes/sec. Utilization?",
      "options": ["~20%", "~40%", "~80%", "~200% (need to scale)"],
      "correct": 1,
      "explanation": "DB writes: 10,000 × 2 = 20,000 writes/sec. Utilization: 20,000 ÷ 50,000 = 40%.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 10,000 and 2 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-021",
      "type": "multiple-choice",
      "question": "Recommendation engine: 50M users, precompute top 100 recs per user daily. Each rec = 8 bytes (item ID). Storage for all recommendations?",
      "options": ["~4 GB", "~40 GB", "~400 GB", "~4 TB"],
      "correct": 1,
      "explanation": "Total: 50M × 100 × 8 bytes = 40B bytes = 40 GB.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 50M and 100 in aligned units before deciding on an implementation approach. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-022",
      "type": "multiple-choice",
      "question": "Map tiles: World at zoom 18 = 69B tiles. Each tile 20 KB. With 5x versioning for updates. Total storage?",
      "options": ["~1.4 PB", "~6.9 PB", "~34.5 PB", "~69 PB"],
      "correct": 1,
      "explanation": "Base: 69B × 20 KB = 1.38 PB. With 5 versions: 1.38 × 5 = 6.9 PB.",
      "detailedExplanation": "Generalize from map tiles: World at zoom 18 = 69B tiles to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 18 and 69B in aligned units before deciding on an implementation approach. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-023",
      "type": "multiple-choice",
      "question": "CDN design: 1 PB content library. 80% of requests hit 1% of content. Size hot tier to serve 80% of requests from memory?",
      "options": ["~1 TB", "~10 TB", "~100 TB", "~500 TB"],
      "correct": 1,
      "explanation": "Hot content: 1% of 1 PB = 10 TB. This serves 80% of requests. (Pareto principle in action.)",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that ignore sustained peak transfer constraints. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Keep quantities like 1 and 80 in aligned units before deciding on an implementation approach. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-024",
      "type": "multiple-choice",
      "question": "Taxi dispatch: City has 50,000 taxis. Each updates location every 5 seconds (100 bytes). Match requests at 1000 rides/min. Location write throughput?",
      "options": [
        "~1,000 writes/sec",
        "~10,000 writes/sec",
        "~100,000 writes/sec",
        "~1,000,000 writes/sec"
      ],
      "correct": 1,
      "explanation": "Location writes: 50,000 taxis ÷ 5 sec = 10,000 writes/sec.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. If values like 50,000 and 5 seconds appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-025",
      "type": "multiple-choice",
      "question": "ML training: 1TB dataset, train for 100 epochs. Each epoch reads full dataset. Disk throughput at 500 MB/s. Minimum read time?",
      "options": ["~2 hours", "~6 hours", "~56 hours", "~200 hours"],
      "correct": 2,
      "explanation": "Total reads: 1 TB × 100 = 100 TB. Time: 100 TB ÷ 500 MB/s = 200,000 sec = 55.6 hours.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. Keep quantities like 1TB and 100 in aligned units before deciding on an implementation approach. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-026",
      "type": "multiple-choice",
      "question": "Web crawler: Crawl 1B pages/day, each page averages 100 KB. Parse and extract 5 KB of text per page. Raw vs processed storage ratio?",
      "options": [
        "100 TB raw, 5 TB processed (20:1)",
        "100 TB raw, 500 GB processed (200:1)",
        "10 PB raw, 500 TB processed (20:1)",
        "1 PB raw, 50 TB processed (20:1)"
      ],
      "correct": 0,
      "explanation": "Raw: 1B × 100 KB = 100 TB. Processed: 1B × 5 KB = 5 TB. Ratio: 20:1.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard storage decisions that optimize one dimension while violating another core requirement. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Numbers such as 1B and 100 KB should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-027",
      "type": "multiple-choice",
      "question": "Stock exchange: 1M trades/day, each creates 5 events (order, match, confirm, settle, report). Events processed by 10 downstream systems. Daily message count?",
      "options": [
        "~5M messages",
        "~50M messages",
        "~500M messages",
        "~5B messages"
      ],
      "correct": 1,
      "explanation": "Events: 1M × 5 = 5M. Fan-out to 10 systems: 5M × 10 = 50M messages.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 1M and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-028",
      "type": "multiple-choice",
      "question": "Healthcare records: 100M patients, average 500 KB record, updated 5 times/year. Keep all versions forever. After 10 years, storage?",
      "options": ["~50 TB", "~250 TB", "~2.5 PB", "~25 PB"],
      "correct": 2,
      "explanation": "Base records: 100M × 500 KB = 50 TB. Updates: 100M × 5 × 500 KB × 10 years = 2.5 PB (assuming full record stored per version). Total: ~2.5 PB.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Keep quantities like 100M and 500 KB in aligned units before deciding on an implementation approach. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-029",
      "type": "multiple-choice",
      "question": "API gateway: 100K RPM, average response 10 KB. Log full request/response for debugging. 24-hour log retention. Storage?",
      "options": ["~14 GB", "~144 GB", "~1.4 TB", "~14 TB"],
      "correct": 2,
      "explanation": "Daily requests: 100K × 60 × 24 = 144M. Storage: 144M × 10 KB = 1.44 TB.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 100K and 10 KB should be normalized first so downstream reasoning stays consistent. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-030",
      "type": "multiple-choice",
      "question": "Backup system: 100 TB database, daily incremental 1%, weekly full. Monthly storage for backups?",
      "options": ["~430 TB", "~500 TB", "~700 TB", "~1 PB"],
      "correct": 0,
      "explanation": "Weekly fulls: 4 × 100 TB = 400 TB. Daily incrementals: ~28 × 1 TB = 28 TB. Total: ~428 TB.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 100 TB and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-031",
      "type": "multiple-choice",
      "question": "Real-time bidding: 1M ad requests/sec, each request sent to 50 bidders, 10ms timeout. Bidder response at 1 KB. Aggregate inbound bandwidth from bidders?",
      "options": ["~50 GB/s", "~500 GB/s", "~5 TB/s", "~50 TB/s"],
      "correct": 0,
      "explanation": "Responses: 1M × 50 = 50M responses/sec. Bandwidth: 50M × 1 KB = 50 GB/s inbound.",
      "detailedExplanation": "Generalize from real-time bidding: 1M ad requests/sec, each request sent to 50 bidders, 10ms timeout to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 1M and 50 in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-032",
      "type": "multiple-choice",
      "question": "Code repository: 10M repos, average 100 MB each. Each repo cloned 10 times/day on average. Daily egress?",
      "options": ["~1 PB", "~10 PB", "~100 PB", "~1 EB"],
      "correct": 1,
      "explanation": "Daily clones: 10M × 10 = 100M. Egress: 100M × 100 MB = 10 PB.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer the answer that correctly handles unit conversion and link-capacity limits. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Numbers such as 10M and 100 MB should be normalized first so downstream reasoning stays consistent. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-033",
      "type": "multiple-choice",
      "question": "Sensor network: 1M sensors, 10 readings/min, 100 bytes each. Keep raw data for 24 hours. Raw storage needed?",
      "options": ["~144 GB", "~1.44 TB", "~14.4 TB", "~144 TB"],
      "correct": 1,
      "explanation": "Per hour: 1M × 10 × 60 × 100 bytes = 60 GB. 24 hours: 60 × 24 = 1.44 TB raw storage.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 1M and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-034",
      "type": "multiple-choice",
      "question": "E-commerce search: 10M products, 1 KB metadata each. Index takes 3x raw size. Update 10% of products daily. Daily index rebuild time at 100 MB/s write?",
      "options": ["~5 minutes", "~50 minutes", "~5 hours", "~50 hours"],
      "correct": 0,
      "explanation": "Full index: 10M × 1 KB × 3 = 30 GB. Write time: 30 GB ÷ 100 MB/s = 300 sec = 5 minutes. (Incremental updates would be faster.)",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that conflict with the primary access pattern or index strategy. Schema and index choices should follow access patterns and write/read amplification constraints. Keep quantities like 10M and 1 KB in aligned units before deciding on an implementation approach. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-035",
      "type": "multiple-choice",
      "question": "Social media: Post goes viral - 10M views in 1 hour. Video is 50 MB. With 90% CDN cache hit, origin serves?",
      "options": [
        "~50 TB from origin",
        "~5 TB from origin",
        "~500 GB from origin",
        "~50 GB from origin"
      ],
      "correct": 0,
      "explanation": "Cache miss: 10% of 10M = 1M requests to origin. Origin egress: 1M × 50 MB = 50 TB. (This is why CDN cache hit rate matters!)",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 10M and 1 hour should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-036",
      "type": "multiple-choice",
      "question": "Auth service: 100M users, each generates 10 sessions/month, session token = 64 bytes. Sessions expire in 30 days. Max concurrent session storage?",
      "options": ["~64 GB", "~640 GB", "~6.4 TB", "~64 TB"],
      "correct": 0,
      "explanation": "Sessions/month: 100M × 10 = 1B. With 30-day expiry ≈ 1 month of sessions active: 1B × 64 bytes = 64 GB.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard storage decisions that optimize one dimension while violating another core requirement. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Keep quantities like 100M and 10 in aligned units before deciding on an implementation approach. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-037",
      "type": "multiple-choice",
      "question": "Monitoring system: 10,000 hosts, 100 metrics each, 15-sec scrape interval. Time-series DB at 8 bytes/point. Daily storage?",
      "options": ["~46 GB", "~460 GB", "~4.6 TB", "~46 TB"],
      "correct": 0,
      "explanation": "Points/day: 10K × 100 metrics × 5,760 points/day = 5.76B points. Storage: 5.76B × 8 bytes = 46 GB.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Numbers such as 10,000 and 100 should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-038",
      "type": "multiple-choice",
      "question": "Document search: 100M documents, average 10 KB. Build inverted index with 5000 unique terms per doc, 12 bytes per posting. Index size?",
      "options": ["~60 TB", "~6 TB", "~600 GB", "~60 GB"],
      "correct": 1,
      "explanation": "Postings: 100M × 5000 = 500B. Size: 500B × 12 bytes = 6 TB.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. If values like 100M and 10 KB appear, convert them into one unit basis before comparison. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-039",
      "type": "multiple-choice",
      "question": "Fleet management: 100K vehicles, GPS ping every 30 sec. Calculate nearest 10 vehicles to rider in 100ms. With 1M ride requests/day, daily distance calculations?",
      "options": [
        "~100M calculations",
        "~1B calculations",
        "~10B calculations",
        "~100B calculations"
      ],
      "correct": 3,
      "explanation": "Each request compares against 100K vehicles = 100K distance calcs. Daily: 1M × 100K = 100B calculations. (This is why spatial indexing matters.)",
      "detailedExplanation": "Generalize from fleet management: 100K vehicles, GPS ping every 30 sec to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 100K and 30 sec should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-040",
      "type": "multiple-choice",
      "question": "Video upload: User uploads 1 hour 4K video (50 GB). Transcode to 5 resolutions. Each resolution takes 2x real-time on 1 CPU. Total CPU-hours?",
      "options": [
        "~5 CPU-hours",
        "~10 CPU-hours",
        "~50 CPU-hours",
        "~100 CPU-hours"
      ],
      "correct": 1,
      "explanation": "Per resolution: 1 hour × 2 = 2 CPU-hours. 5 resolutions: 10 CPU-hours total.",
      "detailedExplanation": "Generalize from video upload: User uploads 1 hour 4K video (50 GB) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 1 hour and 4K in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-041",
      "type": "multiple-choice",
      "question": "Multiplayer game: 100 players in a match, 60 tick rate, 50 bytes game state per player. Server broadcasts full state to all players. Bandwidth per match?",
      "options": ["~30 MB/s", "~300 MB/s", "~3 GB/s", "~30 GB/s"],
      "correct": 0,
      "explanation": "State size: 100 × 50 = 5 KB. Broadcasts/sec: 60. To 100 players: 60 × 5 KB × 100 = 30 MB/s per match.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 100 and 60 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-042",
      "type": "multiple-choice",
      "question": "Data warehouse: 10 TB daily ingest. Queries scan 30% of data on average. 1000 queries/day. Each query gets dedicated throughput at 1 GB/s. Daily query time?",
      "options": ["~50 minutes", "~8 hours", "~83 hours", "~833 hours"],
      "correct": 2,
      "explanation": "Scan per query: 10 TB × 30% = 3 TB. Time per query: 3 TB ÷ 1 GB/s = 3000 sec = 50 min. Daily: 1000 × 50 min = 833 hours. But queries can run in parallel...",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 10 TB and 30 appear, convert them into one unit basis before comparison. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-043",
      "type": "multiple-choice",
      "question": "Image recognition API: 10M images/day, each 500 KB. Pre-process (resize) to 100 KB, then inference at 50ms/image on GPU. Daily GPU-hours?",
      "options": [
        "~14 GPU-hours",
        "~139 GPU-hours",
        "~1,390 GPU-hours",
        "~13,900 GPU-hours"
      ],
      "correct": 1,
      "explanation": "Inference time: 10M × 50ms = 500M ms = 500,000 sec = 139 GPU-hours.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. If values like 10M and 500 KB appear, convert them into one unit basis before comparison. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-044",
      "type": "multiple-choice",
      "question": "Content moderation: 50M posts/day, 10% flagged for review, each review takes ML model 200ms. GPU capacity needed to process same-day?",
      "options": ["~12 GPUs", "~29 GPUs", "~116 GPUs", "~290 GPUs"],
      "correct": 0,
      "explanation": "Reviews: 50M × 10% = 5M. Total GPU time: 5M × 200ms = 1M seconds. GPUs needed: 1M sec ÷ 86,400 sec/day = 11.6 ≈ 12 GPUs.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 50M and 10 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-045",
      "type": "multiple-choice",
      "question": "Blockchain node: 1 MB block every 10 min, store full history (700K blocks). Each block distributed to 10K nodes. Monthly bandwidth for new blocks globally?",
      "options": ["~4 TB", "~40 TB", "~400 TB", "~4 PB"],
      "correct": 1,
      "explanation": "Blocks/month: 6/hour × 24 × 30 = 4,320 blocks = 4.32 GB. Distributed to 10K nodes: 4.32 GB × 10K = 43.2 TB ≈ 40 TB global bandwidth.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard bandwidth plans that omit overhead and burst behavior. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Numbers such as 1 MB and 10 min should be normalized first so downstream reasoning stays consistent. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-046",
      "type": "multiple-choice",
      "question": "Job scheduler: 1M jobs/day, average 5 minutes runtime. Jobs distributed across 1000-server cluster. Average cluster utilization?",
      "options": ["~35%", "~70%", "~140%", "~350%"],
      "correct": 3,
      "explanation": "Work needed: 1M × 5 min = 5M server-minutes/day. Capacity: 1000 servers × 1440 min/day = 1.44M server-minutes. Utilization: 5M ÷ 1.44M = 347% - cluster is overloaded and needs more servers!",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 1M and 5 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-047",
      "type": "multiple-choice",
      "question": "DNS service: 1M domains, each has 5 records averaging 100 bytes. 10B queries/day, 99% cache hit at resolver. Authoritative server query load?",
      "options": ["~1,200 QPS", "~12,000 QPS", "~120,000 QPS", "~1.2M QPS"],
      "correct": 0,
      "explanation": "Cache miss: 1% of 10B = 100M queries/day to authoritative. QPS: 100M ÷ 86,400 = 1,157 ≈ 1,200 QPS.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 1M and 5 in aligned units before deciding on an implementation approach. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-048",
      "type": "multiple-choice",
      "question": "Feature store: 10M users × 500 features each, 8 bytes per feature. Refresh features every hour. Hourly write throughput?",
      "options": ["~40 GB/hour", "~400 GB/hour", "~4 TB/hour", "~40 TB/hour"],
      "correct": 0,
      "explanation": "Data: 10M × 500 × 8 = 40 GB. Hourly refresh: 40 GB/hour write throughput.",
      "detailedExplanation": "Generalize from feature store: 10M users × 500 features each, 8 bytes per feature to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 10M and 500 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-049",
      "type": "multiple-choice",
      "question": "Live streaming: 10K concurrent streamers at 5 Mbps each. Average 1000 viewers per stream. Total egress bandwidth?",
      "options": ["~50 Gbps", "~500 Gbps", "~5 Tbps", "~50 Tbps"],
      "correct": 3,
      "explanation": "Total viewers: 10K × 1000 = 10M. Egress: 10M × 5 Mbps = 50 Tbps.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 10K and 5 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-050",
      "type": "multiple-choice",
      "question": "Fraud detection: 1M transactions/hour. ML model runs 5ms per transaction. Flag 0.1% as suspicious for human review. If reviewer handles 100 cases/day, reviewers needed?",
      "options": [
        "~10 reviewers",
        "~24 reviewers",
        "~100 reviewers",
        "~240 reviewers"
      ],
      "correct": 3,
      "explanation": "Daily transactions: 24M. Suspicious: 24M × 0.1% = 24,000 cases. Reviewers: 24,000 ÷ 100 = 240.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 1M and 5ms should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-051",
      "type": "multiple-choice",
      "question": "Geospatial database: 1B points of interest, each 200 bytes + 16 bytes coordinates. Spatial index adds 50% overhead. Total storage?",
      "options": ["~216 GB", "~324 GB", "~432 GB", "~648 GB"],
      "correct": 1,
      "explanation": "Raw: 1B × (200 + 16) = 216 GB. With index (+50%): 216 × 1.5 = 324 GB.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard modeling choices that look clean but perform poorly for the target queries. Schema and index choices should follow access patterns and write/read amplification constraints. Numbers such as 1B and 200 should be normalized first so downstream reasoning stays consistent. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-052",
      "type": "multiple-choice",
      "question": "Video transcoding farm: 10,000 hours of video uploaded/day. Each hour transcoded to 6 formats, each takes 30 min on one core. Servers needed (32-core each)?",
      "options": [
        "~40 servers",
        "~125 servers",
        "~400 servers",
        "~1,250 servers"
      ],
      "correct": 1,
      "explanation": "Work: 10K hours × 6 × 0.5 hours = 30K core-hours/day. Daily capacity per server: 32 × 24 = 768 core-hours. Servers: 30K ÷ 768 = 39 servers minimum. With headroom: ~40-125.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 10,000 hours and 6 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-053",
      "type": "multiple-choice",
      "question": "Chat history search: 1B messages, average 100 bytes. Full-text index is 2x message size. Search query scans 0.1% of index. Single-threaded query throughput at 1 GB/s disk?",
      "options": [
        "~5 queries/sec",
        "~50 queries/sec",
        "~500 queries/sec",
        "~5,000 queries/sec"
      ],
      "correct": 0,
      "explanation": "Messages: 1B × 100 bytes = 100 GB. Index: 200 GB. Query scan: 200 GB × 0.1% = 200 MB per query. At 1 GB/s: 1000 MB ÷ 200 MB = 5 queries/sec.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer the schema/index decision that minimizes query and write amplification for this workload. Choose data shape based on workload paths, not on normalization dogma alone. Keep quantities like 1B and 100 in aligned units before deciding on an implementation approach. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-054",
      "type": "multiple-choice",
      "question": "Smart home: 50M homes, 10 devices each, device sends 1 event/minute, 200 bytes per event. Daily events and storage?",
      "options": [
        "~720B events, ~144 TB",
        "~72B events, ~14.4 TB",
        "~7.2B events, ~1.44 TB",
        "~720M events, ~144 GB"
      ],
      "correct": 0,
      "explanation": "Devices: 50M × 10 = 500M. Daily events: 500M × 1440 = 720B. Storage: 720B × 200 bytes = 144 TB.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 50M and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-055",
      "type": "multiple-choice",
      "question": "A/B testing: 100M users, 20 concurrent experiments, each user in 5 experiments. Record assignment (32 bytes) and 10 events (50 bytes each) per user per experiment. Daily storage?",
      "options": ["~2.9 GB", "~29 GB", "~290 GB", "~2.9 TB"],
      "correct": 2,
      "explanation": "Per user: 5 experiments × (32 + 10 × 50) = 5 × 532 = 2,660 bytes. Total: 100M × 2,660 = 266 GB ≈ 290 GB.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. If values like 100M and 20 appear, convert them into one unit basis before comparison. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-056",
      "type": "multiple-choice",
      "question": "Email delivery: 1B emails/day. Each email attempt: DNS lookup (1ms), connection (50ms), send (100ms average). Sequential processing throughput per server?",
      "options": [
        "~6.6 emails/sec",
        "~66 emails/sec",
        "~660 emails/sec",
        "~6,600 emails/sec"
      ],
      "correct": 0,
      "explanation": "Time per email: 1 + 50 + 100 = 151ms. Per server: 1000 ÷ 151 = 6.6 emails/sec. (This is why connection pooling and pipelining matter.)",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 1B and 1ms appear, convert them into one unit basis before comparison. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-057",
      "type": "multiple-choice",
      "question": "Database migration: 10 TB source, replicate at 100 MB/s, while handling 1000 writes/sec (1 KB each). Time to initial sync?",
      "options": ["~28 hours", "~56 hours", "~112 hours", "~224 hours"],
      "correct": 0,
      "explanation": "Sync time: 10 TB ÷ 100 MB/s = 100,000 sec = 27.8 hours. New writes during sync: 1000 × 1 KB × 100,000 = 100 GB (negligible vs 10 TB). Total: ~28 hours.",
      "detailedExplanation": "Generalize from database migration: 10 TB source, replicate at 100 MB/s, while handling 1000 writes/sec to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that ignore sustained peak transfer constraints. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Numbers such as 10 TB and 100 MB should be normalized first so downstream reasoning stays consistent. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-058",
      "type": "multiple-choice",
      "question": "Rate limiter: 10M users, each gets 100 requests/minute limit. Store counter per user (16 bytes). With 1-minute sliding window (60 buckets), memory needed?",
      "options": ["~160 MB", "~1.6 GB", "~9.6 GB", "~96 GB"],
      "correct": 2,
      "explanation": "Per user: 60 buckets × 16 bytes = 960 bytes. Total: 10M × 960 = 9.6 GB.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 10M and 100 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-059",
      "type": "multiple-choice",
      "question": "Global load balancer: 100 regions, health check each region every 5 sec. Update DNS with 30-sec TTL. If region fails, max time until traffic rerouted?",
      "options": ["~5 seconds", "~35 seconds", "~65 seconds", "~95 seconds"],
      "correct": 1,
      "explanation": "Worst case: health check just passed, failure happens immediately after, next check in 5 sec detects failure, DNS updated, clients wait up to 30 sec TTL. Total: 5 + 30 = 35 seconds max.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 100 and 5 sec in aligned units before deciding on an implementation approach. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-060",
      "type": "multiple-choice",
      "question": "Object storage: 10B objects, 500 KB average. Lifecycle rule moves objects older than 30 days to cold storage. Cold storage size (ignoring new objects)?",
      "options": ["~500 TB", "~5 PB", "~50 PB", "~500 PB"],
      "correct": 1,
      "explanation": "10B objects × 500 KB = 5 PB total. Most objects (older than 30 days) are in cold storage ≈ 5 PB.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 10B and 500 KB appear, convert them into one unit basis before comparison. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-061",
      "type": "multiple-choice",
      "question": "Social graph: Query 'friends of friends' for user with 500 friends, each having 500 friends. Unique users in result (assuming 50% overlap)?",
      "options": [
        "~125,000 users",
        "~250,000 users",
        "~500,000 users",
        "~1,000,000 users"
      ],
      "correct": 0,
      "explanation": "Friends of friends: 500 × 500 = 250,000. With 50% overlap: 250,000 × 0.5 = 125,000 unique users.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 500 and 50 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-062",
      "type": "multiple-choice",
      "question": "Event sourcing: 10M events/day, average 500 bytes. Snapshots every 1000 events at 10 KB each. Keep all events + snapshots. Daily storage?",
      "options": [
        "~5 GB events + ~100 MB snapshots",
        "~5 GB events + ~1 GB snapshots",
        "~50 GB events + ~100 MB snapshots",
        "~50 GB events + ~1 GB snapshots"
      ],
      "correct": 0,
      "explanation": "Events: 10M × 500 bytes = 5 GB. Snapshots: 10M ÷ 1000 × 10 KB = 100 MB.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 10M and 500 in aligned units before deciding on an implementation approach. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-063",
      "type": "multiple-choice",
      "question": "Caching layer: 100K QPS, 10% cache miss, each miss costs $0.0001 (DB query). Monthly cost of cache misses?",
      "options": ["~$2,600", "~$26,000", "~$260,000", "~$2.6M"],
      "correct": 3,
      "explanation": "Misses/sec: 100K × 10% = 10K. Monthly misses: 10K × 2.6M sec = 26B. Cost: 26B × $0.0001 = $2.6M.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 100K and 10 in aligned units before deciding on an implementation approach. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-064",
      "type": "multiple-choice",
      "question": "Log shipping: 1000 servers, 1 GB logs/day each. Ship compressed (10x) every hour. Network bandwidth per server during ship?",
      "options": [
        "~2.3 KB/s average",
        "~23 KB/s average",
        "~230 KB/s average",
        "~2.3 MB/s average"
      ],
      "correct": 0,
      "explanation": "Daily compressed: 1 GB ÷ 10 = 100 MB. Hourly: 100 MB ÷ 24 = 4.2 MB. Bandwidth: 4.2 MB ÷ 3600 sec ≈ 1.2 KB/s average. Closest is 2.3 KB/s.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 1000 and 1 GB in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-065",
      "type": "multiple-choice",
      "question": "Microservices: 50 services, each calls average 3 downstream services per request. 10K external requests/sec. Internal service calls/sec?",
      "options": [
        "~30K calls/sec",
        "~150K calls/sec",
        "~1.5M calls/sec",
        "~15M calls/sec"
      ],
      "correct": 0,
      "explanation": "Each external request generates: 1 + 3 = 4 hops initially, but if each downstream also calls 3... First level: 10K. Each generates 3 more: 30K internal calls. (Assuming 1 level depth shown.)",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 50 and 3 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-066",
      "type": "multiple-choice",
      "question": "Time-series forecasting: 1M metrics, model generates 24-hour forecast in 10ms per metric. Total forecast compute time?",
      "options": ["~2.8 hours", "~28 hours", "~280 hours", "~2,800 hours"],
      "correct": 0,
      "explanation": "Total: 1M metrics × 10ms = 10,000 sec = 2.78 hours of compute time.",
      "detailedExplanation": "Generalize from time-series forecasting: 1M metrics, model generates 24-hour forecast in 10ms per metric to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that rely on linear intuition for compounding workloads. Growth answers improve when tied to a concrete capacity decision threshold. Numbers such as 1M and 24 should be normalized first so downstream reasoning stays consistent. Common pitfall: treating compounding as linear change.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-067",
      "type": "multiple-choice",
      "question": "CI/CD: 500 repos, each builds 5 times/day, build takes 10 minutes, uses 4 CPU cores. Total daily compute?",
      "options": [
        "~167 core-hours",
        "~417 core-hours",
        "~1,667 core-hours",
        "~4,167 core-hours"
      ],
      "correct": 2,
      "explanation": "Builds: 500 × 5 = 2,500/day. Core-minutes: 2,500 × 10 × 4 = 100,000. Core-hours: 100,000 ÷ 60 = 1,667.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 500 and 5 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-068",
      "type": "multiple-choice",
      "question": "Graph database: 100M nodes (100 bytes each), 1B edges (50 bytes each). Index on node properties adds 20% overhead. Total storage?",
      "options": ["~12 GB", "~62 GB", "~120 GB", "~620 GB"],
      "correct": 1,
      "explanation": "Nodes: 100M × 100 = 10 GB. Edges: 1B × 50 = 50 GB. Index: 10 GB × 0.2 = 2 GB. Total: 62 GB.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that conflict with the primary access pattern or index strategy. Schema and index choices should follow access patterns and write/read amplification constraints. Keep quantities like 100M and 100 in aligned units before deciding on an implementation approach. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-069",
      "type": "multiple-choice",
      "question": "Image pipeline: 1M images/day uploaded. Generate 5 thumbnail sizes (2 KB, 10 KB, 50 KB, 200 KB, 500 KB). Total thumbnail storage after 1 year?",
      "options": ["~28 TB", "~278 TB", "~2.78 PB", "~27.8 PB"],
      "correct": 1,
      "explanation": "Per image: 2 + 10 + 50 + 200 + 500 = 762 KB. Daily: 1M × 762 KB = 762 GB. Annual: 762 × 365 = 278 TB.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. If values like 1M and 5 appear, convert them into one unit basis before comparison. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-070",
      "type": "multiple-choice",
      "question": "Webhook delivery: 10M events/day, each triggers webhook to 3 endpoints average. 5% failure rate, retry 3 times with exponential backoff. Total daily HTTP requests?",
      "options": [
        "~30M requests",
        "~31.5M requests",
        "~34.5M requests",
        "~45M requests"
      ],
      "correct": 2,
      "explanation": "Initial: 10M × 3 = 30M. Failures: 30M × 5% = 1.5M. Retries (up to 3): ~1.5M × 3 = 4.5M extra. But some retries succeed early... Estimate: 30M + ~4.5M = 34.5M.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 10M and 3 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-071",
      "type": "multiple-choice",
      "question": "Audit log: 50M API calls/day. Each log entry: 512 bytes. Compliance requires 7-year retention with immutable storage. Storage after 7 years?",
      "options": ["~64 TB", "~640 TB", "~6.4 PB", "~64 PB"],
      "correct": 0,
      "explanation": "Daily: 50M × 512 = 25.6 GB. 7 years: 25.6 × 365 × 7 = 65.4 TB ≈ 64 TB.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. If values like 50M and 512 appear, convert them into one unit basis before comparison. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-072",
      "type": "multiple-choice",
      "question": "Real-time analytics: 1B events/day ingested. Each event joins with 3 dimension tables (1M rows each). Query 95th percentile latency target: 100ms. Events/sec that meet SLA with proper indexing?",
      "options": [
        "Depends on query pattern and hardware",
        "~1,000 events/sec always achievable",
        "~10,000 events/sec with sharding",
        "~100,000 events/sec with columnar storage"
      ],
      "correct": 0,
      "explanation": "100ms p95 depends heavily on: index design, join strategy, data distribution, hardware specs, query complexity. Can't determine from given info alone. This tests recognizing when more context is needed.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. If values like 1B and 3 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-073",
      "type": "multiple-choice",
      "question": "Distributed cache: 50 nodes, each 64 GB RAM. Keys are 100 bytes, values average 1 KB. Replication factor 2. Max unique keys?",
      "options": ["~1.5B keys", "~3B keys", "~6B keys", "~12B keys"],
      "correct": 0,
      "explanation": "Total RAM: 50 × 64 GB = 3.2 TB. With replication 2: effective 1.6 TB. Per key: 100 + 1000 = 1.1 KB. Keys: 1.6 TB ÷ 1.1 KB = 1.45B keys.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Keep quantities like 50 and 64 GB in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-074",
      "type": "multiple-choice",
      "question": "Search ranking: 100M documents, rank top 1000 for each query. Scoring takes 1 μs per document. 10K QPS. CPU cores needed just for ranking?",
      "options": [
        "~100 cores",
        "~1,000 cores",
        "~10,000 cores",
        "~100,000 cores"
      ],
      "correct": 1,
      "explanation": "Per query: 100M × 1μs = 100 sec? No, that's if scoring all docs. With index, score top candidates. But question says 'rank top 1000' implying all docs considered... 100M × 1μs = 100 sec per query isn't feasible. Assuming index narrows to 100K candidates: 100K × 1μs = 0.1 sec. At 10K QPS: 1000 cores.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that conflict with the primary access pattern or index strategy. Choose data shape based on workload paths, not on normalization dogma alone. Numbers such as 100M and 1000 should be normalized first so downstream reasoning stays consistent. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-075",
      "type": "multiple-choice",
      "question": "Billing system: 10M customers, usage aggregated hourly. Each aggregation reads 1000 usage records (100 bytes each). Monthly compute for aggregation?",
      "options": [
        "~7.2 TB data processed",
        "~72 TB data processed",
        "~720 TB data processed",
        "~7.2 PB data processed"
      ],
      "correct": 2,
      "explanation": "Per customer per hour: 1000 × 100 = 100 KB. Hourly total: 10M × 100 KB = 1 TB. Monthly: 1 TB × 24 × 30 = 720 TB processed.",
      "detailedExplanation": "Generalize from billing system: 10M customers, usage aggregated hourly to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that rely on linear intuition for compounding workloads. Growth answers improve when tied to a concrete capacity decision threshold. If values like 10M and 1000 appear, convert them into one unit basis before comparison. Common pitfall: postponing scaling work until after constraint breach.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-076",
      "type": "multiple-choice",
      "question": "Content delivery: Video on demand, 10M titles, each 2 GB average. 90-10 rule (10% of content gets 90% of views). Hot storage for 90% of views?",
      "options": ["~200 TB", "~2 PB", "~18 PB", "~20 PB"],
      "correct": 1,
      "explanation": "Hot content: 10% of 10M = 1M titles × 2 GB = 2 PB serves 90% of views.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Keep quantities like 10M and 2 GB in aligned units before deciding on an implementation approach. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-077",
      "type": "multiple-choice",
      "question": "Message queue: 1M messages/sec, 10 KB each. Consumers process at 100K messages/sec total. Queue depth after 1 hour if not scaled?",
      "options": [
        "~3.2B messages, ~32 TB",
        "~32M messages, ~320 GB",
        "~3.2M messages, ~32 GB",
        "Queue would be empty"
      ],
      "correct": 0,
      "explanation": "Net accumulation: (1M - 100K) × 3600 = 3.24B messages. Size: 3.24B × 10 KB = 32.4 TB.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 1M and 10 KB in aligned units before deciding on an implementation approach. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-078",
      "type": "multiple-choice",
      "question": "KV store: 99.9% read, 0.1% write. Total 100K ops/sec. Each read 1 KB, each write 10 KB. Network bandwidth?",
      "options": ["~100 MB/s", "~110 MB/s", "~200 MB/s", "~1 GB/s"],
      "correct": 0,
      "explanation": "Reads: 100K × 99.9% = 99.9K × 1 KB = 99.9 MB/s. Writes: 100K × 0.1% = 100 × 10 KB = 1 MB/s. Total: ~101 MB/s ≈ 100 MB/s.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the answer that correctly handles unit conversion and link-capacity limits. Treat network capacity as a steady-state constraint, then test against peak windows. Keep quantities like 99.9 and 0.1 in aligned units before deciding on an implementation approach. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-079",
      "type": "multiple-choice",
      "question": "AB test analysis: 100M users, 1000 metrics per user, analyze daily. Each metric comparison is 1 KB output. Daily report storage?",
      "options": ["~100 GB", "~1 TB", "~10 TB", "~100 TB"],
      "correct": 3,
      "explanation": "Comparisons: 100M × 1000 = 100B. Storage: 100B × 1 KB = 100 TB. (This is why you sample or pre-aggregate!)",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard storage decisions that optimize one dimension while violating another core requirement. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Keep quantities like 100M and 1000 in aligned units before deciding on an implementation approach. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-080",
      "type": "multiple-choice",
      "question": "Service mesh: 100 services, each has 10 replicas (1000 total). Each replica connects to 5 downstream services (all 10 replicas each). Total outbound connections per replica?",
      "options": [
        "~5 connections",
        "~50 connections",
        "~500 connections",
        "~5,000 connections"
      ],
      "correct": 1,
      "explanation": "Each replica connects to: 5 downstream services × 10 replicas each = 50 outbound connections per replica.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Eliminate approaches that hand-wave conflict resolution or quorum behavior. Strong answers connect quorum/coordination settings to concrete correctness goals. Keep quantities like 100 and 10 in aligned units before deciding on an implementation approach. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-081",
      "type": "multiple-choice",
      "question": "Data replication: Primary in US, replica in EU. 10 GB/hour writes. RTT US-EU: 100ms. Synchronous replication latency overhead?",
      "options": [
        "~100ms per write",
        "~200ms per write (round trip)",
        "Depends on batch size",
        "Negligible with pipelining"
      ],
      "correct": 0,
      "explanation": "Synchronous replication waits for acknowledgment: 1 RTT = 100ms added to each write commit. (Async would be near-zero overhead but risks data loss.)",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject designs that improve throughput while weakening reliability guarantees. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 10 GB and 100ms in aligned units before deciding on an implementation approach. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-082",
      "type": "multiple-choice",
      "question": "Ticket system: Concert with 50K seats. Sales open, 1M users ready. Each page load = 5 API calls. If each user refreshes 10 times in first minute, initial QPS?",
      "options": ["~83K QPS", "~417K QPS", "~833K QPS", "~8.3M QPS"],
      "correct": 2,
      "explanation": "Requests in first minute: 1M users × 10 refreshes × 5 calls = 50M. QPS: 50M ÷ 60 = 833K QPS.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Keep quantities like 50K and 1M in aligned units before deciding on an implementation approach. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-083",
      "type": "multiple-choice",
      "question": "Genomics: 1M patient genomes, 100 GB each. Query 1% of genome (1 GB) per analysis. 10K analyses/day. Daily data scanned?",
      "options": ["~1 TB", "~10 TB", "~100 TB", "~1 PB"],
      "correct": 1,
      "explanation": "Per analysis: 1 GB scanned. Daily: 10K × 1 GB = 10 TB.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 1M and 100 GB should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-084",
      "type": "multiple-choice",
      "question": "Autocomplete: 1B queries/day. Each keystroke triggers suggestion (average 10 keystrokes per query). Suggestion service QPS?",
      "options": ["~11K QPS", "~116K QPS", "~1.16M QPS", "~11.6M QPS"],
      "correct": 1,
      "explanation": "Daily suggestions: 1B × 10 = 10B. QPS: 10B ÷ 86,400 = 115,740 ≈ 116K QPS.",
      "detailedExplanation": "Generalize from autocomplete: 1B queries/day to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 1B and 10 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-085",
      "type": "multiple-choice",
      "question": "Election system: 100M voters over 12-hour voting window. Each vote = 1 KB transaction, must be durably stored. Minimum write throughput?",
      "options": [
        "~2.3K writes/sec",
        "~23K writes/sec",
        "~230K writes/sec",
        "~2.3M writes/sec"
      ],
      "correct": 0,
      "explanation": "Votes: 100M ÷ (12 × 3600) = 2,315 writes/sec. (But need significant headroom for peak hours.)",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Eliminate approaches that hand-wave conflict resolution or quorum behavior. Strong answers connect quorum/coordination settings to concrete correctness goals. Numbers such as 100M and 12 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-086",
      "type": "multiple-choice",
      "question": "Network telemetry: 10K switches, each sends 1000 flow records/sec at 100 bytes each. Central collector ingests all. Ingestion rate?",
      "options": ["~1 GB/s", "~10 GB/s", "~100 GB/s", "~1 TB/s"],
      "correct": 0,
      "explanation": "Records/sec: 10K × 1000 = 10M. Bandwidth: 10M × 100 bytes = 1 GB/s.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer the answer that correctly handles unit conversion and link-capacity limits. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Numbers such as 10K and 1000 should be normalized first so downstream reasoning stays consistent. Common pitfall: planning on average transfer while peak bursts dominate.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-087",
      "type": "multiple-choice",
      "question": "Inventory system: 1M products across 1000 warehouses. Each product-warehouse combo (1B combinations) has quantity (8 bytes). Inventory snapshot size?",
      "options": ["~8 GB", "~80 GB", "~800 GB", "~8 TB"],
      "correct": 0,
      "explanation": "Combinations: 1M × 1000 = 1B. Storage: 1B × 8 bytes = 8 GB.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. If values like 1M and 1000 appear, convert them into one unit basis before comparison. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-088",
      "type": "multiple-choice",
      "question": "PDF generation: 10K reports/hour, each report 50 pages, 100 KB/page. Generation takes 500ms/page. Servers needed (sequential page generation)?",
      "options": [
        "~7 servers",
        "~70 servers",
        "~700 servers",
        "~7,000 servers"
      ],
      "correct": 1,
      "explanation": "Pages/hour: 10K × 50 = 500K. Time/hour: 500K × 0.5s = 250K sec = 69.4 hours of work/hour. Servers: 70.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 10K and 50 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-089",
      "type": "multiple-choice",
      "question": "Shopping cart: 50M active sessions, average 5 items per cart at 500 bytes per item. Session data with 30-min timeout. Memory for cart service?",
      "options": ["~125 GB", "~250 GB", "~500 GB", "~1 TB"],
      "correct": 0,
      "explanation": "Per session: 5 × 500 = 2,500 bytes. Total: 50M × 2.5 KB = 125 GB.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 50M and 5 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-090",
      "type": "multiple-choice",
      "question": "Notification fanout: Celebrity posts to 10M followers. Each notification 1 KB. Fanout via pub/sub with 3 regional replicas. Total data written?",
      "options": ["~10 GB", "~30 GB", "~100 GB", "~300 GB"],
      "correct": 1,
      "explanation": "Notifications: 10M × 1 KB = 10 GB. With 3 replicas: 30 GB total written.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 10M and 1 KB appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-091",
      "type": "multiple-choice",
      "question": "Error budget: 99.9% SLO (43.8 min downtime/month allowed). Burn rate dashboard shows 2x budget burn. Time until SLO breach at this rate?",
      "options": [
        "~22 minutes downtime remaining",
        "~11 days until breach",
        "~15 days until breach",
        "Already breached"
      ],
      "correct": 2,
      "explanation": "At 2x burn rate, budget depletes in half the time: 30 days ÷ 2 = 15 days until budget exhausted.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 99.9 and 43.8 min appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-092",
      "type": "multiple-choice",
      "question": "Mobile sync: 100M users, sync 10 MB data each every 4 hours. 20% delta sync (2 MB). Server bandwidth for sync?",
      "options": ["~14 GB/s", "~140 GB/s", "~1.4 TB/s", "~14 TB/s"],
      "correct": 0,
      "explanation": "Syncs/sec: 100M ÷ (4 × 3600) = 6,944/sec. Bandwidth: 6,944 × 2 MB = 13.9 GB/s.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 100M and 10 MB appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-093",
      "type": "multiple-choice",
      "question": "Leaderboard: 100M players, update score every game (average 10 games/day). Leaderboard shows top 100, recalculated on each query at 10K QPS. Storage for scores?",
      "options": ["~800 MB (user ID + score)", "~8 GB", "~80 GB", "~800 GB"],
      "correct": 0,
      "explanation": "Storage: 100M × 8 bytes (ID + score) = 800 MB. (Sorted set or heap structure.)",
      "detailedExplanation": "Generalize from leaderboard: 100M players, update score every game (average 10 games/day) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. If values like 100M and 10 appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-094",
      "type": "multiple-choice",
      "question": "Data pipeline SLA: 99% of records processed within 5 minutes. 1M records/minute. At 99%, how many records can exceed 5 min per day?",
      "options": [
        "~14,400 records",
        "~144,000 records",
        "~1.44M records",
        "~14.4M records"
      ],
      "correct": 3,
      "explanation": "Daily records: 1M × 1440 = 1.44B. 1% can be slow: 1.44B × 1% = 14.4M records/day.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 99 and 5 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-095",
      "type": "multiple-choice",
      "question": "Container registry: Pull 500 images/sec, average 200 MB each. 70% layer cache hit. Registry egress bandwidth?",
      "options": ["~10 GB/s", "~30 GB/s", "~70 GB/s", "~100 GB/s"],
      "correct": 1,
      "explanation": "Cache miss: 30% of 200 MB = 60 MB per pull. Bandwidth: 500 × 60 MB = 30 GB/s.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 500 and 200 MB should be normalized first so downstream reasoning stays consistent. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-096",
      "type": "multiple-choice",
      "question": "Compliance archival: 10 TB/day, keep 10 years. Storage cost: hot $0.02/GB/month, cold after 30 days at $0.004/GB/month. Year 1 storage cost?",
      "options": ["~$75K", "~$750K", "~$1.8M", "~$7.5M"],
      "correct": 2,
      "explanation": "Year 1 accumulation: 365 × 10 TB = 3.65 PB. Hot (30 days): 300 TB × $0.02 × 12 = $72K. Cold (growing): average ~1.8 PB × $0.004 × 12 = $86K? This is complex. Rough estimate: ~$1-2M range.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the approach that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Keep quantities like 10 TB and 10 in aligned units before deciding on an implementation approach. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-097",
      "type": "multiple-choice",
      "question": "Distributed lock: 10K services acquire locks at 1K locks/sec total. Lock held 100ms average. Contention (locks held simultaneously)?",
      "options": [
        "~10 locks held",
        "~100 locks held",
        "~1,000 locks held",
        "~10,000 locks held"
      ],
      "correct": 1,
      "explanation": "Locks/sec × hold time = concurrent locks. 1000 locks/sec × 0.1 sec = 100 locks held simultaneously on average.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 10K and 1K in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-098",
      "type": "multiple-choice",
      "question": "Batch ETL: Source has 1 TB, transform reads each row 3 times (parse, validate, enrich), writes 1.5 TB output. I/O amplification factor?",
      "options": ["~3x", "~4.5x", "~5.5x", "~6x"],
      "correct": 1,
      "explanation": "Reads: 1 TB × 3 = 3 TB. Writes: 1.5 TB. Total I/O: 4.5 TB for 1 TB source = 4.5x amplification.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard options that depend on unrealistic assumptions hidden in the estimate. A harsh sanity check should identify which assumption is most likely wrong. Keep quantities like 1 TB and 3 in aligned units before deciding on an implementation approach. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-099",
      "type": "multiple-choice",
      "question": "Multi-region active-active: 3 regions, each handles 100K writes/sec locally. Cross-region replication at 150ms RTT. Conflict resolution rate if 0.01% of writes conflict?",
      "options": [
        "~30 conflicts/sec",
        "~300 conflicts/sec",
        "~3,000 conflicts/sec",
        "~30,000 conflicts/sec"
      ],
      "correct": 0,
      "explanation": "Total writes: 300K/sec across 3 regions. Conflicts: 300K × 0.01% = 30 conflicts/sec.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 3 and 100K in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "compound-100",
      "type": "multiple-choice",
      "question": "Full system: 10M DAU, each generates 100 events/day (1 KB each), makes 50 API calls (10 KB response avg), uploads 1 photo/week (2 MB). Daily storage growth and bandwidth?",
      "options": [
        "~1 TB storage/day, ~5 TB bandwidth/day",
        "~4 TB storage/day, ~8 TB bandwidth/day",
        "~1 TB storage/day, ~500 GB bandwidth/day",
        "~3 TB storage/day, ~5 TB bandwidth/day"
      ],
      "correct": 0,
      "explanation": "Events: 10M × 100 × 1 KB = 1 TB/day storage. API bandwidth: 10M × 50 × 10 KB = 5 TB/day egress. Photos: 10M ÷ 7 × 2 MB = 2.9 TB/day storage. Total storage: ~4 TB/day, bandwidth: ~5 TB/day.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 10M and 100 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n001",
      "type": "numeric-input",
      "question": "E-commerce: 5M DAU, 50 page views/user/day, 20 KB/page. Daily bandwidth in TB?",
      "answer": 5,
      "unit": "TB",
      "tolerance": 0.1,
      "explanation": "5M × 50 × 20 KB = 5 billion KB = 5 TB.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. If values like 5M and 50 appear, convert them into one unit basis before comparison. Common pitfall: planning on average transfer while peak bursts dominate.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n002",
      "type": "numeric-input",
      "question": "Chat app: 10M users, 100 messages/user/day, 500 bytes/message. Daily storage in GB?",
      "answer": 500,
      "unit": "GB",
      "tolerance": 0.1,
      "explanation": "10M × 100 × 500 bytes = 500 billion bytes = 500 GB.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 10M and 100 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n003",
      "type": "numeric-input",
      "question": "Video platform: 1M daily uploads, 100 MB average, 3 quality versions. Daily storage growth in TB?",
      "answer": 300,
      "unit": "TB",
      "tolerance": 0.1,
      "explanation": "1M × 100 MB × 3 = 300 million MB = 300 TB.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Normalize units before computing so conversion mistakes do not propagate. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 1M and 100 MB appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n004",
      "type": "numeric-input",
      "question": "Social feed: 100M users, 10% active, check feed 20×/day, 50 posts/load. Daily post reads in billions?",
      "answer": 10,
      "unit": "billion",
      "tolerance": 0.1,
      "explanation": "100M × 0.1 × 20 × 50 = 10 billion post reads.",
      "detailedExplanation": "Generalize from social feed: 100M users, 10% active, check feed 20×/day, 50 posts/load to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 100M and 10 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n005",
      "type": "numeric-input",
      "question": "Ride-sharing: 1M rides/day, driver location updates every 5 seconds for 20-minute average ride. Daily location updates in billions?",
      "answer": 0.24,
      "unit": "billion",
      "tolerance": 0.15,
      "explanation": "1M rides × (20 min × 60 sec / 5 sec) = 1M × 240 = 240 million.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 1M and 5 seconds should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n006",
      "type": "numeric-input",
      "question": "IoT platform: 10M devices, heartbeat every 30 seconds, 100 bytes each. Daily data in GB?",
      "answer": 288,
      "unit": "GB",
      "tolerance": 0.1,
      "explanation": "10M × (86400/30) × 100 bytes = 10M × 2880 × 100 = 2.88 trillion bytes = 288 GB.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 10M and 30 seconds in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n007",
      "type": "numeric-input",
      "question": "Search engine: 50K QPS, each query fans out to 100 shards. Total shard QPS in millions?",
      "answer": 5,
      "unit": "million",
      "tolerance": 0.1,
      "explanation": "50K × 100 = 5 million shard queries per second.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 50K and 100 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n008",
      "type": "numeric-input",
      "question": "Email service: 100M users, 50 emails/user/day, 10 KB average, 90-day retention. Storage in PB?",
      "answer": 4.5,
      "unit": "PB",
      "tolerance": 0.15,
      "explanation": "100M × 50 × 10 KB × 90 = 4.5 quadrillion bytes = 4.5 PB.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 100M and 50 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n009",
      "type": "numeric-input",
      "question": "Gaming: 1M concurrent players, 50 updates/second/player, 200 bytes each. Bandwidth in Gbps?",
      "answer": 80,
      "unit": "Gbps",
      "tolerance": 0.1,
      "explanation": "1M × 50 × 200 = 10 billion bytes/sec = 80 Gbps.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. If values like 1M and 50 appear, convert them into one unit basis before comparison. Common pitfall: missing protocol/compression overhead in capacity math.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n010",
      "type": "numeric-input",
      "question": "Analytics: 1B events/day, 500 bytes/event, 10× for indexes/aggregates. Total daily storage in TB?",
      "answer": 5,
      "unit": "TB",
      "tolerance": 0.15,
      "explanation": "1B × 500 × 10 = 5 trillion bytes = 5 TB.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 1B and 500 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n011",
      "type": "numeric-input",
      "question": "CDN: 10M unique assets, 95% cache hit rate, 1M requests/second. Origin QPS?",
      "answer": 50000,
      "tolerance": 0.1,
      "explanation": "1M × 0.05 = 50,000 QPS to origin.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 10M and 95 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n012",
      "type": "numeric-input",
      "question": "Payment system: 1000 TPS, each transaction writes to 3 databases. Database writes per second?",
      "answer": 3000,
      "tolerance": 0.1,
      "explanation": "1000 × 3 = 3000 database writes per second.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 1000 and 3 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n013",
      "type": "numeric-input",
      "question": "Photo sharing: 500M photos, 3 thumbnail sizes (10KB, 50KB, 200KB), plus 2MB original. Total storage in PB?",
      "answer": 1.13,
      "unit": "PB",
      "tolerance": 0.15,
      "explanation": "500M × (10KB + 50KB + 200KB + 2MB) = 500M × 2.26MB = 1.13 PB.",
      "detailedExplanation": "Generalize from photo sharing: 500M photos, 3 thumbnail sizes (10KB, 50KB, 200KB), plus 2MB original to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 500M and 3 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n014",
      "type": "numeric-input",
      "question": "Notification service: 100M users, 5 notifications/day, 10% enabled push. Daily pushes in millions?",
      "answer": 50,
      "unit": "million",
      "tolerance": 0.1,
      "explanation": "100M × 5 × 0.1 = 50 million daily pushes.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 100M and 5 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n015",
      "type": "numeric-input",
      "question": "Streaming: 1M concurrent viewers, 5 Mbps each, 40% peak:average ratio. Peak bandwidth in Tbps?",
      "answer": 5,
      "unit": "Tbps",
      "tolerance": 0.1,
      "explanation": "1M × 5 Mbps = 5 Tbps at this concurrency level.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 1M and 5 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n016",
      "type": "numeric-input",
      "question": "Logging: 1000 servers, 10 GB logs/server/day, 30-day retention, 50% compression. Storage in TB?",
      "answer": 150,
      "unit": "TB",
      "tolerance": 0.1,
      "explanation": "1000 × 10 GB × 30 × 0.5 = 150 TB.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 1000 and 10 GB appear, convert them into one unit basis before comparison. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n017",
      "type": "numeric-input",
      "question": "API gateway: 100K QPS, 5KB request, 20KB response. Total bandwidth in Gbps?",
      "answer": 20,
      "unit": "Gbps",
      "tolerance": 0.1,
      "explanation": "100K × (5KB + 20KB) = 2.5 GB/s = 20 Gbps.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. Interface decisions should be justified by contract stability and client impact over time. If values like 100K and 5KB appear, convert them into one unit basis before comparison. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-n018",
      "type": "numeric-input",
      "question": "ML inference: 10K QPS, 100ms model latency, 50 concurrent models. GPU servers needed (each handles 500 QPS)?",
      "answer": 20,
      "tolerance": 0.1,
      "explanation": "10K QPS ÷ 500 QPS/server = 20 servers.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 10K and 100ms should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o001",
      "type": "ordering",
      "question": "Rank by daily storage growth (lowest to highest). (Compound Scenarios context)",
      "items": [
        "Chat: 10M users, 100 msg/day, 500B each",
        "Photos: 1M uploads/day, 2MB each",
        "Video: 100K uploads/day, 500MB each",
        "Logs: 1000 servers, 10GB/day each"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Chat: 500GB < Photos: 2TB < Logs: 10TB < Video: 50TB.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Storage decisions should align durability expectations with access and cost behavior. If values like 500GB and 2TB appear, convert them into one unit basis before comparison. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o002",
      "type": "ordering",
      "question": "Rank by QPS generated (lowest to highest).",
      "items": [
        "100K DAU × 10 req/day",
        "1M concurrent WebSockets × 1 msg/10s",
        "10M DAU × 5 req/day",
        "1K TPS payment system"
      ],
      "correctOrder": [0, 3, 2, 1],
      "explanation": "100K×10/86400=12 QPS < 1K TPS < 10M×5/86400=579 QPS < 1M/10=100K QPS.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 100K and 10 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o003",
      "type": "ordering",
      "question": "Rank by bandwidth requirement (lowest to highest).",
      "items": [
        "Chat: 10K QPS × 1KB",
        "API: 10K QPS × 10KB",
        "Images: 1K QPS × 1MB",
        "Video: 100 streams × 5Mbps"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Chat: 10MB/s=80Mbps < API: 100MB/s=800Mbps < Video: 500Mbps < Images: 1GB/s=8Gbps.",
      "detailedExplanation": "Generalize from rank by bandwidth requirement (lowest to highest) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Numbers such as 10MB and 100MB should be normalized first so downstream reasoning stays consistent. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o004",
      "type": "ordering",
      "question": "Rank these systems by infrastructure complexity (lowest to highest).",
      "items": [
        "Static website",
        "CRUD API",
        "Real-time multiplayer game",
        "Global social network"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Static (CDN) < CRUD (DB+API) < Real-time (low latency, state sync) < Global social (all of above at scale).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Build the rank from biggest differences first, then refine with adjacent checks. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o005",
      "type": "ordering",
      "question": "Rank by cost driver (lowest to highest cost impact for a video platform).",
      "items": [
        "Metadata storage",
        "Compute for encoding",
        "Video storage",
        "CDN bandwidth"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Metadata (tiny) < Encoding (one-time) < Storage (grows) < CDN bandwidth (continuous, high volume).",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o006",
      "type": "ordering",
      "question": "Rank by number of servers typically needed (fewest to most) for 10M DAU.",
      "items": [
        "Cache (Redis)",
        "Search (Elasticsearch)",
        "Database (PostgreSQL)",
        "Web servers"
      ],
      "correctOrder": [0, 2, 3, 1],
      "explanation": "Redis: 2-5. PostgreSQL: 3-10. Web: 10-50. Elasticsearch: 10-100 (many shards).",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Place obvious extremes first, then sort the middle by pairwise comparison. Treat freshness policy and invalidation paths as first-class constraints. If values like 10M and 2 appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o007",
      "type": "ordering",
      "question": "Rank by data model complexity (simplest to most complex).",
      "items": [
        "Key-value cache",
        "User profiles",
        "Social graph",
        "Activity feed"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "KV (key→value) < Profiles (structured doc) < Graph (relationships) < Feed (graph + time + ranking).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-o008",
      "type": "ordering",
      "question": "Rank by latency sensitivity (least to most sensitive).",
      "items": [
        "Batch analytics",
        "Email delivery",
        "Web page load",
        "Real-time trading"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Batch (hours ok) < Email (minutes) < Web (seconds) < Trading (microseconds).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m001",
      "type": "multi-select",
      "question": "For a 10M DAU social app, which are reasonable resource estimates?",
      "options": [
        "10 TB database",
        "100 QPS average",
        "1 PB total storage (with media)",
        "20 web servers"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "10 TB DB, 1 PB media storage, 20 servers reasonable. 100 QPS too low (should be ~1K-10K).",
      "detailedExplanation": "Generalize from for a 10M DAU social app, which are reasonable resource estimates to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 10M and 10 TB in aligned units before deciding on an implementation approach. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m002",
      "type": "multi-select",
      "question": "Which significantly increase infrastructure costs?",
      "options": [
        "User-uploaded video",
        "Global low-latency requirement",
        "Real-time features",
        "Text-only content"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Video (storage + CDN), global (multi-region), real-time (more servers) all expensive. Text is cheap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m003",
      "type": "multi-select",
      "question": "A messaging app has 50M MAU, 1M concurrent, 100 msg/user/day. Which are true?",
      "options": [
        "~60K messages/second average",
        "~500GB daily storage (at 100B/msg)",
        "~10Gbps bandwidth for delivery",
        "~100 servers for handling connections"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "50M×100/86400≈58K msg/s ✓. 5B×100B=500GB ✓. 58K×100B≈6MB/s=48Mbps ✗. 1M/10K per server≈100 ✓.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 50M and 1M in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m004",
      "type": "multi-select",
      "question": "For ride-sharing with 100K daily rides, which estimates are reasonable?",
      "options": [
        "~1 GPS update/second during rides",
        "~100GB daily location data",
        "~1K QPS for matching",
        "~10 servers for the platform"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "1 GPS/sec reasonable. 100K×1200updates×100B=12GB not 100GB. 1K QPS for peaks. 10 servers for small scale.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 100K and 1 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m005",
      "type": "multi-select",
      "question": "E-commerce flash sale: normal 1K QPS, sale peaks at 100×. Which preparations help?",
      "options": [
        "Pre-warm caches",
        "Auto-scaling policies",
        "Queue for checkout",
        "Increase database replicas"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All help: pre-warm avoids cold start, auto-scale handles surge, queues smooth spikes, replicas handle reads.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 1K and 100 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m006",
      "type": "multi-select",
      "question": "Video platform: 1M videos, 1B views/month. Which require significant resources?",
      "options": [
        "Thumbnail storage",
        "Video transcoding",
        "CDN bandwidth",
        "View count updates"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Thumbnails small. Transcoding is CPU-heavy. CDN bandwidth is huge cost. 1B writes/month = 400 QPS needs handling.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 1M and 1B appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m007",
      "type": "multi-select",
      "question": "Analytics pipeline: 10B events/day. Which are true?",
      "options": [
        "~115K events/second",
        "~5TB raw data (at 500B/event)",
        "Need distributed processing",
        "Can use single PostgreSQL"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "10B/86400=115K/s ✓. 10B×500B=5TB ✓. Distributed needed ✓. Single PG can't handle this.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 10B and 86400 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-m008",
      "type": "multi-select",
      "question": "Global chat with 100ms latency requirement. Which are necessary?",
      "options": [
        "Multi-region deployment",
        "Edge message routing",
        "Single central database",
        "WebSocket connections"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Multi-region and edge routing needed for latency. WebSockets for real-time. Central DB creates latency bottleneck.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 100ms in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-t001",
      "type": "two-stage",
      "stages": [
        {
          "question": "Photo app: 10M DAU, 5 photos/day uploaded, 2MB each. Daily upload volume?",
          "options": ["~10 TB", "~100 TB", "~1 PB", "~10 PB"],
          "correct": 1,
          "explanation": "10M × 5 × 2MB = 100 TB daily uploads.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 10M and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "With 3 copies for redundancy and 90-day retention, total storage needed?",
          "options": ["~9 PB", "~27 PB", "~90 PB", "~270 PB"],
          "correct": 1,
          "explanation": "100 TB × 90 days × 3 copies = 27 PB.",
          "detailedExplanation": "Generalize from with 3 copies for redundancy and 90-day retention, total storage needed to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Numbers such as 3 and 90 should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints."
        }
      ],
      "explanation": "Photo storage at scale requires accounting for retention and redundancy.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-t002",
      "type": "two-stage",
      "stages": [
        {
          "question": "Streaming service: 10M subscribers, 2 hours/day average watch time, 5 Mbps bitrate. Peak concurrent viewers (10% watching)?",
          "options": ["~100K", "~1M", "~10M", "~100M"],
          "correct": 1,
          "explanation": "10M × 10% = 1M concurrent viewers at peak.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Keep quantities like 10M and 2 hours in aligned units before deciding on an implementation approach. Common pitfall: planning on average transfer while peak bursts dominate."
        },
        {
          "question": "Bandwidth needed for peak concurrency?",
          "options": ["~500 Gbps", "~5 Tbps", "~50 Tbps", "~500 Tbps"],
          "correct": 1,
          "explanation": "1M × 5 Mbps = 5,000 Gbps = 5 Tbps.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 1M and 5 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "explanation": "Streaming bandwidth scales with concurrent viewers.",
      "detailedExplanation": "Generalize from streaming bandwidth scales with concurrent viewers to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Common pitfall: bits-vs-bytes conversion mistakes.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-t003",
      "type": "two-stage",
      "stages": [
        {
          "question": "E-commerce: 1M orders/day, each order queries inventory (1 read), reserves stock (1 write), processes payment (1 write). Database writes/second?",
          "options": [
            "~12 writes/sec",
            "~23 writes/sec",
            "~115 writes/sec",
            "~230 writes/sec"
          ],
          "correct": 1,
          "explanation": "1M orders × 2 writes / 86400 sec = 23 writes/sec average.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 1M and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "If peak is 10× average during flash sales, and DB handles 500 writes/sec, how many DB instances for peak?",
          "options": [
            "1 instance",
            "2 instances",
            "5 instances",
            "10 instances"
          ],
          "correct": 0,
          "explanation": "Peak = 230 writes/sec. 500 > 230, so 1 instance handles peak.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 10 and 500 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "explanation": "Write scaling requires understanding peak loads.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "comp-t004",
      "type": "two-stage",
      "stages": [
        {
          "question": "Search engine: 10K QPS, each query searches 50 shards, each shard returns 100 results (1KB each) to be merged. Data transferred per query?",
          "options": ["~50 KB", "~500 KB", "~5 MB", "~50 MB"],
          "correct": 2,
          "explanation": "50 shards × 100 results × 1KB = 5 MB per query.",
          "detailedExplanation": "Generalize from search engine: 10K QPS, each query searches 50 shards, each shard returns 100 results to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 10K and 50 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "Internal bandwidth needed for query fan-out/gather?",
          "options": ["~500 MB/s", "~5 GB/s", "~50 GB/s", "~500 GB/s"],
          "correct": 2,
          "explanation": "10K QPS × 5 MB = 50 GB/s internal bandwidth.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 10K and 5 MB should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "explanation": "Search fan-out creates significant internal traffic.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ],
      "tags": ["estimation", "compound-scenarios"],
      "difficulty": "principal"
    }
  ]
}
