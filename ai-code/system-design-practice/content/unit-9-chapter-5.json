{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 5,
  "chapterTitle": "Graceful Degradation & Dependency Isolation",
  "chapterDescription": "Preserve core user journeys during dependency degradation with fallback modes and strict boundaries.",
  "problems": [
    {
      "id": "rel-gd-001",
      "type": "multiple-choice",
      "question": "Case Alpha: checkout page render path. Primary reliability risk is non-critical dependency blocking checkout. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Isolate critical path from optional dependencies with strict bulkhead pools.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Checkout page render path should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Isolate critical path from optional dependencies with strict bulkhead pools\" is strongest because it directly addresses non-critical dependency blocking checkout and improves repeatability under stress. This aligns with the extra condition (A rollback window is still available for the next 15 minutes).",
      "detailedExplanation": "Generalize from checkout page render path to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 15 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-002",
      "type": "multiple-choice",
      "question": "Case Beta: profile feed API. Primary reliability risk is fallback mode with stale unsafe data. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Define fallback data-quality tiers and expose degraded semantics explicitly.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Treat profile feed API as a reliability-control decision, not an averages-only optimization. \"Define fallback data-quality tiers and expose degraded semantics explicitly\" is correct since it mitigates fallback mode with stale unsafe data while keeping containment local. The decision remains valid given: Leadership asked for an action that lowers recurrence, not just symptoms.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-003",
      "type": "multiple-choice",
      "question": "Case Gamma: recommendation widget service. Primary reliability risk is kill switch missing for expensive feature. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Use kill switches to disable expensive non-core features within seconds.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "For recommendation widget service, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Use kill switches to disable expensive non-core features within seconds\" outperforms the alternatives because it targets kill switch missing for expensive feature and preserves safe recovery behavior. It is also the most compatible with Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-004",
      "type": "multiple-choice",
      "question": "Case Delta: search autosuggest path. Primary reliability risk is bulkhead boundaries crossed by shared pools. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Return minimal safe responses rather than waiting on slow optional calls."
      ],
      "correct": 3,
      "explanation": "In Graceful Degradation & Dependency Isolation, search autosuggest path fails mainly through bulkhead boundaries crossed by shared pools. The best choice is \"Return minimal safe responses rather than waiting on slow optional calls\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Recent game-day results showed hidden cross-zone coupling.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: invoice generation flow. Primary reliability risk is degraded mode hidden from clients. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Precompute essential fallback artifacts for high-traffic journeys.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Invoice generation flow should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Precompute essential fallback artifacts for high-traffic journeys\" is strongest because it directly addresses degraded mode hidden from clients and improves repeatability under stress. This aligns with the extra condition (Customer impact is concentrated on invariant-critical transactions).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-006",
      "type": "multiple-choice",
      "question": "Case Zeta: notification preference API. Primary reliability risk is partial outage causing full-page failure. Which next move is strongest? The previous mitigation improved averages but not tail behavior. (Graceful Degradation & Dependency Isolation context)",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Apply per-dependency timeouts and fallback chaining with bounded depth.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Treat notification preference API as a reliability-control decision, not an averages-only optimization. \"Apply per-dependency timeouts and fallback chaining with bounded depth\" is correct since it mitigates partial outage causing full-page failure while keeping containment local. The decision remains valid given: The previous mitigation improved averages but not tail behavior.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-007",
      "type": "multiple-choice",
      "question": "Case Eta: chat attachment pipeline. Primary reliability risk is critical path coupled to optional enrichment. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Protect critical write flows from read-path degradation side effects.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "For chat attachment pipeline, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Protect critical write flows from read-path degradation side effects\" outperforms the alternatives because it targets critical path coupled to optional enrichment and preserves safe recovery behavior. It is also the most compatible with Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-008",
      "type": "multiple-choice",
      "question": "Case Theta: dashboard analytics backend. Primary reliability risk is cache fallback TTL too long for safety. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Test degradation scenarios in game days to validate user-journey continuity."
      ],
      "correct": 3,
      "explanation": "In Graceful Degradation & Dependency Isolation, dashboard analytics backend fails mainly through cache fallback TTL too long for safety. The best choice is \"Test degradation scenarios in game days to validate user-journey continuity\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Operations wants a reversible step before broader architecture changes.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-009",
      "type": "multiple-choice",
      "question": "Case Iota: catalog browse service. Primary reliability risk is default fallback overloading primary path. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Separate auth/invariant checks from optional personalization dependencies.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Catalog browse service should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Separate auth/invariant checks from optional personalization dependencies\" is strongest because it directly addresses default fallback overloading primary path and improves repeatability under stress. This aligns with the extra condition (SLO burn rate accelerated after a config rollout this morning).",
      "detailedExplanation": "Generalize from catalog browse service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-010",
      "type": "multiple-choice",
      "question": "Case Kappa: device sync endpoint. Primary reliability risk is degradation policy undocumented per endpoint. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Track degraded-mode activation and recovery as SLO-governed behavior.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "For device sync endpoint, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Track degraded-mode activation and recovery as SLO-governed behavior\" outperforms the alternatives because it targets degradation policy undocumented per endpoint and preserves safe recovery behavior. It is also the most compatible with A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-011",
      "type": "multiple-choice",
      "question": "Case Lambda: checkout page render path. Primary reliability risk is non-critical dependency blocking checkout. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Isolate critical path from optional dependencies with strict bulkhead pools.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "In Graceful Degradation & Dependency Isolation, checkout page render path fails mainly through non-critical dependency blocking checkout. The best choice is \"Isolate critical path from optional dependencies with strict bulkhead pools\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The incident review highlighted missing boundary ownership.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-012",
      "type": "multiple-choice",
      "question": "Case Mu: profile feed API. Primary reliability risk is fallback mode with stale unsafe data. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Define fallback data-quality tiers and expose degraded semantics explicitly."
      ],
      "correct": 3,
      "explanation": "Profile feed API should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Define fallback data-quality tiers and expose degraded semantics explicitly\" is strongest because it directly addresses fallback mode with stale unsafe data and improves repeatability under stress. This aligns with the extra condition (Current runbooks assume fail-stop behavior, but reality is partial failure).",
      "detailedExplanation": "Generalize from profile feed API to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-013",
      "type": "multiple-choice",
      "question": "Case Nu: recommendation widget service. Primary reliability risk is kill switch missing for expensive feature. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Use kill switches to disable expensive non-core features within seconds.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat recommendation widget service as a reliability-control decision, not an averages-only optimization. \"Use kill switches to disable expensive non-core features within seconds\" is correct since it mitigates kill switch missing for expensive feature while keeping containment local. The decision remains valid given: A canary can be deployed immediately if the strategy is clear.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-014",
      "type": "multiple-choice",
      "question": "Case Xi: search autosuggest path. Primary reliability risk is bulkhead boundaries crossed by shared pools. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Return minimal safe responses rather than waiting on slow optional calls.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "For search autosuggest path, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Return minimal safe responses rather than waiting on slow optional calls\" outperforms the alternatives because it targets bulkhead boundaries crossed by shared pools and preserves safe recovery behavior. It is also the most compatible with Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-015",
      "type": "multiple-choice",
      "question": "Case Omicron: invoice generation flow. Primary reliability risk is degraded mode hidden from clients. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Precompute essential fallback artifacts for high-traffic journeys.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "In Graceful Degradation & Dependency Isolation, invoice generation flow fails mainly through degraded mode hidden from clients. The best choice is \"Precompute essential fallback artifacts for high-traffic journeys\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Client retries are already elevated and could amplify mistakes.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-016",
      "type": "multiple-choice",
      "question": "Case Pi: notification preference API. Primary reliability risk is partial outage causing full-page failure. Which next move is strongest? The team must preserve core write correctness under mitigation. (Graceful Degradation & Dependency Isolation context)",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Apply per-dependency timeouts and fallback chaining with bounded depth."
      ],
      "correct": 3,
      "explanation": "Notification preference API should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Apply per-dependency timeouts and fallback chaining with bounded depth\" is strongest because it directly addresses partial outage causing full-page failure and improves repeatability under stress. This aligns with the extra condition (The team must preserve core write correctness under mitigation).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-017",
      "type": "multiple-choice",
      "question": "Case Rho: chat attachment pipeline. Primary reliability risk is critical path coupled to optional enrichment. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Protect critical write flows from read-path degradation side effects.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat chat attachment pipeline as a reliability-control decision, not an averages-only optimization. \"Protect critical write flows from read-path degradation side effects\" is correct since it mitigates critical path coupled to optional enrichment while keeping containment local. The decision remains valid given: Recent staffing changes require simpler operational controls.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-018",
      "type": "multiple-choice",
      "question": "Case Sigma: dashboard analytics backend. Primary reliability risk is cache fallback TTL too long for safety. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Test degradation scenarios in game days to validate user-journey continuity.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "For dashboard analytics backend, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Test degradation scenarios in game days to validate user-journey continuity\" outperforms the alternatives because it targets cache fallback TTL too long for safety and preserves safe recovery behavior. It is also the most compatible with Cross-region latency variance increased during the event.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-019",
      "type": "multiple-choice",
      "question": "Case Tau: catalog browse service. Primary reliability risk is default fallback overloading primary path. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Separate auth/invariant checks from optional personalization dependencies.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "In Graceful Degradation & Dependency Isolation, catalog browse service fails mainly through default fallback overloading primary path. The best choice is \"Separate auth/invariant checks from optional personalization dependencies\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This path mixes latency-sensitive and correctness-sensitive requests.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: device sync endpoint. Primary reliability risk is degradation policy undocumented per endpoint. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Track degraded-mode activation and recovery as SLO-governed behavior."
      ],
      "correct": 3,
      "explanation": "Treat device sync endpoint as a reliability-control decision, not an averages-only optimization. \"Track degraded-mode activation and recovery as SLO-governed behavior\" is correct since it mitigates degradation policy undocumented per endpoint while keeping containment local. The decision remains valid given: The service has one hidden shared component with no backup path.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-021",
      "type": "multiple-choice",
      "question": "Case Phi: checkout page render path. Primary reliability risk is non-critical dependency blocking checkout. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Isolate critical path from optional dependencies with strict bulkhead pools.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "For checkout page render path, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Isolate critical path from optional dependencies with strict bulkhead pools\" outperforms the alternatives because it targets non-critical dependency blocking checkout and preserves safe recovery behavior. It is also the most compatible with The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-022",
      "type": "multiple-choice",
      "question": "Case Chi: profile feed API. Primary reliability risk is fallback mode with stale unsafe data. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Define fallback data-quality tiers and expose degraded semantics explicitly.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Graceful Degradation & Dependency Isolation, profile feed API fails mainly through fallback mode with stale unsafe data. The best choice is \"Define fallback data-quality tiers and expose degraded semantics explicitly\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Change approval favors narrowly scoped policies over global flips.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-023",
      "type": "multiple-choice",
      "question": "Case Psi: recommendation widget service. Primary reliability risk is kill switch missing for expensive feature. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Use kill switches to disable expensive non-core features within seconds.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Recommendation widget service should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Use kill switches to disable expensive non-core features within seconds\" is strongest because it directly addresses kill switch missing for expensive feature and improves repeatability under stress. This aligns with the extra condition (A previous outage showed stale metadata can outlive infrastructure recovery).",
      "detailedExplanation": "Generalize from recommendation widget service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-024",
      "type": "multiple-choice",
      "question": "Case Omega: search autosuggest path. Primary reliability risk is bulkhead boundaries crossed by shared pools. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Return minimal safe responses rather than waiting on slow optional calls."
      ],
      "correct": 3,
      "explanation": "Treat search autosuggest path as a reliability-control decision, not an averages-only optimization. \"Return minimal safe responses rather than waiting on slow optional calls\" is correct since it mitigates bulkhead boundaries crossed by shared pools while keeping containment local. The decision remains valid given: On-call needs mitigation that is observable by explicit metrics.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-025",
      "type": "multiple-choice",
      "question": "Case Atlas: invoice generation flow. Primary reliability risk is degraded mode hidden from clients. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Precompute essential fallback artifacts for high-traffic journeys.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "For invoice generation flow, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Precompute essential fallback artifacts for high-traffic journeys\" outperforms the alternatives because it targets degraded mode hidden from clients and preserves safe recovery behavior. It is also the most compatible with A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-026",
      "type": "multiple-choice",
      "question": "Case Nova: notification preference API. Primary reliability risk is partial outage causing full-page failure. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Apply per-dependency timeouts and fallback chaining with bounded depth.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Graceful Degradation & Dependency Isolation, notification preference API fails mainly through partial outage causing full-page failure. The best choice is \"Apply per-dependency timeouts and fallback chaining with bounded depth\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Business impact is highest in the top 5% of critical flows.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. If values like 5 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-027",
      "type": "multiple-choice",
      "question": "Case Orion: chat attachment pipeline. Primary reliability risk is critical path coupled to optional enrichment. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Protect critical write flows from read-path degradation side effects.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Chat attachment pipeline should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Protect critical write flows from read-path degradation side effects\" is strongest because it directly addresses critical path coupled to optional enrichment and improves repeatability under stress. This aligns with the extra condition (Regional failover is possible but expensive if used prematurely).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-028",
      "type": "multiple-choice",
      "question": "Case Vega: dashboard analytics backend. Primary reliability risk is cache fallback TTL too long for safety. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Test degradation scenarios in game days to validate user-journey continuity."
      ],
      "correct": 3,
      "explanation": "Treat dashboard analytics backend as a reliability-control decision, not an averages-only optimization. \"Test degradation scenarios in game days to validate user-journey continuity\" is correct since it mitigates cache fallback TTL too long for safety while keeping containment local. The decision remains valid given: A hot tenant currently consumes disproportionate worker capacity.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-029",
      "type": "multiple-choice",
      "question": "Case Helios: catalog browse service. Primary reliability risk is default fallback overloading primary path. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Separate auth/invariant checks from optional personalization dependencies.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "For catalog browse service, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Separate auth/invariant checks from optional personalization dependencies\" outperforms the alternatives because it targets default fallback overloading primary path and preserves safe recovery behavior. It is also the most compatible with The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-030",
      "type": "multiple-choice",
      "question": "Case Aurora: device sync endpoint. Primary reliability risk is degradation policy undocumented per endpoint. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Track degraded-mode activation and recovery as SLO-governed behavior.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Device sync endpoint should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Track degraded-mode activation and recovery as SLO-governed behavior\" is strongest because it directly addresses degradation policy undocumented per endpoint and improves repeatability under stress. This aligns with the extra condition (Queue age is rising even though average CPU appears normal).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: checkout page render path. Primary reliability risk is non-critical dependency blocking checkout. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Isolate critical path from optional dependencies with strict bulkhead pools.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Treat checkout page render path as a reliability-control decision, not an averages-only optimization. \"Isolate critical path from optional dependencies with strict bulkhead pools\" is correct since it mitigates non-critical dependency blocking checkout while keeping containment local. The decision remains valid given: A control-plane API is healthy but data-plane errors are increasing.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-032",
      "type": "multiple-choice",
      "question": "Case Pulse: profile feed API. Primary reliability risk is fallback mode with stale unsafe data. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Define fallback data-quality tiers and expose degraded semantics explicitly."
      ],
      "correct": 3,
      "explanation": "For profile feed API, prefer the option that prevents reoccurrence in Graceful Degradation & Dependency Isolation. \"Define fallback data-quality tiers and expose degraded semantics explicitly\" outperforms the alternatives because it targets fallback mode with stale unsafe data and preserves safe recovery behavior. It is also the most compatible with Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-033",
      "type": "multiple-choice",
      "question": "Case Forge: recommendation widget service. Primary reliability risk is kill switch missing for expensive feature. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Use kill switches to disable expensive non-core features within seconds.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "In Graceful Degradation & Dependency Isolation, recommendation widget service fails mainly through kill switch missing for expensive feature. The best choice is \"Use kill switches to disable expensive non-core features within seconds\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Legal/compliance constraints require explicit behavior in degraded mode.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-034",
      "type": "multiple-choice",
      "question": "Case Harbor: search autosuggest path. Primary reliability risk is bulkhead boundaries crossed by shared pools. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Return minimal safe responses rather than waiting on slow optional calls.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Search autosuggest path should be solved at the failure boundary named in Graceful Degradation & Dependency Isolation. \"Return minimal safe responses rather than waiting on slow optional calls\" is strongest because it directly addresses bulkhead boundaries crossed by shared pools and improves repeatability under stress. This aligns with the extra condition (Past incidents show this failure mode recurs every quarter).",
      "detailedExplanation": "Generalize from search autosuggest path to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-035",
      "type": "multiple-choice",
      "question": "Case Vector: invoice generation flow. Primary reliability risk is degraded mode hidden from clients. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Precompute essential fallback artifacts for high-traffic journeys.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Treat invoice generation flow as a reliability-control decision, not an averages-only optimization. \"Precompute essential fallback artifacts for high-traffic journeys\" is correct since it mitigates degraded mode hidden from clients while keeping containment local. The decision remains valid given: User trust impact is tied to visible inconsistency, not only downtime.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout page render path: signal points to bulkhead boundaries crossed by shared pools. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for checkout page render path is mismatched to bulkhead boundaries crossed by shared pools, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for checkout page render path is mismatched to bulkhead boundaries crossed by shared pools, creating repeat reliability incidents\" best matches checkout page render path by targeting bulkhead boundaries crossed by shared pools and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for checkout page render path:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Apply per-dependency timeouts and fallback chaining with bounded depth.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Graceful Degradation & Dependency Isolation: for \"incident diagnosis for checkout page render path:\", which next step is strongest under current constraints, \"Apply per-dependency timeouts and fallback chaining with bounded depth\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile feed API: signal points to degraded mode hidden from clients. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for profile feed API is mismatched to degraded mode hidden from clients, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Profile feed API is a two-step reliability decision. At stage 1, \"The design for profile feed API is mismatched to degraded mode hidden from clients, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around degraded mode hidden from clients.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for profile feed API: signal points\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Protect critical write flows from read-path degradation side effects.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Protect critical write flows from read-path degradation side effects\" best matches Given the diagnosis in \"incident diagnosis for profile feed API: signal points\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation widget service: signal points to partial outage causing full-page failure. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Graceful Degradation & Dependency Isolation, the best answer is \"The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents\". It is the option most directly aligned to partial outage causing full-page failure while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for recommendation widget service:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Test degradation scenarios in game days to validate user-journey continuity."
          ],
          "correct": 3,
          "explanation": "After diagnosing \"incident diagnosis for recommendation widget service:\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Test degradation scenarios in game days to validate user-journey continuity\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search autosuggest path: signal points to critical path coupled to optional enrichment. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for search autosuggest path is mismatched to critical path coupled to optional enrichment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Graceful Degradation & Dependency Isolation: for search autosuggest path, \"The design for search autosuggest path is mismatched to critical path coupled to optional enrichment, creating repeat reliability incidents\" is correct because it addresses critical path coupled to optional enrichment and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for search autosuggest path: signal\", which next change should be prioritized first?",
          "options": [
            "Separate auth/invariant checks from optional personalization dependencies.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Graceful Degradation & Dependency Isolation, the best answer is \"Separate auth/invariant checks from optional personalization dependencies\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for invoice generation flow: signal points to cache fallback TTL too long for safety. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for invoice generation flow is mismatched to cache fallback TTL too long for safety, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Invoice generation flow is a two-step reliability decision. At stage 1, \"The design for invoice generation flow is mismatched to cache fallback TTL too long for safety, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around cache fallback TTL too long for safety.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for invoice generation flow: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Track degraded-mode activation and recovery as SLO-governed behavior.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Track degraded-mode activation and recovery as SLO-governed behavior\" best matches With root cause identified for \"incident diagnosis for invoice generation flow: signal\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification preference API: signal points to default fallback overloading primary path. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for notification preference API is mismatched to default fallback overloading primary path, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Graceful Degradation & Dependency Isolation, the best answer is \"The design for notification preference API is mismatched to default fallback overloading primary path, creating repeat reliability incidents\". It is the option most directly aligned to default fallback overloading primary path while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident diagnosis for notification preference API:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Isolate critical path from optional dependencies with strict bulkhead pools.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "After diagnosing \"incident diagnosis for notification preference API:\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Isolate critical path from optional dependencies with strict bulkhead pools\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat attachment pipeline: signal points to degradation policy undocumented per endpoint. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for chat attachment pipeline is mismatched to degradation policy undocumented per endpoint, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Graceful Degradation & Dependency Isolation: for chat attachment pipeline, \"The design for chat attachment pipeline is mismatched to degradation policy undocumented per endpoint, creating repeat reliability incidents\" is correct because it addresses degradation policy undocumented per endpoint and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for chat attachment pipeline: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Define fallback data-quality tiers and expose degraded semantics explicitly."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Graceful Degradation & Dependency Isolation, the best answer is \"Define fallback data-quality tiers and expose degraded semantics explicitly\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for dashboard analytics backend: signal points to non-critical dependency blocking checkout. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for dashboard analytics backend is mismatched to non-critical dependency blocking checkout, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for dashboard analytics backend is mismatched to non-critical dependency blocking checkout, creating repeat reliability incidents\" best matches dashboard analytics backend by targeting non-critical dependency blocking checkout and lowering repeat risk.",
          "detailedExplanation": "Generalize from incident diagnosis for dashboard analytics backend: signal points to non-critical to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for dashboard analytics backend:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Use kill switches to disable expensive non-core features within seconds.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Graceful Degradation & Dependency Isolation: for Now that \"incident diagnosis for dashboard analytics backend:\" is diagnosed, what is the highest-leverage change to make now, \"Use kill switches to disable expensive non-core features within seconds\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog browse service: signal points to fallback mode with stale unsafe data. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for catalog browse service is mismatched to fallback mode with stale unsafe data, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Catalog browse service is a two-step reliability decision. At stage 1, \"The design for catalog browse service is mismatched to fallback mode with stale unsafe data, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around fallback mode with stale unsafe data.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for catalog browse service: signal\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Return minimal safe responses rather than waiting on slow optional calls.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Return minimal safe responses rather than waiting on slow optional calls\" best matches Using the diagnosis from \"incident diagnosis for catalog browse service: signal\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for device sync endpoint: signal points to kill switch missing for expensive feature. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for device sync endpoint is mismatched to kill switch missing for expensive feature, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Graceful Degradation & Dependency Isolation, the best answer is \"The design for device sync endpoint is mismatched to kill switch missing for expensive feature, creating repeat reliability incidents\". It is the option most directly aligned to kill switch missing for expensive feature while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for device sync endpoint: signal\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Precompute essential fallback artifacts for high-traffic journeys.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for device sync endpoint: signal\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Precompute essential fallback artifacts for high-traffic journeys\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize from graceful Degradation & Dependency Isolation to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout page render path: signal points to bulkhead boundaries crossed by shared pools. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for checkout page render path is mismatched to bulkhead boundaries crossed by shared pools, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Graceful Degradation & Dependency Isolation: for checkout page render path, \"The design for checkout page render path is mismatched to bulkhead boundaries crossed by shared pools, creating repeat reliability incidents\" is correct because it addresses bulkhead boundaries crossed by shared pools and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for checkout page render path:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Apply per-dependency timeouts and fallback chaining with bounded depth."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Graceful Degradation & Dependency Isolation, the best answer is \"Apply per-dependency timeouts and fallback chaining with bounded depth\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile feed API: signal points to degraded mode hidden from clients. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for profile feed API is mismatched to degraded mode hidden from clients, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for profile feed API is mismatched to degraded mode hidden from clients, creating repeat reliability incidents\" best matches profile feed API by targeting degraded mode hidden from clients and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for profile feed API: signal points\", what is the highest-leverage change to make now?",
          "options": [
            "Protect critical write flows from read-path degradation side effects.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Graceful Degradation & Dependency Isolation: for \"incident diagnosis for profile feed API: signal points\", what is the highest-leverage change to make now, \"Protect critical write flows from read-path degradation side effects\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation widget service: signal points to partial outage causing full-page failure. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Recommendation widget service is a two-step reliability decision. At stage 1, \"The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around partial outage causing full-page failure.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for recommendation widget service:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Test degradation scenarios in game days to validate user-journey continuity.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Test degradation scenarios in game days to validate user-journey continuity\" best matches With root cause identified for \"incident diagnosis for recommendation widget service:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search autosuggest path: signal points to critical path coupled to optional enrichment. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for search autosuggest path is mismatched to critical path coupled to optional enrichment, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Graceful Degradation & Dependency Isolation, the best answer is \"The design for search autosuggest path is mismatched to critical path coupled to optional enrichment, creating repeat reliability incidents\". It is the option most directly aligned to critical path coupled to optional enrichment while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for search autosuggest path: signal\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Separate auth/invariant checks from optional personalization dependencies.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "After diagnosing \"incident diagnosis for search autosuggest path: signal\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Separate auth/invariant checks from optional personalization dependencies\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for invoice generation flow: signal points to cache fallback TTL too long for safety. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for invoice generation flow is mismatched to cache fallback TTL too long for safety, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for invoice generation flow is mismatched to cache fallback TTL too long for safety, creating repeat reliability incidents\" best matches invoice generation flow by targeting cache fallback TTL too long for safety and lowering repeat risk.",
          "detailedExplanation": "Generalize from incident diagnosis for invoice generation flow: signal points to cache fallback TTL too to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident diagnosis for invoice generation flow: signal\" is diagnosed, what first move gives the best reliability impact?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Track degraded-mode activation and recovery as SLO-governed behavior."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Graceful Degradation & Dependency Isolation: for Now that \"incident diagnosis for invoice generation flow: signal\" is diagnosed, what first move gives the best reliability impact, \"Track degraded-mode activation and recovery as SLO-governed behavior\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification preference API: signal points to default fallback overloading primary path. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for notification preference API is mismatched to default fallback overloading primary path, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Notification preference API is a two-step reliability decision. At stage 1, \"The design for notification preference API is mismatched to default fallback overloading primary path, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around default fallback overloading primary path.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for notification preference API:\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Isolate critical path from optional dependencies with strict bulkhead pools.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Isolate critical path from optional dependencies with strict bulkhead pools\" best matches In the \"incident diagnosis for notification preference API:\" scenario, what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat attachment pipeline: signal points to degradation policy undocumented per endpoint. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for chat attachment pipeline is mismatched to degradation policy undocumented per endpoint, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Graceful Degradation & Dependency Isolation, the best answer is \"The design for chat attachment pipeline is mismatched to degradation policy undocumented per endpoint, creating repeat reliability incidents\". It is the option most directly aligned to degradation policy undocumented per endpoint while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for chat attachment pipeline: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Define fallback data-quality tiers and expose degraded semantics explicitly.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "After diagnosing \"incident diagnosis for chat attachment pipeline: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Define fallback data-quality tiers and expose degraded semantics explicitly\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for dashboard analytics backend: signal points to non-critical dependency blocking checkout. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for dashboard analytics backend is mismatched to non-critical dependency blocking checkout, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Graceful Degradation & Dependency Isolation: for dashboard analytics backend, \"The design for dashboard analytics backend is mismatched to non-critical dependency blocking checkout, creating repeat reliability incidents\" is correct because it addresses non-critical dependency blocking checkout and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for dashboard analytics backend:\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Use kill switches to disable expensive non-core features within seconds.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Graceful Degradation & Dependency Isolation, the best answer is \"Use kill switches to disable expensive non-core features within seconds\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog browse service: signal points to fallback mode with stale unsafe data. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for catalog browse service is mismatched to fallback mode with stale unsafe data, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for catalog browse service is mismatched to fallback mode with stale unsafe data, creating repeat reliability incidents\" best matches catalog browse service by targeting fallback mode with stale unsafe data and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident diagnosis for catalog browse service: signal\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Return minimal safe responses rather than waiting on slow optional calls."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Graceful Degradation & Dependency Isolation: for \"incident diagnosis for catalog browse service: signal\", what should change first before wider rollout, \"Return minimal safe responses rather than waiting on slow optional calls\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for device sync endpoint: signal points to kill switch missing for expensive feature. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for device sync endpoint is mismatched to kill switch missing for expensive feature, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Device sync endpoint is a two-step reliability decision. At stage 1, \"The design for device sync endpoint is mismatched to kill switch missing for expensive feature, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around kill switch missing for expensive feature.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for device sync endpoint: signal\", what first move gives the best reliability impact?",
          "options": [
            "Precompute essential fallback artifacts for high-traffic journeys.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Precompute essential fallback artifacts for high-traffic journeys\" best matches Given the diagnosis in \"incident diagnosis for device sync endpoint: signal\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout page render path: signal points to bulkhead boundaries crossed by shared pools. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for checkout page render path is mismatched to bulkhead boundaries crossed by shared pools, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Graceful Degradation & Dependency Isolation, the best answer is \"The design for checkout page render path is mismatched to bulkhead boundaries crossed by shared pools, creating repeat reliability incidents\". It is the option most directly aligned to bulkhead boundaries crossed by shared pools while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for checkout page render path:\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Apply per-dependency timeouts and fallback chaining with bounded depth.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for checkout page render path:\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Apply per-dependency timeouts and fallback chaining with bounded depth\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from graceful Degradation & Dependency Isolation to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile feed API: signal points to degraded mode hidden from clients. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for profile feed API is mismatched to degraded mode hidden from clients, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Graceful Degradation & Dependency Isolation: for profile feed API, \"The design for profile feed API is mismatched to degraded mode hidden from clients, creating repeat reliability incidents\" is correct because it addresses degraded mode hidden from clients and improves controllability.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for profile feed API: signal points\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Protect critical write flows from read-path degradation side effects.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Graceful Degradation & Dependency Isolation, the best answer is \"Protect critical write flows from read-path degradation side effects\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation widget service: signal points to partial outage causing full-page failure. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for recommendation widget service is mismatched to partial outage causing full-page failure, creating repeat reliability incidents\" best matches recommendation widget service by targeting partial outage causing full-page failure and lowering repeat risk.",
          "detailedExplanation": "Generalize from incident diagnosis for recommendation widget service: signal points to partial outage to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for recommendation widget service:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Test degradation scenarios in game days to validate user-journey continuity."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Graceful Degradation & Dependency Isolation: for Now that \"incident diagnosis for recommendation widget service:\" is diagnosed, which immediate adjustment best addresses the risk, \"Test degradation scenarios in game days to validate user-journey continuity\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search autosuggest path: signal points to critical path coupled to optional enrichment. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for search autosuggest path is mismatched to critical path coupled to optional enrichment, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Search autosuggest path is a two-step reliability decision. At stage 1, \"The design for search autosuggest path is mismatched to critical path coupled to optional enrichment, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around critical path coupled to optional enrichment.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident diagnosis for search autosuggest path: signal\" scenario, which next change should be prioritized first?",
          "options": [
            "Separate auth/invariant checks from optional personalization dependencies.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Separate auth/invariant checks from optional personalization dependencies\" best matches In the \"incident diagnosis for search autosuggest path: signal\" scenario, which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for invoice generation flow: signal points to cache fallback TTL too long for safety. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for invoice generation flow is mismatched to cache fallback TTL too long for safety, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Graceful Degradation & Dependency Isolation: for invoice generation flow, \"The design for invoice generation flow is mismatched to cache fallback TTL too long for safety, creating repeat reliability incidents\" is correct because it addresses cache fallback TTL too long for safety and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for invoice generation flow: signal\" scenario, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Track degraded-mode activation and recovery as SLO-governed behavior.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Graceful Degradation & Dependency Isolation, the best answer is \"Track degraded-mode activation and recovery as SLO-governed behavior\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-061",
      "type": "multi-select",
      "question": "For this scenario, which indicators most directly reveal cross-domain blast radius? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For this scenario, which indicators most directly reveal cross-domain blast radius, the highest-signal answer is a bundle of controls. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-062",
      "type": "multi-select",
      "question": "For this scenario, which controls reduce hidden single points of failure? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Graceful Degradation & Dependency Isolation, For this scenario, which controls reduce hidden single points of failure needs layered controls, not one silver bullet. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-063",
      "type": "multi-select",
      "question": "For this scenario, during partial failures, which practices improve diagnosis quality? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For this scenario, during partial failures, which practices improve diagnosis quality is intentionally multi-dimensional in Graceful Degradation & Dependency Isolation. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-064",
      "type": "multi-select",
      "question": "For this scenario, what belongs in a useful dependency failure taxonomy? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Graceful Degradation & Dependency Isolation: The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-065",
      "type": "multi-select",
      "question": "For this scenario, which patterns limit correlated failures across zones? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For this scenario, which patterns limit correlated failures across zones, the highest-signal answer is a bundle of controls. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-066",
      "type": "multi-select",
      "question": "For this scenario, which runbook elements increase incident execution reliability? (Select all that apply)",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Graceful Degradation & Dependency Isolation, For this scenario, which runbook elements increase incident execution reliability needs layered controls, not one silver bullet. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-067",
      "type": "multi-select",
      "question": "For this scenario, which signals should trigger graceful isolation first? (Select all that apply)",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For this scenario, which signals should trigger graceful isolation first is intentionally multi-dimensional in Graceful Degradation & Dependency Isolation. The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from signals should trigger graceful isolation first? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-068",
      "type": "multi-select",
      "question": "For this scenario, which architectural choices help contain tenant-induced overload? (Select all that apply)",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Graceful Degradation & Dependency Isolation: The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-069",
      "type": "multi-select",
      "question": "For this scenario, for reliability policies, which items should be explicit per endpoint? (Select all that apply)",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For this scenario, for reliability policies, which items should be explicit per endpoint, the highest-signal answer is a bundle of controls. The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-070",
      "type": "multi-select",
      "question": "For this scenario, which anti-patterns commonly enlarge outage blast radius? (Select all that apply)",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For this scenario, which anti-patterns commonly enlarge outage blast radius is intentionally multi-dimensional in Graceful Degradation & Dependency Isolation. The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from anti-patterns commonly enlarge outage blast radius? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-071",
      "type": "multi-select",
      "question": "For this scenario, what improves confidence in failover assumptions? (Select all that apply)",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Graceful Degradation & Dependency Isolation: The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-072",
      "type": "multi-select",
      "question": "For this scenario, which data is essential when classifying partial vs fail-stop incidents? (Select all that apply)",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For this scenario, which data is essential when classifying partial vs fail-stop incidents, the highest-signal answer is a bundle of controls. The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-073",
      "type": "multi-select",
      "question": "For this scenario, which controls improve safety when control-plane health is uncertain? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Graceful Degradation & Dependency Isolation, For this scenario, which controls improve safety when control-plane health is uncertain needs layered controls, not one silver bullet. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-074",
      "type": "multi-select",
      "question": "For this scenario, for critical writes, which guardrails reduce corruption risk under faults? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For this scenario, for critical writes, which guardrails reduce corruption risk under faults is intentionally multi-dimensional in Graceful Degradation & Dependency Isolation. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-075",
      "type": "multi-select",
      "question": "For this scenario, which recurring reviews keep reliability boundaries accurate over time? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Graceful Degradation & Dependency Isolation: The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-076",
      "type": "multi-select",
      "question": "For this scenario, which decisions help teams align on reliability trade-offs during incidents? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For this scenario, which decisions help teams align on reliability trade-offs during incidents, the highest-signal answer is a bundle of controls. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-077",
      "type": "multi-select",
      "question": "For this scenario, what evidence best shows a mitigation reduced recurrence risk? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Graceful Degradation & Dependency Isolation, For this scenario, what evidence best shows a mitigation reduced recurrence risk needs layered controls, not one silver bullet. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-078",
      "type": "numeric-input",
      "question": "If a service processes 4,200,000 requests/day and 0.22% violate reliability SLO, what is how many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "Graceful Degradation & Dependency Isolation expects quick quantitative triage: If a service processes 4,200,000 requests/day and 0 evaluates to 9240 requests. Any answer within +/-3% is acceptable.",
      "detailedExplanation": "Generalize from service processes 4,200,000 requests/day and 0 to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 4,200 and 000 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-079",
      "type": "numeric-input",
      "question": "If incident queue receives 1,800 items/min and drains 2,050 items/min, what is net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "The operational math for If incident queue receives 1,800 items/min and drains 2,050 items/min, what is net drain rate gives 250 items/min. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep every transformation in one unit system and check order of magnitude at the end. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 1,800 and 2,050 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-080",
      "type": "numeric-input",
      "question": "If retry policy adds 0.35 extra attempts per request at 60,000 req/sec, what is effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "For If retry policy adds 0, the computed target in Graceful Degradation & Dependency Isolation is 81000 attempts/sec. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 0.35 and 60,000 in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-081",
      "type": "numeric-input",
      "question": "If failover takes 18 seconds and happens 21 times/day, what is total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "Graceful Degradation & Dependency Isolation expects quick quantitative triage: If failover takes 18 seconds and happens 21 times/day, what is total failover seconds/day evaluates to 378 seconds. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "Generalize from failover takes 18 seconds and happens 21 times/day to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 18 seconds and 21 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-082",
      "type": "numeric-input",
      "question": "If target p99 latency is 700ms; observed p99 is 980ms, what is percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for If target p99 latency is 700ms; observed p99 is 980ms, what is percent over target gives 40 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 700ms and 980ms should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-083",
      "type": "numeric-input",
      "question": "Evaluate this prompt: if 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for Evaluate this prompt: if 31% of 120,000 requests/min are critical-path, how many critical requests/min: 37200 requests/min. Answers within +/-2% show correct directional reasoning for Graceful Degradation & Dependency Isolation.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 31 and 120,000 in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-084",
      "type": "numeric-input",
      "question": "If error rate drops from 1.2% to 0.3%, what is percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For If error rate drops from 1, the computed target in Graceful Degradation & Dependency Isolation is 75 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 1.2 and 0.3 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-085",
      "type": "numeric-input",
      "question": "If a 7-node quorum system requires majority writes, what is minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Graceful Degradation & Dependency Isolation expects quick quantitative triage: If a 7-node quorum system requires majority writes, what is minimum acknowledgements required evaluates to 4 acks. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 7 and 4 in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-086",
      "type": "numeric-input",
      "question": "If backlog is 48,000 tasks and net drain is 320 tasks/min, what is minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "The operational math for If backlog is 48,000 tasks and net drain is 320 tasks/min, what is minutes to clear backlog gives 150 minutes. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 48,000 and 320 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-087",
      "type": "numeric-input",
      "question": "If a system with 14 zones has 2 unavailable, what is what percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for If a system with 14 zones has 2 unavailable, what is what percent remain available: 85.71 %. Answers within +/-30% show correct directional reasoning for Graceful Degradation & Dependency Isolation.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 14 and 2 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-088",
      "type": "numeric-input",
      "question": "If mTTR improved from 45 min to 30 min, what is percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For If mTTR improved from 45 min to 30 min, what is percent reduction, the computed target in Graceful Degradation & Dependency Isolation is 33.33 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep every transformation in one unit system and check order of magnitude at the end. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 45 min and 30 min in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-089",
      "type": "numeric-input",
      "question": "Evaluate this prompt: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "Graceful Degradation & Dependency Isolation expects quick quantitative triage: Evaluate this prompt: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day evaluates to 225000 operations. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "Generalize from if 9% of 2,500,000 daily operations need manual recovery checks, checks/day to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. If values like 9 and 2,500 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle. Use a graceful degradation & dependency isolation perspective.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order a reliability response lifecycle, the correct ordering runs from Detect and scope affected fault domains to Validate recovery and harden recurrence defenses. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-091",
      "type": "ordering",
      "question": "Order from lowest to highest reliability risk. Focus on graceful degradation & dependency isolation tradeoffs.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Graceful Degradation & Dependency Isolation should start with Isolated dependency with fallback and budget and end with Implicit dependency with no failure policy. Order from lowest to highest reliability risk rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Place obvious extremes first, then sort the middle by pairwise comparison. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-092",
      "type": "ordering",
      "question": "For graceful degradation & dependency isolation, order failover safety steps.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Verify candidate health and freshness must happen before Run failback readiness checks before restoration. That ordering matches incident-safe flow in Graceful Degradation & Dependency Isolation.",
      "detailedExplanation": "Generalize from order failover safety steps to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-093",
      "type": "ordering",
      "question": "Within graceful degradation & dependency isolation, order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Graceful Degradation & Dependency Isolation emphasizes safe recovery order. Beginning at No admission limits and finishing at Priority-aware shedding plus per-domain concurrency bounds keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-094",
      "type": "ordering",
      "question": "In this graceful degradation & dependency isolation context, order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For In this graceful degradation & dependency isolation context, order data recovery execution, the correct ordering runs from Select recovery point by RPO target to Promote and re-enable writes with monitoring. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-095",
      "type": "ordering",
      "question": "Considering graceful degradation & dependency isolation, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Graceful Degradation & Dependency Isolation should start with Define SLIs tied to user impact and end with Review incidents and close corrective actions. Considering graceful degradation & dependency isolation, order reliability operations loop rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-096",
      "type": "ordering",
      "question": "Put these in order of increasing blast radius. (Graceful Degradation & Dependency Isolation context)",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Single process failure must happen before Cross-region control-plane failure. That ordering matches incident-safe flow in Graceful Degradation & Dependency Isolation.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-097",
      "type": "ordering",
      "question": "Order retry-policy maturity. (graceful degradation & dependency isolation lens)",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Graceful Degradation & Dependency Isolation emphasizes safe recovery order. Beginning at Fixed immediate retries and finishing at Jittered backoff with retry budgets and telemetry keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Order by relative scale and bottleneck effect, then validate neighboring items. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-098",
      "type": "ordering",
      "question": "Order degradation sophistication. Use a graceful degradation & dependency isolation perspective.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order degradation sophistication, the correct ordering runs from Undocumented ad hoc fallback to Automated policy-driven degradation with user semantics. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-099",
      "type": "ordering",
      "question": "Order incident command rigor. Focus on graceful degradation & dependency isolation tradeoffs.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Graceful Degradation & Dependency Isolation should start with Ad hoc responders with no roles and end with Role-defined operations plus decision log and action tracking. Order incident command rigor rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-gd-100",
      "type": "ordering",
      "question": "For graceful degradation & dependency isolation, order reliability validation confidence.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Single success in staging must happen before Sustained recovery plus recurrence drill pass. That ordering matches incident-safe flow in Graceful Degradation & Dependency Isolation.",
      "detailedExplanation": "Generalize from order reliability validation confidence to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "graceful-degradation-and-dependency-isolation"],
      "difficulty": "staff-level"
    }
  ]
}
