{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 5,
  "chapterTitle": "Hotspots, Sharding & Work Distribution",
  "chapterDescription": "Mitigating compute hotspots with better partitioning, fair work distribution, and sharding strategies that preserve correctness and resilience.",
  "problems": [
    {
      "id": "sc-hs-001",
      "type": "multiple-choice",
      "question": "A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The imbalance worsened immediately after a minor traffic increase.",
      "options": [
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-002",
      "type": "multiple-choice",
      "question": "A feed fanout workers is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Only two partitions now drive most error budget burn.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-003",
      "type": "multiple-choice",
      "question": "A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Autoscaling adds nodes but hotspot pressure remains localized.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-004",
      "type": "multiple-choice",
      "question": "A image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Rollback of recent deploy did not remove the skew pattern.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-005",
      "type": "multiple-choice",
      "question": "A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Premium-tenant traffic growth made the issue visible this week.",
      "options": [
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        }
      ]
    },
    {
      "id": "sc-hs-006",
      "type": "multiple-choice",
      "question": "A ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Top-key concentration reports align with latency regression.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-007",
      "type": "multiple-choice",
      "question": "A inventory reservation workers is showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Queue age outliers are isolated to a subset of workers.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-008",
      "type": "multiple-choice",
      "question": "A chat message delivery workers is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Dependency saturation appears only on hot partitions.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-009",
      "type": "multiple-choice",
      "question": "A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Cold partitions remain underutilized during the incident.",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-010",
      "type": "multiple-choice",
      "question": "A recommendation candidate generators is showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Retry amplification is concentrated on the same key range.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-hs-011",
      "type": "multiple-choice",
      "question": "A video transcoding workers is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The same shard repeatedly triggers paging alerts.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-012",
      "type": "multiple-choice",
      "question": "A analytics stream processors is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Average fleet metrics look normal despite p99 failures.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-013",
      "type": "multiple-choice",
      "question": "A order enrichment workers is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Regional traffic shift exposed long-standing partition imbalance.",
      "options": [
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-014",
      "type": "multiple-choice",
      "question": "A OCR document processors is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Worker saturation correlates with one low-cardinality attribute.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hs-015",
      "type": "multiple-choice",
      "question": "A shipping quote calculators is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Fairness SLOs broke before global throughput limits.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-016",
      "type": "multiple-choice",
      "question": "A fraud feature extractors is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Canary traffic was healthy until one shard became dominant.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-017",
      "type": "multiple-choice",
      "question": "A profile denormalization workers is showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Backlog growth is nonlinear in only one partition family.",
      "options": [
        "Apply route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-018",
      "type": "multiple-choice",
      "question": "A email rendering fleet is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Hotspot symptoms persist across two independent deployments.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-019",
      "type": "multiple-choice",
      "question": "A IoT event processors is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Top-tenant bursts now crowd out long-tail workloads.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-020",
      "type": "multiple-choice",
      "question": "A audit ingestion workers is showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Per-shard tail latency spread widened 6x overnight.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-hs-021",
      "type": "multiple-choice",
      "question": "A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? Observability confirms skew rather than fleet-wide exhaustion.",
      "options": [
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-022",
      "type": "multiple-choice",
      "question": "A feed fanout workers is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Node churn increased remap concentration to a few workers.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-023",
      "type": "multiple-choice",
      "question": "A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? A single celebrity key dominates read fanout paths.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-024",
      "type": "multiple-choice",
      "question": "A image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Queue drain rate fell even though worker count increased.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hs-025",
      "type": "multiple-choice",
      "question": "A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Latency improves only when hotspot key traffic drops.",
      "options": [
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-026",
      "type": "multiple-choice",
      "question": "A ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Shard-level p99 remains unstable after autoscaling events.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-027",
      "type": "multiple-choice",
      "question": "A inventory reservation workers is showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? The incident reproduces in load tests with skewed inputs.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hs-028",
      "type": "multiple-choice",
      "question": "A chat message delivery workers is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Low-priority queues starve while one tenant remains unconstrained.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-029",
      "type": "multiple-choice",
      "question": "A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Cross-zone traffic balancing did not change hotspot behavior.",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-030",
      "type": "multiple-choice",
      "question": "A recommendation candidate generators is showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Global retries made local hot partitions significantly worse.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-hs-031",
      "type": "multiple-choice",
      "question": "A video transcoding workers is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? Recent partition-key change reduced cardinality unexpectedly.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-032",
      "type": "multiple-choice",
      "question": "A analytics stream processors is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Periodic campaign bursts repeatedly hit identical key ranges.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-033",
      "type": "multiple-choice",
      "question": "A order enrichment workers is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Hot partitions hit memory pressure while others stay cool.",
      "options": [
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-034",
      "type": "multiple-choice",
      "question": "A OCR document processors is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Error concentration is tied to deterministic hash collisions.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-035",
      "type": "multiple-choice",
      "question": "A shipping quote calculators is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? The same work class monopolizes lease renewals under load.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-hs-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first under peak traffic?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness during deploy churn?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action with tenant fairness constraints?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a inventory reservation workers, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck while preserving ordering guarantees?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery workers, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers without increasing retry storms?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a notification dispatch fleet, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "notification dispatch fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming skew in notification dispatch fleet, what mitigation should be implemented first during cross-zone failover?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for notification dispatch fleet: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a recommendation candidate generators, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "recommendation candidate generators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "Which next change best reduces hotspot pressure in recommendation candidate generators without breaking correctness for celebrity-key bursts?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for recommendation candidate generators: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a video transcoding workers, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "video transcoding workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "With root cause verified in video transcoding workers, what is the highest-leverage follow-up action under uneven task runtimes?",
          "options": [
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for video transcoding workers: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a analytics stream processors, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "analytics stream processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "For this analytics stream processors incident, which mitigation most directly addresses the partition bottleneck while keeping p99 SLO?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for analytics stream processors: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a order enrichment workers, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "order enrichment workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in order enrichment workers before adding more workers?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for order enrichment workers: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a OCR document processors, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "OCR document processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming skew in OCR document processors, what mitigation should be implemented first with strict cost caps?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for OCR document processors: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a shipping quote calculators, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "shipping quote calculators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "Which next change best reduces hotspot pressure in shipping quote calculators without breaking correctness during campaign spikes?",
          "options": [
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for shipping quote calculators: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a fraud feature extractors, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "fraud feature extractors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "With root cause verified in fraud feature extractors, what is the highest-leverage follow-up action for mixed read/write load?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for fraud feature extractors: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a profile denormalization workers, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "profile denormalization workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "For this profile denormalization workers incident, which mitigation most directly addresses the partition bottleneck with dependency limits?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for profile denormalization workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a email rendering fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "email rendering fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in email rendering fleet without global locks?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for email rendering fleet: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a IoT event processors, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "IoT event processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming skew in IoT event processors, what mitigation should be implemented first during shard migration?",
          "options": [
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for IoT event processors: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a audit ingestion workers, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "audit ingestion workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "Which next change best reduces hotspot pressure in audit ingestion workers without breaking correctness with long-tail tenant protection?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for audit ingestion workers: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a tenant-aware job queue, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "tenant-aware job queue shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "With root cause verified in tenant-aware job queue, what is the highest-leverage follow-up action before next traffic wave?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for tenant-aware job queue: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a feed fanout workers, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "feed fanout workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "For this feed fanout workers incident, which mitigation most directly addresses the partition bottleneck while keeping idempotent retries?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for feed fanout workers: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a search indexing pipeline, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "search indexing pipeline shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in search indexing pipeline under control-plane limits?",
          "options": [
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for search indexing pipeline: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first for low-cardinality keys?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness with stale-ring risk?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action while minimizing remap impact?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a inventory reservation workers, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck with queue-age regression?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery workers, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers during incident mitigation?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-061",
      "type": "multi-select",
      "question": "Which signals most directly indicate partition hotspots? (Select all that apply)",
      "options": [
        "Per-partition queue age outliers",
        "High variance in shard CPU/utilization",
        "Flat global average latency only",
        "Top-key request concentration trends"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Hotspots appear as skew and outliers; averages hide them.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-062",
      "type": "multi-select",
      "question": "Which techniques help mitigate hot-key write pressure? (Select all that apply)",
      "options": [
        "Key salting with bounded fan-in merge",
        "Tenant isolation for heavy writers",
        "One global writer lock for all keys",
        "Adaptive shard split for sustained hotspots"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Use controlled distribution tactics that preserve semantics.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-063",
      "type": "multi-select",
      "question": "Work stealing is most effective when which conditions hold? (Select all that apply)",
      "options": [
        "Tasks are mostly independent",
        "Fairness/priority constraints are explicit",
        "Strict total order across all tasks is mandatory",
        "Steal limits avoid starving original queues"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Work stealing helps imbalance, but fairness and ordering constraints matter.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-064",
      "type": "multi-select",
      "question": "For consistent hashing, which design choices reduce remap pain during scale events? (Select all that apply)",
      "options": [
        "Use virtual nodes per physical node",
        "Keep key-space mapping deterministic",
        "Rehash all keys on every deployment",
        "Balance token ownership by capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Virtual nodes and deterministic capacity-aware ownership smooth membership changes.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-065",
      "type": "multi-select",
      "question": "Which trade-offs are true for ordered vs unordered processing? (Select all that apply)",
      "options": [
        "Global ordering usually reduces parallelism",
        "Partition-local ordering can scale better",
        "Ordering requirements never affect throughput",
        "Relaxing order can simplify hotspot mitigation"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ordering guarantees can constrain partitioning and concurrency.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-066",
      "type": "multi-select",
      "question": "Which controls prevent one tenant from starving shared worker pools? (Select all that apply)",
      "options": [
        "Per-tenant concurrency quotas",
        "Weighted fair scheduling",
        "Infinite retries for premium tenants",
        "Backpressure per tenant queue"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Fairness controls enforce isolation in shared systems.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-067",
      "type": "multi-select",
      "question": "Good shard rebalancing safety practices include which? (Select all that apply)",
      "options": [
        "Move data incrementally with canary checks",
        "Track read/write error rates during moves",
        "Do all shard moves simultaneously",
        "Keep rollback path for token ownership"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Controlled migration lowers risk and improves recoverability.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-068",
      "type": "multi-select",
      "question": "Which patterns indicate skew from low-cardinality partition keys? (Select all that apply)",
      "options": [
        "Few partitions dominate traffic",
        "Long-tail partitions remain mostly idle",
        "Uniform per-partition throughput",
        "Frequent hotspot rotation tied to single attributes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Low cardinality concentrates load and causes persistent imbalance.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-069",
      "type": "multi-select",
      "question": "Which metrics are useful before introducing dynamic shard splitting? (Select all that apply)",
      "options": [
        "Per-shard QPS growth slope",
        "Per-shard storage growth and compaction cost",
        "Office hours attendance",
        "Per-shard p99 and queue age"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Splitting should be triggered from sustained load and latency evidence.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-070",
      "type": "multi-select",
      "question": "For queue-based work distribution, which practices reduce stragglers? (Select all that apply)",
      "options": [
        "Bound max lease duration with renewals",
        "Prefer smaller task chunks where possible",
        "Assign all largest tasks to one worker",
        "Allow speculative retry for stalled tasks with idempotency"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Lease management, chunking, and safe speculation improve tail completion time.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-071",
      "type": "multi-select",
      "question": "Which statements about hotspot caches are valid? (Select all that apply)",
      "options": [
        "Replicated read caches can absorb hot-key fanout",
        "Write-through caches can still bottleneck origin writes",
        "Caches remove need for consistency policy",
        "Cache invalidation cost must be included in design"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Caches help read hotspots but dont eliminate consistency and write limits.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-072",
      "type": "multi-select",
      "question": "Which anti-patterns worsen hotspot incidents? (Select all that apply)",
      "options": [
        "Scaling only on global averages",
        "Ignoring top-N key concentration reports",
        "Using partition-level dashboards and alerts",
        "Removing fairness controls under pressure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ignoring skew signals and fairness causes repeated hotspot failures.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-073",
      "type": "multi-select",
      "question": "When choosing a partition key, which properties are generally desirable? (Select all that apply)",
      "options": [
        "High cardinality with stable distribution",
        "Alignment with dominant access pattern",
        "Hard dependence on current fleet size",
        "Ability to evolve when workload shifts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "A resilient key choice balances distribution and access efficiency over time.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-074",
      "type": "multi-select",
      "question": "Which steps improve confidence in re-sharding plans? (Select all that apply)",
      "options": [
        "Simulate remap percentage before rollout",
        "Estimate migration bandwidth and duration",
        "Skip data integrity verification to move faster",
        "Define abort criteria and rollback checkpoints"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Dry-run analysis and explicit guardrails reduce migration risk.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-075",
      "type": "multi-select",
      "question": "Which strategies help preserve correctness during unordered parallel execution? (Select all that apply)",
      "options": [
        "Idempotent handlers",
        "Commutative updates where possible",
        "Assume exactly-once delivery by default",
        "Deduplication keys on side effects"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correctness under parallelism needs idempotency and dedupe controls.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-076",
      "type": "multi-select",
      "question": "Which signals suggest work stealing is harming fairness? (Select all that apply)",
      "options": [
        "Low-priority queues never drain",
        "High-priority queues remain bounded",
        "Tenant latency SLO violations become uneven",
        "Stealers consistently drain same source queues"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Fairness regressions show up as starvation and uneven tenant latency.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-077",
      "type": "multi-select",
      "question": "Which actions reduce retry-amplified partition hotspots? (Select all that apply)",
      "options": [
        "Partition-scoped retry budgets",
        "Exponential backoff with jitter",
        "Immediate synchronized retries from all clients",
        "Circuit breaking for saturated partitions"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Budgeted retries and jitter limit synchronized pressure on hot partitions.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-hs-078",
      "type": "numeric-input",
      "question": "A queue has 1,200,000 jobs. Worker fleet drains 28,000 jobs/min while producers add 16,000 jobs/min. How many minutes to clear backlog?",
      "answer": 100,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Net drain is 12,000 jobs/min. 1,200,000 / 12,000 = 100 minutes.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hs-079",
      "type": "numeric-input",
      "question": "A hot partition handles 18,000 rps but safe target is 7,500 rps/partition. Minimum partitions needed for that key range?",
      "answer": 3,
      "unit": "partitions",
      "tolerance": 0,
      "explanation": "18,000 / 7,500 = 2.4, so round up to 3 partitions.",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hs-080",
      "type": "numeric-input",
      "question": "Consistent hash ring has 240 virtual nodes over 12 physical nodes. Average virtual nodes per physical node?",
      "answer": 20,
      "unit": "vnodes",
      "tolerance": 0,
      "explanation": "240 / 12 = 20 virtual nodes per physical node.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-081",
      "type": "numeric-input",
      "question": "Top 1% of keys account for 42% of traffic at 90,000 rps total. How much rps does top 1% represent?",
      "answer": 37800,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.42 * 90,000 = 37,800 rps.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-082",
      "type": "numeric-input",
      "question": "A rebalancing plan moves 3.6 TB at sustained 300 MB/s. Approximate hours to complete transfer?",
      "answer": 3.33,
      "unit": "hours",
      "tolerance": 0.08,
      "explanation": "3.6 TB  3,600,000 MB. 3,600,000 / 300 = 12,000s = 3.33 hours.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-083",
      "type": "numeric-input",
      "question": "Worker pool has 64 workers. Fair scheduler reserves 25% capacity for premium tier. How many workers are effectively reserved?",
      "answer": 16,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "0.25 * 64 = 16 workers.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-084",
      "type": "numeric-input",
      "question": "Shard A p99 is 1,400ms, shard B p99 is 220ms. By what multiple is shard A p99 higher?",
      "answer": 6.36,
      "unit": "x",
      "tolerance": 0.1,
      "explanation": "1,400 / 220 = 6.36x.",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-085",
      "type": "numeric-input",
      "question": "A queue system processes 75,000 jobs/min with 300 workers. If per-worker throughput drops 20% from skew, effective jobs/min?",
      "answer": 60000,
      "unit": "jobs/min",
      "tolerance": 0.01,
      "explanation": "Effective throughput = 75,000 * 0.8 = 60,000 jobs/min.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hs-086",
      "type": "numeric-input",
      "question": "Re-sharding reduces remapped keys from 48% to 14%. How many percentage points improvement?",
      "answer": 34,
      "unit": "percentage points",
      "tolerance": 0,
      "explanation": "48 - 14 = 34 percentage points.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-087",
      "type": "numeric-input",
      "question": "At 24 partitions, one partition holds 19% of load. Ideal equal share would be what percent per partition?",
      "answer": 4.17,
      "unit": "%",
      "tolerance": 0.05,
      "explanation": "Equal share is 100 / 24 = 4.17% per partition.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-hs-088",
      "type": "numeric-input",
      "question": "A worker lease timeout is 45s. Average task runtime is 18s. Timeout should be at least how many times average runtime?",
      "answer": 2.5,
      "unit": "x",
      "tolerance": 0.05,
      "explanation": "45 / 18 = 2.5x.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-089",
      "type": "numeric-input",
      "question": "A hotspot mitigation lowers tail latency from 2,400ms to 900ms. What percent reduction is this?",
      "answer": 62.5,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "(2,400 - 900) / 2,400 = 62.5% reduction.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-090",
      "type": "ordering",
      "question": "Order a practical hotspot incident response flow.",
      "items": [
        "Detect skew with partition-level metrics",
        "Contain impact with fairness/rate controls",
        "Apply partition/work-distribution fix",
        "Validate and codify prevention guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Contain blast radius first, then fix root cause and institutionalize guardrails.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-091",
      "type": "ordering",
      "question": "Order partition-key design steps from first to last.",
      "items": [
        "Map dominant access patterns",
        "Evaluate key cardinality/distribution",
        "Simulate growth and hotspot scenarios",
        "Finalize with evolution strategy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Design keys from workload access first, then validate future skew resilience.",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-092",
      "type": "ordering",
      "question": "Order by increasing remap impact during node membership changes.",
      "items": [
        "Modulo hashing",
        "Naive range rebalance",
        "Consistent hashing",
        "Consistent hashing with virtual nodes"
      ],
      "correctOrder": [3, 2, 1, 0],
      "explanation": "Virtual-node consistent hashing tends to minimize remap impact best.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-093",
      "type": "ordering",
      "question": "Order work distribution strategies from least to most fairness-aware.",
      "items": [
        "Single global FIFO without tenant isolation",
        "Round-robin worker assignment",
        "Weighted fair queues by tenant",
        "Weighted fair queues with per-tenant quotas and backpressure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Fairness increases as explicit tenant controls are added.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-094",
      "type": "ordering",
      "question": "Order shard migration rollout from safest to riskiest.",
      "items": [
        "Canary move with rollback checkpoints",
        "Incremental batch moves",
        "Large single-wave migration",
        "All shards moved at once"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer migration is gradual and observable with rollback capability.",
      "detailedExplanation": "Compare relative scale first, then confirm neighboring items pairwise to lock in the order. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-095",
      "type": "ordering",
      "question": "Order by strongest hotspot diagnosis quality.",
      "items": [
        "Global average CPU only",
        "Per-node CPU averages",
        "Per-partition utilization with top-key analysis",
        "Per-partition utilization plus queue age and tail latency decomposition"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnosis quality improves with granular skew and latency decomposition.",
      "detailedExplanation": "Order by relative impact rather than exact values, then verify the sequence one boundary at a time. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-096",
      "type": "ordering",
      "question": "Order queue processing controls for straggler reduction.",
      "items": [
        "Add task lease/heartbeat",
        "Bound task size where possible",
        "Enable selective speculative retry",
        "Tune retry dedupe/idempotency guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Progressive controls reduce long-tail completions safely.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-hs-097",
      "type": "ordering",
      "question": "Order by increasing ordering guarantee strength.",
      "items": [
        "Unordered processing",
        "Per-partition ordering",
        "Per-tenant ordering",
        "Global total ordering"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger ordering guarantees generally reduce available parallelism.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-098",
      "type": "ordering",
      "question": "Order mitigation options for hot tenant isolation from quickest to slowest to implement.",
      "items": [
        "Per-tenant throttles",
        "Weighted scheduler tuning",
        "Dedicated partition pool",
        "Full data model repartitioning"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Incremental controls are usually faster than repartitioning changes.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-099",
      "type": "ordering",
      "question": "Order by increasing risk of unfair starvation.",
      "items": [
        "Fair queues with quotas",
        "Work stealing with fairness caps",
        "Unbounded stealing from low-priority queues",
        "No fairness controls with bursty premium traffic"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Starvation risk grows as fairness protections are removed.",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-hs-100",
      "type": "ordering",
      "question": "Order validation loop after hotspot fixes.",
      "items": [
        "Compare pre/post partition heatmaps",
        "Check tail latency and error budgets",
        "Run controlled load replay",
        "Lock in alerts for recurrence signals"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Validate skew reduction, user impact, stress behavior, then codify monitoring.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    }
  ]
}
