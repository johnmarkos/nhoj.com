{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 5,
  "chapterTitle": "Hotspots, Sharding & Work Distribution",
  "chapterDescription": "Mitigating compute hotspots with better partitioning, fair work distribution, and sharding strategies that preserve correctness and resilience.",
  "problems": [
    {
      "id": "sc-hs-001",
      "type": "multiple-choice",
      "question": "A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The imbalance worsened immediately after a minor traffic increase.",
      "options": [
        "Introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-002",
      "type": "multiple-choice",
      "question": "Feed fanout workers are showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Only two partitions now drive most error budget burn.",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A feed fanout workers is showing sustained p99 regressions due to celebrity-key traffic skew, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-003",
      "type": "multiple-choice",
      "question": "A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Autoscaling adds nodes but hotspot pressure remains localized.",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-004",
      "type": "multiple-choice",
      "question": "An image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Rollback of recent deploy did not remove the skew pattern.",
      "options": [
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For an image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-005",
      "type": "multiple-choice",
      "question": "A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Premium-tenant traffic growth made the issue visible this week.",
      "options": [
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-006",
      "type": "multiple-choice",
      "question": "An ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Top-key concentration reports align with latency regression.",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For an ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize from ad auction compute tier is showing sustained p99 regressions due to sequential IDs to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-007",
      "type": "multiple-choice",
      "question": "Inventory reservation workers are showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Queue age outliers are isolated to a subset of workers.",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For inventory reservation workers showing sustained p99 regressions due to shard-per-region imbalance, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-008",
      "type": "multiple-choice",
      "question": "Chat message delivery workers are showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Dependency saturation appears only on hot partitions.",
      "options": [
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A chat message delivery workers is showing sustained p99 regressions due to uneven worker assignment, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-009",
      "type": "multiple-choice",
      "question": "A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Cold partitions remain underutilized during the incident.",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-010",
      "type": "multiple-choice",
      "question": "Recommendation candidate generators are showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Retry amplification is concentrated on the same key range.",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A recommendation candidate generators is showing sustained p99 regressions due to retry storms targeting same partition, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-011",
      "type": "multiple-choice",
      "question": "Video transcoding workers are showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The same shard repeatedly triggers paging alerts.",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A video transcoding workers is showing sustained p99 regressions due to low-cardinality partition key, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-012",
      "type": "multiple-choice",
      "question": "Analytics stream processors are showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Average fleet metrics look normal despite p99 failures.",
      "options": [
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For analytics stream processors showing sustained p99 regressions due to celebrity-key traffic skew, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-013",
      "type": "multiple-choice",
      "question": "Order enrichment workers are showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Regional traffic shift exposed long-standing partition imbalance.",
      "options": [
        "Split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For order enrichment workers showing sustained p99 regressions due to time-bucket key with synchronized spikes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-014",
      "type": "multiple-choice",
      "question": "OCR document processors are showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Worker saturation correlates with one low-cardinality attribute.",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For OCR document processors showing sustained p99 regressions due to sticky hash ring after node churn, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-015",
      "type": "multiple-choice",
      "question": "Shipping quote calculators are showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Fairness SLOs broke before global throughput limits.",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A shipping quote calculators is showing sustained p99 regressions due to single hot tenant dominating writes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Generalize from shipping quote calculators is showing sustained p99 regressions due to single hot to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-016",
      "type": "multiple-choice",
      "question": "A fraud feature extractors is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Canary traffic was healthy until one shard became dominant.",
      "options": [
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A fraud feature extractors is showing sustained p99 regressions due to sequential IDs concentrating recent traffic, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-017",
      "type": "multiple-choice",
      "question": "Profile denormalization workers are showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Backlog growth is nonlinear in only one partition family.",
      "options": [
        "Route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A profile denormalization workers is showing sustained p99 regressions due to shard-per-region imbalance, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-018",
      "type": "multiple-choice",
      "question": "An email rendering fleet is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Hotspot symptoms persist across two independent deployments.",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For an email rendering fleet is showing sustained p99 regressions due to uneven worker assignment, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-019",
      "type": "multiple-choice",
      "question": "IoT event processors are showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Top-tenant bursts now crowd out long-tail workloads.",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For IoT event processors showing sustained p99 regressions due to straggler tasks on large partitions, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-020",
      "type": "multiple-choice",
      "question": "Audit ingestion workers are showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Per-shard tail latency spread widened 6x overnight.",
      "options": [
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For audit ingestion workers showing sustained p99 regressions due to retry storms targeting same partition, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize from audit ingestion workers is showing sustained p99 regressions due to retry storms to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 6x should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-021",
      "type": "multiple-choice",
      "question": "A multi-tenant reconciliation queue has one shard at 92% CPU while others sit under 20%, and the hot shard owns most enterprise tenants. Which intervention is strongest for the next deploy?",
      "options": [
        "Re-key partitions with a tenant+workload salt (or composite key) so heavy tenants split across shards while preserving ordering boundaries.",
        "Increase global retries and keep the current partition strategy.",
        "Route all enterprise jobs to the currently fastest worker for cache warmth.",
        "Add workers without changing partitioning or shard-level telemetry."
      ],
      "correct": 0,
      "explanation": "This incident is partition-skew, not fleet under-capacity. Re-keying with a composite/salted key directly addresses load concentration while keeping correctness boundaries explicit; retries or blind scaling keep the same hot partition behavior.",
      "detailedExplanation": "The operational clue is asymmetric shard pressure, not global saturation. Favor the change that redistributes key ownership with bounded remap risk, then validate by shard-level p95/p99 spread and queue-depth convergence. Common pitfall: adding capacity while leaving the skew source untouched.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-022",
      "type": "multiple-choice",
      "question": "A feed fanout service uses consistent hashing; after node churn, two virtual-node ranges now absorb most celebrity traffic and p99 latency spikes. What is the strongest next step?",
      "options": [
        "Increase global retries and keep the same ring layout.",
        "Pin celebrity traffic to one warm worker.",
        "Scale worker count only and defer ring changes.",
        "Rebalance the hash ring with more virtual nodes (and controlled remap) to spread celebrity-key load."
      ],
      "correct": 3,
      "explanation": "The failure mode is ring imbalance after churn. Rebalancing with higher virtual-node granularity and controlled remap directly reduces concentration on celebrity keys; retries or single-worker pinning amplify hotspots.",
      "detailedExplanation": "Consistent-hash incidents should be solved at placement policy, not only at worker count. Choose the action that restores even key distribution and preserves predictable remap behavior during future churn. Common pitfall: treating hotspot traffic as a retry problem instead of a distribution problem.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-023",
      "type": "multiple-choice",
      "question": "A recommendation fanout service shards by `region_id`; one region now carries 68% of traffic and queue age is rising only on that shard. Which next step is strongest?",
      "options": [
        "Pin the hottest region to one pre-warmed worker for locality.",
        "Add generic workers and keep current keying unchanged.",
        "Split dominant regions into finer partition keys (or dedicated shards) while keeping ordering boundaries explicit.",
        "Increase retry depth so stragglers eventually catch up."
      ],
      "correct": 2,
      "explanation": "This is concentrated-key skew. Repartitioning heavy regions is the only option that attacks the placement imbalance directly; scaling or retry changes leave the hot shard as the bottleneck.",
      "detailedExplanation": "Use the shard-level asymmetry as the primary signal: one shard is overloaded while others are healthy. The strongest answer changes key distribution with explicit correctness boundaries, then verifies recovery via per-shard queue age and p99 convergence. Common pitfall: treating skew as a fleet-wide capacity issue.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-024",
      "type": "multiple-choice",
      "question": "A media-transcoding queue shows chronic unfairness: small jobs starve behind long-running jobs on a few overloaded workers after rebalance. Which next step is strongest?",
      "options": [
        "Scale worker count only and keep current assignment behavior.",
        "Introduce load-aware work stealing with fairness quotas so overloaded workers can offload safely.",
        "Increase retry aggressiveness to force faster completion.",
        "Route all short jobs to the currently fastest worker."
      ],
      "correct": 1,
      "explanation": "The issue is work-distribution fairness, not raw capacity. Load-aware work stealing with explicit fairness guardrails reduces starvation and tail latency without collapsing correctness boundaries.",
      "detailedExplanation": "When long jobs monopolize a subset of workers, prioritize scheduling controls that rebalance active work while protecting ordering/invariant constraints. The right metric check is reduced queue-age skew and improved percentile fairness across workers, not just average throughput. Common pitfall: retries that amplify congestion instead of redistributing work.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-025",
      "type": "multiple-choice",
      "question": "In the payment risk scorer, one tenant now dominates writes and p99 spikes persist unless that tenant is throttled. Which next step is strongest?",
      "options": [
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "This is hot-tenant amplification. Partition-level rate limits and backpressure reduce localized overload at the source while preserving decision correctness.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-026",
      "type": "multiple-choice",
      "question": "An ad auction compute tier hashes mostly sequential IDs, and newly created IDs are piling onto a few shards while p99 stays unstable after autoscaling. Which next step is strongest?",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "Autoscaling adds capacity but does not break skew from sequential-key locality. Dynamic hot-partition splitting is the direct control because it redistributes load where contention is concentrated.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-027",
      "type": "multiple-choice",
      "question": "Inventory reservation workers are imbalanced by shard-per-region assignment, and load tests with skewed regional demand reproduce the p99 failures. Which next step is strongest?",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "Regional skew is read-path concentration on a subset of shards. Routing hot-key reads through replicated cache tiers relieves those shards without weakening reservation correctness boundaries.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-028",
      "type": "multiple-choice",
      "question": "Chat delivery workers show persistent p99 tails from uneven worker assignment, and low-priority queues starve under one noisy tenant. Which next step is strongest?",
      "options": [
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership."
      ],
      "correct": 1,
      "explanation": "The failure mode is fairness collapse between ordered and unordered traffic classes. Splitting those streams lets you protect ordering-critical flow while preventing noisy-neighbor starvation.",
      "detailedExplanation": "Generalize from chat message delivery workers is showing sustained p99 regressions due to uneven worker to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-029",
      "type": "multiple-choice",
      "question": "Notification dispatch has straggler-heavy large partitions, and p99 remains poor even after cross-zone balancing. Which next step is strongest?",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "Cross-zone balancing cannot fix retry amplification on already hot partitions. Bounding retries per partition contains local blast radius and stops tail-latency feedback loops.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-030",
      "type": "multiple-choice",
      "question": "Recommendation candidate generation is suffering retry storms on the same partition, and global retries made that partition worse. Which next step is strongest?",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "Retry storms are queue-scheduling failures under skew, not a fleet-wide capacity issue. Queue-depth-aware worker scheduling prioritizes backlog relief where pressure is highest.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-031",
      "type": "multiple-choice",
      "question": "Video transcoding workers regressed after a partition-key change reduced cardinality; hot keys now dominate p99. Which next step is strongest?",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "Low-cardinality keys collapse many jobs onto too few partitions. Introducing salting or composite keys restores distribution entropy and removes persistent hotspots.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-032",
      "type": "multiple-choice",
      "question": "An analytics stream processor receives campaign bursts on celebrity keys, repeatedly overloading the same hash ranges. Which next step is strongest?",
      "options": [
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership."
      ],
      "correct": 1,
      "explanation": "Bursty celebrity keys stress consistent-hash ranges unevenly after churn and growth. Rebalancing with sufficient virtual nodes evens placement and reduces repeated hot-range concentration.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-033",
      "type": "multiple-choice",
      "question": "Order enrichment workers use time-bucket keys and now hit synchronized spikes where a few partitions run hot while others stay cool. Which next step is strongest?",
      "options": [
        "Split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window."
      ],
      "correct": 0,
      "explanation": "Synchronized bucket spikes indicate deterministic co-location of heavy tenants. Splitting heavy tenants into dedicated partitions directly addresses the memory and queue pressure source.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-034",
      "type": "multiple-choice",
      "question": "An OCR processing fleet shows sticky-hash-ring skew after node churn, with deterministic collisions concentrating errors. Which next step is strongest?",
      "options": [
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists.",
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "Ring stickiness plus collisions creates unfair queue ownership across workers. Load-aware work stealing with fairness limits rebalances active work without letting one class monopolize execution.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-035",
      "type": "multiple-choice",
      "question": "Shipping quote calculators exhibit hot-tenant write dominance, and one work class monopolizes lease renewals during load spikes. Which next step is strongest?",
      "options": [
        "Bias more traffic toward highest-throughput workers to improve cache warmth before rebalancing shard ownership.",
        "Scale total worker count first and postpone partition-level diagnostics until after the next observability window.",
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Tune retry backoff globally and keep current distribution strategy while monitoring whether skew pressure persists."
      ],
      "correct": 2,
      "explanation": "The bottleneck is per-partition contention from one dominant class. Partition-level rate limits and backpressure enforce fairness at the hot boundary and restore tail behavior.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first under peak traffic?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness during deploy churn?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "Generalize from hotspots, Sharding & Work Distribution to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action with tenant fairness constraints?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an inventory reservation worker fleet, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Generalize from in a inventory reservation workers, incident review shows sequential IDs concentrating to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck while preserving ordering guarantees?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery worker fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers without increasing retry storms?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a notification dispatch fleet, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "notification dispatch fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming skew in notification dispatch fleet, what mitigation should be implemented first during cross-zone failover?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for notification dispatch fleet: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Generalize from after confirming skew in notification dispatch fleet, what mitigation should be to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a recommendation candidate generation tier, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "recommendation candidate generators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Which next change best reduces hotspot pressure in recommendation candidate generators without breaking correctness for celebrity-key bursts?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for recommendation candidate generators: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "Generalize from hotspots, Sharding & Work Distribution to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a video transcoding worker fleet, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "video transcoding workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause verified in video transcoding workers, what is the highest-leverage follow-up action under uneven task runtimes?",
          "options": [
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for video transcoding workers: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an analytics stream processing tier, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "analytics stream processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Generalize from in a analytics stream processors, incident review shows low-cardinality partition key to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For this analytics stream processors incident, which mitigation most directly addresses the partition bottleneck while keeping p99 SLO?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for analytics stream processors: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an order enrichment worker fleet, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "order enrichment workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in order enrichment workers before adding more workers?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for order enrichment workers: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an OCR document processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "OCR document processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming skew in OCR document processors, what mitigation should be implemented first with strict cost caps?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for OCR document processors: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a shipping quote calculation fleet, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "shipping quote calculators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Which next change best reduces hotspot pressure in shipping quote calculators without breaking correctness during campaign spikes?",
          "options": [
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for shipping quote calculators: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a fraud feature extractors, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "fraud feature extractors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause verified in fraud feature extractors, what is the highest-leverage follow-up action for mixed read/write load?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for fraud feature extractors: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a profile denormalization worker fleet, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "profile denormalization workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For this profile denormalization workers incident, which mitigation most directly addresses the partition bottleneck with dependency limits?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for profile denormalization workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Generalize from for this profile denormalization workers incident, which mitigation most directly to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an email rendering fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "email rendering fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in email rendering fleet without global locks?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for email rendering fleet: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an IoT event processing fleet, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "IoT event processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming skew in IoT event processors, what mitigation should be implemented first during shard migration?",
          "options": [
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for IoT event processors: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "Generalize from hotspots, Sharding & Work Distribution to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an audit ingestion worker fleet, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "audit ingestion workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Which next change best reduces hotspot pressure in audit ingestion workers without breaking correctness with long-tail tenant protection?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for audit ingestion workers: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Generalize from next change best reduces hotspot pressure in audit ingestion workers without breaking to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a tenant-aware job queue, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "tenant-aware job queue shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause verified in tenant-aware job queue, what is the highest-leverage follow-up action before next traffic wave?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for tenant-aware job queue: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a feed fanout worker fleet, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "feed fanout workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For this feed fanout workers incident, which mitigation most directly addresses the partition bottleneck while keeping idempotent retries?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for feed fanout workers: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a search indexing pipeline, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "search indexing pipeline shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in search indexing pipeline under control-plane limits?",
          "options": [
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for search indexing pipeline: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first for low-cardinality keys?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Generalize from in a payment risk scorer, incident review shows sticky hash ring after node churn to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness with stale-ring risk?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action while minimizing remap impact?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an inventory reservation worker fleet, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck with queue-age regression?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize from hotspots, Sharding & Work Distribution to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery worker fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers during incident mitigation?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-061",
      "type": "multi-select",
      "question": "Which signals most directly indicate partition hotspots? (Select all that apply)",
      "options": [
        "Per-partition queue age outliers",
        "High variance in shard CPU/utilization",
        "Flat global average latency only",
        "Top-key request concentration trends"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Hotspots appear as skew and outliers; averages hide them.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-062",
      "type": "multi-select",
      "question": "Which techniques help mitigate hot-key write pressure? (Select all that apply)",
      "options": [
        "Key salting with bounded fan-in merge",
        "Tenant isolation for heavy writers",
        "One global writer lock for all keys",
        "Adaptive shard split for sustained hotspots"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Use controlled distribution tactics that preserve semantics.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-063",
      "type": "multi-select",
      "question": "Work stealing is most effective when which conditions hold? (Select all that apply)",
      "options": [
        "Tasks are mostly independent",
        "Fairness/priority constraints are explicit",
        "Strict total order across all tasks is mandatory",
        "Steal limits avoid starving original queues"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Work stealing helps imbalance, but fairness and ordering constraints matter.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-064",
      "type": "multi-select",
      "question": "For consistent hashing, which design choices reduce remap pain during scale events? (Select all that apply)",
      "options": [
        "Use virtual nodes per physical node",
        "Keep key-space mapping deterministic",
        "Rehash all keys on every deployment",
        "Balance token ownership by capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Virtual nodes and deterministic capacity-aware ownership smooth membership changes.",
      "detailedExplanation": "Generalize from for consistent hashing, which design choices reduce remap pain during scale events? to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-065",
      "type": "multi-select",
      "question": "Which trade-offs are true for ordered vs unordered processing? (Select all that apply)",
      "options": [
        "Global ordering usually reduces parallelism",
        "Partition-local ordering can scale better",
        "Ordering requirements never affect throughput",
        "Relaxing order can simplify hotspot mitigation"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ordering guarantees can constrain partitioning and concurrency.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-066",
      "type": "multi-select",
      "question": "Which controls prevent one tenant from starving shared worker pools? (Select all that apply)",
      "options": [
        "Per-tenant concurrency quotas",
        "Weighted fair scheduling",
        "Infinite retries for premium tenants",
        "Backpressure per tenant queue"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Fairness controls enforce isolation in shared systems.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-067",
      "type": "multi-select",
      "question": "Good shard rebalancing safety practices include which? (Select all that apply)",
      "options": [
        "Move data incrementally with canary checks",
        "Track read/write error rates during moves",
        "Do all shard moves simultaneously",
        "Keep rollback path for token ownership"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Controlled migration lowers risk and improves recoverability.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-068",
      "type": "multi-select",
      "question": "Which patterns indicate skew from low-cardinality partition keys? (Select all that apply)",
      "options": [
        "Few partitions dominate traffic",
        "Long-tail partitions remain mostly idle",
        "Uniform per-partition throughput",
        "Frequent hotspot rotation tied to single attributes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Low cardinality concentrates load and causes persistent imbalance.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-069",
      "type": "multi-select",
      "question": "Which metrics are useful before introducing dynamic shard splitting? (Select all that apply)",
      "options": [
        "Per-shard QPS growth slope",
        "Per-shard storage growth and compaction cost",
        "Office hours attendance",
        "Per-shard p99 and queue age"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Splitting should be triggered from sustained load and latency evidence.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-070",
      "type": "multi-select",
      "question": "For queue-based work distribution, which practices reduce stragglers? (Select all that apply)",
      "options": [
        "Bound max lease duration with renewals",
        "Prefer smaller task chunks where possible",
        "Assign all largest tasks to one worker",
        "Allow speculative retry for stalled tasks with idempotency"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Lease management, chunking, and safe speculation improve tail completion time.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-071",
      "type": "multi-select",
      "question": "Which statements about hotspot caches are valid? (Select all that apply)",
      "options": [
        "Replicated read caches can absorb hot-key fanout",
        "Write-through caches can still bottleneck origin writes",
        "Caches remove need for consistency policy",
        "Cache invalidation cost must be included in design"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Caches help read hotspots but dont eliminate consistency and write limits.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-072",
      "type": "multi-select",
      "question": "Which anti-patterns worsen hotspot incidents? (Select all that apply)",
      "options": [
        "Scaling only on global averages",
        "Ignoring top-N key concentration reports",
        "Using partition-level dashboards and alerts",
        "Removing fairness controls under pressure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ignoring skew signals and fairness causes repeated hotspot failures.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-073",
      "type": "multi-select",
      "question": "When choosing a partition key, which properties are generally desirable? (Select all that apply)",
      "options": [
        "High cardinality with stable distribution",
        "Alignment with dominant access pattern",
        "Hard dependence on current fleet size",
        "Ability to evolve when workload shifts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "A resilient key choice balances distribution and access efficiency over time.",
      "detailedExplanation": "Generalize from choosing a partition key, which properties are generally desirable? (Select all that to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-074",
      "type": "multi-select",
      "question": "Which steps improve confidence in re-sharding plans? (Select all that apply)",
      "options": [
        "Simulate remap percentage before rollout",
        "Estimate migration bandwidth and duration",
        "Skip data integrity verification to move faster",
        "Define abort criteria and rollback checkpoints"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Dry-run analysis and explicit guardrails reduce migration risk.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-075",
      "type": "multi-select",
      "question": "Which strategies help preserve correctness during unordered parallel execution? (Select all that apply)",
      "options": [
        "Idempotent handlers",
        "Commutative updates where possible",
        "Assume exactly-once delivery by default",
        "Deduplication keys on side effects"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correctness under parallelism needs idempotency and dedupe controls.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-076",
      "type": "multi-select",
      "question": "Which signals suggest work stealing is harming fairness? (Select all that apply)",
      "options": [
        "Low-priority queues never drain",
        "High-priority queues remain bounded",
        "Tenant latency SLO violations become uneven",
        "Stealers consistently drain same source queues"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Fairness regressions show up as starvation and uneven tenant latency.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-077",
      "type": "multi-select",
      "question": "Which actions reduce retry-amplified partition hotspots? (Select all that apply)",
      "options": [
        "Partition-scoped retry budgets",
        "Exponential backoff with jitter",
        "Immediate synchronized retries from all clients",
        "Circuit breaking for saturated partitions"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Budgeted retries and jitter limit synchronized pressure on hot partitions.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-078",
      "type": "numeric-input",
      "question": "A queue has 1,200,000 jobs. Worker fleet drains 28,000 jobs/min while producers add 16,000 jobs/min. How many minutes to clear backlog?",
      "answer": 100,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Net drain is 12,000 jobs/min. 1,200,000 / 12,000 = 100 minutes.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 1,200 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-079",
      "type": "numeric-input",
      "question": "A hot partition handles 18,000 rps but safe target is 7,500 rps/partition. Minimum partitions needed for that key range?",
      "answer": 3,
      "unit": "partitions",
      "tolerance": 0,
      "explanation": "18,000 / 7,500 = 2.4, so round up to 3 partitions.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 18,000 rps and 7,500 rps appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-080",
      "type": "numeric-input",
      "question": "Consistent hash ring has 240 virtual nodes over 12 physical nodes. Average virtual nodes per physical node?",
      "answer": 20,
      "unit": "vnodes",
      "tolerance": 0,
      "explanation": "240 / 12 = 20 virtual nodes per physical node.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 240 and 12 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-081",
      "type": "numeric-input",
      "question": "Top 1% of keys account for 42% of traffic at 90,000 rps total. How much rps does top 1% represent?",
      "answer": 37800,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.42 * 90,000 = 37,800 rps.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 1 and 42 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-082",
      "type": "numeric-input",
      "question": "A rebalancing plan moves 3.6 TB at sustained 300 MB/s. Approximate hours to complete transfer?",
      "answer": 3.33,
      "unit": "hours",
      "tolerance": 0.08,
      "explanation": "3.6 TB  3,600,000 MB. 3,600,000 / 300 = 12,000s = 3.33 hours.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. Treat network capacity as a steady-state constraint, then test against peak windows. If values like 3.6 TB and 300 MB appear, convert them into one unit basis before comparison. Common pitfall: planning on average transfer while peak bursts dominate.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-083",
      "type": "numeric-input",
      "question": "Worker pool has 64 workers. Fair scheduler reserves 25% capacity for premium tier. How many workers are effectively reserved?",
      "answer": 16,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "0.25 * 64 = 16 workers.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 64 and 25 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-084",
      "type": "numeric-input",
      "question": "Shard A p99 is 1,400ms, shard B p99 is 220ms. By what multiple is shard A p99 higher?",
      "answer": 6.36,
      "unit": "x",
      "tolerance": 0.1,
      "explanation": "1,400 / 220 = 6.36x.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 1,400ms and 220ms appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-085",
      "type": "numeric-input",
      "question": "A queue system processes 75,000 jobs/min with 300 workers. If per-worker throughput drops 20% from skew, effective jobs/min?",
      "answer": 60000,
      "unit": "jobs/min",
      "tolerance": 0.01,
      "explanation": "Effective throughput = 75,000 * 0.8 = 60,000 jobs/min.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 75,000 and 300 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-086",
      "type": "numeric-input",
      "question": "Re-sharding reduces remapped keys from 48% to 14%. How many percentage points improvement?",
      "answer": 34,
      "unit": "percentage points",
      "tolerance": 0,
      "explanation": "48 - 14 = 34 percentage points.",
      "detailedExplanation": "Generalize from re-sharding reduces remapped keys from 48% to 14% to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 48 and 14 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-087",
      "type": "numeric-input",
      "question": "At 24 partitions, one partition holds 19% of load. Ideal equal share would be what percent per partition?",
      "answer": 4.17,
      "unit": "%",
      "tolerance": 0.05,
      "explanation": "Equal share is 100 / 24 = 4.17% per partition.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Normalize units before computing so conversion mistakes do not propagate. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 24 and 19 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-088",
      "type": "numeric-input",
      "question": "A worker lease timeout is 45s. Average task runtime is 18s. Timeout should be at least how many times average runtime?",
      "answer": 2.5,
      "unit": "x",
      "tolerance": 0.05,
      "explanation": "45 / 18 = 2.5x.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep every transformation in one unit system and check order of magnitude at the end. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 45s and 18s in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-089",
      "type": "numeric-input",
      "question": "A hotspot mitigation lowers tail latency from 2,400ms to 900ms. What percent reduction is this?",
      "answer": 62.5,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "(2,400 - 900) / 2,400 = 62.5% reduction.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 2,400ms and 900ms appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-090",
      "type": "ordering",
      "question": "Order a practical hotspot incident response flow.",
      "items": [
        "Detect skew with partition-level metrics",
        "Contain impact with fairness/rate controls",
        "Apply partition/work-distribution fix",
        "Validate and codify prevention guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Contain blast radius first, then fix root cause and institutionalize guardrails.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Build the rank from biggest differences first, then refine with adjacent checks. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-091",
      "type": "ordering",
      "question": "Order partition-key design steps from first to last.",
      "items": [
        "Map dominant access patterns",
        "Evaluate key cardinality/distribution",
        "Simulate growth and hotspot scenarios",
        "Finalize with evolution strategy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Design keys from workload access first, then validate future skew resilience.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Place obvious extremes first, then sort the middle by pairwise comparison. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-092",
      "type": "ordering",
      "question": "Order by increasing remap impact during node membership changes.",
      "items": [
        "Modulo hashing",
        "Naive range rebalance",
        "Consistent hashing",
        "Consistent hashing with virtual nodes"
      ],
      "correctOrder": [3, 2, 1, 0],
      "explanation": "Virtual-node consistent hashing tends to minimize remap impact best.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-093",
      "type": "ordering",
      "question": "Order work distribution strategies from least to most fairness-aware.",
      "items": [
        "Single global FIFO without tenant isolation",
        "Round-robin worker assignment",
        "Weighted fair queues by tenant",
        "Weighted fair queues with per-tenant quotas and backpressure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Fairness increases as explicit tenant controls are added.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-094",
      "type": "ordering",
      "question": "Order shard migration rollout from safest to riskiest.",
      "items": [
        "Canary move with rollback checkpoints",
        "Incremental batch moves",
        "Large single-wave migration",
        "All shards moved at once"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer migration is gradual and observable with rollback capability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-095",
      "type": "ordering",
      "question": "Order by strongest hotspot diagnosis quality.",
      "items": [
        "Global average CPU only",
        "Per-node CPU averages",
        "Per-partition utilization with top-key analysis",
        "Per-partition utilization plus queue age and tail latency decomposition"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnosis quality improves with granular skew and latency decomposition.",
      "detailedExplanation": "Generalize from order by strongest hotspot diagnosis quality to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-096",
      "type": "ordering",
      "question": "Order queue processing controls for straggler reduction.",
      "items": [
        "Add task lease/heartbeat",
        "Bound task size where possible",
        "Enable selective speculative retry",
        "Tune retry dedupe/idempotency guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Progressive controls reduce long-tail completions safely.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Place obvious extremes first, then sort the middle by pairwise comparison. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-097",
      "type": "ordering",
      "question": "Order by increasing ordering guarantee strength.",
      "items": [
        "Unordered processing",
        "Per-partition ordering",
        "Per-tenant ordering",
        "Global total ordering"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger ordering guarantees generally reduce available parallelism.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-098",
      "type": "ordering",
      "question": "Order mitigation options for hot tenant isolation from quickest to slowest to implement.",
      "items": [
        "Per-tenant throttles",
        "Weighted scheduler tuning",
        "Dedicated partition pool",
        "Full data model repartitioning"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Incremental controls are usually faster than repartitioning changes.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-099",
      "type": "ordering",
      "question": "Order by increasing risk of unfair starvation.",
      "items": [
        "Fair queues with quotas",
        "Work stealing with fairness caps",
        "Unbounded stealing from low-priority queues",
        "No fairness controls with bursty premium traffic"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Starvation risk grows as fairness protections are removed.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-100",
      "type": "ordering",
      "question": "Order validation loop after hotspot fixes.",
      "items": [
        "Compare pre/post partition heatmaps",
        "Check tail latency and error budgets",
        "Run controlled load replay",
        "Lock in alerts for recurrence signals"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Validate skew reduction, user impact, stress behavior, then codify monitoring.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    }
  ]
}
