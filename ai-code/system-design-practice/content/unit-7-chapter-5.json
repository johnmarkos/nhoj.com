{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 5,
  "chapterTitle": "Hotspots, Sharding & Work Distribution",
  "chapterDescription": "Mitigating compute hotspots with better partitioning, fair work distribution, and sharding strategies that preserve correctness and resilience.",
  "problems": [
    {
      "id": "sc-hs-001",
      "type": "multiple-choice",
      "question": "A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The imbalance worsened immediately after a minor traffic increase.",
      "options": [
        "Introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Read this as a scenario about \"tenant-aware job queue is showing sustained p99 regressions due to low-cardinality\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-002",
      "type": "multiple-choice",
      "question": "Feed fanout workers are showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Only two partitions now drive most error budget burn.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A feed fanout workers is showing sustained p99 regressions due to celebrity-key traffic skew, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The key clue in this question is \"feed fanout workers is showing sustained p99 regressions due to celebrity-key traffic\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-003",
      "type": "multiple-choice",
      "question": "A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Autoscaling adds nodes but hotspot pressure remains localized.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Start from \"search indexing pipeline is showing sustained p99 regressions due to time-bucket key\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-004",
      "type": "multiple-choice",
      "question": "An image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Rollback of recent deploy did not remove the skew pattern.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For an image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "If you keep \"image processing fleet is showing sustained p99 regressions due to sticky hash ring\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-005",
      "type": "multiple-choice",
      "question": "A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Premium-tenant traffic growth made the issue visible this week.",
      "options": [
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "The core signal here is \"payment risk scorer is showing sustained p99 regressions due to single hot tenant\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-006",
      "type": "multiple-choice",
      "question": "An ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Top-key concentration reports align with latency regression.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For an ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"ad auction compute tier is showing sustained p99 regressions due to sequential IDs\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-007",
      "type": "multiple-choice",
      "question": "Inventory reservation workers are showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Queue age outliers are isolated to a subset of workers.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For inventory reservation workers showing sustained p99 regressions due to shard-per-region imbalance, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "This prompt is really about \"inventory reservation workers is showing sustained p99 regressions due to\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-008",
      "type": "multiple-choice",
      "question": "Chat message delivery workers are showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Dependency saturation appears only on hot partitions.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A chat message delivery workers is showing sustained p99 regressions due to uneven worker assignment, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"chat message delivery workers is showing sustained p99 regressions due to uneven worker\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-009",
      "type": "multiple-choice",
      "question": "A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Cold partitions remain underutilized during the incident.",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Read this as a scenario about \"notification dispatch fleet is showing sustained p99 regressions due to straggler tasks\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-010",
      "type": "multiple-choice",
      "question": "Recommendation candidate generators are showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Retry amplification is concentrated on the same key range.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A recommendation candidate generators is showing sustained p99 regressions due to retry storms targeting same partition, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"recommendation candidate generators is showing sustained p99 regressions due to retry\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-011",
      "type": "multiple-choice",
      "question": "Video transcoding workers are showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The same shard repeatedly triggers paging alerts.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A video transcoding workers is showing sustained p99 regressions due to low-cardinality partition key, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "The key clue in this question is \"video transcoding workers is showing sustained p99 regressions due to low-cardinality\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-012",
      "type": "multiple-choice",
      "question": "Analytics stream processors are showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Average fleet metrics look normal despite p99 failures.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For analytics stream processors showing sustained p99 regressions due to celebrity-key traffic skew, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"analytics stream processors is showing sustained p99 regressions due to celebrity-key\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-013",
      "type": "multiple-choice",
      "question": "Order enrichment workers are showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Regional traffic shift exposed long-standing partition imbalance.",
      "options": [
        "Split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For order enrichment workers showing sustained p99 regressions due to time-bucket key with synchronized spikes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "The decision turns on \"order enrichment workers is showing sustained p99 regressions due to time-bucket key\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-014",
      "type": "multiple-choice",
      "question": "OCR document processors are showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Worker saturation correlates with one low-cardinality attribute.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For OCR document processors showing sustained p99 regressions due to sticky hash ring after node churn, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "This prompt is really about \"oCR document processors is showing sustained p99 regressions due to sticky hash ring\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-015",
      "type": "multiple-choice",
      "question": "Shipping quote calculators are showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Fairness SLOs broke before global throughput limits.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A shipping quote calculators is showing sustained p99 regressions due to single hot tenant dominating writes, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "Use \"shipping quote calculators is showing sustained p99 regressions due to single hot\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-016",
      "type": "multiple-choice",
      "question": "A fraud feature extractors is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Canary traffic was healthy until one shard became dominant.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A fraud feature extractors is showing sustained p99 regressions due to sequential IDs concentrating recent traffic, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The core signal here is \"fraud feature extractors is showing sustained p99 regressions due to sequential IDs\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-017",
      "type": "multiple-choice",
      "question": "Profile denormalization workers are showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Backlog growth is nonlinear in only one partition family.",
      "options": [
        "Route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For A profile denormalization workers is showing sustained p99 regressions due to shard-per-region imbalance, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "If you keep \"profile denormalization workers is showing sustained p99 regressions due to\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-018",
      "type": "multiple-choice",
      "question": "An email rendering fleet is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Hotspot symptoms persist across two independent deployments.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For an email rendering fleet is showing sustained p99 regressions due to uneven worker assignment, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"email rendering fleet is showing sustained p99 regressions due to uneven worker\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-019",
      "type": "multiple-choice",
      "question": "IoT event processors are showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Top-tenant bursts now crowd out long-tail workloads.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For IoT event processors showing sustained p99 regressions due to straggler tasks on large partitions, this is the strongest fit in Hotspots, Sharding & Work Distribution.",
      "detailedExplanation": "The key clue in this question is \"ioT event processors is showing sustained p99 regressions due to straggler tasks on\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-020",
      "type": "multiple-choice",
      "question": "Audit ingestion workers are showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Per-shard tail latency spread widened 6x overnight.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew. For audit ingestion workers showing sustained p99 regressions due to retry storms targeting same partition, this is the strongest fit in Hotspots, Sharding & Work Distribution. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"audit ingestion workers is showing sustained p99 regressions due to retry storms\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 6x should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-021",
      "type": "multiple-choice",
      "question": "A multi-tenant reconciliation queue has one shard at 92% CPU while others sit under 20%, and the hot shard owns most enterprise tenants. Which intervention is strongest for the next deploy?",
      "options": [
        "Re-key partitions with a tenant+workload salt (or composite key) so heavy tenants split across shards while preserving ordering boundaries.",
        "Increase global retries and keep the current partition strategy.",
        "Route all enterprise jobs to the currently fastest worker for cache warmth.",
        "Add workers without changing partitioning or shard-level telemetry."
      ],
      "correct": 0,
      "explanation": "This incident is partition-skew, not fleet under-capacity. Re-keying with a composite/salted key directly addresses load concentration while keeping correctness boundaries explicit; retries or blind scaling keep the same hot partition behavior.",
      "detailedExplanation": "The operational clue is asymmetric shard pressure, not global saturation. Favor the change that redistributes key ownership with bounded remap risk, then validate by shard-level p95/p99 spread and queue-depth convergence. Common pitfall: adding capacity while leaving the skew source untouched.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-022",
      "type": "multiple-choice",
      "question": "A feed fanout service uses consistent hashing; after node churn, two virtual-node ranges now absorb most celebrity traffic and p99 latency spikes. What is the strongest next step?",
      "options": [
        "Increase global retries and keep the same ring layout.",
        "Pin celebrity traffic to one warm worker.",
        "Scale worker count only and defer ring changes.",
        "Rebalance the hash ring with more virtual nodes (and controlled remap) to spread celebrity-key load."
      ],
      "correct": 3,
      "explanation": "The failure mode is ring imbalance after churn. Rebalancing with higher virtual-node granularity and controlled remap directly reduces concentration on celebrity keys; retries or single-worker pinning amplify hotspots.",
      "detailedExplanation": "Consistent-hash incidents should be solved at placement policy, not only at worker count. Choose the action that restores even key distribution and preserves predictable remap behavior during future churn. Common pitfall: treating hotspot traffic as a retry problem instead of a distribution problem.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-023",
      "type": "multiple-choice",
      "question": "A recommendation fanout service shards by `region_id`; one region now carries 68% of traffic and queue age is rising only on that shard. Which next step is strongest?",
      "options": [
        "Pin the hottest region to one pre-warmed worker for locality.",
        "Add generic workers and keep current keying unchanged.",
        "Split dominant regions into finer partition keys (or dedicated shards) while keeping ordering boundaries explicit.",
        "Increase retry depth so stragglers eventually catch up."
      ],
      "correct": 2,
      "explanation": "This is concentrated-key skew. Repartitioning heavy regions is the only option that attacks the placement imbalance directly; scaling or retry changes leave the hot shard as the bottleneck.",
      "detailedExplanation": "Use the shard-level asymmetry as the primary signal: one shard is overloaded while others are healthy. The strongest answer changes key distribution with explicit correctness boundaries, then verifies recovery via per-shard queue age and p99 convergence. Common pitfall: treating skew as a fleet-wide capacity issue.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-024",
      "type": "multiple-choice",
      "question": "A media-transcoding queue shows chronic unfairness: small jobs starve behind long-running jobs on a few overloaded workers after rebalance. Which next step is strongest?",
      "options": [
        "Scale worker count only and keep current assignment behavior.",
        "Introduce load-aware work stealing with fairness quotas so overloaded workers can offload safely.",
        "Increase retry aggressiveness to force faster completion.",
        "Route all short jobs to the currently fastest worker."
      ],
      "correct": 1,
      "explanation": "The issue is work-distribution fairness, not raw capacity. Load-aware work stealing with explicit fairness guardrails reduces starvation and tail latency without collapsing correctness boundaries.",
      "detailedExplanation": "When long jobs monopolize a subset of workers, prioritize scheduling controls that rebalance active work while protecting ordering/invariant constraints. The right metric check is reduced queue-age skew and improved percentile fairness across workers, not just average throughput. Common pitfall: retries that amplify congestion instead of redistributing work.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-025",
      "type": "multiple-choice",
      "question": "In the payment risk scorer, one tenant now dominates writes and p99 spikes persist unless that tenant is throttled. Which next step is strongest?",
      "options": [
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "This is hot-tenant amplification. Partition-level rate limits and backpressure reduce localized overload at the source while preserving decision correctness.",
      "detailedExplanation": "Start from \"payment risk scorer is showing sustained p99 regressions due to single hot tenant\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-026",
      "type": "multiple-choice",
      "question": "An ad auction compute tier hashes mostly sequential IDs, and newly created IDs are piling onto a few shards while p99 stays unstable after autoscaling. Which next step is strongest?",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "Autoscaling adds capacity but does not break skew from sequential-key locality. Dynamic hot-partition splitting is the direct control because it redistributes load where contention is concentrated.",
      "detailedExplanation": "The decision turns on \"ad auction compute tier is showing sustained p99 regressions due to sequential IDs\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-027",
      "type": "multiple-choice",
      "question": "Inventory reservation workers are imbalanced by shard-per-region assignment, and load tests with skewed regional demand reproduce the p99 failures. Which next step is strongest?",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "Regional skew is read-path concentration on a subset of shards. Routing hot-key reads through replicated cache tiers relieves those shards without weakening reservation correctness boundaries.",
      "detailedExplanation": "Read this as a scenario about \"inventory reservation workers is showing sustained p99 regressions due to\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-028",
      "type": "multiple-choice",
      "question": "Chat delivery workers show persistent p99 tails from uneven worker assignment, and low-priority queues starve under one noisy tenant. Which next step is strongest?",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "The failure mode is fairness collapse between ordered and unordered traffic classes. Splitting those streams lets you protect ordering-critical flow while preventing noisy-neighbor starvation.",
      "detailedExplanation": "Use \"chat message delivery workers is showing sustained p99 regressions due to uneven worker\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-029",
      "type": "multiple-choice",
      "question": "Notification dispatch has straggler-heavy large partitions, and p99 remains poor even after cross-zone balancing. Which next step is strongest?",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "Cross-zone balancing cannot fix retry amplification on already hot partitions. Bounding retries per partition contains local blast radius and stops tail-latency feedback loops.",
      "detailedExplanation": "This prompt is really about \"notification dispatch fleet is showing sustained p99 regressions due to straggler tasks\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-030",
      "type": "multiple-choice",
      "question": "Recommendation candidate generation is suffering retry storms on the same partition, and global retries made that partition worse. Which next step is strongest?",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "Retry storms are queue-scheduling failures under skew, not a fleet-wide capacity issue. Queue-depth-aware worker scheduling prioritizes backlog relief where pressure is highest.",
      "detailedExplanation": "Read this as a scenario about \"recommendation candidate generators is showing sustained p99 regressions due to retry\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-031",
      "type": "multiple-choice",
      "question": "Video transcoding workers regressed after a partition-key change reduced cardinality; hot keys now dominate p99. Which next step is strongest?",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "Low-cardinality keys collapse many jobs onto too few partitions. Introducing salting or composite keys restores distribution entropy and removes persistent hotspots.",
      "detailedExplanation": "The decision turns on \"video transcoding workers is showing sustained p99 regressions due to low-cardinality\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-032",
      "type": "multiple-choice",
      "question": "An analytics stream processor receives campaign bursts on celebrity keys, repeatedly overloading the same hash ranges. Which next step is strongest?",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "Bursty celebrity keys stress consistent-hash ranges unevenly after churn and growth. Rebalancing with sufficient virtual nodes evens placement and reduces repeated hot-range concentration.",
      "detailedExplanation": "Start from \"analytics stream processors is showing sustained p99 regressions due to celebrity-key\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-033",
      "type": "multiple-choice",
      "question": "Order enrichment workers use time-bucket keys and now hit synchronized spikes where a few partitions run hot while others stay cool. Which next step is strongest?",
      "options": [
        "Split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "Synchronized bucket spikes indicate deterministic co-location of heavy tenants. Splitting heavy tenants into dedicated partitions directly addresses the memory and queue pressure source.",
      "detailedExplanation": "The key clue in this question is \"order enrichment workers is showing sustained p99 regressions due to time-bucket key\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-034",
      "type": "multiple-choice",
      "question": "An OCR processing fleet shows sticky-hash-ring skew after node churn, with deterministic collisions concentrating errors. Which next step is strongest?",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "Ring stickiness plus collisions creates unfair queue ownership across workers. Load-aware work stealing with fairness limits rebalances active work without letting one class monopolize execution.",
      "detailedExplanation": "The core signal here is \"oCR document processors is showing sustained p99 regressions due to sticky hash ring\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-035",
      "type": "multiple-choice",
      "question": "Shipping quote calculators exhibit hot-tenant write dominance, and one work class monopolizes lease renewals during load spikes. Which next step is strongest?",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "The bottleneck is per-partition contention from one dominant class. Partition-level rate limits and backpressure enforce fairness at the hot boundary and restore tail behavior.",
      "detailedExplanation": "If you keep \"shipping quote calculators is showing sustained p99 regressions due to single hot\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Start from \"in a image processing fleet, incident review shows time-bucket key with synchronized\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first under peak traffic?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The decision turns on \"after confirming skew in image processing fleet, what mitigation should be implemented\". Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "This prompt is really about \"hotspots, Sharding & Work Distribution\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The decision turns on \"in a payment risk scorer, incident review shows sticky hash ring after node churn\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness during deploy churn?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Start from \"next change best reduces hotspot pressure in payment risk scorer without breaking\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "Use \"hotspots, Sharding & Work Distribution\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Read this as a scenario about \"in a ad auction compute tier, incident review shows single hot tenant dominating writes\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action with tenant fairness constraints?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The key clue in this question is \"with root cause verified in ad auction compute tier, what is the highest-leverage\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"hotspots, Sharding & Work Distribution\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an inventory reservation worker fleet, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use \"in a inventory reservation workers, incident review shows sequential IDs concentrating\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck while preserving ordering guarantees?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The core signal here is \"for this inventory reservation workers incident, which mitigation most directly\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"hotspots, Sharding & Work Distribution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery worker fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The key clue in this question is \"in a chat message delivery workers, incident review shows shard-per-region imbalance\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers without increasing retry storms?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Read this as a scenario about \"the strongest immediate remediation for the confirmed skew in chat message delivery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"hotspots, Sharding & Work Distribution\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a notification dispatch fleet, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "notification dispatch fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The core signal here is \"in a notification dispatch fleet, incident review shows uneven worker assignment\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming skew in notification dispatch fleet, what mitigation should be implemented first during cross-zone failover?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for notification dispatch fleet: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use \"after confirming skew in notification dispatch fleet, what mitigation should be\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The core signal here is \"hotspots, Sharding & Work Distribution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a recommendation candidate generation tier, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "recommendation candidate generators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The decision turns on \"in a recommendation candidate generators, incident review shows straggler tasks on\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Which next change best reduces hotspot pressure in recommendation candidate generators without breaking correctness for celebrity-key bursts?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for recommendation candidate generators: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Start from \"next change best reduces hotspot pressure in recommendation candidate generators\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "Use \"hotspots, Sharding & Work Distribution\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a video transcoding worker fleet, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "video transcoding workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Start from \"in a video transcoding workers, incident review shows retry storms targeting same\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause verified in video transcoding workers, what is the highest-leverage follow-up action under uneven task runtimes?",
          "options": [
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for video transcoding workers: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The decision turns on \"with root cause verified in video transcoding workers, what is the highest-leverage\". Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "This prompt is really about \"hotspots, Sharding & Work Distribution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an analytics stream processing tier, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "analytics stream processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use \"in a analytics stream processors, incident review shows low-cardinality partition key\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For this analytics stream processors incident, which mitigation most directly addresses the partition bottleneck while keeping p99 SLO?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for analytics stream processors: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The core signal here is \"for this analytics stream processors incident, which mitigation most directly addresses\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The decision turns on \"hotspots, Sharding & Work Distribution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an order enrichment worker fleet, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "order enrichment workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Read this as a scenario about \"in a order enrichment workers, incident review shows celebrity-key traffic skew\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in order enrichment workers before adding more workers?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for order enrichment workers: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The key clue in this question is \"the strongest immediate remediation for the confirmed skew in order enrichment workers\". Solve this as chained reasoning where stage two must respect stage one assumptions. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"hotspots, Sharding & Work Distribution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an OCR document processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "OCR document processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "If you keep \"in a OCR document processors, incident review shows time-bucket key with synchronized\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming skew in OCR document processors, what mitigation should be implemented first with strict cost caps?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for OCR document processors: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "This prompt is really about \"after confirming skew in OCR document processors, what mitigation should be implemented\". Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"hotspots, Sharding & Work Distribution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a shipping quote calculation fleet, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "shipping quote calculators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "This prompt is really about \"in a shipping quote calculators, incident review shows sticky hash ring after node churn\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Which next change best reduces hotspot pressure in shipping quote calculators without breaking correctness during campaign spikes?",
          "options": [
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for shipping quote calculators: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "If you keep \"next change best reduces hotspot pressure in shipping quote calculators without\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "Start from \"hotspots, Sharding & Work Distribution\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a fraud feature extractors, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "fraud feature extractors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The key clue in this question is \"in a fraud feature extractors, incident review shows single hot tenant dominating writes\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause verified in fraud feature extractors, what is the highest-leverage follow-up action for mixed read/write load?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for fraud feature extractors: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Read this as a scenario about \"with root cause verified in fraud feature extractors, what is the highest-leverage\". Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load."
        }
      ],
      "detailedExplanation": "If you keep \"hotspots, Sharding & Work Distribution\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a profile denormalization worker fleet, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "profile denormalization workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The core signal here is \"in a profile denormalization workers, incident review shows sequential IDs\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For this profile denormalization workers incident, which mitigation most directly addresses the partition bottleneck with dependency limits?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for profile denormalization workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use \"for this profile denormalization workers incident, which mitigation most directly\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"hotspots, Sharding & Work Distribution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an email rendering fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "email rendering fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Start from \"in a email rendering fleet, incident review shows shard-per-region imbalance\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in email rendering fleet without global locks?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for email rendering fleet: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The decision turns on \"the strongest immediate remediation for the confirmed skew in email rendering fleet\". Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"hotspots, Sharding & Work Distribution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an IoT event processing fleet, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "IoT event processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The decision turns on \"in a IoT event processors, incident review shows uneven worker assignment\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming skew in IoT event processors, what mitigation should be implemented first during shard migration?",
          "options": [
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for IoT event processors: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Start from \"after confirming skew in IoT event processors, what mitigation should be implemented\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "Use \"hotspots, Sharding & Work Distribution\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an audit ingestion worker fleet, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "audit ingestion workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The core signal here is \"in a audit ingestion workers, incident review shows straggler tasks on large partitions\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Which next change best reduces hotspot pressure in audit ingestion workers without breaking correctness with long-tail tenant protection?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for audit ingestion workers: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Use \"next change best reduces hotspot pressure in audit ingestion workers without breaking\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "The core signal here is \"hotspots, Sharding & Work Distribution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a tenant-aware job queue, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "tenant-aware job queue shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The key clue in this question is \"in a tenant-aware job queue, incident review shows retry storms targeting same partition\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause verified in tenant-aware job queue, what is the highest-leverage follow-up action before next traffic wave?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for tenant-aware job queue: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Read this as a scenario about \"with root cause verified in tenant-aware job queue, what is the highest-leverage\". Solve this as chained reasoning where stage two must respect stage one assumptions. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "If you keep \"hotspots, Sharding & Work Distribution\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a feed fanout worker fleet, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "feed fanout workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "This prompt is really about \"in a feed fanout workers, incident review shows low-cardinality partition key\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For this feed fanout workers incident, which mitigation most directly addresses the partition bottleneck while keeping idempotent retries?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for feed fanout workers: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "If you keep \"for this feed fanout workers incident, which mitigation most directly addresses the\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Start from \"hotspots, Sharding & Work Distribution\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a search indexing pipeline, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "search indexing pipeline shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "If you keep \"in a search indexing pipeline, incident review shows celebrity-key traffic skew\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in search indexing pipeline under control-plane limits?",
          "options": [
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for search indexing pipeline: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "This prompt is really about \"the strongest immediate remediation for the confirmed skew in search indexing pipeline\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"hotspots, Sharding & Work Distribution\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Read this as a scenario about \"in a image processing fleet, incident review shows time-bucket key with synchronized\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first for low-cardinality keys?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The key clue in this question is \"after confirming skew in image processing fleet, what mitigation should be implemented\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"hotspots, Sharding & Work Distribution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Use \"in a payment risk scorer, incident review shows sticky hash ring after node churn\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness with stale-ring risk?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The core signal here is \"next change best reduces hotspot pressure in payment risk scorer without breaking\". Solve this as chained reasoning where stage two must respect stage one assumptions. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "The decision turns on \"hotspots, Sharding & Work Distribution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "Start from \"in a ad auction compute tier, incident review shows single hot tenant dominating writes\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action while minimizing remap impact?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "The decision turns on \"with root cause verified in ad auction compute tier, what is the highest-leverage\". Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"hotspots, Sharding & Work Distribution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "In an inventory reservation worker fleet, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "The decision turns on \"in a inventory reservation workers, incident review shows sequential IDs concentrating\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck with queue-age regression?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "Start from \"for this inventory reservation workers incident, which mitigation most directly\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Use \"hotspots, Sharding & Work Distribution\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery worker fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage.",
          "detailedExplanation": "If you keep \"in a chat message delivery workers, incident review shows shard-per-region imbalance\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers during incident mitigation?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
          "detailedExplanation": "This prompt is really about \"the strongest immediate remediation for the confirmed skew in chat message delivery\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"hotspots, Sharding & Work Distribution\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-061",
      "type": "multi-select",
      "question": "Which signals most directly indicate partition hotspots? (Select all that apply)",
      "options": [
        "Per-partition queue age outliers",
        "High variance in shard CPU/utilization",
        "Flat global average latency only",
        "Top-key request concentration trends"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Hotspots appear as skew and outliers; averages hide them.",
      "detailedExplanation": "Start from \"signals most directly indicate partition hotspots? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-062",
      "type": "multi-select",
      "question": "Which techniques help mitigate hot-key write pressure? (Select all that apply)",
      "options": [
        "Key salting with bounded fan-in merge",
        "Tenant isolation for heavy writers",
        "One global writer lock for all keys",
        "Adaptive shard split for sustained hotspots"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Use controlled distribution tactics that preserve semantics.",
      "detailedExplanation": "The decision turns on \"techniques help mitigate hot-key write pressure? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-063",
      "type": "multi-select",
      "question": "Work stealing is most effective when which conditions hold? (Select all that apply)",
      "options": [
        "Tasks are mostly independent",
        "Fairness/priority constraints are explicit",
        "Strict total order across all tasks is mandatory",
        "Steal limits avoid starving original queues"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Work stealing helps imbalance, but fairness and ordering constraints matter.",
      "detailedExplanation": "Read this as a scenario about \"work stealing is most effective when which conditions hold? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-064",
      "type": "multi-select",
      "question": "For consistent hashing, which design choices reduce remap pain during scale events? (Select all that apply)",
      "options": [
        "Use virtual nodes per physical node",
        "Keep key-space mapping deterministic",
        "Rehash all keys on every deployment",
        "Balance token ownership by capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Virtual nodes and deterministic capacity-aware ownership smooth membership changes.",
      "detailedExplanation": "Use \"for consistent hashing, which design choices reduce remap pain during scale events?\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-065",
      "type": "multi-select",
      "question": "Which trade-offs are true for ordered vs unordered processing? (Select all that apply)",
      "options": [
        "Global ordering usually reduces parallelism",
        "Partition-local ordering can scale better",
        "Ordering requirements never affect throughput",
        "Relaxing order can simplify hotspot mitigation"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ordering guarantees can constrain partitioning and concurrency.",
      "detailedExplanation": "This prompt is really about \"trade-offs are true for ordered vs unordered processing? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-066",
      "type": "multi-select",
      "question": "Which controls prevent one tenant from starving shared worker pools? (Select all that apply)",
      "options": [
        "Per-tenant concurrency quotas",
        "Weighted fair scheduling",
        "Infinite retries for premium tenants",
        "Backpressure per tenant queue"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Fairness controls enforce isolation in shared systems.",
      "detailedExplanation": "If you keep \"controls prevent one tenant from starving shared worker pools? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-067",
      "type": "multi-select",
      "question": "Good shard rebalancing safety practices include which? (Select all that apply)",
      "options": [
        "Move data incrementally with canary checks",
        "Track read/write error rates during moves",
        "Do all shard moves simultaneously",
        "Keep rollback path for token ownership"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Controlled migration lowers risk and improves recoverability.",
      "detailedExplanation": "The core signal here is \"good shard rebalancing safety practices include which? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-068",
      "type": "multi-select",
      "question": "Which patterns indicate skew from low-cardinality partition keys? (Select all that apply)",
      "options": [
        "Few partitions dominate traffic",
        "Long-tail partitions remain mostly idle",
        "Uniform per-partition throughput",
        "Frequent hotspot rotation tied to single attributes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Low cardinality concentrates load and causes persistent imbalance.",
      "detailedExplanation": "The key clue in this question is \"patterns indicate skew from low-cardinality partition keys? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-069",
      "type": "multi-select",
      "question": "Which metrics are useful before introducing dynamic shard splitting? (Select all that apply)",
      "options": [
        "Per-shard QPS growth slope",
        "Per-shard storage growth and compaction cost",
        "Office hours attendance",
        "Per-shard p99 and queue age"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Splitting should be triggered from sustained load and latency evidence.",
      "detailedExplanation": "Start from \"metrics are useful before introducing dynamic shard splitting? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-070",
      "type": "multi-select",
      "question": "For queue-based work distribution, which practices reduce stragglers? (Select all that apply)",
      "options": [
        "Bound max lease duration with renewals",
        "Prefer smaller task chunks where possible",
        "Assign all largest tasks to one worker",
        "Allow speculative retry for stalled tasks with idempotency"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Lease management, chunking, and safe speculation improve tail completion time.",
      "detailedExplanation": "The core signal here is \"for queue-based work distribution, which practices reduce stragglers? (Select all that\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-071",
      "type": "multi-select",
      "question": "Which statements about hotspot caches are valid? (Select all that apply)",
      "options": [
        "Replicated read caches can absorb hot-key fanout",
        "Write-through caches can still bottleneck origin writes",
        "Caches remove need for consistency policy",
        "Cache invalidation cost must be included in design"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Caches help read hotspots but dont eliminate consistency and write limits.",
      "detailedExplanation": "If you keep \"statements about hotspot caches are valid? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-072",
      "type": "multi-select",
      "question": "Which anti-patterns worsen hotspot incidents? (Select all that apply)",
      "options": [
        "Scaling only on global averages",
        "Ignoring top-N key concentration reports",
        "Using partition-level dashboards and alerts",
        "Removing fairness controls under pressure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ignoring skew signals and fairness causes repeated hotspot failures.",
      "detailedExplanation": "This prompt is really about \"anti-patterns worsen hotspot incidents? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-073",
      "type": "multi-select",
      "question": "When choosing a partition key, which properties are generally desirable? (Select all that apply)",
      "options": [
        "High cardinality with stable distribution",
        "Alignment with dominant access pattern",
        "Hard dependence on current fleet size",
        "Ability to evolve when workload shifts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "A resilient key choice balances distribution and access efficiency over time.",
      "detailedExplanation": "Use \"choosing a partition key, which properties are generally desirable? (Select all that\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-074",
      "type": "multi-select",
      "question": "Which steps improve confidence in re-sharding plans? (Select all that apply)",
      "options": [
        "Simulate remap percentage before rollout",
        "Estimate migration bandwidth and duration",
        "Skip data integrity verification to move faster",
        "Define abort criteria and rollback checkpoints"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Dry-run analysis and explicit guardrails reduce migration risk.",
      "detailedExplanation": "Read this as a scenario about \"steps improve confidence in re-sharding plans? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-075",
      "type": "multi-select",
      "question": "Which strategies help preserve correctness during unordered parallel execution? (Select all that apply)",
      "options": [
        "Idempotent handlers",
        "Commutative updates where possible",
        "Assume exactly-once delivery by default",
        "Deduplication keys on side effects"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correctness under parallelism needs idempotency and dedupe controls.",
      "detailedExplanation": "The decision turns on \"strategies help preserve correctness during unordered parallel execution? (Select all\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-076",
      "type": "multi-select",
      "question": "Which signals suggest work stealing is harming fairness? (Select all that apply)",
      "options": [
        "Low-priority queues never drain",
        "High-priority queues remain bounded",
        "Tenant latency SLO violations become uneven",
        "Stealers consistently drain same source queues"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Fairness regressions show up as starvation and uneven tenant latency.",
      "detailedExplanation": "Start from \"signals suggest work stealing is harming fairness? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-077",
      "type": "multi-select",
      "question": "Which actions reduce retry-amplified partition hotspots? (Select all that apply)",
      "options": [
        "Partition-scoped retry budgets",
        "Exponential backoff with jitter",
        "Immediate synchronized retries from all clients",
        "Circuit breaking for saturated partitions"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Budgeted retries and jitter limit synchronized pressure on hot partitions.",
      "detailedExplanation": "The key clue in this question is \"actions reduce retry-amplified partition hotspots? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-078",
      "type": "numeric-input",
      "question": "A queue has 1,200,000 jobs. Worker fleet drains 28,000 jobs/min while producers add 16,000 jobs/min. How many minutes to clear backlog?",
      "answer": 100,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Net drain is 12,000 jobs/min. 1,200,000 / 12,000 = 100 minutes.",
      "detailedExplanation": "The core signal here is \"queue has 1,200,000 jobs\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 1,200 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-079",
      "type": "numeric-input",
      "question": "A hot partition handles 18,000 rps but safe target is 7,500 rps/partition. Minimum partitions needed for that key range?",
      "answer": 3,
      "unit": "partitions",
      "tolerance": 0,
      "explanation": "18,000 / 7,500 = 2.4, so round up to 3 partitions.",
      "detailedExplanation": "If you keep \"hot partition handles 18,000 rps but safe target is 7,500 rps/partition\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. Throughput is only one part; replay behavior and consumer lag handling matter equally. If values like 18,000 rps and 7,500 rps appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-080",
      "type": "numeric-input",
      "question": "Consistent hash ring has 240 virtual nodes over 12 physical nodes. Average virtual nodes per physical node?",
      "answer": 20,
      "unit": "vnodes",
      "tolerance": 0,
      "explanation": "240 / 12 = 20 virtual nodes per physical node.",
      "detailedExplanation": "The decision turns on \"consistent hash ring has 240 virtual nodes over 12 physical nodes\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 240 and 12 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-081",
      "type": "numeric-input",
      "question": "Top 1% of keys account for 42% of traffic at 90,000 rps total. How much rps does top 1% represent?",
      "answer": 37800,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.42 * 90,000 = 37,800 rps.",
      "detailedExplanation": "Read this as a scenario about \"top 1% of keys account for 42% of traffic at 90,000 rps total\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 1 and 42 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-082",
      "type": "numeric-input",
      "question": "A rebalancing plan moves 3.6 TB at sustained 300 MB/s. Approximate hours to complete transfer?",
      "answer": 3.33,
      "unit": "hours",
      "tolerance": 0.08,
      "explanation": "3.6 TB  3,600,000 MB. 3,600,000 / 300 = 12,000s = 3.33 hours.",
      "detailedExplanation": "The key clue in this question is \"rebalancing plan moves 3\". Normalize units before computing so conversion mistakes do not propagate. Treat network capacity as a steady-state constraint, then test against peak windows. If values like 3.6 TB and 300 MB appear, convert them into one unit basis before comparison. Common pitfall: planning on average transfer while peak bursts dominate.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-083",
      "type": "numeric-input",
      "question": "Worker pool has 64 workers. Fair scheduler reserves 25% capacity for premium tier. How many workers are effectively reserved?",
      "answer": 16,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "0.25 * 64 = 16 workers.",
      "detailedExplanation": "Start from \"worker pool has 64 workers\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 64 and 25 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-084",
      "type": "numeric-input",
      "question": "Shard A p99 is 1,400ms, shard B p99 is 220ms. By what multiple is shard A p99 higher?",
      "answer": 6.36,
      "unit": "x",
      "tolerance": 0.1,
      "explanation": "1,400 / 220 = 6.36x.",
      "detailedExplanation": "If you keep \"shard A p99 is 1,400ms, shard B p99 is 220ms\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 1,400ms and 220ms appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-085",
      "type": "numeric-input",
      "question": "A queue system processes 75,000 jobs/min with 300 workers. If per-worker throughput drops 20% from skew, effective jobs/min?",
      "answer": 60000,
      "unit": "jobs/min",
      "tolerance": 0.01,
      "explanation": "Effective throughput = 75,000 * 0.8 = 60,000 jobs/min.",
      "detailedExplanation": "The core signal here is \"queue system processes 75,000 jobs/min with 300 workers\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 75,000 and 300 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-086",
      "type": "numeric-input",
      "question": "Re-sharding reduces remapped keys from 48% to 14%. How many percentage points improvement?",
      "answer": 34,
      "unit": "percentage points",
      "tolerance": 0,
      "explanation": "48 - 14 = 34 percentage points.",
      "detailedExplanation": "Use \"re-sharding reduces remapped keys from 48% to 14%\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 48 and 14 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-087",
      "type": "numeric-input",
      "question": "At 24 partitions, one partition holds 19% of load. Ideal equal share would be what percent per partition?",
      "answer": 4.17,
      "unit": "%",
      "tolerance": 0.05,
      "explanation": "Equal share is 100 / 24 = 4.17% per partition.",
      "detailedExplanation": "This prompt is really about \"at 24 partitions, one partition holds 19% of load\". Normalize units before computing so conversion mistakes do not propagate. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 24 and 19 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-088",
      "type": "numeric-input",
      "question": "A worker lease timeout is 45s. Average task runtime is 18s. Timeout should be at least how many times average runtime?",
      "answer": 2.5,
      "unit": "x",
      "tolerance": 0.05,
      "explanation": "45 / 18 = 2.5x.",
      "detailedExplanation": "The decision turns on \"worker lease timeout is 45s\". Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 45s and 18s in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-089",
      "type": "numeric-input",
      "question": "A hotspot mitigation lowers tail latency from 2,400ms to 900ms. What percent reduction is this?",
      "answer": 62.5,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "(2,400 - 900) / 2,400 = 62.5% reduction.",
      "detailedExplanation": "Read this as a scenario about \"hotspot mitigation lowers tail latency from 2,400ms to 900ms\". Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 2,400ms and 900ms appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-090",
      "type": "ordering",
      "question": "Order a practical hotspot incident response flow.",
      "items": [
        "Detect skew with partition-level metrics",
        "Contain impact with fairness/rate controls",
        "Apply partition/work-distribution fix",
        "Validate and codify prevention guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Contain blast radius first, then fix root cause and institutionalize guardrails.",
      "detailedExplanation": "Start from \"order a practical hotspot incident response flow\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-091",
      "type": "ordering",
      "question": "Order partition-key design steps from first to last.",
      "items": [
        "Map dominant access patterns",
        "Evaluate key cardinality/distribution",
        "Simulate growth and hotspot scenarios",
        "Finalize with evolution strategy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Design keys from workload access first, then validate future skew resilience.",
      "detailedExplanation": "The key clue in this question is \"order partition-key design steps from first to last\". Place obvious extremes first, then sort the middle by pairwise comparison. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-092",
      "type": "ordering",
      "question": "Order by increasing remap impact during node membership changes.",
      "items": [
        "Modulo hashing",
        "Naive range rebalance",
        "Consistent hashing",
        "Consistent hashing with virtual nodes"
      ],
      "correctOrder": [3, 2, 1, 0],
      "explanation": "Virtual-node consistent hashing tends to minimize remap impact best.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing remap impact during node membership changes\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-093",
      "type": "ordering",
      "question": "Order work distribution strategies from least to most fairness-aware.",
      "items": [
        "Single global FIFO without tenant isolation",
        "Round-robin worker assignment",
        "Weighted fair queues by tenant",
        "Weighted fair queues with per-tenant quotas and backpressure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Fairness increases as explicit tenant controls are added.",
      "detailedExplanation": "The decision turns on \"order work distribution strategies from least to most fairness-aware\". Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-094",
      "type": "ordering",
      "question": "Order shard migration rollout from safest to riskiest.",
      "items": [
        "Canary move with rollback checkpoints",
        "Incremental batch moves",
        "Large single-wave migration",
        "All shards moved at once"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer migration is gradual and observable with rollback capability.",
      "detailedExplanation": "This prompt is really about \"order shard migration rollout from safest to riskiest\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-095",
      "type": "ordering",
      "question": "Order by strongest hotspot diagnosis quality.",
      "items": [
        "Global average CPU only",
        "Per-node CPU averages",
        "Per-partition utilization with top-key analysis",
        "Per-partition utilization plus queue age and tail latency decomposition"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnosis quality improves with granular skew and latency decomposition.",
      "detailedExplanation": "Use \"order by strongest hotspot diagnosis quality\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-096",
      "type": "ordering",
      "question": "Order queue processing controls for straggler reduction.",
      "items": [
        "Add task lease/heartbeat",
        "Bound task size where possible",
        "Enable selective speculative retry",
        "Tune retry dedupe/idempotency guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Progressive controls reduce long-tail completions safely.",
      "detailedExplanation": "The core signal here is \"order queue processing controls for straggler reduction\". Place obvious extremes first, then sort the middle by pairwise comparison. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-097",
      "type": "ordering",
      "question": "Order by increasing ordering guarantee strength.",
      "items": [
        "Unordered processing",
        "Per-partition ordering",
        "Per-tenant ordering",
        "Global total ordering"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger ordering guarantees generally reduce available parallelism.",
      "detailedExplanation": "If you keep \"order by increasing ordering guarantee strength\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-098",
      "type": "ordering",
      "question": "Order mitigation options for hot tenant isolation from quickest to slowest to implement.",
      "items": [
        "Per-tenant throttles",
        "Weighted scheduler tuning",
        "Dedicated partition pool",
        "Full data model repartitioning"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Incremental controls are usually faster than repartitioning changes.",
      "detailedExplanation": "Start from \"order mitigation options for hot tenant isolation from quickest to slowest to implement\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-099",
      "type": "ordering",
      "question": "Order by increasing risk of unfair starvation.",
      "items": [
        "Fair queues with quotas",
        "Work stealing with fairness caps",
        "Unbounded stealing from low-priority queues",
        "No fairness controls with bursty premium traffic"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Starvation risk grows as fairness protections are removed.",
      "detailedExplanation": "The key clue in this question is \"order by increasing risk of unfair starvation\". Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    },
    {
      "id": "sc-hs-100",
      "type": "ordering",
      "question": "Order validation loop after hotspot fixes.",
      "items": [
        "Compare pre/post partition heatmaps",
        "Check tail latency and error budgets",
        "Run controlled load replay",
        "Lock in alerts for recurrence signals"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Validate skew reduction, user impact, stress behavior, then codify monitoring.",
      "detailedExplanation": "Read this as a scenario about \"order validation loop after hotspot fixes\". Order by relative scale and bottleneck effect, then validate neighboring items. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "hotspots-sharding-and-work-distribution"],
      "difficulty": "senior"
    }
  ]
}
