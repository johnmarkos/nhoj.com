{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 5,
  "chapterTitle": "Hotspots, Sharding & Work Distribution",
  "chapterDescription": "Mitigating compute hotspots with better partitioning, fair work distribution, and sharding strategies that preserve correctness and resilience.",
  "problems": [
    {
      "id": "sc-hs-001",
      "type": "multiple-choice",
      "question": "A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The imbalance worsened immediately after a minor traffic increase.",
      "options": [
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-002",
      "type": "multiple-choice",
      "question": "A feed fanout workers is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Only two partitions now drive most error budget burn.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-003",
      "type": "multiple-choice",
      "question": "A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Autoscaling adds nodes but hotspot pressure remains localized.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-004",
      "type": "multiple-choice",
      "question": "A image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Rollback of recent deploy did not remove the skew pattern.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-005",
      "type": "multiple-choice",
      "question": "A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Premium-tenant traffic growth made the issue visible this week.",
      "options": [
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-006",
      "type": "multiple-choice",
      "question": "A ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Top-key concentration reports align with latency regression.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-007",
      "type": "multiple-choice",
      "question": "A inventory reservation workers is showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Queue age outliers are isolated to a subset of workers.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-008",
      "type": "multiple-choice",
      "question": "A chat message delivery workers is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Dependency saturation appears only on hot partitions.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-009",
      "type": "multiple-choice",
      "question": "A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Cold partitions remain underutilized during the incident.",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-010",
      "type": "multiple-choice",
      "question": "A recommendation candidate generators is showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Retry amplification is concentrated on the same key range.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-011",
      "type": "multiple-choice",
      "question": "A video transcoding workers is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? The same shard repeatedly triggers paging alerts.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-012",
      "type": "multiple-choice",
      "question": "A analytics stream processors is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Average fleet metrics look normal despite p99 failures.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-013",
      "type": "multiple-choice",
      "question": "A order enrichment workers is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Regional traffic shift exposed long-standing partition imbalance.",
      "options": [
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-014",
      "type": "multiple-choice",
      "question": "A OCR document processors is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Worker saturation correlates with one low-cardinality attribute.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-015",
      "type": "multiple-choice",
      "question": "A shipping quote calculators is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Fairness SLOs broke before global throughput limits.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-016",
      "type": "multiple-choice",
      "question": "A fraud feature extractors is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Canary traffic was healthy until one shard became dominant.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-017",
      "type": "multiple-choice",
      "question": "A profile denormalization workers is showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? Backlog growth is nonlinear in only one partition family.",
      "options": [
        "Apply route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-018",
      "type": "multiple-choice",
      "question": "A email rendering fleet is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Hotspot symptoms persist across two independent deployments.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-019",
      "type": "multiple-choice",
      "question": "A IoT event processors is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Top-tenant bursts now crowd out long-tail workloads.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-020",
      "type": "multiple-choice",
      "question": "A audit ingestion workers is showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Per-shard tail latency spread widened 6x overnight.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-021",
      "type": "multiple-choice",
      "question": "A tenant-aware job queue is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? Observability confirms skew rather than fleet-wide exhaustion.",
      "options": [
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-022",
      "type": "multiple-choice",
      "question": "A feed fanout workers is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Node churn increased remap concentration to a few workers.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-023",
      "type": "multiple-choice",
      "question": "A search indexing pipeline is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? A single celebrity key dominates read fanout paths.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-024",
      "type": "multiple-choice",
      "question": "A image processing fleet is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Queue drain rate fell even though worker count increased.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-025",
      "type": "multiple-choice",
      "question": "A payment risk scorer is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? Latency improves only when hotspot key traffic drops.",
      "options": [
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-026",
      "type": "multiple-choice",
      "question": "A ad auction compute tier is showing sustained p99 regressions due to sequential IDs concentrating recent traffic. Which next step is strongest? Shard-level p99 remains unstable after autoscaling events.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply use dynamic shard splitting for hot partitions to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-027",
      "type": "multiple-choice",
      "question": "A inventory reservation workers is showing sustained p99 regressions due to shard-per-region imbalance. Which next step is strongest? The incident reproduces in load tests with skewed inputs.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply route hot-key reads through replicated cache tiers to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-028",
      "type": "multiple-choice",
      "question": "A chat message delivery workers is showing sustained p99 regressions due to uneven worker assignment. Which next step is strongest? Low-priority queues starve while one tenant remains unconstrained.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply separate ordered streams from unordered workloads to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-029",
      "type": "multiple-choice",
      "question": "A notification dispatch fleet is showing sustained p99 regressions due to straggler tasks on large partitions. Which next step is strongest? Cross-zone traffic balancing did not change hotspot behavior.",
      "options": [
        "Apply bound retries per partition to avoid amplification to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-030",
      "type": "multiple-choice",
      "question": "A recommendation candidate generators is showing sustained p99 regressions due to retry storms targeting same partition. Which next step is strongest? Global retries made local hot partitions significantly worse.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply adopt queue-depth-aware worker scheduling to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-031",
      "type": "multiple-choice",
      "question": "A video transcoding workers is showing sustained p99 regressions due to low-cardinality partition key. Which next step is strongest? Recent partition-key change reduced cardinality unexpectedly.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply introduce salting or composite partition keys to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-032",
      "type": "multiple-choice",
      "question": "A analytics stream processors is showing sustained p99 regressions due to celebrity-key traffic skew. Which next step is strongest? Periodic campaign bursts repeatedly hit identical key ranges.",
      "options": [
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply rebalance with virtual nodes in consistent hashing to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth."
      ],
      "correct": 1,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-033",
      "type": "multiple-choice",
      "question": "A order enrichment workers is showing sustained p99 regressions due to time-bucket key with synchronized spikes. Which next step is strongest? Hot partitions hit memory pressure while others stay cool.",
      "options": [
        "Apply split heavy tenants into dedicated partitions to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only."
      ],
      "correct": 0,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-034",
      "type": "multiple-choice",
      "question": "A OCR document processors is showing sustained p99 regressions due to sticky hash ring after node churn. Which next step is strongest? Error concentration is tied to deterministic hash collisions.",
      "options": [
        "Increase global retries and keep the same distribution strategy.",
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply apply load-aware work stealing with fairness limits to reduce skew while preserving correctness boundaries."
      ],
      "correct": 3,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-035",
      "type": "multiple-choice",
      "question": "A shipping quote calculators is showing sustained p99 regressions due to single hot tenant dominating writes. Which next step is strongest? The same work class monopolizes lease renewals under load.",
      "options": [
        "Pin all traffic to the currently fastest worker for cache warmth.",
        "Ignore partition-level metrics and scale total worker count only.",
        "Apply add partition-level rate limits and backpressure to reduce skew while preserving correctness boundaries.",
        "Increase global retries and keep the same distribution strategy."
      ],
      "correct": 2,
      "explanation": "When hotspots drive tail latency, fix key distribution or work assignment first; global scaling alone rarely removes skew."
    },
    {
      "id": "sc-hs-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first under peak traffic?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness during deploy churn?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action with tenant fairness constraints?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a inventory reservation workers, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck while preserving ordering guarantees?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery workers, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers without increasing retry storms?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a notification dispatch fleet, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "notification dispatch fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "After confirming skew in notification dispatch fleet, what mitigation should be implemented first during cross-zone failover?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for notification dispatch fleet: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a recommendation candidate generators, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "recommendation candidate generators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "Which next change best reduces hotspot pressure in recommendation candidate generators without breaking correctness for celebrity-key bursts?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for recommendation candidate generators: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a video transcoding workers, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "video transcoding workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "With root cause verified in video transcoding workers, what is the highest-leverage follow-up action under uneven task runtimes?",
          "options": [
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for video transcoding workers: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a analytics stream processors, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "analytics stream processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "For this analytics stream processors incident, which mitigation most directly addresses the partition bottleneck while keeping p99 SLO?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for analytics stream processors: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a order enrichment workers, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "order enrichment workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in order enrichment workers before adding more workers?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for order enrichment workers: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a OCR document processors, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "OCR document processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "After confirming skew in OCR document processors, what mitigation should be implemented first with strict cost caps?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for OCR document processors: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a shipping quote calculators, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "shipping quote calculators shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "Which next change best reduces hotspot pressure in shipping quote calculators without breaking correctness during campaign spikes?",
          "options": [
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for shipping quote calculators: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a fraud feature extractors, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "fraud feature extractors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "With root cause verified in fraud feature extractors, what is the highest-leverage follow-up action for mixed read/write load?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for fraud feature extractors: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a profile denormalization workers, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "profile denormalization workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "For this profile denormalization workers incident, which mitigation most directly addresses the partition bottleneck with dependency limits?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for profile denormalization workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a email rendering fleet, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "email rendering fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in email rendering fleet without global locks?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for email rendering fleet: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a IoT event processors, incident review shows uneven worker assignment. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from uneven worker assignment is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "IoT event processors shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "After confirming skew in IoT event processors, what mitigation should be implemented first during shard migration?",
          "options": [
            "Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for IoT event processors: Prioritize adopt queue-depth-aware worker scheduling and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a audit ingestion workers, incident review shows straggler tasks on large partitions. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from straggler tasks on large partitions is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "audit ingestion workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "Which next change best reduces hotspot pressure in audit ingestion workers without breaking correctness with long-tail tenant protection?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for audit ingestion workers: Prioritize introduce salting or composite partition keys and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a tenant-aware job queue, incident review shows retry storms targeting same partition. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from retry storms targeting same partition is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "tenant-aware job queue shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "With root cause verified in tenant-aware job queue, what is the highest-leverage follow-up action before next traffic wave?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for tenant-aware job queue: Prioritize rebalance with virtual nodes in consistent hashing and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a feed fanout workers, incident review shows low-cardinality partition key. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from low-cardinality partition key is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "feed fanout workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "For this feed fanout workers incident, which mitigation most directly addresses the partition bottleneck while keeping idempotent retries?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for feed fanout workers: Prioritize split heavy tenants into dedicated partitions and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a search indexing pipeline, incident review shows celebrity-key traffic skew. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from celebrity-key traffic skew is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "search indexing pipeline shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in search indexing pipeline under control-plane limits?",
          "options": [
            "Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for search indexing pipeline: Prioritize apply load-aware work stealing with fairness limits and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a image processing fleet, incident review shows time-bucket key with synchronized spikes. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from time-bucket key with synchronized spikes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "image processing fleet shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "After confirming skew in image processing fleet, what mitigation should be implemented first for low-cardinality keys?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for image processing fleet: Prioritize add partition-level rate limits and backpressure and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a payment risk scorer, incident review shows sticky hash ring after node churn. What is the most likely primary diagnosis?",
          "options": [
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sticky hash ring after node churn is creating localized saturation despite spare cluster capacity."
          ],
          "correct": 3,
          "explanation": "payment risk scorer shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "Which next change best reduces hotspot pressure in payment risk scorer without breaking correctness with stale-ring risk?",
          "options": [
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis."
          ],
          "correct": 2,
          "explanation": "Start with the mitigation that directly removes skew pressure for payment risk scorer: Prioritize use dynamic shard splitting for hot partitions and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a ad auction compute tier, incident review shows single hot tenant dominating writes. What is the most likely primary diagnosis?",
          "options": [
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from single hot tenant dominating writes is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics."
          ],
          "correct": 2,
          "explanation": "ad auction compute tier shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "With root cause verified in ad auction compute tier, what is the highest-leverage follow-up action while minimizing remap impact?",
          "options": [
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers."
          ],
          "correct": 1,
          "explanation": "Start with the mitigation that directly removes skew pressure for ad auction compute tier: Prioritize route hot-key reads through replicated cache tiers and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a inventory reservation workers, incident review shows sequential IDs concentrating recent traffic. What is the most likely primary diagnosis?",
          "options": [
            "Global QPS is low, so no action is required.",
            "Skewed partitioning from sequential IDs concentrating recent traffic is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern."
          ],
          "correct": 1,
          "explanation": "inventory reservation workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "For this inventory reservation workers incident, which mitigation most directly addresses the partition bottleneck with queue-age regression?",
          "options": [
            "Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms.",
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity."
          ],
          "correct": 0,
          "explanation": "Start with the mitigation that directly removes skew pressure for inventory reservation workers: Prioritize separate ordered streams from unordered workloads and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "In a chat message delivery workers, incident review shows shard-per-region imbalance. What is the most likely primary diagnosis?",
          "options": [
            "Skewed partitioning from shard-per-region imbalance is creating localized saturation despite spare cluster capacity.",
            "The autoscaler is always wrong regardless of metrics.",
            "Only DNS TTL settings can explain this failure pattern.",
            "Global QPS is low, so no action is required."
          ],
          "correct": 0,
          "explanation": "chat message delivery workers shows localized saturation and skew, so the dominant issue is partitioning/work-distribution imbalance rather than fleet-wide shortage."
        },
        {
          "question": "What is the strongest immediate remediation for the confirmed skew in chat message delivery workers during incident mitigation?",
          "options": [
            "Double worker count with no partition analysis.",
            "Disable fairness controls so the hottest key can monopolize workers.",
            "Route all partitions through one coordinator for simplicity.",
            "Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
          ],
          "correct": 3,
          "explanation": "Start with the mitigation that directly removes skew pressure for chat message delivery workers: Prioritize bound retries per partition to avoid amplification and validate with per-partition utilization/latency histograms."
        }
      ]
    },
    {
      "id": "sc-hs-061",
      "type": "multi-select",
      "question": "Which signals most directly indicate partition hotspots? (Select all that apply)",
      "options": [
        "Per-partition queue age outliers",
        "High variance in shard CPU/utilization",
        "Flat global average latency only",
        "Top-key request concentration trends"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Hotspots appear as skew and outliers; averages hide them."
    },
    {
      "id": "sc-hs-062",
      "type": "multi-select",
      "question": "Which techniques help mitigate hot-key write pressure? (Select all that apply)",
      "options": [
        "Key salting with bounded fan-in merge",
        "Tenant isolation for heavy writers",
        "One global writer lock for all keys",
        "Adaptive shard split for sustained hotspots"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Use controlled distribution tactics that preserve semantics."
    },
    {
      "id": "sc-hs-063",
      "type": "multi-select",
      "question": "Work stealing is most effective when which conditions hold? (Select all that apply)",
      "options": [
        "Tasks are mostly independent",
        "Fairness/priority constraints are explicit",
        "Strict total order across all tasks is mandatory",
        "Steal limits avoid starving original queues"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Work stealing helps imbalance, but fairness and ordering constraints matter."
    },
    {
      "id": "sc-hs-064",
      "type": "multi-select",
      "question": "For consistent hashing, which design choices reduce remap pain during scale events? (Select all that apply)",
      "options": [
        "Use virtual nodes per physical node",
        "Keep key-space mapping deterministic",
        "Rehash all keys on every deployment",
        "Balance token ownership by capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Virtual nodes and deterministic capacity-aware ownership smooth membership changes."
    },
    {
      "id": "sc-hs-065",
      "type": "multi-select",
      "question": "Which trade-offs are true for ordered vs unordered processing? (Select all that apply)",
      "options": [
        "Global ordering usually reduces parallelism",
        "Partition-local ordering can scale better",
        "Ordering requirements never affect throughput",
        "Relaxing order can simplify hotspot mitigation"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ordering guarantees can constrain partitioning and concurrency."
    },
    {
      "id": "sc-hs-066",
      "type": "multi-select",
      "question": "Which controls prevent one tenant from starving shared worker pools? (Select all that apply)",
      "options": [
        "Per-tenant concurrency quotas",
        "Weighted fair scheduling",
        "Infinite retries for premium tenants",
        "Backpressure per tenant queue"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Fairness controls enforce isolation in shared systems."
    },
    {
      "id": "sc-hs-067",
      "type": "multi-select",
      "question": "Good shard rebalancing safety practices include which? (Select all that apply)",
      "options": [
        "Move data incrementally with canary checks",
        "Track read/write error rates during moves",
        "Do all shard moves simultaneously",
        "Keep rollback path for token ownership"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Controlled migration lowers risk and improves recoverability."
    },
    {
      "id": "sc-hs-068",
      "type": "multi-select",
      "question": "Which patterns indicate skew from low-cardinality partition keys? (Select all that apply)",
      "options": [
        "Few partitions dominate traffic",
        "Long-tail partitions remain mostly idle",
        "Uniform per-partition throughput",
        "Frequent hotspot rotation tied to single attributes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Low cardinality concentrates load and causes persistent imbalance."
    },
    {
      "id": "sc-hs-069",
      "type": "multi-select",
      "question": "Which metrics are useful before introducing dynamic shard splitting? (Select all that apply)",
      "options": [
        "Per-shard QPS growth slope",
        "Per-shard storage growth and compaction cost",
        "Office hours attendance",
        "Per-shard p99 and queue age"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Splitting should be triggered from sustained load and latency evidence."
    },
    {
      "id": "sc-hs-070",
      "type": "multi-select",
      "question": "For queue-based work distribution, which practices reduce stragglers? (Select all that apply)",
      "options": [
        "Bound max lease duration with renewals",
        "Prefer smaller task chunks where possible",
        "Assign all largest tasks to one worker",
        "Allow speculative retry for stalled tasks with idempotency"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Lease management, chunking, and safe speculation improve tail completion time."
    },
    {
      "id": "sc-hs-071",
      "type": "multi-select",
      "question": "Which statements about hotspot caches are valid? (Select all that apply)",
      "options": [
        "Replicated read caches can absorb hot-key fanout",
        "Write-through caches can still bottleneck origin writes",
        "Caches remove need for consistency policy",
        "Cache invalidation cost must be included in design"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Caches help read hotspots but dont eliminate consistency and write limits."
    },
    {
      "id": "sc-hs-072",
      "type": "multi-select",
      "question": "Which anti-patterns worsen hotspot incidents? (Select all that apply)",
      "options": [
        "Scaling only on global averages",
        "Ignoring top-N key concentration reports",
        "Using partition-level dashboards and alerts",
        "Removing fairness controls under pressure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ignoring skew signals and fairness causes repeated hotspot failures."
    },
    {
      "id": "sc-hs-073",
      "type": "multi-select",
      "question": "When choosing a partition key, which properties are generally desirable? (Select all that apply)",
      "options": [
        "High cardinality with stable distribution",
        "Alignment with dominant access pattern",
        "Hard dependence on current fleet size",
        "Ability to evolve when workload shifts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "A resilient key choice balances distribution and access efficiency over time."
    },
    {
      "id": "sc-hs-074",
      "type": "multi-select",
      "question": "Which steps improve confidence in re-sharding plans? (Select all that apply)",
      "options": [
        "Simulate remap percentage before rollout",
        "Estimate migration bandwidth and duration",
        "Skip data integrity verification to move faster",
        "Define abort criteria and rollback checkpoints"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Dry-run analysis and explicit guardrails reduce migration risk."
    },
    {
      "id": "sc-hs-075",
      "type": "multi-select",
      "question": "Which strategies help preserve correctness during unordered parallel execution? (Select all that apply)",
      "options": [
        "Idempotent handlers",
        "Commutative updates where possible",
        "Assume exactly-once delivery by default",
        "Deduplication keys on side effects"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correctness under parallelism needs idempotency and dedupe controls."
    },
    {
      "id": "sc-hs-076",
      "type": "multi-select",
      "question": "Which signals suggest work stealing is harming fairness? (Select all that apply)",
      "options": [
        "Low-priority queues never drain",
        "High-priority queues remain bounded",
        "Tenant latency SLO violations become uneven",
        "Stealers consistently drain same source queues"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Fairness regressions show up as starvation and uneven tenant latency."
    },
    {
      "id": "sc-hs-077",
      "type": "multi-select",
      "question": "Which actions reduce retry-amplified partition hotspots? (Select all that apply)",
      "options": [
        "Partition-scoped retry budgets",
        "Exponential backoff with jitter",
        "Immediate synchronized retries from all clients",
        "Circuit breaking for saturated partitions"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Budgeted retries and jitter limit synchronized pressure on hot partitions."
    },
    {
      "id": "sc-hs-078",
      "type": "numeric-input",
      "question": "A queue has 1,200,000 jobs. Worker fleet drains 28,000 jobs/min while producers add 16,000 jobs/min. How many minutes to clear backlog?",
      "answer": 100,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Net drain is 12,000 jobs/min. 1,200,000 / 12,000 = 100 minutes."
    },
    {
      "id": "sc-hs-079",
      "type": "numeric-input",
      "question": "A hot partition handles 18,000 rps but safe target is 7,500 rps/partition. Minimum partitions needed for that key range?",
      "answer": 3,
      "unit": "partitions",
      "tolerance": 0,
      "explanation": "18,000 / 7,500 = 2.4, so round up to 3 partitions."
    },
    {
      "id": "sc-hs-080",
      "type": "numeric-input",
      "question": "Consistent hash ring has 240 virtual nodes over 12 physical nodes. Average virtual nodes per physical node?",
      "answer": 20,
      "unit": "vnodes",
      "tolerance": 0,
      "explanation": "240 / 12 = 20 virtual nodes per physical node."
    },
    {
      "id": "sc-hs-081",
      "type": "numeric-input",
      "question": "Top 1% of keys account for 42% of traffic at 90,000 rps total. How much rps does top 1% represent?",
      "answer": 37800,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.42 * 90,000 = 37,800 rps."
    },
    {
      "id": "sc-hs-082",
      "type": "numeric-input",
      "question": "A rebalancing plan moves 3.6 TB at sustained 300 MB/s. Approximate hours to complete transfer?",
      "answer": 3.33,
      "unit": "hours",
      "tolerance": 0.08,
      "explanation": "3.6 TB  3,600,000 MB. 3,600,000 / 300 = 12,000s = 3.33 hours."
    },
    {
      "id": "sc-hs-083",
      "type": "numeric-input",
      "question": "Worker pool has 64 workers. Fair scheduler reserves 25% capacity for premium tier. How many workers are effectively reserved?",
      "answer": 16,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "0.25 * 64 = 16 workers."
    },
    {
      "id": "sc-hs-084",
      "type": "numeric-input",
      "question": "Shard A p99 is 1,400ms, shard B p99 is 220ms. By what multiple is shard A p99 higher?",
      "answer": 6.36,
      "unit": "x",
      "tolerance": 0.1,
      "explanation": "1,400 / 220 = 6.36x."
    },
    {
      "id": "sc-hs-085",
      "type": "numeric-input",
      "question": "A queue system processes 75,000 jobs/min with 300 workers. If per-worker throughput drops 20% from skew, effective jobs/min?",
      "answer": 60000,
      "unit": "jobs/min",
      "tolerance": 0.01,
      "explanation": "Effective throughput = 75,000 * 0.8 = 60,000 jobs/min."
    },
    {
      "id": "sc-hs-086",
      "type": "numeric-input",
      "question": "Re-sharding reduces remapped keys from 48% to 14%. How many percentage points improvement?",
      "answer": 34,
      "unit": "percentage points",
      "tolerance": 0,
      "explanation": "48 - 14 = 34 percentage points."
    },
    {
      "id": "sc-hs-087",
      "type": "numeric-input",
      "question": "At 24 partitions, one partition holds 19% of load. Ideal equal share would be what percent per partition?",
      "answer": 4.17,
      "unit": "%",
      "tolerance": 0.05,
      "explanation": "Equal share is 100 / 24 = 4.17% per partition."
    },
    {
      "id": "sc-hs-088",
      "type": "numeric-input",
      "question": "A worker lease timeout is 45s. Average task runtime is 18s. Timeout should be at least how many times average runtime?",
      "answer": 2.5,
      "unit": "x",
      "tolerance": 0.05,
      "explanation": "45 / 18 = 2.5x."
    },
    {
      "id": "sc-hs-089",
      "type": "numeric-input",
      "question": "A hotspot mitigation lowers tail latency from 2,400ms to 900ms. What percent reduction is this?",
      "answer": 62.5,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "(2,400 - 900) / 2,400 = 62.5% reduction."
    },
    {
      "id": "sc-hs-090",
      "type": "ordering",
      "question": "Order a practical hotspot incident response flow.",
      "items": [
        "Detect skew with partition-level metrics",
        "Contain impact with fairness/rate controls",
        "Apply partition/work-distribution fix",
        "Validate and codify prevention guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Contain blast radius first, then fix root cause and institutionalize guardrails."
    },
    {
      "id": "sc-hs-091",
      "type": "ordering",
      "question": "Order partition-key design steps from first to last.",
      "items": [
        "Map dominant access patterns",
        "Evaluate key cardinality/distribution",
        "Simulate growth and hotspot scenarios",
        "Finalize with evolution strategy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Design keys from workload access first, then validate future skew resilience."
    },
    {
      "id": "sc-hs-092",
      "type": "ordering",
      "question": "Order by increasing remap impact during node membership changes.",
      "items": [
        "Modulo hashing",
        "Naive range rebalance",
        "Consistent hashing",
        "Consistent hashing with virtual nodes"
      ],
      "correctOrder": [3, 2, 1, 0],
      "explanation": "Virtual-node consistent hashing tends to minimize remap impact best."
    },
    {
      "id": "sc-hs-093",
      "type": "ordering",
      "question": "Order work distribution strategies from least to most fairness-aware.",
      "items": [
        "Single global FIFO without tenant isolation",
        "Round-robin worker assignment",
        "Weighted fair queues by tenant",
        "Weighted fair queues with per-tenant quotas and backpressure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Fairness increases as explicit tenant controls are added."
    },
    {
      "id": "sc-hs-094",
      "type": "ordering",
      "question": "Order shard migration rollout from safest to riskiest.",
      "items": [
        "Canary move with rollback checkpoints",
        "Incremental batch moves",
        "Large single-wave migration",
        "All shards moved at once"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer migration is gradual and observable with rollback capability."
    },
    {
      "id": "sc-hs-095",
      "type": "ordering",
      "question": "Order by strongest hotspot diagnosis quality.",
      "items": [
        "Global average CPU only",
        "Per-node CPU averages",
        "Per-partition utilization with top-key analysis",
        "Per-partition utilization plus queue age and tail latency decomposition"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnosis quality improves with granular skew and latency decomposition."
    },
    {
      "id": "sc-hs-096",
      "type": "ordering",
      "question": "Order queue processing controls for straggler reduction.",
      "items": [
        "Add task lease/heartbeat",
        "Bound task size where possible",
        "Enable selective speculative retry",
        "Tune retry dedupe/idempotency guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Progressive controls reduce long-tail completions safely."
    },
    {
      "id": "sc-hs-097",
      "type": "ordering",
      "question": "Order by increasing ordering guarantee strength.",
      "items": [
        "Unordered processing",
        "Per-partition ordering",
        "Per-tenant ordering",
        "Global total ordering"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger ordering guarantees generally reduce available parallelism."
    },
    {
      "id": "sc-hs-098",
      "type": "ordering",
      "question": "Order mitigation options for hot tenant isolation from quickest to slowest to implement.",
      "items": [
        "Per-tenant throttles",
        "Weighted scheduler tuning",
        "Dedicated partition pool",
        "Full data model repartitioning"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Incremental controls are usually faster than repartitioning changes."
    },
    {
      "id": "sc-hs-099",
      "type": "ordering",
      "question": "Order by increasing risk of unfair starvation.",
      "items": [
        "Fair queues with quotas",
        "Work stealing with fairness caps",
        "Unbounded stealing from low-priority queues",
        "No fairness controls with bursty premium traffic"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Starvation risk grows as fairness protections are removed."
    },
    {
      "id": "sc-hs-100",
      "type": "ordering",
      "question": "Order validation loop after hotspot fixes.",
      "items": [
        "Compare pre/post partition heatmaps",
        "Check tail latency and error budgets",
        "Run controlled load replay",
        "Lock in alerts for recurrence signals"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Validate skew reduction, user impact, stress behavior, then codify monitoring."
    }
  ]
}
