{
  "chapterTitle": "Distributed Caching",
  "problems": [
    {
      "id": "distcache-001",
      "type": "multiple-choice",
      "question": "What is the primary challenge that consistent hashing solves in distributed caches?",
      "options": [
        "Reducing memory usage",
        "Minimizing key redistribution when nodes are added or removed",
        "Improving cache hit rates",
        "Reducing network latency"
      ],
      "correct": 1,
      "explanation": "Consistent hashing maps keys to a ring of positions. When a node joins/leaves, only keys mapped to that node's range need to move. With naive modulo hashing (key % N), adding a node changes N and remaps most keys.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-002",
      "type": "numeric-input",
      "question": "A distributed cache uses modulo hashing (key % N) with 10 nodes. If you add 1 node, approximately what percentage of keys need to move?",
      "answer": 91,
      "tolerance": 0.05,
      "unit": "%",
      "explanation": "With modulo hashing, key % 10 vs key % 11 produces different results for most keys. Approximately N/(N+1) = 10/11 ≈ 91% of keys change their assigned node.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 10 and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-003",
      "type": "numeric-input",
      "question": "The same cache uses consistent hashing. Adding 1 node to 10 nodes moves approximately what percentage of keys?",
      "answer": 9,
      "tolerance": 0.15,
      "unit": "%",
      "explanation": "With consistent hashing, only keys in the new node's range move—approximately 1/(N+1) = 1/11 ≈ 9% of keys. This is dramatically less disruption than modulo hashing.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Treat freshness policy and invalidation paths as first-class constraints. Numbers such as 1 and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-004",
      "type": "multiple-choice",
      "question": "In consistent hashing, what are 'virtual nodes' (vnodes)?",
      "options": [
        "Backup nodes that take over on failure",
        "Multiple positions on the hash ring per physical node for better distribution",
        "Simulated nodes for testing",
        "Nodes in different data centers"
      ],
      "correct": 1,
      "explanation": "Each physical node is assigned multiple positions (virtual nodes) on the hash ring. This improves load distribution—a node with 100 vnodes gets ~100 segments of the ring instead of 1 large segment.",
      "detailedExplanation": "Generalize from in consistent hashing, what are 'virtual nodes' (vnodes) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 100 and 1 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-005",
      "type": "multiple-choice",
      "question": "Why do virtual nodes improve load balancing in consistent hashing?",
      "options": [
        "They reduce network hops",
        "They spread each node's responsibility across many small ring segments instead of one large segment",
        "They cache more data",
        "They reduce memory usage"
      ],
      "correct": 1,
      "explanation": "With one position per node, segment sizes vary widely based on hash distribution. With many vnodes, the law of large numbers applies—each node gets roughly equal total coverage despite random positions.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-006",
      "type": "two-stage",
      "stages": [
        {
          "question": "A consistent hash ring has 3 nodes at positions 0, 120, and 240 (degrees). A key hashes to position 100. Which node owns it?",
          "options": ["Node at 0", "Node at 120", "Node at 240"],
          "correct": 1,
          "explanation": "Keys are assigned to the next node clockwise on the ring. From 100, the next node clockwise is at 120.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 3 and 0 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "The node at 120 fails. Which node now owns the key at position 100?",
          "options": ["Node at 0", "Node at 240", "Key becomes unavailable"],
          "correct": 1,
          "explanation": "With node 120 gone, the next clockwise node from 100 is at 240. Only keys in 120's range (1-120) move to 240; keys in 0's range (241-360) stay with node 0.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 120 and 100 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-007",
      "type": "multiple-choice",
      "question": "What is 'cache sharding'?",
      "options": [
        "Breaking the cache into pieces stored across multiple nodes",
        "Encrypting cache data",
        "Compressing cached values",
        "Sharing cache between applications"
      ],
      "correct": 0,
      "explanation": "Sharding partitions the cache keyspace across multiple nodes. Each node holds a subset of keys. This allows horizontal scaling—add more nodes to increase total capacity.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-008",
      "type": "multi-select",
      "question": "Which are benefits of cache sharding? (Select all that apply)",
      "options": [
        "Horizontal scalability (more nodes = more capacity)",
        "Parallel reads across different keys",
        "Automatic failover",
        "Reduced memory per node"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Sharding enables horizontal scaling, parallel access to different shards, and smaller memory footprint per node. Automatic failover requires additional mechanisms (replication, sentinel)—sharding alone doesn't provide it.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-009",
      "type": "multiple-choice",
      "question": "What is 'client-side sharding' in distributed caching?",
      "options": [
        "Cache stored in the browser",
        "Client library determines which server holds each key",
        "Clients cache data locally",
        "Sharding based on client IP"
      ],
      "correct": 1,
      "explanation": "In client-side sharding, the client library hashes each key to determine which cache server to contact. No central coordinator—each client independently routes requests. Simple but requires client updates when topology changes.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-010",
      "type": "multiple-choice",
      "question": "What is a disadvantage of client-side sharding?",
      "options": [
        "Higher latency",
        "All clients must be updated when cache topology changes",
        "Cannot scale beyond one server",
        "Requires special hardware"
      ],
      "correct": 1,
      "explanation": "Every client embeds the shard map. When nodes are added/removed, all clients need configuration updates (or smart clients with auto-discovery). Inconsistent views between clients can cause misrouted requests.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-011",
      "type": "multiple-choice",
      "question": "What is a 'cache proxy' like Twemproxy or mcrouter?",
      "options": [
        "A CDN service",
        "An intermediary that routes cache requests to the appropriate shard",
        "A backup cache server",
        "A cache monitoring tool"
      ],
      "correct": 1,
      "explanation": "Cache proxies sit between clients and cache servers. Clients send all requests to the proxy, which routes to the correct shard. This centralizes sharding logic—clients don't need to know the topology.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-012",
      "type": "multi-select",
      "question": "Which are advantages of using a cache proxy over client-side sharding? (Select all that apply)",
      "options": [
        "Topology changes don't require client updates",
        "Connection pooling reduces connections to cache servers",
        "Lower latency than direct connection",
        "Simpler client implementation"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Proxies hide topology from clients (no updates needed), pool connections (fewer connections to backend), and simplify clients. However, proxies add a network hop, increasing latency slightly.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-013",
      "type": "multiple-choice",
      "question": "Redis Cluster uses 16384 hash slots. Why this specific number?",
      "options": [
        "Arbitrary choice",
        "Balances granularity for resharding with reasonable metadata size",
        "Matches TCP port range",
        "Maximum nodes supported"
      ],
      "correct": 1,
      "explanation": "16384 slots allows fine-grained resharding (move individual slots) while keeping slot metadata small (2KB bitmap per node). More slots = finer granularity but more metadata; fewer = coarser resharding.",
      "detailedExplanation": "Generalize from redis Cluster uses 16384 hash slots to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 16384 and 2KB appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-014",
      "type": "multiple-choice",
      "question": "In Redis Cluster, how is a key mapped to a slot?",
      "options": [
        "Modulo of string length",
        "CRC16 of key, then modulo 16384",
        "MD5 hash",
        "Random assignment"
      ],
      "correct": 1,
      "explanation": "Redis Cluster computes CRC16(key) % 16384 to determine the slot. If the key contains {...}, only the content inside braces is hashed—this allows co-locating related keys on the same slot.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 16384 appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-015",
      "type": "multiple-choice",
      "question": "Redis keys 'user:{123}:profile' and 'user:{123}:settings' will be on the same slot. Why?",
      "options": [
        "They have the same prefix",
        "Only the content inside {} is hashed (hash tags)",
        "They're both user keys",
        "Random chance"
      ],
      "correct": 1,
      "explanation": "Redis hash tags: when a key contains {...}, only the substring inside braces is hashed. Both keys hash '123', so they land on the same slot. This enables multi-key operations on related data.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 123 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-016",
      "type": "multiple-choice",
      "question": "What happens when a Redis Cluster client sends a command to the wrong node?",
      "options": [
        "The command fails",
        "The node returns a MOVED redirect with the correct node",
        "The node forwards the request internally",
        "The command executes on the wrong data"
      ],
      "correct": 1,
      "explanation": "Redis Cluster nodes respond with MOVED <slot> <ip:port> when they don't own the requested slot. Smart clients cache the slot→node mapping and update it on MOVED responses.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-017",
      "type": "multiple-choice",
      "question": "What is 'ASK' vs 'MOVED' in Redis Cluster?",
      "options": [
        "ASK is for reads, MOVED is for writes",
        "MOVED means permanent slot migration; ASK means migration in progress",
        "ASK is faster than MOVED",
        "They're identical"
      ],
      "correct": 1,
      "explanation": "MOVED indicates a slot permanently lives on another node—client should update its map. ASK means the slot is being migrated; try the other node once but don't update the map yet.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-018",
      "type": "multiple-choice",
      "question": "What is a 'hot key' problem in distributed caching (Distributed Caching context)?",
      "options": [
        "Keys that are frequently updated",
        "A single key receiving disproportionate traffic, overloading one node",
        "Keys with high TTL",
        "Encrypted keys"
      ],
      "correct": 1,
      "explanation": "Hot keys concentrate traffic on one node regardless of how many shards exist. If a viral tweet's cache key gets 100K requests/sec, that one node becomes the bottleneck while others sit idle.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Numbers such as 100K should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-019",
      "type": "multi-select",
      "question": "Which are solutions to the hot key problem? (Select all that apply)",
      "options": [
        "Replicate hot keys across multiple nodes",
        "Add random suffix to key and read from any replica",
        "Use local caching in addition to distributed cache",
        "Increase the hot node's memory"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Solutions distribute hot key load: replicate the key, add random suffixes (key:1, key:2, key:3) and load balance reads, or cache locally (L1 cache in app). More memory on one node doesn't help if CPU/network is bottlenecked.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Treat freshness policy and invalidation paths as first-class constraints. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-020",
      "type": "two-stage",
      "stages": [
        {
          "question": "A hot key 'trending:post:123' gets 50K reads/sec. One Redis node handles 10K ops/sec max. You add random suffixes (key:0 through key:4) and replicate the value. How many reads/sec can you now handle?",
          "options": ["10K", "25K", "50K", "Unlimited"],
          "correct": 2,
          "explanation": "With 5 key variants spread across different nodes (or same node but enabling parallel ops), you can handle 5 × 10K = 50K reads/sec if load is evenly distributed across suffixes.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. If values like 123 and 50K appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes."
        },
        {
          "question": "What's the main operational challenge with this approach?",
          "options": [
            "Increased latency",
            "Must update all 5 keys on write (consistency)",
            "More memory usage",
            "Client complexity"
          ],
          "correct": 1,
          "explanation": "When the value changes, you must update all 5 keys atomically or accept temporary inconsistency. Write amplification and consistency management are the key challenges.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: using weak consistency for strict invariants."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-021",
      "type": "multiple-choice",
      "question": "What is 'cache replication' in distributed systems?",
      "options": [
        "Copying cache code across servers",
        "Maintaining copies of cached data on multiple nodes for availability",
        "Duplicating requests",
        "Backing up cache to disk"
      ],
      "correct": 1,
      "explanation": "Cache replication keeps copies of data on multiple nodes. If the primary fails, a replica can serve requests. It trades storage efficiency for availability—same data exists in multiple places.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-022",
      "type": "multiple-choice",
      "question": "Redis replication is asynchronous by default. What does this mean?",
      "options": [
        "Writes complete before replication",
        "Primary acknowledges writes before replicas receive them",
        "Replicas run on different threads",
        "Replication happens on a schedule"
      ],
      "correct": 1,
      "explanation": "Asynchronous replication: primary acknowledges the write to the client, then sends it to replicas in the background. Lower latency but risks data loss if primary fails before replicating.",
      "detailedExplanation": "Generalize from redis replication is asynchronous by default to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-023",
      "type": "multiple-choice",
      "question": "What is the trade-off between synchronous and asynchronous replication?",
      "options": [
        "Sync is faster",
        "Sync ensures no data loss but adds latency; async is faster but may lose recent writes on failure",
        "Async uses more memory",
        "No trade-off, sync is always better"
      ],
      "correct": 1,
      "explanation": "Synchronous replication waits for replica acknowledgment before confirming writes—no data loss but higher latency. Asynchronous confirms immediately—lower latency but recent writes may be lost if primary fails before replicating.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-024",
      "type": "multiple-choice",
      "question": "What is Redis Sentinel (Distributed Caching context)?",
      "options": [
        "A security scanner",
        "A monitoring and automatic failover system for Redis",
        "A cache proxy",
        "A data encryption tool"
      ],
      "correct": 1,
      "explanation": "Sentinel monitors Redis primary and replicas. If primary fails, Sentinel coordinates failover: promotes a replica to primary and reconfigures other replicas to follow the new primary. Provides high availability.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-025",
      "type": "multiple-choice",
      "question": "Why do you need multiple Sentinel instances (typically 3+)?",
      "options": [
        "Load balancing",
        "To avoid false positives through quorum-based failure detection",
        "To cache more data",
        "Geographic distribution"
      ],
      "correct": 1,
      "explanation": "A single Sentinel might incorrectly think the primary is down (network partition to Sentinel). Multiple Sentinels vote—failover only occurs if a quorum agrees the primary is unreachable. This prevents split-brain.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 3 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-026",
      "type": "numeric-input",
      "question": "You have 5 Redis Sentinels. What's the minimum quorum to avoid split-brain during network partitions?",
      "answer": 3,
      "tolerance": "exact",
      "unit": "sentinels",
      "explanation": "Quorum must be > N/2 to ensure only one partition can achieve it. With 5 Sentinels, quorum of 3 means both partitions can't independently elect a new primary—at most one partition has 3+ Sentinels.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 5 and 2 in aligned units before deciding on an implementation approach. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-027",
      "type": "multiple-choice",
      "question": "What is 'split-brain' in distributed caching?",
      "options": [
        "A cache corruption issue",
        "Two nodes both acting as primary, accepting conflicting writes",
        "Cache data split across datacenters",
        "Brain-teaser interview questions"
      ],
      "correct": 1,
      "explanation": "Split-brain occurs when a network partition causes two nodes to both believe they're the primary. Each accepts writes independently, leading to data divergence that's difficult to reconcile.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that ignore delivery semantics or backpressure behavior. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-028",
      "type": "multi-select",
      "question": "Which mechanisms help prevent split-brain? (Select all that apply)",
      "options": [
        "Quorum-based leader election",
        "Fencing (STONITH)",
        "Increasing cache size",
        "Node liveness checks"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Quorum ensures only one partition can elect a leader. Fencing (STONITH: Shoot The Other Node In The Head) forcibly shuts down the old primary. Liveness checks detect failures. Cache size is unrelated.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-029",
      "type": "multiple-choice",
      "question": "In Redis Cluster, what happens during a node failure?",
      "options": [
        "All data is lost",
        "Cluster pauses until node returns",
        "Replica is promoted to primary for that node's slots",
        "Slots are redistributed to remaining nodes"
      ],
      "correct": 2,
      "explanation": "If a primary fails and has a replica, the replica is promoted to primary for those slots. If no replica exists, the cluster may enter a fail state for those slots (configurable via cluster-require-full-coverage).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-030",
      "type": "multiple-choice",
      "question": "Redis Cluster requires minimum how many primary nodes?",
      "options": ["1", "2", "3", "5"],
      "correct": 2,
      "explanation": "Redis Cluster requires at least 3 primary nodes for the cluster protocol to function properly. With 3 primaries, losing 1 still leaves a majority to agree on cluster state.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 3 and 1 appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-031",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a Redis Cluster with 3 primaries. For high availability, how many replicas per primary are recommended?",
          "options": ["0", "1", "2", "3"],
          "correct": 1,
          "explanation": "At minimum, 1 replica per primary. This allows any single primary to fail—its replica takes over. More replicas add redundancy but increase memory usage.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 3 and 1 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With 3 primaries and 1 replica each, how many total Redis instances do you need?",
          "options": ["3", "4", "6", "9"],
          "correct": 2,
          "explanation": "3 primaries + 3 replicas = 6 total instances. Each primary has its dedicated replica for failover.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 3 and 1 in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize from distributed Caching to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-032",
      "type": "multiple-choice",
      "question": "What is 'cache coherence' in distributed systems (Distributed Caching context)?",
      "options": [
        "Keeping cache data organized",
        "Ensuring all cache nodes have consistent views of the same data",
        "Cache compression",
        "Coherent naming conventions"
      ],
      "correct": 1,
      "explanation": "Cache coherence means maintaining consistency across distributed caches. When data is updated on one node, other nodes should eventually (or immediately) see the same value. This is challenging in distributed systems.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-033",
      "type": "multiple-choice",
      "question": "Which consistency model does a typical distributed cache with replication provide?",
      "options": [
        "Strong consistency",
        "Eventual consistency",
        "Linearizability",
        "Serializability"
      ],
      "correct": 1,
      "explanation": "Most distributed caches (Redis with async replication, Memcached) provide eventual consistency. Writes propagate over time; reads may temporarily see stale data from replicas. Strong consistency requires synchronous replication.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-034",
      "type": "multiple-choice",
      "question": "You write to a Redis primary and immediately read from a replica. The read returns stale data. What happened?",
      "options": [
        "Bug in Redis",
        "Replication lag—the write hasn't propagated to the replica yet",
        "Cache miss",
        "Wrong key used"
      ],
      "correct": 1,
      "explanation": "Asynchronous replication means replicas lag behind the primary. Reading from replica immediately after writing to primary may return the old value. This is expected behavior, not a bug.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-035",
      "type": "multi-select",
      "question": "How can you ensure read-your-writes consistency with Redis replicas? (Select all that apply)",
      "options": [
        "Always read from primary",
        "Use WAIT command to ensure replication before responding",
        "Track write timestamp and wait before reading from replica",
        "Use larger TTL"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Read from primary ensures you see your writes. WAIT blocks until N replicas acknowledge. Tracking timestamps lets you wait for replication before reading from replica. TTL is unrelated to consistency.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-036",
      "type": "multiple-choice",
      "question": "What is 'thundering herd' in the context of distributed caching?",
      "options": [
        "Too many cache servers",
        "Many clients simultaneously requesting an expired key, all hitting the database",
        "Network congestion",
        "Memory overflow"
      ],
      "correct": 1,
      "explanation": "When a popular cached item expires, many concurrent requests find it missing and all query the database simultaneously. This spike can overwhelm the database. It's worse in distributed caches with more clients.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-037",
      "type": "multi-select",
      "question": "Which techniques prevent thundering herd? (Select all that apply)",
      "options": [
        "Cache locking (only one request fetches on miss)",
        "Stale-while-revalidate",
        "Probabilistic early expiration",
        "Increasing database connections"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Locking ensures one request refills cache while others wait. Stale-while-revalidate serves old data while refreshing. Probabilistic early expiration spreads refreshes over time. More DB connections just delays the overload.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-038",
      "type": "multiple-choice",
      "question": "Implementing distributed cache locking for thundering herd prevention requires what?",
      "options": [
        "Larger cache",
        "Coordination mechanism (e.g., Redis SETNX) so only one client acquires lock",
        "Faster network",
        "More database replicas"
      ],
      "correct": 1,
      "explanation": "Distributed locking needs atomic operations. Redis SETNX (SET if Not eXists) or SET with NX flag atomically acquires a lock. First client gets the lock and fetches data; others wait or retry.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Client A acquires a distributed lock to refresh a cache entry. It starts a slow database query. What's a risk?",
          "options": [
            "Lock never releases if A crashes",
            "Other clients fetch duplicate data",
            "Cache becomes read-only",
            "Database crashes"
          ],
          "correct": 0,
          "explanation": "If A crashes or takes too long, the lock remains held indefinitely, blocking all other clients from refreshing the cache entry.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates."
        },
        {
          "question": "How do you mitigate this risk?",
          "options": [
            "Use faster database",
            "Set TTL on the lock so it auto-expires",
            "Don't use locking",
            "Restart A automatically"
          ],
          "correct": 1,
          "explanation": "Lock TTL ensures automatic release if the holder crashes or is slow. SET key value NX EX 30 acquires a lock that expires in 30 seconds. The holder should complete and release before expiry.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. If values like 30 and 30 seconds appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "Generalize from distributed Caching to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-040",
      "type": "multiple-choice",
      "question": "What is 'cache warming' for distributed caches?",
      "options": [
        "Increasing cache memory",
        "Pre-populating cache with expected hot data before receiving traffic",
        "Raising server temperature",
        "Gradual traffic increase"
      ],
      "correct": 1,
      "explanation": "After cache restart, it's empty—all requests hit the database (cold cache). Cache warming pre-loads hot data (from DB, logs, or previous cache dump) to achieve good hit rates before taking full traffic.",
      "detailedExplanation": "Generalize from 'cache warming' for distributed caches to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-041",
      "type": "multi-select",
      "question": "Which are common cache warming strategies? (Select all that apply)",
      "options": [
        "Replay recent request logs against the cache",
        "Load from database using 'hot keys' list",
        "Restore from RDB/AOF dump",
        "Wait for organic traffic to warm cache"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Log replay simulates recent traffic. Hot keys lists identify popular items to preload. RDB/AOF restore recovers previous cache state. Organic warming works but causes initial performance degradation.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-042",
      "type": "multiple-choice",
      "question": "What is 'cache stamping' or 'dog-pile effect'?",
      "options": [
        "Cache data corruption",
        "Multiple servers generating the same cached value simultaneously on expiry",
        "Cache key collisions",
        "Excessive memory usage"
      ],
      "correct": 1,
      "explanation": "Similar to thundering herd—when a popular item expires, multiple application servers simultaneously try to regenerate and cache it. This wastes resources computing the same value multiple times.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-043",
      "type": "multiple-choice",
      "question": "You have caches in US-East and EU-West. A user in EU writes data. How do you ensure EU readers see the new value?",
      "options": [
        "It happens automatically with any cache",
        "Cross-region replication or invalidation must be configured",
        "Users must clear their browser cache",
        "Not possible"
      ],
      "correct": 1,
      "explanation": "Distributed caches in different regions don't automatically sync. You need cross-region replication (data copied) or invalidation (EU notified to refetch). Both add latency and complexity.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-044",
      "type": "multiple-choice",
      "question": "What is 'write-behind cross-region replication'?",
      "options": [
        "Writes to local region cache, then asynchronously replicates to other regions",
        "Writes go to all regions synchronously",
        "Writes only to the primary region",
        "Writes bypassing the cache"
      ],
      "correct": 0,
      "explanation": "Write-behind replication writes to the local region first (low latency), then asynchronously propagates to other regions. Improves write latency but introduces cross-region consistency lag.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard choices that violate required invariants during concurrent or failed states. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "A global application has caches in US and EU with async cross-region replication. Write latency in US, replication lag to EU is 100ms. A US user writes, then an EU user reads after 50ms. What happens?",
          "options": [
            "EU user sees new value",
            "EU user sees stale value (replication not complete)",
            "Read fails",
            "Read waits for replication"
          ],
          "correct": 1,
          "explanation": "With 100ms replication lag and only 50ms elapsed, the write hasn't reached EU yet. EU user reads from local cache and gets the stale value.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Numbers such as 100ms and 50ms should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "If strong consistency is required for this data, what's the solution?",
          "options": [
            "Increase cache size",
            "Route reads/writes to a single primary region",
            "Faster network",
            "More replicas"
          ],
          "correct": 1,
          "explanation": "For strong consistency, all reads/writes for specific data should go to one region (primary). Other regions can have read replicas with eventual consistency, but writes go to primary. Trade-off: higher write latency from remote regions.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: ignoring conflict resolution behavior."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-046",
      "type": "multiple-choice",
      "question": "What is 'lease' mechanism in distributed caching (as used by Facebook's Memcache)?",
      "options": [
        "Renting cache servers",
        "A token given on cache miss that must be presented to set the value, preventing stale sets",
        "Time-limited cache entries",
        "Agreement to use the cache"
      ],
      "correct": 1,
      "explanation": "On cache miss, Memcache returns a lease token. To set the value, you must provide this token. If another client already set the value (invalidating the lease), your stale set is rejected. Prevents race conditions.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-047",
      "type": "multiple-choice",
      "question": "Facebook's TAO uses 'look-aside' caching. What does this mean?",
      "options": [
        "Cache is beside the application",
        "Application checks cache first; on miss, fetches from DB and populates cache",
        "Cache automatically fetches from DB",
        "Side-channel caching"
      ],
      "correct": 1,
      "explanation": "Look-aside (cache-aside): Application explicitly manages the cache. Check cache → if miss, query DB → store result in cache. The cache doesn't know about the database; the application coordinates.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-048",
      "type": "multiple-choice",
      "question": "What is 'look-through' caching?",
      "options": [
        "Transparent cache that application doesn't manage",
        "The cache sits in front of the database and handles misses automatically",
        "Looking at cache contents",
        "Cache debugging mode"
      ],
      "correct": 1,
      "explanation": "Look-through (read-through): Cache transparently fetches from database on miss. Application only talks to cache. Simpler application logic but cache must understand how to load data.",
      "detailedExplanation": "Generalize from 'look-through' caching to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-049",
      "type": "multi-select",
      "question": "Which are challenges with cross-datacenter cache invalidation? (Select all that apply)",
      "options": [
        "Network latency between datacenters",
        "Message delivery reliability (lost invalidations)",
        "Clock synchronization",
        "Ordering of invalidations and writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Cross-DC invalidation faces: latency (messages take time), reliability (networks fail, messages lost), ordering (invalidation may arrive before/after write). Clock sync matters but isn't the primary challenge for invalidation.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-050",
      "type": "multiple-choice",
      "question": "What is 'mcsqueal' in Facebook's memcache architecture?",
      "options": [
        "A cache compression algorithm",
        "A daemon that tails MySQL's commit log to invalidate cache entries",
        "A cache query language",
        "A monitoring tool"
      ],
      "correct": 1,
      "explanation": "mcsqueal watches MySQL's binary log (commits) and sends invalidations to memcache when data changes. This ensures cache invalidation happens reliably based on actual database commits.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-051",
      "type": "multiple-choice",
      "question": "What is 'regional pool' vs 'wildcard' (all-region) in Facebook's caching?",
      "options": [
        "Pool size differences",
        "Regional pools cache region-specific data; wildcards cache data needed by all regions",
        "Security zones",
        "Memory allocation strategies"
      ],
      "correct": 1,
      "explanation": "Regional pools hold data accessed primarily within one region. Wildcard/all-region pools hold globally-accessed data (like user profiles). This optimizes replication—regional data doesn't need cross-region sync.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-052",
      "type": "multiple-choice",
      "question": "In a sharded distributed cache, what happens if one shard becomes unavailable?",
      "options": [
        "All cache operations fail",
        "Only keys mapped to that shard are unavailable; others work normally",
        "Keys automatically redistribute",
        "Cache switches to backup mode"
      ],
      "correct": 1,
      "explanation": "Sharding isolates failures. Keys on the failed shard become unavailable (cache misses → database), but keys on other shards continue working. This partial availability is better than total failure.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-053",
      "type": "numeric-input",
      "question": "A cache is sharded across 10 nodes. One node fails. Approximately what percentage of cache requests now miss (assuming uniform distribution)?",
      "answer": 10,
      "tolerance": 0.05,
      "unit": "%",
      "explanation": "With uniform distribution, each shard holds ~10% of keys. That shard's keys become misses, so ~10% of requests now miss (hit the database). The other 90% still hit cache.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. Treat freshness policy and invalidation paths as first-class constraints. If values like 10 and 90 appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your 10-shard cache handles 100K requests/sec with 95% hit rate. One shard fails. Previous miss rate was 5K/sec to DB. What's the new miss rate?",
          "options": ["5K/sec", "10K/sec", "15K/sec", "100K/sec"],
          "correct": 2,
          "explanation": "Original: 5K/sec misses. Failed shard adds ~10K/sec more misses (10% of traffic). Total: 5K + 10K = 15K/sec misses hitting the database.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 10 and 100K in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "Your database can handle 12K/sec. What happens?",
          "options": [
            "Database handles it fine",
            "Database becomes overloaded (15K > 12K capacity)",
            "Requests queue up",
            "Cache automatically recovers"
          ],
          "correct": 1,
          "explanation": "15K/sec exceeds DB capacity of 12K/sec. Database becomes overloaded—queries slow down, timeouts increase, potentially cascading failures. This is why cache availability matters so much.",
          "detailedExplanation": "Generalize from your database can handle 12K/sec to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 12K and 15K in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-055",
      "type": "multiple-choice",
      "question": "What is 'consistent hashing with bounded loads'?",
      "options": [
        "Limiting items per node",
        "Extension that limits how much more than average any node can take, redistributing overflow",
        "Hashing with encryption",
        "Consistent hashing with compression"
      ],
      "correct": 1,
      "explanation": "Bounded loads consistent hashing ensures no node exceeds (1 + ε) × average load. If a node would exceed its bound, the key is assigned to the next node on the ring. This smooths hot spots.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 1 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-056",
      "type": "multiple-choice",
      "question": "What is 'jump consistent hash'?",
      "options": [
        "Hashing with random jumps",
        "A fast consistent hash algorithm using simple arithmetic, ideal for sequentially numbered nodes",
        "Hashing that skips nodes",
        "Hash-based load balancing"
      ],
      "correct": 1,
      "explanation": "Jump consistent hash is a simple, fast algorithm that maps keys to buckets. It only requires knowing the number of buckets (not bucket IDs), uses minimal memory, and distributes keys evenly. Great for statically numbered shards.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-057",
      "type": "multi-select",
      "question": "Which are trade-offs of using more replicas in a distributed cache? (Select all that apply)",
      "options": [
        "Higher availability on node failures",
        "More read throughput (can read from replicas)",
        "Higher memory cost (data duplicated)",
        "Faster writes"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "More replicas improve availability (more copies) and read throughput (distribute reads). But memory cost increases (same data stored N times). Writes don't speed up—they must propagate to all replicas.",
      "detailedExplanation": "Generalize from trade-offs of using more replicas in a distributed cache? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-058",
      "type": "multiple-choice",
      "question": "What is 'connection pooling' important for distributed caches?",
      "options": [
        "Reduces memory usage",
        "Reduces overhead of establishing new TCP connections for each request",
        "Improves cache hit rate",
        "Enables encryption"
      ],
      "correct": 1,
      "explanation": "TCP connection setup takes time (handshake, potentially TLS). Connection pools maintain open connections for reuse. This reduces latency and load on both client and server for high-request-rate workloads.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-059",
      "type": "numeric-input",
      "question": "A service has 100 application servers, each with a pool of 10 connections per cache shard. With 5 shards, how many total connections does each shard receive?",
      "answer": 1000,
      "tolerance": "exact",
      "unit": "connections",
      "explanation": "Each of 100 servers maintains 10 connections to each shard. Per shard: 100 × 10 = 1,000 connections. This connection count can become a scaling bottleneck.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Normalize units before computing so conversion mistakes do not propagate. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. If values like 100 and 10 appear, convert them into one unit basis before comparison. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-060",
      "type": "multiple-choice",
      "question": "Redis Cluster mode has limitations on multi-key operations. Why?",
      "options": [
        "Protocol limitation",
        "Keys may be on different nodes, making atomic operations impossible without coordination",
        "Memory constraints",
        "Security reasons"
      ],
      "correct": 1,
      "explanation": "In Redis Cluster, keys on different slots may be on different nodes. Operations like MGET, MSET, or transactions across multiple keys only work if all keys are on the same slot (achieved via hash tags).",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-061",
      "type": "multiple-choice",
      "question": "How do you run a Lua script atomically across keys in Redis Cluster?",
      "options": [
        "It works automatically",
        "All keys accessed by the script must be in the same slot (use hash tags)",
        "Not possible in Cluster mode",
        "Use special Lua syntax"
      ],
      "correct": 1,
      "explanation": "Redis executes Lua scripts atomically on a single node. In Cluster mode, all keys accessed by the script must hash to the same slot. Use hash tags to ensure this: {user123}:profile, {user123}:settings.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-062",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to atomically increment two counters 'pageviews:home' and 'pageviews:total' in Redis Cluster. Can you use MULTI/EXEC transaction?",
          "options": [
            "Yes, transactions work across any keys",
            "No, keys may be on different nodes",
            "Yes, if on the same node",
            "No, transactions aren't supported in Cluster"
          ],
          "correct": 1,
          "explanation": "These keys likely hash to different slots/nodes. MULTI/EXEC only works within a single node. The transaction would fail or operate non-atomically.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes."
        },
        {
          "question": "How would you fix this to enable atomic operation?",
          "options": [
            "Use hash tags: {pageviews}:home and {pageviews}:total",
            "Use Lua script without hash tags",
            "Run two separate INCR commands",
            "Use pipeline"
          ],
          "correct": 0,
          "explanation": "Hash tags {pageviews} ensure both keys hash to the same slot: {pageviews}:home and {pageviews}:total. Now they're on the same node and MULTI/EXEC or Lua works atomically.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-063",
      "type": "multiple-choice",
      "question": "What is 'pipeline' in Redis and why is it useful for distributed caching?",
      "options": [
        "A data transformation flow",
        "Sending multiple commands without waiting for each response, reducing round-trips",
        "A connection type",
        "A replication mechanism"
      ],
      "correct": 1,
      "explanation": "Pipelining batches multiple commands into one network round-trip. Instead of send--send-you send-send-send-. This dramatically reduces latency for bulk operations, especially over high-latency networks.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-064",
      "type": "numeric-input",
      "question": "Network round-trip to cache is 1ms. Executing 100 commands sequentially takes how long minimum (network only)?",
      "answer": 100,
      "tolerance": "exact",
      "unit": "ms",
      "explanation": "Each command requires a round-trip: 100 commands × 1ms = 100ms minimum (ignoring processing time). Pipelining could reduce this to ~1ms (one round-trip for all commands).",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 1ms and 100 in aligned units before deciding on an implementation approach. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-065",
      "type": "multiple-choice",
      "question": "What is a limitation of pipelining in Redis Cluster?",
      "options": [
        "Commands can't use pipelines",
        "Pipeline commands must all go to the same node (smart clients split pipelines by node)",
        "Pipelines are slower",
        "Pipelines don't work over TLS"
      ],
      "correct": 1,
      "explanation": "In Cluster, different keys may be on different nodes. A pipeline must be split—commands for each node are pipelined separately. Smart clients handle this automatically, but it adds complexity.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-066",
      "type": "multiple-choice",
      "question": "What is 'request coalescing' or 'request deduplication' in distributed caching?",
      "options": [
        "Merging similar data",
        "Combining multiple simultaneous requests for the same key into one backend request",
        "Compressing requests",
        "Removing duplicate cache entries"
      ],
      "correct": 1,
      "explanation": "When many clients request the same key simultaneously (e.g., during thundering herd), coalescing makes one backend request and returns the result to all waiting clients. Reduces backend load.",
      "detailedExplanation": "Generalize from 'request coalescing' or 'request deduplication' in distributed caching to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-067",
      "type": "multi-select",
      "question": "Which cache proxy features help with distributed caching at scale? (Select all that apply)",
      "options": [
        "Connection pooling to backends",
        "Request coalescing",
        "Automatic sharding",
        "Built-in data persistence to disk"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Common proxy features: Connection pooling reduces backend connections. Coalescing reduces duplicate requests. Auto-sharding hides topology. Data persistence is a backend feature (Redis RDB/AOF), not a proxy responsibility — proxies route requests, they don't store data.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-068",
      "type": "multiple-choice",
      "question": "mcrouter (Facebook's Memcached proxy) can do 'shadow' requests. What is this?",
      "options": [
        "Hiding requests from logs",
        "Sending copies of requests to a test cluster without affecting production",
        "Encrypted requests",
        "Background requests"
      ],
      "correct": 1,
      "explanation": "Shadow mode duplicates requests to a second cluster (e.g., new version). Results from shadow are ignored. This allows testing new deployments with real production traffic without risk.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-069",
      "type": "multiple-choice",
      "question": "What is 'cache-through' vs 'cache-aside' in terms of write handling?",
      "options": [
        "Same thing",
        "Cache-through: cache handles DB writes; cache-aside: app handles DB writes",
        "Cache-through is for reads only",
        "Cache-aside has no cache"
      ],
      "correct": 1,
      "explanation": "Cache-through (write-through/write-behind): writes go to cache, which writes to DB. Cache-aside: app writes to DB directly, then updates/invalidates cache. Cache-aside gives app more control; cache-through simplifies app logic.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-070",
      "type": "two-stage",
      "stages": [
        {
          "question": "Distributed cache across 3 AZs in one region. One AZ loses network connectivity. What happens to cache operations?",
          "options": [
            "All operations fail",
            "Operations to that AZ's nodes fail; others succeed",
            "Operations automatically reroute",
            "Cache switches to readonly"
          ],
          "correct": 1,
          "explanation": "Without automatic failover, operations targeting nodes in the isolated AZ fail (timeout). Operations to other AZs continue. Keys on isolated nodes become unavailable.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 3 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "You have replicas in each AZ. How does this help?",
          "options": [
            "No change—replicas fail too",
            "Failover promotes replica in healthy AZ to primary",
            "Replicas serve reads but writes still fail",
            "Replicas automatically sync"
          ],
          "correct": 1,
          "explanation": "With cross-AZ replicas, failover can promote a replica in a healthy AZ to primary. The isolated AZ's nodes are demoted/removed. Service continues with potentially higher latency or reduced capacity.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-071",
      "type": "multiple-choice",
      "question": "What is 'rack awareness' in distributed caching?",
      "options": [
        "Physical rack management",
        "Placing replicas across different failure domains (racks/AZs) for resilience",
        "Rack-mounted servers only",
        "Tracking rack utilization"
      ],
      "correct": 1,
      "explanation": "Rack awareness ensures replicas aren't all in the same failure domain. If a rack/AZ fails, replicas in other domains survive. Redis Cluster and many distributed systems support replica placement policies.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-072",
      "type": "multiple-choice",
      "question": "What metric indicates cache cluster health issues?",
      "options": [
        "Memory usage exactly at limit",
        "Significant variance in latency percentiles across nodes",
        "Consistent hit rate",
        "Low eviction rate"
      ],
      "correct": 1,
      "explanation": "If some nodes have much higher p99 latency than others, there may be hot spots, hardware issues, or unbalanced sharding. Healthy clusters have relatively consistent performance across nodes.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-073",
      "type": "multi-select",
      "question": "Which metrics should you monitor for distributed cache health? (Select all that apply)",
      "options": [
        "Hit rate per node",
        "Memory usage per node",
        "Replication lag",
        "Total number of unique keys ever written"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Key metrics: Hit rate shows cache effectiveness. Memory usage indicates capacity. Replication lag affects consistency. Total unique keys ever written is a cumulative counter that grows monotonically — it doesn't indicate current health or problems.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-074",
      "type": "multiple-choice",
      "question": "What is 'cache cluster rebalancing'?",
      "options": [
        "Restarting nodes",
        "Redistributing keys/slots across nodes when nodes are added or removed",
        "Memory defragmentation",
        "Balancing hit rates"
      ],
      "correct": 1,
      "explanation": "Rebalancing moves data when cluster topology changes. Adding a node? Move some keys to it. Removing a node? Move its keys elsewhere. This should be gradual to avoid performance impact.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-075",
      "type": "multiple-choice",
      "question": "During Redis Cluster slot migration, what happens to operations on migrating keys?",
      "options": [
        "Operations fail",
        "ASKING redirects guide clients to the new location",
        "Operations are queued",
        "Operations go to both nodes"
      ],
      "correct": 1,
      "explanation": "During migration, a key might be on source or target. Clients receive ASK redirects to try the target. This ensures continuous operation during migration, though with some redirect overhead.",
      "detailedExplanation": "Generalize from during Redis Cluster slot migration, what happens to operations on migrating keys to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-076",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're adding a new shard to a 4-shard consistent hash cluster. To minimize disruption, how should keys migrate?",
          "options": [
            "Move 100% of keys from one old shard",
            "Move ~20% of keys from each existing shard to new shard",
            "Move keys randomly",
            "Don't migrate—only new keys go to new shard"
          ],
          "correct": 1,
          "explanation": "New shard should get 1/5 = 20% of total keys, taken proportionally from each existing shard. This balances load while minimizing disruption (each shard loses only 20% of its keys).",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 4 and 1 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "What happens to the keys being migrated during the migration process?",
          "options": [
            "Keys unavailable during migration",
            "Keys readable from old location, writes might go to new location",
            "Keys duplicated temporarily",
            "Keys deleted and must be refetched"
          ],
          "correct": 2,
          "explanation": "Typically, keys are copied (not moved) during migration—exist in both places temporarily. Once fully copied, ownership transfers. This ensures availability during migration.",
          "detailedExplanation": "Generalize from happens to the keys being migrated during the migration process to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-077",
      "type": "multiple-choice",
      "question": "What is 'gossip protocol' in Redis Cluster?",
      "options": [
        "Logging protocol",
        "Nodes periodically exchange cluster state information with random peers",
        "Client-server communication",
        "Replication protocol"
      ],
      "correct": 1,
      "explanation": "Gossip: each node periodically pings random other nodes, exchanging cluster state (who's alive, who owns which slots). This disseminates information without central coordination—eventually all nodes converge on the same view.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-078",
      "type": "multiple-choice",
      "question": "What is 'cluster bus' in Redis Cluster?",
      "options": [
        "Public transportation for data",
        "A dedicated port/protocol for node-to-node cluster communication",
        "The main client protocol",
        "A message queue"
      ],
      "correct": 1,
      "explanation": "Redis Cluster uses a separate 'cluster bus' port (typically data port + 10000) for node-to-node communication: gossip, failover coordination, and configuration. This keeps cluster traffic separate from client traffic.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 10000 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-079",
      "type": "numeric-input",
      "question": "Redis Cluster nodes need to communicate. With N nodes, how does connection count between nodes scale?",
      "answer": 2,
      "tolerance": 0.2,
      "unit": "polynomial degree",
      "explanation": "Each node connects to every other node for gossip: N × (N-1) / 2 = O(N²) connections. Cluster size is practically limited by this quadratic scaling of inter-node connections.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-080",
      "type": "multiple-choice",
      "question": "What is 'cluster-require-full-coverage' in Redis Cluster?",
      "options": [
        "Requires data on all nodes",
        "If any slot is unassigned/unreachable, the cluster refuses to serve any requests",
        "Full replication requirement",
        "Coverage testing mode"
      ],
      "correct": 1,
      "explanation": "When 'yes' (default), cluster rejects all commands if any slot is unavailable. When 'no', cluster serves requests for available slots while unavailable slots error. Trade-off: consistency vs. partial availability.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-081",
      "type": "multi-select",
      "question": "Which are advantages of Redis Cluster over Sentinel with sharding? (Select all that apply)",
      "options": [
        "Built-in sharding with automatic slot management",
        "No single point of failure (decentralized)",
        "Simpler client implementation",
        "Multi-key operations across all keys"
      ],
      "correctIndices": [0, 1],
      "explanation": "Cluster has built-in sharding and decentralized architecture (no Sentinel coordinator). However, clients need Cluster-aware drivers (more complex), and multi-key operations are limited to same-slot keys.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-082",
      "type": "multiple-choice",
      "question": "What is 'read replicas' scaling pattern in distributed caching?",
      "options": [
        "Replicas that only read from primary",
        "Directing read traffic to replicas to scale read throughput beyond primary's capacity",
        "Read-only cache mode",
        "Backup replicas"
      ],
      "correct": 1,
      "explanation": "Read replicas receive writes from primary and serve read traffic. For read-heavy workloads, add replicas to scale read throughput. Writes still go to primary. Trade-off: eventual consistency for reads.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-083",
      "type": "numeric-input",
      "question": "Primary handles 50K reads/sec max. You add 2 read replicas. Assuming even distribution, what's the new max read throughput?",
      "answer": 150,
      "tolerance": 0.05,
      "unit": "K reads/sec",
      "explanation": "With 1 primary + 2 replicas, each handling 50K reads/sec: 3 × 50K = 150K reads/sec total capacity. Reads are distributed across all three nodes.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 50K and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-084",
      "type": "two-stage",
      "stages": [
        {
          "question": "You scale to 1 primary + 5 read replicas. Write throughput is limited by what?",
          "options": [
            "Total of all replicas",
            "Primary alone (all writes go there)",
            "Network bandwidth",
            "Client count"
          ],
          "correct": 1,
          "explanation": "Writes go only to primary, which replicates to replicas. Write throughput = primary's capacity. Adding replicas scales reads, not writes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 1 and 5 in aligned units before deciding on an implementation approach. Common pitfall: choosing tier by cost only and missing access constraints."
        },
        {
          "question": "To scale write throughput, what architecture change is needed?",
          "options": [
            "More read replicas",
            "Sharding (multiple primaries for different key ranges)",
            "Larger primary instance",
            "Write batching"
          ],
          "correct": 1,
          "explanation": "Sharding splits the keyspace across multiple primaries. Each primary handles writes for its keys. With N shards, write capacity is ~N× single primary capacity.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead."
        }
      ],
      "detailedExplanation": "Generalize from distributed Caching to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-085",
      "type": "multiple-choice",
      "question": "What is 'cache topology' in distributed caching?",
      "options": [
        "Physical server layout",
        "The arrangement of cache nodes (shards, replicas, proxies) and how they connect",
        "Network topology only",
        "Memory layout"
      ],
      "correct": 1,
      "explanation": "Cache topology describes the system structure: how many shards, replicas per shard, whether there's a proxy layer, how nodes are distributed across failure domains. Topology decisions impact scalability and reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-086",
      "type": "multiple-choice",
      "question": "What is 'cache stampede prevention' at the cluster level?",
      "options": [
        "Preventing cache server crashes",
        "Coordinated mechanisms to prevent all nodes from simultaneously fetching the same missing key",
        "Load balancing",
        "Connection limiting"
      ],
      "correct": 1,
      "explanation": "Cluster-level stampede prevention ensures that when a key expires/is missing, only one node/request fetches from the database. Others wait for the result. Requires distributed locking or leader election.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-087",
      "type": "multiple-choice",
      "question": "What challenge does 'cluster-wide hot key' present that per-node hot key doesn't?",
      "options": [
        "Higher latency",
        "Key is hot across all traffic, but data lives on one node—distributing it requires replication/client-side caching",
        "More memory usage",
        "Network congestion"
      ],
      "correct": 1,
      "explanation": "A hot key's shard becomes the bottleneck for the entire cluster. Unlike random hot spots (fixable with rebalancing), a single hot key can't be split. Solutions: replicate the key, add local caching, or shard within the key.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-088",
      "type": "multi-select",
      "question": "Which are signs of a hot key in a distributed cache cluster? (Select all that apply)",
      "options": [
        "One node has much higher CPU than others",
        "One node has much higher network traffic",
        "One node has much higher latency",
        "Overall cache size is unbalanced"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Hot keys cause traffic imbalance: affected node has higher CPU (processing requests), network (data transfer), and latency (queuing). Cache size imbalance suggests sharding issues, not hot keys specifically.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-089",
      "type": "multiple-choice",
      "question": "What is 'local cache' in front of distributed cache?",
      "options": [
        "Cache on local disk",
        "In-process cache on each application server as L1, with distributed cache as L2",
        "Cache in same datacenter",
        "Cache on same machine"
      ],
      "correct": 1,
      "explanation": "Local (L1) cache is in-process memory on each app server. Distributed cache is L2. Hot data lives in L1 (fastest, per-server). Misses fall through to L2. This reduces network calls for very hot data.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-090",
      "type": "two-stage",
      "stages": [
        {
          "question": "You add L1 local caches (100MB each) on 10 app servers in front of an L2 Redis cluster. What's a challenge with this approach?",
          "options": [
            "Too much memory",
            "L1 caches may have different/stale versions of the same key",
            "L2 becomes useless",
            "Lower hit rate"
          ],
          "correct": 1,
          "explanation": "Each L1 cache is independent. Updates to L2 don't automatically invalidate L1. Server A may have stale V1 while server B has V2. This is L1 cache coherence problem.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 100MB and 10 in aligned units before deciding on an implementation approach. Common pitfall: hot-key skew causing uneven load."
        },
        {
          "question": "How can you mitigate L1 cache staleness?",
          "options": [
            "Don't use L1 cache",
            "Use very short TTL on L1 and/or pub-sub invalidation",
            "Make L1 larger",
            "Sync L1 caches directly"
          ],
          "correct": 1,
          "explanation": "Short L1 TTL limits staleness duration. Pub-sub invalidation (Redis pub/sub when data changes) actively clears L1 entries. Balance: shorter TTL = less stale but more L2 traffic.",
          "detailedExplanation": "Generalize from you mitigate L1 cache staleness to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-091",
      "type": "multiple-choice",
      "question": "Netflix's EVCache uses what technique for cross-region consistency?",
      "options": [
        "Synchronous replication",
        "Invalidation messages sent to other regions on write",
        "Single global region",
        "No cross-region support"
      ],
      "correct": 1,
      "explanation": "EVCache sends invalidation messages to other regions when data is written. Other regions invalidate their local cache; data is refetched on next access. This provides eventual consistency without full replication.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-092",
      "type": "multiple-choice",
      "question": "What is 'cache tiering' across storage types?",
      "options": [
        "Multiple cache providers",
        "Hot data in fastest storage (RAM), warm in slower (SSD), cold in slowest (HDD/network)",
        "Pricing tiers",
        "Security tiers"
      ],
      "correct": 1,
      "explanation": "Cache tiering uses multiple storage tiers: RAM for hot data (fastest, expensive), SSD for warm data (fast, cheaper), HDD/network for cold. Items demote between tiers based on access patterns.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-093",
      "type": "multi-select",
      "question": "Which technologies can serve as distributed cache? (Select all that apply)",
      "options": [
        "Redis Cluster",
        "Memcached with consistent hashing",
        "Apache Ignite",
        "SQLite"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Redis Cluster, Memcached, and Ignite are distributed caching solutions. SQLite is an embedded single-file database with no built-in distribution or clustering — it's designed for local, single-process use, not distributed caching.",
      "detailedExplanation": "Generalize from technologies can serve as distributed cache? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-094",
      "type": "multiple-choice",
      "question": "What is 'write-around' caching in distributed systems?",
      "options": [
        "Writing to all replicas",
        "Writes go directly to database, cache is populated only on read",
        "Writing to cache only",
        "Circular write pattern"
      ],
      "correct": 1,
      "explanation": "Write-around: writes bypass cache, go straight to DB. Cache is filled only when data is read. Good when written data isn't immediately read. Avoids caching data that may never be accessed.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-095",
      "type": "multiple-choice",
      "question": "What is the CAP theorem implication for distributed caching?",
      "options": [
        "Distributed caches can have all three: consistency, availability, partition tolerance",
        "Must choose between consistency and availability during network partitions",
        "Caching doesn't involve CAP theorem",
        "Only affects databases"
      ],
      "correct": 1,
      "explanation": "During network partitions, distributed caches must choose: CP (consistent but may be unavailable) or AP (available but may serve stale data). Most caches choose AP—serve stale data rather than fail.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-096",
      "type": "two-stage",
      "stages": [
        {
          "question": "Network partition splits your cache cluster. Nodes can't reach each other but clients can reach some nodes. What happens with eventual consistency (AP)?",
          "options": [
            "All operations fail",
            "Operations succeed but partitions may diverge; reconciled after partition heals",
            "Only reads work",
            "System shuts down"
          ],
          "correct": 1,
          "explanation": "AP systems continue operating during partition. Each partition accepts writes independently. After partition heals, systems reconcile—possibly with conflicts if same keys were modified differently.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates."
        },
        {
          "question": "What happens with strong consistency (CP)?",
          "options": [
            "Same as AP",
            "Minority partition refuses operations to prevent divergence",
            "Both partitions continue normally",
            "All data is deleted"
          ],
          "correct": 1,
          "explanation": "CP systems require majority quorum. The minority partition cannot confirm writes, so it refuses operations. Only the majority partition serves requests. This prevents divergence at the cost of availability.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-097",
      "type": "multiple-choice",
      "question": "What is 'cache-aside with fence tokens' pattern?",
      "options": [
        "Physical barriers around cache servers",
        "Using tokens to prevent stale cache writes from overwriting newer data",
        "Token-based authentication",
        "Rate limiting tokens"
      ],
      "correct": 1,
      "explanation": "Fence tokens are monotonic IDs from database. When updating cache, check the token: only update if your token is higher than the current cached token. Prevents races where stale data overwrites fresh data.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-098",
      "type": "multi-select",
      "question": "Which are best practices for distributed cache operations? (Select all that apply)",
      "options": [
        "Set reasonable TTLs on all entries",
        "Handle cache failures gracefully (fallback to database)",
        "Monitor hit rates and latency across nodes",
        "Use synchronous replication for all data"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "TTLs prevent stale data and memory exhaustion. Graceful failure handling prevents cache outages from becoming full outages. Monitoring enables early detection of issues. Sync replication is often too slow—use it selectively.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-099",
      "type": "multiple-choice",
      "question": "What is 'cache aside with double-check locking'?",
      "options": [
        "Checking cache twice",
        "On miss, acquire lock, check cache again (might have been filled while waiting), only then fetch from DB",
        "Double encryption",
        "Two-phase locking"
      ],
      "correct": 1,
      "explanation": "After acquiring a lock for cache fill, check cache again—another thread may have filled it while you waited for the lock. This prevents duplicate database queries in high-concurrency scenarios.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    },
    {
      "id": "distcache-100",
      "type": "multiple-choice",
      "question": "When designing a distributed caching layer, what should you decide first?",
      "options": [
        "Which technology to use",
        "How many nodes to deploy",
        "What data to cache and consistency requirements",
        "Network topology"
      ],
      "correct": 2,
      "explanation": "Start with: what data needs caching? How fresh must it be? Can stale data be served? What's the read/write ratio? These requirements drive architecture decisions (technology, topology, replication strategy).",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["caching", "distributed-caching"],
      "difficulty": "senior"
    }
  ]
}
