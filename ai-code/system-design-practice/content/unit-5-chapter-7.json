{
  "chapterTitle": "Distributed Caching",
  "problems": [
    {
      "id": "distcache-001",
      "type": "multiple-choice",
      "question": "What is the primary challenge that consistent hashing solves in distributed caches?",
      "options": [
        "Reducing memory usage",
        "Minimizing key redistribution when nodes are added or removed",
        "Improving cache hit rates",
        "Reducing network latency"
      ],
      "correct": 1,
      "explanation": "Consistent hashing maps keys to a ring of positions. When a node joins/leaves, only keys mapped to that node's range need to move. With naive modulo hashing (key % N), adding a node changes N and remaps most keys.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-002",
      "type": "numeric-input",
      "question": "A distributed cache uses modulo hashing (key % N) with 10 nodes. If you add 1 node, approximately what percentage of keys need to move?",
      "answer": 91,
      "tolerance": 0.05,
      "unit": "%",
      "explanation": "With modulo hashing, key % 10 vs key % 11 produces different results for most keys. Approximately N/(N+1) = 10/11 ≈ 91% of keys change their assigned node.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-003",
      "type": "numeric-input",
      "question": "The same cache uses consistent hashing. Adding 1 node to 10 nodes moves approximately what percentage of keys?",
      "answer": 9,
      "tolerance": 0.15,
      "unit": "%",
      "explanation": "With consistent hashing, only keys in the new node's range move—approximately 1/(N+1) = 1/11 ≈ 9% of keys. This is dramatically less disruption than modulo hashing.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-004",
      "type": "multiple-choice",
      "question": "In consistent hashing, what are 'virtual nodes' (vnodes)?",
      "options": [
        "Backup nodes that take over on failure",
        "Multiple positions on the hash ring per physical node for better distribution",
        "Simulated nodes for testing",
        "Nodes in different data centers"
      ],
      "correct": 1,
      "explanation": "Each physical node is assigned multiple positions (virtual nodes) on the hash ring. This improves load distribution—a node with 100 vnodes gets ~100 segments of the ring instead of 1 large segment.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-005",
      "type": "multiple-choice",
      "question": "Why do virtual nodes improve load balancing in consistent hashing?",
      "options": [
        "They reduce network hops",
        "They spread each node's responsibility across many small ring segments instead of one large segment",
        "They cache more data",
        "They reduce memory usage"
      ],
      "correct": 1,
      "explanation": "With one position per node, segment sizes vary widely based on hash distribution. With many vnodes, the law of large numbers applies—each node gets roughly equal total coverage despite random positions.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-006",
      "type": "two-stage",
      "stages": [
        {
          "question": "A consistent hash ring has 3 nodes at positions 0, 120, and 240 (degrees). A key hashes to position 100. Which node owns it?",
          "options": ["Node at 0", "Node at 120", "Node at 240"],
          "correct": 1,
          "explanation": "Keys are assigned to the next node clockwise on the ring. From 100, the next node clockwise is at 120.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "The node at 120 fails. Which node now owns the key at position 100?",
          "options": ["Node at 0", "Node at 240", "Key becomes unavailable"],
          "correct": 1,
          "explanation": "With node 120 gone, the next clockwise node from 100 is at 240. Only keys in 120's range (1-120) move to 240; keys in 0's range (241-360) stay with node 0.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-007",
      "type": "multiple-choice",
      "question": "What is 'cache sharding'?",
      "options": [
        "Breaking the cache into pieces stored across multiple nodes",
        "Encrypting cache data",
        "Compressing cached values",
        "Sharing cache between applications"
      ],
      "correct": 0,
      "explanation": "Sharding partitions the cache keyspace across multiple nodes. Each node holds a subset of keys. This allows horizontal scaling—add more nodes to increase total capacity.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "distcache-008",
      "type": "multi-select",
      "question": "Which are benefits of cache sharding? (Select all that apply)",
      "options": [
        "Horizontal scalability (more nodes = more capacity)",
        "Parallel reads across different keys",
        "Automatic failover",
        "Reduced memory per node"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Sharding enables horizontal scaling, parallel access to different shards, and smaller memory footprint per node. Automatic failover requires additional mechanisms (replication, sentinel)—sharding alone doesn't provide it.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-009",
      "type": "multiple-choice",
      "question": "What is 'client-side sharding' in distributed caching?",
      "options": [
        "Cache stored in the browser",
        "Client library determines which server holds each key",
        "Clients cache data locally",
        "Sharding based on client IP"
      ],
      "correct": 1,
      "explanation": "In client-side sharding, the client library hashes each key to determine which cache server to contact. No central coordinator—each client independently routes requests. Simple but requires client updates when topology changes.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-010",
      "type": "multiple-choice",
      "question": "What is a disadvantage of client-side sharding?",
      "options": [
        "Higher latency",
        "All clients must be updated when cache topology changes",
        "Cannot scale beyond one server",
        "Requires special hardware"
      ],
      "correct": 1,
      "explanation": "Every client embeds the shard map. When nodes are added/removed, all clients need configuration updates (or smart clients with auto-discovery). Inconsistent views between clients can cause misrouted requests.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-011",
      "type": "multiple-choice",
      "question": "What is a 'cache proxy' like Twemproxy or mcrouter?",
      "options": [
        "A CDN service",
        "An intermediary that routes cache requests to the appropriate shard",
        "A backup cache server",
        "A cache monitoring tool"
      ],
      "correct": 1,
      "explanation": "Cache proxies sit between clients and cache servers. Clients send all requests to the proxy, which routes to the correct shard. This centralizes sharding logic—clients don't need to know the topology.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-012",
      "type": "multi-select",
      "question": "Which are advantages of using a cache proxy over client-side sharding? (Select all that apply)",
      "options": [
        "Topology changes don't require client updates",
        "Connection pooling reduces connections to cache servers",
        "Lower latency than direct connection",
        "Simpler client implementation"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Proxies hide topology from clients (no updates needed), pool connections (fewer connections to backend), and simplify clients. However, proxies add a network hop, increasing latency slightly.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "distcache-013",
      "type": "multiple-choice",
      "question": "Redis Cluster uses 16384 hash slots. Why this specific number?",
      "options": [
        "Arbitrary choice",
        "Balances granularity for resharding with reasonable metadata size",
        "Matches TCP port range",
        "Maximum nodes supported"
      ],
      "correct": 1,
      "explanation": "16384 slots allows fine-grained resharding (move individual slots) while keeping slot metadata small (2KB bitmap per node). More slots = finer granularity but more metadata; fewer = coarser resharding.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-014",
      "type": "multiple-choice",
      "question": "In Redis Cluster, how is a key mapped to a slot?",
      "options": [
        "Modulo of string length",
        "CRC16 of key, then modulo 16384",
        "MD5 hash",
        "Random assignment"
      ],
      "correct": 1,
      "explanation": "Redis Cluster computes CRC16(key) % 16384 to determine the slot. If the key contains {...}, only the content inside braces is hashed—this allows co-locating related keys on the same slot.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-015",
      "type": "multiple-choice",
      "question": "Redis keys 'user:{123}:profile' and 'user:{123}:settings' will be on the same slot. Why?",
      "options": [
        "They have the same prefix",
        "Only the content inside {} is hashed (hash tags)",
        "They're both user keys",
        "Random chance"
      ],
      "correct": 1,
      "explanation": "Redis hash tags: when a key contains {...}, only the substring inside braces is hashed. Both keys hash '123', so they land on the same slot. This enables multi-key operations on related data.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-016",
      "type": "multiple-choice",
      "question": "What happens when a Redis Cluster client sends a command to the wrong node?",
      "options": [
        "The command fails",
        "The node returns a MOVED redirect with the correct node",
        "The node forwards the request internally",
        "The command executes on the wrong data"
      ],
      "correct": 1,
      "explanation": "Redis Cluster nodes respond with MOVED <slot> <ip:port> when they don't own the requested slot. Smart clients cache the slot→node mapping and update it on MOVED responses.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-017",
      "type": "multiple-choice",
      "question": "What is 'ASK' vs 'MOVED' in Redis Cluster?",
      "options": [
        "ASK is for reads, MOVED is for writes",
        "MOVED means permanent slot migration; ASK means migration in progress",
        "ASK is faster than MOVED",
        "They're identical"
      ],
      "correct": 1,
      "explanation": "MOVED indicates a slot permanently lives on another node—client should update its map. ASK means the slot is being migrated; try the other node once but don't update the map yet.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-018",
      "type": "multiple-choice",
      "question": "What is a 'hot key' problem in distributed caching?",
      "options": [
        "Keys that are frequently updated",
        "A single key receiving disproportionate traffic, overloading one node",
        "Keys with high TTL",
        "Encrypted keys"
      ],
      "correct": 1,
      "explanation": "Hot keys concentrate traffic on one node regardless of how many shards exist. If a viral tweet's cache key gets 100K requests/sec, that one node becomes the bottleneck while others sit idle.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-019",
      "type": "multi-select",
      "question": "Which are solutions to the hot key problem? (Select all that apply)",
      "options": [
        "Replicate hot keys across multiple nodes",
        "Add random suffix to key and read from any replica",
        "Use local caching in addition to distributed cache",
        "Increase the hot node's memory"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Solutions distribute hot key load: replicate the key, add random suffixes (key:1, key:2, key:3) and load balance reads, or cache locally (L1 cache in app). More memory on one node doesn't help if CPU/network is bottlenecked.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "distcache-020",
      "type": "two-stage",
      "stages": [
        {
          "question": "A hot key 'trending:post:123' gets 50K reads/sec. One Redis node handles 10K ops/sec max. You add random suffixes (key:0 through key:4) and replicate the value. How many reads/sec can you now handle?",
          "options": ["10K", "25K", "50K", "Unlimited"],
          "correct": 2,
          "explanation": "With 5 key variants spread across different nodes (or same node but enabling parallel ops), you can handle 5 × 10K = 50K reads/sec if load is evenly distributed across suffixes.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "What's the main operational challenge with this approach?",
          "options": [
            "Increased latency",
            "Must update all 5 keys on write (consistency)",
            "More memory usage",
            "Client complexity"
          ],
          "correct": 1,
          "explanation": "When the value changes, you must update all 5 keys atomically or accept temporary inconsistency. Write amplification and consistency management are the key challenges.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-021",
      "type": "multiple-choice",
      "question": "What is 'cache replication' in distributed systems?",
      "options": [
        "Copying cache code across servers",
        "Maintaining copies of cached data on multiple nodes for availability",
        "Duplicating requests",
        "Backing up cache to disk"
      ],
      "correct": 1,
      "explanation": "Cache replication keeps copies of data on multiple nodes. If the primary fails, a replica can serve requests. It trades storage efficiency for availability—same data exists in multiple places.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-022",
      "type": "multiple-choice",
      "question": "Redis replication is asynchronous by default. What does this mean?",
      "options": [
        "Writes complete before replication",
        "Primary acknowledges writes before replicas receive them",
        "Replicas run on different threads",
        "Replication happens on a schedule"
      ],
      "correct": 1,
      "explanation": "Asynchronous replication: primary acknowledges the write to the client, then sends it to replicas in the background. Lower latency but risks data loss if primary fails before replicating.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-023",
      "type": "multiple-choice",
      "question": "What is the trade-off between synchronous and asynchronous replication?",
      "options": [
        "Sync is faster",
        "Sync ensures no data loss but adds latency; async is faster but may lose recent writes on failure",
        "Async uses more memory",
        "No trade-off, sync is always better"
      ],
      "correct": 1,
      "explanation": "Synchronous replication waits for replica acknowledgment before confirming writes—no data loss but higher latency. Asynchronous confirms immediately—lower latency but recent writes may be lost if primary fails before replicating.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-024",
      "type": "multiple-choice",
      "question": "What is Redis Sentinel?",
      "options": [
        "A security scanner",
        "A monitoring and automatic failover system for Redis",
        "A cache proxy",
        "A data encryption tool"
      ],
      "correct": 1,
      "explanation": "Sentinel monitors Redis primary and replicas. If primary fails, Sentinel coordinates failover: promotes a replica to primary and reconfigures other replicas to follow the new primary. Provides high availability.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-025",
      "type": "multiple-choice",
      "question": "Why do you need multiple Sentinel instances (typically 3+)?",
      "options": [
        "Load balancing",
        "To avoid false positives through quorum-based failure detection",
        "To cache more data",
        "Geographic distribution"
      ],
      "correct": 1,
      "explanation": "A single Sentinel might incorrectly think the primary is down (network partition to Sentinel). Multiple Sentinels vote—failover only occurs if a quorum agrees the primary is unreachable. This prevents split-brain.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-026",
      "type": "numeric-input",
      "question": "You have 5 Redis Sentinels. What's the minimum quorum to avoid split-brain during network partitions?",
      "answer": 3,
      "tolerance": "exact",
      "unit": "sentinels",
      "explanation": "Quorum must be > N/2 to ensure only one partition can achieve it. With 5 Sentinels, quorum of 3 means both partitions can't independently elect a new primary—at most one partition has 3+ Sentinels.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "distcache-027",
      "type": "multiple-choice",
      "question": "What is 'split-brain' in distributed caching?",
      "options": [
        "A cache corruption issue",
        "Two nodes both acting as primary, accepting conflicting writes",
        "Cache data split across datacenters",
        "Brain-teaser interview questions"
      ],
      "correct": 1,
      "explanation": "Split-brain occurs when a network partition causes two nodes to both believe they're the primary. Each accepts writes independently, leading to data divergence that's difficult to reconcile.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "distcache-028",
      "type": "multi-select",
      "question": "Which mechanisms help prevent split-brain? (Select all that apply)",
      "options": [
        "Quorum-based leader election",
        "Fencing (STONITH)",
        "Increasing cache size",
        "Node liveness checks"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Quorum ensures only one partition can elect a leader. Fencing (STONITH: Shoot The Other Node In The Head) forcibly shuts down the old primary. Liveness checks detect failures. Cache size is unrelated.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "distcache-029",
      "type": "multiple-choice",
      "question": "In Redis Cluster, what happens during a node failure?",
      "options": [
        "All data is lost",
        "Cluster pauses until node returns",
        "Replica is promoted to primary for that node's slots",
        "Slots are redistributed to remaining nodes"
      ],
      "correct": 2,
      "explanation": "If a primary fails and has a replica, the replica is promoted to primary for those slots. If no replica exists, the cluster may enter a fail state for those slots (configurable via cluster-require-full-coverage).",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-030",
      "type": "multiple-choice",
      "question": "Redis Cluster requires minimum how many primary nodes?",
      "options": ["1", "2", "3", "5"],
      "correct": 2,
      "explanation": "Redis Cluster requires at least 3 primary nodes for the cluster protocol to function properly. With 3 primaries, losing 1 still leaves a majority to agree on cluster state.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-031",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a Redis Cluster with 3 primaries. For high availability, how many replicas per primary are recommended?",
          "options": ["0", "1", "2", "3"],
          "correct": 1,
          "explanation": "At minimum, 1 replica per primary. This allows any single primary to fail—its replica takes over. More replicas add redundancy but increase memory usage.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "With 3 primaries and 1 replica each, how many total Redis instances do you need?",
          "options": ["3", "4", "6", "9"],
          "correct": 2,
          "explanation": "3 primaries + 3 replicas = 6 total instances. Each primary has its dedicated replica for failover.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-032",
      "type": "multiple-choice",
      "question": "What is 'cache coherence' in distributed systems?",
      "options": [
        "Keeping cache data organized",
        "Ensuring all cache nodes have consistent views of the same data",
        "Cache compression",
        "Coherent naming conventions"
      ],
      "correct": 1,
      "explanation": "Cache coherence means maintaining consistency across distributed caches. When data is updated on one node, other nodes should eventually (or immediately) see the same value. This is challenging in distributed systems.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-033",
      "type": "multiple-choice",
      "question": "Which consistency model does a typical distributed cache with replication provide?",
      "options": [
        "Strong consistency",
        "Eventual consistency",
        "Linearizability",
        "Serializability"
      ],
      "correct": 1,
      "explanation": "Most distributed caches (Redis with async replication, Memcached) provide eventual consistency. Writes propagate over time; reads may temporarily see stale data from replicas. Strong consistency requires synchronous replication.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "distcache-034",
      "type": "multiple-choice",
      "question": "You write to a Redis primary and immediately read from a replica. The read returns stale data. What happened?",
      "options": [
        "Bug in Redis",
        "Replication lag—the write hasn't propagated to the replica yet",
        "Cache miss",
        "Wrong key used"
      ],
      "correct": 1,
      "explanation": "Asynchronous replication means replicas lag behind the primary. Reading from replica immediately after writing to primary may return the old value. This is expected behavior, not a bug.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-035",
      "type": "multi-select",
      "question": "How can you ensure read-your-writes consistency with Redis replicas? (Select all that apply)",
      "options": [
        "Always read from primary",
        "Use WAIT command to ensure replication before responding",
        "Track write timestamp and wait before reading from replica",
        "Use larger TTL"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Read from primary ensures you see your writes. WAIT blocks until N replicas acknowledge. Tracking timestamps lets you wait for replication before reading from replica. TTL is unrelated to consistency.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-036",
      "type": "multiple-choice",
      "question": "What is 'thundering herd' in the context of distributed caching?",
      "options": [
        "Too many cache servers",
        "Many clients simultaneously requesting an expired key, all hitting the database",
        "Network congestion",
        "Memory overflow"
      ],
      "correct": 1,
      "explanation": "When a popular cached item expires, many concurrent requests find it missing and all query the database simultaneously. This spike can overwhelm the database. It's worse in distributed caches with more clients.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-037",
      "type": "multi-select",
      "question": "Which techniques prevent thundering herd? (Select all that apply)",
      "options": [
        "Cache locking (only one request fetches on miss)",
        "Stale-while-revalidate",
        "Probabilistic early expiration",
        "Increasing database connections"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Locking ensures one request refills cache while others wait. Stale-while-revalidate serves old data while refreshing. Probabilistic early expiration spreads refreshes over time. More DB connections just delays the overload.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-038",
      "type": "multiple-choice",
      "question": "Implementing distributed cache locking for thundering herd prevention requires what?",
      "options": [
        "Larger cache",
        "Coordination mechanism (e.g., Redis SETNX) so only one client acquires lock",
        "Faster network",
        "More database replicas"
      ],
      "correct": 1,
      "explanation": "Distributed locking needs atomic operations. Redis SETNX (SET if Not eXists) or SET with NX flag atomically acquires a lock. First client gets the lock and fetches data; others wait or retry.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "distcache-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Client A acquires a distributed lock to refresh a cache entry. It starts a slow database query. What's a risk?",
          "options": [
            "Lock never releases if A crashes",
            "Other clients fetch duplicate data",
            "Cache becomes read-only",
            "Database crashes"
          ],
          "correct": 0,
          "explanation": "If A crashes or takes too long, the lock remains held indefinitely, blocking all other clients from refreshing the cache entry.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "How do you mitigate this risk?",
          "options": [
            "Use faster database",
            "Set TTL on the lock so it auto-expires",
            "Don't use locking",
            "Restart A automatically"
          ],
          "correct": 1,
          "explanation": "Lock TTL ensures automatic release if the holder crashes or is slow. SET key value NX EX 30 acquires a lock that expires in 30 seconds. The holder should complete and release before expiry.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-040",
      "type": "multiple-choice",
      "question": "What is 'cache warming' for distributed caches?",
      "options": [
        "Increasing cache memory",
        "Pre-populating cache with expected hot data before receiving traffic",
        "Raising server temperature",
        "Gradual traffic increase"
      ],
      "correct": 1,
      "explanation": "After cache restart, it's empty—all requests hit the database (cold cache). Cache warming pre-loads hot data (from DB, logs, or previous cache dump) to achieve good hit rates before taking full traffic.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-041",
      "type": "multi-select",
      "question": "Which are common cache warming strategies? (Select all that apply)",
      "options": [
        "Replay recent request logs against the cache",
        "Load from database using 'hot keys' list",
        "Restore from RDB/AOF dump",
        "Wait for organic traffic to warm cache"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Log replay simulates recent traffic. Hot keys lists identify popular items to preload. RDB/AOF restore recovers previous cache state. Organic warming works but causes initial performance degradation.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-042",
      "type": "multiple-choice",
      "question": "What is 'cache stamping' or 'dog-pile effect'?",
      "options": [
        "Cache data corruption",
        "Multiple servers generating the same cached value simultaneously on expiry",
        "Cache key collisions",
        "Excessive memory usage"
      ],
      "correct": 1,
      "explanation": "Similar to thundering herd—when a popular item expires, multiple application servers simultaneously try to regenerate and cache it. This wastes resources computing the same value multiple times.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-043",
      "type": "multiple-choice",
      "question": "You have caches in US-East and EU-West. A user in EU writes data. How do you ensure EU readers see the new value?",
      "options": [
        "It happens automatically with any cache",
        "Cross-region replication or invalidation must be configured",
        "Users must clear their browser cache",
        "Not possible"
      ],
      "correct": 1,
      "explanation": "Distributed caches in different regions don't automatically sync. You need cross-region replication (data copied) or invalidation (EU notified to refetch). Both add latency and complexity.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-044",
      "type": "multiple-choice",
      "question": "What is 'write-behind cross-region replication'?",
      "options": [
        "Writes to local region cache, then asynchronously replicates to other regions",
        "Writes go to all regions synchronously",
        "Writes only to the primary region",
        "Writes bypassing the cache"
      ],
      "correct": 0,
      "explanation": "Write-behind replication writes to the local region first (low latency), then asynchronously propagates to other regions. Improves write latency but introduces cross-region consistency lag.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "A global application has caches in US and EU with async cross-region replication. Write latency in US, replication lag to EU is 100ms. A US user writes, then an EU user reads after 50ms. What happens?",
          "options": [
            "EU user sees new value",
            "EU user sees stale value (replication not complete)",
            "Read fails",
            "Read waits for replication"
          ],
          "correct": 1,
          "explanation": "With 100ms replication lag and only 50ms elapsed, the write hasn't reached EU yet. EU user reads from local cache and gets the stale value.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "If strong consistency is required for this data, what's the solution?",
          "options": [
            "Increase cache size",
            "Route reads/writes to a single primary region",
            "Faster network",
            "More replicas"
          ],
          "correct": 1,
          "explanation": "For strong consistency, all reads/writes for specific data should go to one region (primary). Other regions can have read replicas with eventual consistency, but writes go to primary. Trade-off: higher write latency from remote regions.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-046",
      "type": "multiple-choice",
      "question": "What is 'lease' mechanism in distributed caching (as used by Facebook's Memcache)?",
      "options": [
        "Renting cache servers",
        "A token given on cache miss that must be presented to set the value, preventing stale sets",
        "Time-limited cache entries",
        "Agreement to use the cache"
      ],
      "correct": 1,
      "explanation": "On cache miss, Memcache returns a lease token. To set the value, you must provide this token. If another client already set the value (invalidating the lease), your stale set is rejected. Prevents race conditions.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ]
    },
    {
      "id": "distcache-047",
      "type": "multiple-choice",
      "question": "Facebook's TAO uses 'look-aside' caching. What does this mean?",
      "options": [
        "Cache is beside the application",
        "Application checks cache first; on miss, fetches from DB and populates cache",
        "Cache automatically fetches from DB",
        "Side-channel caching"
      ],
      "correct": 1,
      "explanation": "Look-aside (cache-aside): Application explicitly manages the cache. Check cache → if miss, query DB → store result in cache. The cache doesn't know about the database; the application coordinates.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-048",
      "type": "multiple-choice",
      "question": "What is 'look-through' caching?",
      "options": [
        "Transparent cache that application doesn't manage",
        "The cache sits in front of the database and handles misses automatically",
        "Looking at cache contents",
        "Cache debugging mode"
      ],
      "correct": 1,
      "explanation": "Look-through (read-through): Cache transparently fetches from database on miss. Application only talks to cache. Simpler application logic but cache must understand how to load data.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "distcache-049",
      "type": "multi-select",
      "question": "Which are challenges with cross-datacenter cache invalidation? (Select all that apply)",
      "options": [
        "Network latency between datacenters",
        "Message delivery reliability (lost invalidations)",
        "Clock synchronization",
        "Ordering of invalidations and writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Cross-DC invalidation faces: latency (messages take time), reliability (networks fail, messages lost), ordering (invalidation may arrive before/after write). Clock sync matters but isn't the primary challenge for invalidation.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-050",
      "type": "multiple-choice",
      "question": "What is 'mcsqueal' in Facebook's memcache architecture?",
      "options": [
        "A cache compression algorithm",
        "A daemon that tails MySQL's commit log to invalidate cache entries",
        "A cache query language",
        "A monitoring tool"
      ],
      "correct": 1,
      "explanation": "mcsqueal watches MySQL's binary log (commits) and sends invalidations to memcache when data changes. This ensures cache invalidation happens reliably based on actual database commits.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-051",
      "type": "multiple-choice",
      "question": "What is 'regional pool' vs 'wildcard' (all-region) in Facebook's caching?",
      "options": [
        "Pool size differences",
        "Regional pools cache region-specific data; wildcards cache data needed by all regions",
        "Security zones",
        "Memory allocation strategies"
      ],
      "correct": 1,
      "explanation": "Regional pools hold data accessed primarily within one region. Wildcard/all-region pools hold globally-accessed data (like user profiles). This optimizes replication—regional data doesn't need cross-region sync.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-052",
      "type": "multiple-choice",
      "question": "In a sharded distributed cache, what happens if one shard becomes unavailable?",
      "options": [
        "All cache operations fail",
        "Only keys mapped to that shard are unavailable; others work normally",
        "Keys automatically redistribute",
        "Cache switches to backup mode"
      ],
      "correct": 1,
      "explanation": "Sharding isolates failures. Keys on the failed shard become unavailable (cache misses → database), but keys on other shards continue working. This partial availability is better than total failure.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-053",
      "type": "numeric-input",
      "question": "A cache is sharded across 10 nodes. One node fails. Approximately what percentage of cache requests now miss (assuming uniform distribution)?",
      "answer": 10,
      "tolerance": 0.05,
      "unit": "%",
      "explanation": "With uniform distribution, each shard holds ~10% of keys. That shard's keys become misses, so ~10% of requests now miss (hit the database). The other 90% still hit cache.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your 10-shard cache handles 100K requests/sec with 95% hit rate. One shard fails. Previous miss rate was 5K/sec to DB. What's the new miss rate?",
          "options": ["5K/sec", "10K/sec", "15K/sec", "100K/sec"],
          "correct": 2,
          "explanation": "Original: 5K/sec misses. Failed shard adds ~10K/sec more misses (10% of traffic). Total: 5K + 10K = 15K/sec misses hitting the database.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "Your database can handle 12K/sec. What happens?",
          "options": [
            "Database handles it fine",
            "Database becomes overloaded (15K > 12K capacity)",
            "Requests queue up",
            "Cache automatically recovers"
          ],
          "correct": 1,
          "explanation": "15K/sec exceeds DB capacity of 12K/sec. Database becomes overloaded—queries slow down, timeouts increase, potentially cascading failures. This is why cache availability matters so much.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-055",
      "type": "multiple-choice",
      "question": "What is 'consistent hashing with bounded loads'?",
      "options": [
        "Limiting items per node",
        "Extension that limits how much more than average any node can take, redistributing overflow",
        "Hashing with encryption",
        "Consistent hashing with compression"
      ],
      "correct": 1,
      "explanation": "Bounded loads consistent hashing ensures no node exceeds (1 + ε) × average load. If a node would exceed its bound, the key is assigned to the next node on the ring. This smooths hot spots.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-056",
      "type": "multiple-choice",
      "question": "What is 'jump consistent hash'?",
      "options": [
        "Hashing with random jumps",
        "A fast consistent hash algorithm using simple arithmetic, ideal for sequentially numbered nodes",
        "Hashing that skips nodes",
        "Hash-based load balancing"
      ],
      "correct": 1,
      "explanation": "Jump consistent hash is a simple, fast algorithm that maps keys to buckets. It only requires knowing the number of buckets (not bucket IDs), uses minimal memory, and distributes keys evenly. Great for statically numbered shards.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-057",
      "type": "multi-select",
      "question": "Which are trade-offs of using more replicas in a distributed cache? (Select all that apply)",
      "options": [
        "Higher availability on node failures",
        "More read throughput (can read from replicas)",
        "Higher memory cost (data duplicated)",
        "Faster writes"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "More replicas improve availability (more copies) and read throughput (distribute reads). But memory cost increases (same data stored N times). Writes don't speed up—they must propagate to all replicas.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-058",
      "type": "multiple-choice",
      "question": "What is 'connection pooling' important for distributed caches?",
      "options": [
        "Reduces memory usage",
        "Reduces overhead of establishing new TCP connections for each request",
        "Improves cache hit rate",
        "Enables encryption"
      ],
      "correct": 1,
      "explanation": "TCP connection setup takes time (handshake, potentially TLS). Connection pools maintain open connections for reuse. This reduces latency and load on both client and server for high-request-rate workloads.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-059",
      "type": "numeric-input",
      "question": "A service has 100 application servers, each with a pool of 10 connections per cache shard. With 5 shards, how many total connections does each shard receive?",
      "answer": 1000,
      "tolerance": "exact",
      "unit": "connections",
      "explanation": "Each of 100 servers maintains 10 connections to each shard. Per shard: 100 × 10 = 1,000 connections. This connection count can become a scaling bottleneck.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-060",
      "type": "multiple-choice",
      "question": "Redis Cluster mode has limitations on multi-key operations. Why?",
      "options": [
        "Protocol limitation",
        "Keys may be on different nodes, making atomic operations impossible without coordination",
        "Memory constraints",
        "Security reasons"
      ],
      "correct": 1,
      "explanation": "In Redis Cluster, keys on different slots may be on different nodes. Operations like MGET, MSET, or transactions across multiple keys only work if all keys are on the same slot (achieved via hash tags).",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-061",
      "type": "multiple-choice",
      "question": "How do you run a Lua script atomically across keys in Redis Cluster?",
      "options": [
        "It works automatically",
        "All keys accessed by the script must be in the same slot (use hash tags)",
        "Not possible in Cluster mode",
        "Use special Lua syntax"
      ],
      "correct": 1,
      "explanation": "Redis executes Lua scripts atomically on a single node. In Cluster mode, all keys accessed by the script must hash to the same slot. Use hash tags to ensure this: {user123}:profile, {user123}:settings.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-062",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to atomically increment two counters 'pageviews:home' and 'pageviews:total' in Redis Cluster. Can you use MULTI/EXEC transaction?",
          "options": [
            "Yes, transactions work across any keys",
            "No, keys may be on different nodes",
            "Yes, if on the same node",
            "No, transactions aren't supported in Cluster"
          ],
          "correct": 1,
          "explanation": "These keys likely hash to different slots/nodes. MULTI/EXEC only works within a single node. The transaction would fail or operate non-atomically.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "How would you fix this to enable atomic operation?",
          "options": [
            "Use hash tags: {pageviews}:home and {pageviews}:total",
            "Use Lua script without hash tags",
            "Run two separate INCR commands",
            "Use pipeline"
          ],
          "correct": 0,
          "explanation": "Hash tags {pageviews} ensure both keys hash to the same slot: {pageviews}:home and {pageviews}:total. Now they're on the same node and MULTI/EXEC or Lua works atomically.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-063",
      "type": "multiple-choice",
      "question": "What is 'pipeline' in Redis and why is it useful for distributed caching?",
      "options": [
        "A data transformation flow",
        "Sending multiple commands without waiting for each response, reducing round-trips",
        "A connection type",
        "A replication mechanism"
      ],
      "correct": 1,
      "explanation": "Pipelining batches multiple commands into one network round-trip. Instead of send--send-you send-send-send-. This dramatically reduces latency for bulk operations, especially over high-latency networks.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "distcache-064",
      "type": "numeric-input",
      "question": "Network round-trip to cache is 1ms. Executing 100 commands sequentially takes how long minimum (network only)?",
      "answer": 100,
      "tolerance": "exact",
      "unit": "ms",
      "explanation": "Each command requires a round-trip: 100 commands × 1ms = 100ms minimum (ignoring processing time). Pipelining could reduce this to ~1ms (one round-trip for all commands).",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "distcache-065",
      "type": "multiple-choice",
      "question": "What is a limitation of pipelining in Redis Cluster?",
      "options": [
        "Commands can't use pipelines",
        "Pipeline commands must all go to the same node (smart clients split pipelines by node)",
        "Pipelines are slower",
        "Pipelines don't work over TLS"
      ],
      "correct": 1,
      "explanation": "In Cluster, different keys may be on different nodes. A pipeline must be split—commands for each node are pipelined separately. Smart clients handle this automatically, but it adds complexity.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-066",
      "type": "multiple-choice",
      "question": "What is 'request coalescing' or 'request deduplication' in distributed caching?",
      "options": [
        "Merging similar data",
        "Combining multiple simultaneous requests for the same key into one backend request",
        "Compressing requests",
        "Removing duplicate cache entries"
      ],
      "correct": 1,
      "explanation": "When many clients request the same key simultaneously (e.g., during thundering herd), coalescing makes one backend request and returns the result to all waiting clients. Reduces backend load.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-067",
      "type": "multi-select",
      "question": "Which cache proxy features help with distributed caching at scale? (Select all that apply)",
      "options": [
        "Connection pooling to backends",
        "Request coalescing",
        "Automatic sharding",
        "Built-in data persistence to disk"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Common proxy features: Connection pooling reduces backend connections. Coalescing reduces duplicate requests. Auto-sharding hides topology. Data persistence is a backend feature (Redis RDB/AOF), not a proxy responsibility — proxies route requests, they don't store data.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-068",
      "type": "multiple-choice",
      "question": "mcrouter (Facebook's Memcached proxy) can do 'shadow' requests. What is this?",
      "options": [
        "Hiding requests from logs",
        "Sending copies of requests to a test cluster without affecting production",
        "Encrypted requests",
        "Background requests"
      ],
      "correct": 1,
      "explanation": "Shadow mode duplicates requests to a second cluster (e.g., new version). Results from shadow are ignored. This allows testing new deployments with real production traffic without risk.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-069",
      "type": "multiple-choice",
      "question": "What is 'cache-through' vs 'cache-aside' in terms of write handling?",
      "options": [
        "Same thing",
        "Cache-through: cache handles DB writes; cache-aside: app handles DB writes",
        "Cache-through is for reads only",
        "Cache-aside has no cache"
      ],
      "correct": 1,
      "explanation": "Cache-through (write-through/write-behind): writes go to cache, which writes to DB. Cache-aside: app writes to DB directly, then updates/invalidates cache. Cache-aside gives app more control; cache-through simplifies app logic.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-070",
      "type": "two-stage",
      "stages": [
        {
          "question": "Distributed cache across 3 AZs in one region. One AZ loses network connectivity. What happens to cache operations?",
          "options": [
            "All operations fail",
            "Operations to that AZ's nodes fail; others succeed",
            "Operations automatically reroute",
            "Cache switches to readonly"
          ],
          "correct": 1,
          "explanation": "Without automatic failover, operations targeting nodes in the isolated AZ fail (timeout). Operations to other AZs continue. Keys on isolated nodes become unavailable.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "You have replicas in each AZ. How does this help?",
          "options": [
            "No change—replicas fail too",
            "Failover promotes replica in healthy AZ to primary",
            "Replicas serve reads but writes still fail",
            "Replicas automatically sync"
          ],
          "correct": 1,
          "explanation": "With cross-AZ replicas, failover can promote a replica in a healthy AZ to primary. The isolated AZ's nodes are demoted/removed. Service continues with potentially higher latency or reduced capacity.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-071",
      "type": "multiple-choice",
      "question": "What is 'rack awareness' in distributed caching?",
      "options": [
        "Physical rack management",
        "Placing replicas across different failure domains (racks/AZs) for resilience",
        "Rack-mounted servers only",
        "Tracking rack utilization"
      ],
      "correct": 1,
      "explanation": "Rack awareness ensures replicas aren't all in the same failure domain. If a rack/AZ fails, replicas in other domains survive. Redis Cluster and many distributed systems support replica placement policies.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-072",
      "type": "multiple-choice",
      "question": "What metric indicates cache cluster health issues?",
      "options": [
        "Memory usage exactly at limit",
        "Significant variance in latency percentiles across nodes",
        "Consistent hit rate",
        "Low eviction rate"
      ],
      "correct": 1,
      "explanation": "If some nodes have much higher p99 latency than others, there may be hot spots, hardware issues, or unbalanced sharding. Healthy clusters have relatively consistent performance across nodes.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "distcache-073",
      "type": "multi-select",
      "question": "Which metrics should you monitor for distributed cache health? (Select all that apply)",
      "options": [
        "Hit rate per node",
        "Memory usage per node",
        "Replication lag",
        "Total number of unique keys ever written"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Key metrics: Hit rate shows cache effectiveness. Memory usage indicates capacity. Replication lag affects consistency. Total unique keys ever written is a cumulative counter that grows monotonically — it doesn't indicate current health or problems.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-074",
      "type": "multiple-choice",
      "question": "What is 'cache cluster rebalancing'?",
      "options": [
        "Restarting nodes",
        "Redistributing keys/slots across nodes when nodes are added or removed",
        "Memory defragmentation",
        "Balancing hit rates"
      ],
      "correct": 1,
      "explanation": "Rebalancing moves data when cluster topology changes. Adding a node? Move some keys to it. Removing a node? Move its keys elsewhere. This should be gradual to avoid performance impact.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-075",
      "type": "multiple-choice",
      "question": "During Redis Cluster slot migration, what happens to operations on migrating keys?",
      "options": [
        "Operations fail",
        "ASKING redirects guide clients to the new location",
        "Operations are queued",
        "Operations go to both nodes"
      ],
      "correct": 1,
      "explanation": "During migration, a key might be on source or target. Clients receive ASK redirects to try the target. This ensures continuous operation during migration, though with some redirect overhead.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-076",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're adding a new shard to a 4-shard consistent hash cluster. To minimize disruption, how should keys migrate?",
          "options": [
            "Move 100% of keys from one old shard",
            "Move ~20% of keys from each existing shard to new shard",
            "Move keys randomly",
            "Don't migrate—only new keys go to new shard"
          ],
          "correct": 1,
          "explanation": "New shard should get 1/5 = 20% of total keys, taken proportionally from each existing shard. This balances load while minimizing disruption (each shard loses only 20% of its keys).",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        },
        {
          "question": "What happens to the keys being migrated during the migration process?",
          "options": [
            "Keys unavailable during migration",
            "Keys readable from old location, writes might go to new location",
            "Keys duplicated temporarily",
            "Keys deleted and must be refetched"
          ],
          "correct": 2,
          "explanation": "Typically, keys are copied (not moved) during migration—exist in both places temporarily. Once fully copied, ownership transfers. This ensures availability during migration.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-077",
      "type": "multiple-choice",
      "question": "What is 'gossip protocol' in Redis Cluster?",
      "options": [
        "Logging protocol",
        "Nodes periodically exchange cluster state information with random peers",
        "Client-server communication",
        "Replication protocol"
      ],
      "correct": 1,
      "explanation": "Gossip: each node periodically pings random other nodes, exchanging cluster state (who's alive, who owns which slots). This disseminates information without central coordination—eventually all nodes converge on the same view.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-078",
      "type": "multiple-choice",
      "question": "What is 'cluster bus' in Redis Cluster?",
      "options": [
        "Public transportation for data",
        "A dedicated port/protocol for node-to-node cluster communication",
        "The main client protocol",
        "A message queue"
      ],
      "correct": 1,
      "explanation": "Redis Cluster uses a separate 'cluster bus' port (typically data port + 10000) for node-to-node communication: gossip, failover coordination, and configuration. This keeps cluster traffic separate from client traffic.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-079",
      "type": "numeric-input",
      "question": "Redis Cluster nodes need to communicate. With N nodes, how does connection count between nodes scale?",
      "answer": 2,
      "tolerance": 0.2,
      "unit": "polynomial degree",
      "explanation": "Each node connects to every other node for gossip: N × (N-1) / 2 = O(N²) connections. Cluster size is practically limited by this quadratic scaling of inter-node connections.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-080",
      "type": "multiple-choice",
      "question": "What is 'cluster-require-full-coverage' in Redis Cluster?",
      "options": [
        "Requires data on all nodes",
        "If any slot is unassigned/unreachable, the cluster refuses to serve any requests",
        "Full replication requirement",
        "Coverage testing mode"
      ],
      "correct": 1,
      "explanation": "When 'yes' (default), cluster rejects all commands if any slot is unavailable. When 'no', cluster serves requests for available slots while unavailable slots error. Trade-off: consistency vs. partial availability.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-081",
      "type": "multi-select",
      "question": "Which are advantages of Redis Cluster over Sentinel with sharding? (Select all that apply)",
      "options": [
        "Built-in sharding with automatic slot management",
        "No single point of failure (decentralized)",
        "Simpler client implementation",
        "Multi-key operations across all keys"
      ],
      "correctIndices": [0, 1],
      "explanation": "Cluster has built-in sharding and decentralized architecture (no Sentinel coordinator). However, clients need Cluster-aware drivers (more complex), and multi-key operations are limited to same-slot keys.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-082",
      "type": "multiple-choice",
      "question": "What is 'read replicas' scaling pattern in distributed caching?",
      "options": [
        "Replicas that only read from primary",
        "Directing read traffic to replicas to scale read throughput beyond primary's capacity",
        "Read-only cache mode",
        "Backup replicas"
      ],
      "correct": 1,
      "explanation": "Read replicas receive writes from primary and serve read traffic. For read-heavy workloads, add replicas to scale read throughput. Writes still go to primary. Trade-off: eventual consistency for reads.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-083",
      "type": "numeric-input",
      "question": "Primary handles 50K reads/sec max. You add 2 read replicas. Assuming even distribution, what's the new max read throughput?",
      "answer": 150,
      "tolerance": 0.05,
      "unit": "K reads/sec",
      "explanation": "With 1 primary + 2 replicas, each handling 50K reads/sec: 3 × 50K = 150K reads/sec total capacity. Reads are distributed across all three nodes.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-084",
      "type": "two-stage",
      "stages": [
        {
          "question": "You scale to 1 primary + 5 read replicas. Write throughput is limited by what?",
          "options": [
            "Total of all replicas",
            "Primary alone (all writes go there)",
            "Network bandwidth",
            "Client count"
          ],
          "correct": 1,
          "explanation": "Writes go only to primary, which replicates to replicas. Write throughput = primary's capacity. Adding replicas scales reads, not writes.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "To scale write throughput, what architecture change is needed?",
          "options": [
            "More read replicas",
            "Sharding (multiple primaries for different key ranges)",
            "Larger primary instance",
            "Write batching"
          ],
          "correct": 1,
          "explanation": "Sharding splits the keyspace across multiple primaries. Each primary handles writes for its keys. With N shards, write capacity is ~N× single primary capacity.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-085",
      "type": "multiple-choice",
      "question": "What is 'cache topology' in distributed caching?",
      "options": [
        "Physical server layout",
        "The arrangement of cache nodes (shards, replicas, proxies) and how they connect",
        "Network topology only",
        "Memory layout"
      ],
      "correct": 1,
      "explanation": "Cache topology describes the system structure: how many shards, replicas per shard, whether there's a proxy layer, how nodes are distributed across failure domains. Topology decisions impact scalability and reliability.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "distcache-086",
      "type": "multiple-choice",
      "question": "What is 'cache stampede prevention' at the cluster level?",
      "options": [
        "Preventing cache server crashes",
        "Coordinated mechanisms to prevent all nodes from simultaneously fetching the same missing key",
        "Load balancing",
        "Connection limiting"
      ],
      "correct": 1,
      "explanation": "Cluster-level stampede prevention ensures that when a key expires/is missing, only one node/request fetches from the database. Others wait for the result. Requires distributed locking or leader election.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-087",
      "type": "multiple-choice",
      "question": "What challenge does 'cluster-wide hot key' present that per-node hot key doesn't?",
      "options": [
        "Higher latency",
        "Key is hot across all traffic, but data lives on one node—distributing it requires replication/client-side caching",
        "More memory usage",
        "Network congestion"
      ],
      "correct": 1,
      "explanation": "A hot key's shard becomes the bottleneck for the entire cluster. Unlike random hot spots (fixable with rebalancing), a single hot key can't be split. Solutions: replicate the key, add local caching, or shard within the key.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-088",
      "type": "multi-select",
      "question": "Which are signs of a hot key in a distributed cache cluster? (Select all that apply)",
      "options": [
        "One node has much higher CPU than others",
        "One node has much higher network traffic",
        "One node has much higher latency",
        "Overall cache size is unbalanced"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Hot keys cause traffic imbalance: affected node has higher CPU (processing requests), network (data transfer), and latency (queuing). Cache size imbalance suggests sharding issues, not hot keys specifically.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "distcache-089",
      "type": "multiple-choice",
      "question": "What is 'local cache' in front of distributed cache?",
      "options": [
        "Cache on local disk",
        "In-process cache on each application server as L1, with distributed cache as L2",
        "Cache in same datacenter",
        "Cache on same machine"
      ],
      "correct": 1,
      "explanation": "Local (L1) cache is in-process memory on each app server. Distributed cache is L2. Hot data lives in L1 (fastest, per-server). Misses fall through to L2. This reduces network calls for very hot data.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "distcache-090",
      "type": "two-stage",
      "stages": [
        {
          "question": "You add L1 local caches (100MB each) on 10 app servers in front of an L2 Redis cluster. What's a challenge with this approach?",
          "options": [
            "Too much memory",
            "L1 caches may have different/stale versions of the same key",
            "L2 becomes useless",
            "Lower hit rate"
          ],
          "correct": 1,
          "explanation": "Each L1 cache is independent. Updates to L2 don't automatically invalidate L1. Server A may have stale V1 while server B has V2. This is L1 cache coherence problem.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "How can you mitigate L1 cache staleness?",
          "options": [
            "Don't use L1 cache",
            "Use very short TTL on L1 and/or pub-sub invalidation",
            "Make L1 larger",
            "Sync L1 caches directly"
          ],
          "correct": 1,
          "explanation": "Short L1 TTL limits staleness duration. Pub-sub invalidation (Redis pub/sub when data changes) actively clears L1 entries. Balance: shorter TTL = less stale but more L2 traffic.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-091",
      "type": "multiple-choice",
      "question": "Netflix's EVCache uses what technique for cross-region consistency?",
      "options": [
        "Synchronous replication",
        "Invalidation messages sent to other regions on write",
        "Single global region",
        "No cross-region support"
      ],
      "correct": 1,
      "explanation": "EVCache sends invalidation messages to other regions when data is written. Other regions invalidate their local cache; data is refetched on next access. This provides eventual consistency without full replication.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "distcache-092",
      "type": "multiple-choice",
      "question": "What is 'cache tiering' across storage types?",
      "options": [
        "Multiple cache providers",
        "Hot data in fastest storage (RAM), warm in slower (SSD), cold in slowest (HDD/network)",
        "Pricing tiers",
        "Security tiers"
      ],
      "correct": 1,
      "explanation": "Cache tiering uses multiple storage tiers: RAM for hot data (fastest, expensive), SSD for warm data (fast, cheaper), HDD/network for cold. Items demote between tiers based on access patterns.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "distcache-093",
      "type": "multi-select",
      "question": "Which technologies can serve as distributed cache? (Select all that apply)",
      "options": [
        "Redis Cluster",
        "Memcached with consistent hashing",
        "Apache Ignite",
        "SQLite"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Redis Cluster, Memcached, and Ignite are distributed caching solutions. SQLite is an embedded single-file database with no built-in distribution or clustering — it's designed for local, single-process use, not distributed caching.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-094",
      "type": "multiple-choice",
      "question": "What is 'write-around' caching in distributed systems?",
      "options": [
        "Writing to all replicas",
        "Writes go directly to database, cache is populated only on read",
        "Writing to cache only",
        "Circular write pattern"
      ],
      "correct": 1,
      "explanation": "Write-around: writes bypass cache, go straight to DB. Cache is filled only when data is read. Good when written data isn't immediately read. Avoids caching data that may never be accessed.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-095",
      "type": "multiple-choice",
      "question": "What is the CAP theorem implication for distributed caching?",
      "options": [
        "Distributed caches can have all three: consistency, availability, partition tolerance",
        "Must choose between consistency and availability during network partitions",
        "Caching doesn't involve CAP theorem",
        "Only affects databases"
      ],
      "correct": 1,
      "explanation": "During network partitions, distributed caches must choose: CP (consistent but may be unavailable) or AP (available but may serve stale data). Most caches choose AP—serve stale data rather than fail.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "distcache-096",
      "type": "two-stage",
      "stages": [
        {
          "question": "Network partition splits your cache cluster. Nodes can't reach each other but clients can reach some nodes. What happens with eventual consistency (AP)?",
          "options": [
            "All operations fail",
            "Operations succeed but partitions may diverge; reconciled after partition heals",
            "Only reads work",
            "System shuts down"
          ],
          "correct": 1,
          "explanation": "AP systems continue operating during partition. Each partition accepts writes independently. After partition heals, systems reconcile—possibly with conflicts if same keys were modified differently.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee."
        },
        {
          "question": "What happens with strong consistency (CP)?",
          "options": [
            "Same as AP",
            "Minority partition refuses operations to prevent divergence",
            "Both partitions continue normally",
            "All data is deleted"
          ],
          "correct": 1,
          "explanation": "CP systems require majority quorum. The minority partition cannot confirm writes, so it refuses operations. Only the majority partition serves requests. This prevents divergence at the cost of availability.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. A strong caching answer specifies key design, invalidation behavior, and the acceptable staleness window, not just a cache hit-rate target.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-097",
      "type": "multiple-choice",
      "question": "What is 'cache-aside with fence tokens' pattern?",
      "options": [
        "Physical barriers around cache servers",
        "Using tokens to prevent stale cache writes from overwriting newer data",
        "Token-based authentication",
        "Rate limiting tokens"
      ],
      "correct": 1,
      "explanation": "Fence tokens are monotonic IDs from database. When updating cache, check the token: only update if your token is higher than the current cached token. Prevents races where stale data overwrites fresh data.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ]
    },
    {
      "id": "distcache-098",
      "type": "multi-select",
      "question": "Which are best practices for distributed cache operations? (Select all that apply)",
      "options": [
        "Set reasonable TTLs on all entries",
        "Handle cache failures gracefully (fallback to database)",
        "Monitor hit rates and latency across nodes",
        "Use synchronous replication for all data"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "TTLs prevent stale data and memory exhaustion. Graceful failure handling prevents cache outages from becoming full outages. Monitoring enables early detection of issues. Sync replication is often too slow—use it selectively.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "distcache-099",
      "type": "multiple-choice",
      "question": "What is 'cache aside with double-check locking'?",
      "options": [
        "Checking cache twice",
        "On miss, acquire lock, check cache again (might have been filled while waiting), only then fetch from DB",
        "Double encryption",
        "Two-phase locking"
      ],
      "correct": 1,
      "explanation": "After acquiring a lock for cache fill, check cache again—another thread may have filled it while you waited for the lock. This prevents duplicate database queries in high-concurrency scenarios.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "distcache-100",
      "type": "multiple-choice",
      "question": "When designing a distributed caching layer, what should you decide first?",
      "options": [
        "Which technology to use",
        "How many nodes to deploy",
        "What data to cache and consistency requirements",
        "Network topology"
      ],
      "correct": 2,
      "explanation": "Start with: what data needs caching? How fresh must it be? Can stale data be served? What's the read/write ratio? These requirements drive architecture decisions (technology, topology, replication strategy).",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    }
  ]
}
