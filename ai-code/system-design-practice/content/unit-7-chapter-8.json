{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 8,
  "chapterTitle": "Scaling Compute Scenarios",
  "chapterDescription": "Integrated scenario practice combining bottleneck triage, load balancing, autoscaling, hotspot mitigation, compute-platform choices, and multi-region failover trade-offs.",
  "problems": [
    {
      "id": "sc-scn-001",
      "type": "multiple-choice",
      "question": "Scenario: global commerce checkout during regional degradation. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? The issue started immediately after a traffic policy update.",
      "options": [
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-002",
      "type": "multiple-choice",
      "question": "Scenario: flash-sale inventory service with hotspot keys. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Only one dependency tier reached saturation first.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-003",
      "type": "multiple-choice",
      "question": "Scenario: chat platform with reconnect storm after AZ fault. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Canary behavior diverged from full-fleet behavior.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-004",
      "type": "multiple-choice",
      "question": "Scenario: ticketing API under bot-driven burst traffic. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Global averages looked healthy while p99 regressed.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-scn-005",
      "type": "multiple-choice",
      "question": "Scenario: fraud scoring service with uneven tenant load. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Retry amplification appeared in one tenant segment.",
      "options": [
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-006",
      "type": "multiple-choice",
      "question": "Scenario: ad bidding pipeline with strict p99 budget. Primary symptom is ordered-processing constraints limiting parallelism. What is the strongest next move? Backlog growth remained localized after scale-out.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-scn-007",
      "type": "multiple-choice",
      "question": "Scenario: video metadata API during cross-region failover. Primary symptom is noisy-neighbor contention in shared nodes. What is the strongest next move? Regional failover timing exceeded the documented runbook.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-scn-008",
      "type": "multiple-choice",
      "question": "Scenario: notification system with delayed queue recovery. Primary symptom is control-plane change blast radius across regions. What is the strongest next move? Incident mitigations must preserve strict correctness boundaries.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-009",
      "type": "multiple-choice",
      "question": "Scenario: payment authorization path with dependency saturation. Primary symptom is insufficient standby capacity for evacuation. What is the strongest next move? Ops capacity is constrained and changes must be high leverage.",
      "options": [
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-scn-010",
      "type": "multiple-choice",
      "question": "Scenario: search frontend under cold-start regressions. Primary symptom is cost cap conflict with strict latency SLO. What is the strongest next move? The business event peak repeats in two hours.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-011",
      "type": "multiple-choice",
      "question": "Scenario: webhook ingestion tier with retry amplification. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? A prior rollback did not remove the hotspot pattern.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-012",
      "type": "multiple-choice",
      "question": "Scenario: gaming session backend with sticky-state pressure. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Tail latency and error-budget burn rose together.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-013",
      "type": "multiple-choice",
      "question": "Scenario: identity token service during rollout + traffic shift. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Control-plane drift increased uncertainty in diagnostics.",
      "options": [
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ]
    },
    {
      "id": "sc-scn-014",
      "type": "multiple-choice",
      "question": "Scenario: log ingestion pipeline with skewed partitions. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Capacity caps cannot be removed due to budget guardrails.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-scn-015",
      "type": "multiple-choice",
      "question": "Scenario: ride dispatch API with latency-sensitive routing. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Service owners require a reversible first step.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-scn-016",
      "type": "multiple-choice",
      "question": "Scenario: document collaboration backend with hot tenants. Primary symptom is ordered-processing constraints limiting parallelism. What is the strongest next move? Customer-facing impact is concentrated in one geography.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-017",
      "type": "multiple-choice",
      "question": "Scenario: pricing service with cache-miss storm. Primary symptom is noisy-neighbor contention in shared nodes. What is the strongest next move? Hot partitions stayed hot even after worker expansion.",
      "options": [
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "sc-scn-018",
      "type": "multiple-choice",
      "question": "Scenario: support messaging system during control-plane drift. Primary symptom is control-plane change blast radius across regions. What is the strongest next move? Failback safety is a hard requirement in this incident.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-019",
      "type": "multiple-choice",
      "question": "Scenario: catalog read API with stale replica lag. Primary symptom is insufficient standby capacity for evacuation. What is the strongest next move? Admission controls are available but not yet tuned.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-scn-020",
      "type": "multiple-choice",
      "question": "Scenario: compliance scanner with long-running job backlog. Primary symptom is cost cap conflict with strict latency SLO. What is the strongest next move? Team must avoid introducing new platform lock-in quickly.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-021",
      "type": "multiple-choice",
      "question": "Scenario: global commerce checkout during regional degradation. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? Standby region promotion is possible but dependency-limited.",
      "options": [
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-022",
      "type": "multiple-choice",
      "question": "Scenario: flash-sale inventory service with hotspot keys. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Some workloads in the path require strict ordering.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-023",
      "type": "multiple-choice",
      "question": "Scenario: chat platform with reconnect storm after AZ fault. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Connection-heavy clients are causing reconnect surges.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-024",
      "type": "multiple-choice",
      "question": "Scenario: ticketing API under bot-driven burst traffic. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Rate-limiting policy currently treats all tenants equally.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-scn-025",
      "type": "multiple-choice",
      "question": "Scenario: fraud scoring service with uneven tenant load. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Previous incidents showed similar partition-skew signatures.",
      "options": [
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-026",
      "type": "multiple-choice",
      "question": "Scenario: ad bidding pipeline with strict p99 budget. Primary symptom is ordered-processing constraints limiting parallelism. What is the strongest next move? The fix must be validated within this shift handoff window.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-scn-027",
      "type": "multiple-choice",
      "question": "Scenario: video metadata API during cross-region failover. Primary symptom is noisy-neighbor contention in shared nodes. What is the strongest next move? Metrics freshness lag makes fast feedback difficult.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-scn-028",
      "type": "multiple-choice",
      "question": "Scenario: notification system with delayed queue recovery. Primary symptom is control-plane change blast radius across regions. What is the strongest next move? Current autoscaling policy has no scale-in stabilization.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-029",
      "type": "multiple-choice",
      "question": "Scenario: payment authorization path with dependency saturation. Primary symptom is insufficient standby capacity for evacuation. What is the strongest next move? One region has weaker warm capacity than the others.",
      "options": [
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-scn-030",
      "type": "multiple-choice",
      "question": "Scenario: search frontend under cold-start regressions. Primary symptom is cost cap conflict with strict latency SLO. What is the strongest next move? Runbook ownership is clear but thresholds need tuning.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-031",
      "type": "multiple-choice",
      "question": "Scenario: webhook ingestion tier with retry amplification. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? The mitigation should minimize blast radius to adjacent systems.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-032",
      "type": "multiple-choice",
      "question": "Scenario: gaming session backend with sticky-state pressure. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Workload mix changed after recent product feature launch.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-033",
      "type": "multiple-choice",
      "question": "Scenario: identity token service during rollout + traffic shift. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Low-priority traffic can be degraded if explicitly controlled.",
      "options": [
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ]
    },
    {
      "id": "sc-scn-034",
      "type": "multiple-choice",
      "question": "Scenario: log ingestion pipeline with skewed partitions. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Executive review requested explicit SLO/cost trade-off rationale.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-035",
      "type": "multiple-choice",
      "question": "Scenario: ride dispatch API with latency-sensitive routing. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Post-incident prevention is required as part of the first fix.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "sc-scn-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat platform with reconnect storm after AZ fault. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in chat platform with reconnect storm after AZ fault.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under strict p99 SLO pressure?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticketing API under bot-driven burst traffic. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in ticketing API under bot-driven burst traffic."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while preventing retry amplification?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: fraud scoring service with uneven tenant load. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in fraud scoring service with uneven tenant load.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during staged regional evacuation?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ad bidding pipeline with strict p99 budget. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in ad bidding pipeline with strict p99 budget.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action without violating cost guardrails?",
          "options": [
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: video metadata API during cross-region failover. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in video metadata API during cross-region failover.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while preserving correctness boundaries?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: notification system with delayed queue recovery. Current incident signal is cost cap conflict with strict latency SLO. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cost cap conflict with strict latency SLO and current scaling/routing controls in notification system with delayed queue recovery."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during rapid traffic growth?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: pre-scale standby regions and validate RTO/RPO with game-day drills.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Growth planning should show today vs horizon requirements side by side because compounding makes small percentages material quickly."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: payment authorization path with dependency saturation. Current incident signal is queue age surge despite moderate average CPU. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between queue age surge despite moderate average CPU and current scaling/routing controls in payment authorization path with dependency saturation.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with uncertain telemetry confidence?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: define explicit degradation mode and admission controls when cap pressure is reached.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: search frontend under cold-start regressions. Current incident signal is partition-level saturation hidden by fleet averages. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between partition-level saturation hidden by fleet averages and current scaling/routing controls in search frontend under cold-start regressions.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action before full rollout promotion?",
          "options": [
            "Execute this plan first: prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: webhook ingestion tier with retry amplification. Current incident signal is cold path latency after scale-from-low baseline. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between cold path latency after scale-from-low baseline and current scaling/routing controls in webhook ingestion tier with retry amplification.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during dependency saturation?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: gaming session backend with sticky-state pressure. Current incident signal is regional dependency cap hit during failover. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between regional dependency cap hit during failover and current scaling/routing controls in gaming session backend with sticky-state pressure."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while containing blast radius?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: identity token service during rollout + traffic shift. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in identity token service during rollout + traffic shift.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under hotspot tenant skew?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: log ingestion pipeline with skewed partitions. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in log ingestion pipeline with skewed partitions.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during cold-start-sensitive periods?",
          "options": [
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ride dispatch API with latency-sensitive routing. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in ride dispatch API with latency-sensitive routing.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with rollback readiness requirements?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: document collaboration backend with hot tenants. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in document collaboration backend with hot tenants."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while keeping failback safe?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: pricing service with cache-miss storm. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in pricing service with cache-miss storm.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with mixed ordered/unordered workloads?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: support messaging system during control-plane drift. Current incident signal is cost cap conflict with strict latency SLO. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cost cap conflict with strict latency SLO and current scaling/routing controls in support messaging system during control-plane drift.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under high queue-age variance?",
          "options": [
            "Execute this plan first: pre-scale standby regions and validate RTO/RPO with game-day drills.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: catalog read API with stale replica lag. Current incident signal is queue age surge despite moderate average CPU. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between queue age surge despite moderate average CPU and current scaling/routing controls in catalog read API with stale replica lag.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while protecting premium-tenant SLOs?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: define explicit degradation mode and admission controls when cap pressure is reached."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: compliance scanner with long-running job backlog. Current incident signal is partition-level saturation hidden by fleet averages. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between partition-level saturation hidden by fleet averages and current scaling/routing controls in compliance scanner with long-running job backlog."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during control-plane instability?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: global commerce checkout during regional degradation. Current incident signal is cold path latency after scale-from-low baseline. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cold path latency after scale-from-low baseline and current scaling/routing controls in global commerce checkout during regional degradation.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with limited ops bandwidth?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: flash-sale inventory service with hotspot keys. Current incident signal is regional dependency cap hit during failover. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between regional dependency cap hit during failover and current scaling/routing controls in flash-sale inventory service with hotspot keys.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action before next peak event?",
          "options": [
            "Execute this plan first: raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat platform with reconnect storm after AZ fault. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in chat platform with reconnect storm after AZ fault.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with lag-sensitive read paths?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticketing API under bot-driven burst traffic. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in ticketing API under bot-driven burst traffic."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during migration overlap?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: fraud scoring service with uneven tenant load. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in fraud scoring service with uneven tenant load.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while constraining concurrency spikes?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ad bidding pipeline with strict p99 budget. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in ad bidding pipeline with strict p99 budget.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under partial regional outage?",
          "options": [
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: video metadata API during cross-region failover. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in video metadata API during cross-region failover.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with incident-to-steady-state transition?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Translate workload into per-node limits and peak demand so scaling thresholds and headroom decisions are explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-061",
      "type": "multi-select",
      "question": "In end-to-end scaling incidents, which diagnostics are highest value first? (Select all that apply)",
      "options": [
        "Queue age and tail latency by partition/region",
        "Dependency saturation and error skew",
        "Only global mean CPU",
        "Traffic-shift/failover event timeline"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Integrated diagnosis needs granular signals and timeline correlation, not only global means.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-062",
      "type": "multi-select",
      "question": "Which controls reduce blast radius during emergency traffic shifts? (Select all that apply)",
      "options": [
        "Regional traffic caps",
        "Dependency-aware ramp sequencing",
        "Immediate full cutover by default",
        "Rollback thresholds with ownership"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Guarded shift and rollback controls limit cascade risk.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-063",
      "type": "multi-select",
      "question": "For hotspot + autoscaling interactions, which practices help? (Select all that apply)",
      "options": [
        "Fix key skew before broad scale-out",
        "Use shard-aware signals",
        "Rely on fleet averages only",
        "Apply fair scheduling/quotas"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Skew-aware controls prevent ineffective scale-out and unfair starvation.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-064",
      "type": "multi-select",
      "question": "Compute selection in scenario triage should consider which jointly? (Select all that apply)",
      "options": [
        "Runtime limits vs job profile",
        "Cold-start sensitivity",
        "Weekly meeting availability only",
        "Isolation and operability requirements"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Platform fit is multidimensional: runtime shape, latency sensitivity, and operational constraints.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-065",
      "type": "multi-select",
      "question": "Which signs indicate scale-in policy is unsafe during incidents? (Select all that apply)",
      "options": [
        "In-flight work loss during downscale",
        "Replica oscillation after short dips",
        "Stable queue age and no retries",
        "Frequent rollback to prior capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Unsafe scale-in shows churn, dropped work, and unstable rollback behavior.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-066",
      "type": "multi-select",
      "question": "For multi-region + sharding scenarios, which safeguards are useful? (Select all that apply)",
      "options": [
        "Regional isolation boundaries",
        "Per-partition retry budgets",
        "Single global mutable control without scoping",
        "Failover drills with RTO/RPO measurement"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Scoping, retry control, and tested failover guard against cascading failures.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-scn-067",
      "type": "multi-select",
      "question": "Which interventions are valid under cost cap pressure with strict SLOs? (Select all that apply)",
      "options": [
        "Admission control for low-priority load",
        "Graceful degradation mode",
        "Remove all safety caps and hope",
        "Protect critical-path capacity floors"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "When caps and SLOs conflict, enforce priority/degradation policies explicitly.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-068",
      "type": "multi-select",
      "question": "What makes an integrated migration plan safer? (Select all that apply)",
      "options": [
        "Canary by traffic segment",
        "Baseline/post-change SLO+cost comparison",
        "Big-bang cutover only",
        "Clear fallback ownership/runbook"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Safe migration combines staged rollout, measurement, and owned fallback.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-069",
      "type": "multi-select",
      "question": "Which metrics should be watched together during incident stabilization? (Select all that apply)",
      "options": [
        "Tail latency",
        "Error budget burn rate",
        "Queue age/backlog",
        "Only deployment count"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Stabilization requires service quality, reliability, and work-congestion metrics together.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-070",
      "type": "multi-select",
      "question": "Which anti-patterns commonly appear in compute-scaling scenarios? (Select all that apply)",
      "options": [
        "Treating retries as free capacity",
        "Ignoring partition/regional skew",
        "Canarying policy changes",
        "Applying one metric to all workloads"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Oversimplified controls and blind scaling are recurrent failure modes.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-071",
      "type": "multi-select",
      "question": "For ordered-processing bottlenecks, which options are valid? (Select all that apply)",
      "options": [
        "Isolate strict-order path",
        "Relax order where domain allows",
        "Assume ordering has zero throughput cost",
        "Use idempotent handlers for safe retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ordering guarantees constrain parallelism and should be scoped intentionally.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-072",
      "type": "multi-select",
      "question": "What supports confident failback after incident containment? (Select all that apply)",
      "options": [
        "Gradual traffic restoration",
        "Consistency/integrity checks",
        "Immediate 100% restore always",
        "Rollback triggers if regression appears"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failback should be staged, validated, and reversible.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-073",
      "type": "multi-select",
      "question": "In mixed compute estates (VM/container/serverless), which governance helps? (Select all that apply)",
      "options": [
        "Platform selection rubric per workload",
        "Shared reliability guardrails",
        "Unbounded platform sprawl",
        "Periodic cost/perf review by service tier"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Mixed estates are sustainable with explicit selection rules and shared guardrails.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-074",
      "type": "multi-select",
      "question": "Which signs indicate control-plane risk is under-managed? (Select all that apply)",
      "options": [
        "Global policy blasts without canary",
        "No audited rollback path",
        "Regional staged rollout discipline",
        "Frequent config drift surprises"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Control-plane safety requires scoped rollout, auditability, and rollback readiness.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-075",
      "type": "multi-select",
      "question": "During reconnect storms, which controls matter most? (Select all that apply)",
      "options": [
        "Connection admission backpressure",
        "Jittered reconnect guidance",
        "Infinite immediate client retries",
        "Warm capacity buffer on connection gateways"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Reconnect storms need coordinated client and server-side damping.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-076",
      "type": "multi-select",
      "question": "Which are valid scenario-level success criteria after mitigation? (Select all that apply)",
      "options": [
        "SLO recovery sustained across peak window",
        "Reduced error-budget burn",
        "Higher mean CPU regardless user impact",
        "Cost returns near pre-incident bounds"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Success should be measured by user impact, reliability burn, and cost stability.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-077",
      "type": "multi-select",
      "question": "Which preparations improve incident response on compute scaling paths? (Select all that apply)",
      "options": [
        "Predefined degradation modes",
        "Owner-mapped runbooks",
        "Ad hoc undocumented actions only",
        "Regular game-day simulations"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Preparedness increases speed and correctness of response under pressure.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-078",
      "type": "numeric-input",
      "question": "Backlog is 960,000 jobs. Net drain after mitigation is 24,000 jobs/min. Minutes to clear backlog?",
      "answer": 40,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "960,000 / 24,000 = 40 minutes.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-079",
      "type": "numeric-input",
      "question": "Primary region loses 45% capacity. Remaining regions can absorb 18,000 rps and 12,000 rps. Maximum total failover traffic they can absorb?",
      "answer": 30000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "18,000 + 12,000 = 30,000 rps.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-080",
      "type": "numeric-input",
      "question": "A hotspot shard handles 22% of 80,000 rps. How much rps hits that shard?",
      "answer": 17600,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.22 * 80,000 = 17,600 rps.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-081",
      "type": "numeric-input",
      "question": "Queue age SLO is 90s. Current queue age is 225s. Percent over SLO?",
      "answer": 150,
      "unit": "%",
      "tolerance": 0.5,
      "explanation": "(225-90)/90 = 150% over target.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-scn-082",
      "type": "numeric-input",
      "question": "Autoscaling adds 8 workers/min. Starting at 32 workers, how many after 6 minutes without scale-in?",
      "answer": 80,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "32 + 8*6 = 80 workers.",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-083",
      "type": "numeric-input",
      "question": "A retry policy adds 0.25 extra requests per original request. At 48,000 original rps, effective rps?",
      "answer": 60000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "48,000 * 1.25 = 60,000 rps.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-scn-084",
      "type": "numeric-input",
      "question": "Standby region warm pool is 140 instances. Promotion needs 196. Additional instances required?",
      "answer": 56,
      "unit": "instances",
      "tolerance": 0,
      "explanation": "196 - 140 = 56 instances.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-085",
      "type": "numeric-input",
      "question": "p99 improved from 1800ms to 720ms after mitigation. Percent reduction?",
      "answer": 60,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1800-720)/1800 = 60% reduction.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-086",
      "type": "numeric-input",
      "question": "A tier runs at 70% target utilization with 35,000 rps demand. Required raw capacity?",
      "answer": 50000,
      "unit": "rps",
      "tolerance": 0.02,
      "explanation": "35,000 / 0.7 = 50,000 rps.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-087",
      "type": "numeric-input",
      "question": "Failover drill has 5 stages of 3 minutes each before full promotion. Total drill minutes?",
      "answer": 15,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "5 * 3 = 15 minutes.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-088",
      "type": "numeric-input",
      "question": "Per-partition cap is 4,500 rps. Hot partition receives 9,900 rps. Minimum equal split partitions needed?",
      "answer": 3,
      "unit": "partitions",
      "tolerance": 0,
      "explanation": "9,900 / 4,500 = 2.2, so at least 3 partitions.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-scn-089",
      "type": "numeric-input",
      "question": "Degradation mode sheds 12% of 75,000 rps low-priority traffic. How much rps is shed?",
      "answer": 9000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.12 * 75,000 = 9,000 rps.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-090",
      "type": "ordering",
      "question": "Order a robust integrated incident response loop.",
      "items": [
        "Scope bottleneck with granular signals",
        "Apply blast-radius containment controls",
        "Execute targeted mitigation",
        "Validate and codify guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnose, contain, mitigate, and institutionalize.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-091",
      "type": "ordering",
      "question": "Order by fastest to slowest mitigation impact (typical).",
      "items": [
        "Traffic/rate guardrails",
        "Autoscaling policy tuning",
        "Partition strategy adjustment",
        "Major platform migration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Immediate controls usually act faster than architectural shifts.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-092",
      "type": "ordering",
      "question": "Order by increasing observability granularity.",
      "items": [
        "Global averages",
        "Per-region metrics",
        "Per-service and per-shard metrics",
        "Per-shard plus top-key and queue-age outliers"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Finer granularity improves hotspot diagnosis precision.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-093",
      "type": "ordering",
      "question": "Order failover/failback safety from weakest to strongest.",
      "items": [
        "Untested manual steps",
        "Documented runbook only",
        "Runbook with periodic drills",
        "Runbook with drills and rollback triggers"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Tested and guarded workflows are safest.",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-094",
      "type": "ordering",
      "question": "Order by increasing risk of retry amplification.",
      "items": [
        "Budgeted retries with jitter",
        "Bounded retries without jitter",
        "High retry limits during errors",
        "Immediate unbounded synchronized retries"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Loose, synchronized retry behavior amplifies incidents.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-scn-095",
      "type": "ordering",
      "question": "Order platform-change rollout safety.",
      "items": [
        "Canary subset rollout",
        "Incremental traffic ramps",
        "Large batch cutover",
        "Immediate full cutover"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Smaller progressive steps reduce change risk.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-096",
      "type": "ordering",
      "question": "Order by increasing ordering guarantee strength.",
      "items": [
        "Unordered processing",
        "Partition-local ordering",
        "Tenant-local ordering",
        "Global total ordering"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger ordering generally constrains parallelism more.",
      "detailedExplanation": "Compare relative scale first, then confirm neighboring items pairwise to lock in the order. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-097",
      "type": "ordering",
      "question": "Order by strongest evidence quality for mitigation success.",
      "items": [
        "Anecdotal operator impression",
        "Global mean metric improvement",
        "SLO + error-budget recovery across peak",
        "SLO/cost recovery plus recurrence guardrail validation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "High-confidence validation combines user impact, reliability, cost, and prevention signals.",
      "detailedExplanation": "Order by relative impact rather than exact values, then verify the sequence one boundary at a time. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-098",
      "type": "ordering",
      "question": "Order escalation path for capacity-vs-cost conflict.",
      "items": [
        "Enable low-priority shedding",
        "Protect critical-path floor",
        "Tune autoscaling and routing to cap pressure",
        "Revisit capacity budget and architecture constraints"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start with tactical controls, then adjust system and planning assumptions.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-scn-099",
      "type": "ordering",
      "question": "Order by increasing control-plane blast-radius safety.",
      "items": [
        "Global one-shot policy update",
        "Large regional batch update",
        "Regional canary update",
        "Regional canary with automated rollback"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer control-plane changes scope impact and add rollback automation.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    },
    {
      "id": "sc-scn-100",
      "type": "ordering",
      "question": "Order scenario-mitigation lifecycle.",
      "items": [
        "Detect pattern recurrence risk",
        "Define hypothesis + guardrails",
        "Implement and monitor in canary",
        "Promote and archive lessons"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Explicit hypotheses and guardrails improve repeatability and learning.",
      "detailedExplanation": "Compare relative scale first, then confirm neighboring items pairwise to lock in the order. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ]
    }
  ]
}
