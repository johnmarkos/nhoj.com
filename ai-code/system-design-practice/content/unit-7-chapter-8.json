{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 8,
  "chapterTitle": "Scaling Compute Scenarios",
  "chapterDescription": "Integrated scenario practice combining bottleneck triage, load balancing, autoscaling, hotspot mitigation, compute-platform choices, and multi-region failover trade-offs.",
  "problems": [
    {
      "id": "sc-scn-001",
      "type": "multiple-choice",
      "question": "Scenario: global commerce checkout during regional degradation. Observed symptom: queue age surge despite moderate average CPU. What is the strongest next move? The issue started immediately after a traffic policy update.",
      "options": [
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Given the observed bottleneck and guardrails, integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For global commerce checkout during regional degradation, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-002",
      "type": "multiple-choice",
      "question": "Scenario: flash-sale inventory service with hotspot keys. Observed symptom: partition-level saturation hidden by fleet averages. What is the strongest next move? Only one dependency tier reached saturation first.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
      ],
      "correct": 3,
      "explanation": "From an incident-first perspective, integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For flash-sale inventory service with hotspot keys, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-003",
      "type": "multiple-choice",
      "question": "Scenario: chat platform with reconnect storm after AZ fault. Observed symptom: cold path latency after scale-from-low baseline. What is the strongest next move? Canary behavior diverged from full-fleet behavior.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Under the stated reliability and cost constraints, integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For chat platform with reconnect storm after AZ fault, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-004",
      "type": "multiple-choice",
      "question": "Scenario: ticketing API under bot-driven burst traffic. Observed symptom: regional dependency cap hit during failover. What is the strongest next move? Global averages looked healthy while p99 regressed.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Prioritizing blast-radius reduction first, integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ticketing API under bot-driven burst traffic, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-005",
      "type": "multiple-choice",
      "question": "Scenario: fraud scoring service with uneven tenant load. Observed symptom: retry storms amplifying hotspot pressure. What is the strongest next move? Retry amplification appeared in one tenant segment.",
      "options": [
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "With latency and correctness objectives explicit, integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For fraud scoring service with uneven tenant load, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-006",
      "type": "multiple-choice",
      "question": "Scenario: ad bidding pipeline with strict p99 budget. Observed symptom: ordered-processing constraints limiting parallelism. What is the strongest next move? Backlog growth remained localized after scale-out.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths."
      ],
      "correct": 3,
      "explanation": "Looking at rollback safety and operational load, integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ad bidding pipeline with strict p99 budget, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-007",
      "type": "multiple-choice",
      "question": "Scenario: video metadata API during cross-region failover. Observed symptom: noisy-neighbor contention in shared nodes. What is the strongest next move? Regional failover timing exceeded the documented runbook.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For video metadata API during cross-region failover, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-008",
      "type": "multiple-choice",
      "question": "Scenario: notification system with delayed queue recovery. Observed symptom: control-plane change blast radius across regions. What is the strongest next move? Incident mitigations must preserve strict correctness boundaries.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For notification system with delayed queue recovery, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize from scenario: notification system with delayed queue recovery to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-009",
      "type": "multiple-choice",
      "question": "Scenario: payment authorization path with dependency saturation. Observed symptom: insufficient standby capacity for evacuation. What is the strongest next move? Ops capacity is constrained and changes must be high leverage.",
      "options": [
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For payment authorization path with dependency saturation, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-010",
      "type": "multiple-choice",
      "question": "Scenario: search frontend under cold-start regressions. Observed symptom: cost cap conflict with strict latency SLO. What is the strongest next move? The business event peak repeats in two hours.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For search frontend under cold-start regressions, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-011",
      "type": "multiple-choice",
      "question": "Scenario: webhook ingestion tier with retry amplification. Observed symptom: queue age surge despite moderate average CPU. What is the strongest next move? A prior rollback did not remove the hotspot pattern.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For webhook ingestion tier with retry amplification, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-012",
      "type": "multiple-choice",
      "question": "Scenario: gaming session backend with sticky-state pressure. Observed symptom: partition-level saturation hidden by fleet averages. What is the strongest next move? Tail latency and error-budget burn rose together.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For gaming session backend with sticky-state pressure, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-013",
      "type": "multiple-choice",
      "question": "Scenario: identity token service during rollout + traffic shift. Observed symptom: cold path latency after scale-from-low baseline. What is the strongest next move? Control-plane drift increased uncertainty in diagnostics.",
      "options": [
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For identity token service during rollout + traffic shift, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-014",
      "type": "multiple-choice",
      "question": "Scenario: log ingestion pipeline with skewed partitions. Observed symptom: regional dependency cap hit during failover. What is the strongest next move? Capacity caps cannot be removed due to budget guardrails.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For log ingestion pipeline with skewed partitions, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-015",
      "type": "multiple-choice",
      "question": "Scenario: ride dispatch API with latency-sensitive routing. Observed symptom: retry storms amplifying hotspot pressure. What is the strongest next move? Service owners require a reversible first step.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ride dispatch API with latency-sensitive routing, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-016",
      "type": "multiple-choice",
      "question": "Scenario: document collaboration backend with hot tenants. Observed symptom: ordered-processing constraints limiting parallelism. What is the strongest next move? Customer-facing impact is concentrated in one geography.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For document collaboration backend with hot tenants, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-017",
      "type": "multiple-choice",
      "question": "Scenario: pricing service with cache-miss storm. Observed symptom: noisy-neighbor contention in shared nodes. What is the strongest next move? Hot partitions stayed hot even after worker expansion.",
      "options": [
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For pricing service with cache-miss storm, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "Generalize from scenario: pricing service with cache-miss storm to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-018",
      "type": "multiple-choice",
      "question": "Scenario: support messaging system during control-plane drift. Observed symptom: control-plane change blast radius across regions. What is the strongest next move? Failback safety is a hard requirement in this incident.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For support messaging system during control-plane drift, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-019",
      "type": "multiple-choice",
      "question": "Scenario: catalog read API with stale replica lag. Observed symptom: insufficient standby capacity for evacuation. What is the strongest next move? Admission controls are available but not yet tuned.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For catalog read API with stale replica lag, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-020",
      "type": "multiple-choice",
      "question": "Scenario: compliance scanner with long-running job backlog. Observed symptom: cost cap conflict with strict latency SLO. What is the strongest next move? Team must avoid introducing new platform lock-in quickly.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For compliance scanner with long-running job backlog, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-021",
      "type": "multiple-choice",
      "question": "Scenario: global commerce checkout during regional degradation. Observed symptom: queue age surge despite moderate average CPU. What is the strongest next move? Standby region promotion is possible but dependency-limited.",
      "options": [
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For global commerce checkout during regional degradation, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-022",
      "type": "multiple-choice",
      "question": "Scenario: flash-sale inventory service with hotspot keys. Observed symptom: partition-level saturation hidden by fleet averages. What is the strongest next move? Some workloads in the path require strict ordering.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For flash-sale inventory service with hotspot keys, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-023",
      "type": "multiple-choice",
      "question": "Scenario: chat platform with reconnect storm after AZ fault. Observed symptom: cold path latency after scale-from-low baseline. What is the strongest next move? Connection-heavy clients are causing reconnect surges.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For chat platform with reconnect storm after AZ fault, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-024",
      "type": "multiple-choice",
      "question": "Scenario: ticketing API under bot-driven burst traffic. Observed symptom: regional dependency cap hit during failover. What is the strongest next move? Rate-limiting policy currently treats all tenants equally.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ticketing API under bot-driven burst traffic, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-025",
      "type": "multiple-choice",
      "question": "Scenario: fraud scoring service with uneven tenant load. Observed symptom: retry storms amplifying hotspot pressure. What is the strongest next move? Previous incidents showed similar partition-skew signatures.",
      "options": [
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For fraud scoring service with uneven tenant load, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-026",
      "type": "multiple-choice",
      "question": "Scenario: ad bidding pipeline with strict p99 budget. Observed symptom: ordered-processing constraints limiting parallelism. What is the strongest next move? The fix must be validated within this shift handoff window.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ad bidding pipeline with strict p99 budget, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize from scenario: ad bidding pipeline with strict p99 budget to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-027",
      "type": "multiple-choice",
      "question": "Scenario: video metadata API during cross-region failover. Observed symptom: noisy-neighbor contention in shared nodes. What is the strongest next move? Metrics freshness lag makes fast feedback difficult.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For video metadata API during cross-region failover, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-028",
      "type": "multiple-choice",
      "question": "Scenario: notification system with delayed queue recovery. Observed symptom: control-plane change blast radius across regions. What is the strongest next move? Current autoscaling policy has no scale-in stabilization.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For notification system with delayed queue recovery, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-029",
      "type": "multiple-choice",
      "question": "Scenario: payment authorization path with dependency saturation. Observed symptom: insufficient standby capacity for evacuation. What is the strongest next move? One region has weaker warm capacity than the others.",
      "options": [
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For payment authorization path with dependency saturation, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-030",
      "type": "multiple-choice",
      "question": "Scenario: search frontend under cold-start regressions. Observed symptom: cost cap conflict with strict latency SLO. What is the strongest next move? Runbook ownership is clear but thresholds need tuning.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For search frontend under cold-start regressions, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-031",
      "type": "multiple-choice",
      "question": "Scenario: webhook ingestion tier with retry amplification. Observed symptom: queue age surge despite moderate average CPU. What is the strongest next move? The mitigation should minimize blast radius to adjacent systems.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For webhook ingestion tier with retry amplification, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-032",
      "type": "multiple-choice",
      "question": "Scenario: gaming session backend with sticky-state pressure. Observed symptom: partition-level saturation hidden by fleet averages. What is the strongest next move? Workload mix changed after recent product feature launch.",
      "options": [
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For gaming session backend with sticky-state pressure, this response is the most defensible call for Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-033",
      "type": "multiple-choice",
      "question": "Scenario: identity token service during rollout + traffic shift. Observed symptom: cold path latency after scale-from-low baseline. What is the strongest next move? Low-priority traffic can be degraded if explicitly controlled.",
      "options": [
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For identity token service during rollout + traffic shift, this option best addresses the stated constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-034",
      "type": "multiple-choice",
      "question": "Scenario: log ingestion pipeline with skewed partitions. Observed symptom: regional dependency cap hit during failover. What is the strongest next move? Executive review requested explicit SLO/cost trade-off rationale.",
      "options": [
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass.",
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For log ingestion pipeline with skewed partitions, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-035",
      "type": "multiple-choice",
      "question": "Scenario: ride dispatch API with latency-sensitive routing. Observed symptom: retry storms amplifying hotspot pressure. What is the strongest next move? Post-incident prevention is required as part of the first fix.",
      "options": [
        "Raise retry budgets globally to protect availability while deferring architectural changes until post-incident review.",
        "Use fleet-level averages as primary guidance first, then investigate partition and regional variance if regressions persist.",
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale all tiers proportionally first to restore headroom, then decompose bottlenecks in a later optimization pass."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ride dispatch API with latency-sensitive routing, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios.",
      "detailedExplanation": "Generalize from scenario: ride dispatch API with latency-sensitive routing to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat platform with reconnect storm after AZ fault. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in chat platform with reconnect storm after AZ fault.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: chat platform with reconnect storm after AZ fault, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under strict p99 SLO pressure?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision under strict p99 SLO pressure, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize from after confirming diagnosis, what is the strongest next action under strict p99 SLO to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticketing API under bot-driven burst traffic. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in ticketing API under bot-driven burst traffic."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ticketing API under bot-driven burst traffic, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while preventing retry amplification?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision while preventing retry amplification, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: fraud scoring service with uneven tenant load. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in fraud scoring service with uneven tenant load.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: fraud scoring service with uneven tenant load, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during staged regional evacuation?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision during staged regional evacuation, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ad bidding pipeline with strict p99 budget. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in ad bidding pipeline with strict p99 budget.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ad bidding pipeline with strict p99 budget, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action without violating cost guardrails?",
          "options": [
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision without violating cost guardrails, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: video metadata API during cross-region failover. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in video metadata API during cross-region failover.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: video metadata API during cross-region failover, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while preserving correctness boundaries?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision while preserving correctness boundaries, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: notification system with delayed queue recovery. Current incident signal is cost cap conflict with strict latency SLO. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cost cap conflict with strict latency SLO and current scaling/routing controls in notification system with delayed queue recovery."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: notification system with delayed queue recovery, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during rapid traffic growth?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: pre-scale standby regions and validate RTO/RPO with game-day drills.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision during rapid traffic growth, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: postponing scaling work until after constraint breach."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: payment authorization path with dependency saturation. Current incident signal is queue age surge despite moderate average CPU. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between queue age surge despite moderate average CPU and current scaling/routing controls in payment authorization path with dependency saturation.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: payment authorization path with dependency saturation, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize from scenario review: payment authorization path with dependency saturation to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with uncertain telemetry confidence?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: define explicit degradation mode and admission controls when cap pressure is reached.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision with uncertain telemetry confidence, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: search frontend under cold-start regressions. Current incident signal is partition-level saturation hidden by fleet averages. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between partition-level saturation hidden by fleet averages and current scaling/routing controls in search frontend under cold-start regressions.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: search frontend under cold-start regressions, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action before full rollout promotion?",
          "options": [
            "Execute this plan first: prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision before full rollout promotion, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: webhook ingestion tier with retry amplification. Current incident signal is cold path latency after scale-from-low baseline. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between cold path latency after scale-from-low baseline and current scaling/routing controls in webhook ingestion tier with retry amplification.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: webhook ingestion tier with retry amplification, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during dependency saturation?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision during dependency saturation, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize from scaling Compute Scenarios to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: gaming session backend with sticky-state pressure. Current incident signal is regional dependency cap hit during failover. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between regional dependency cap hit during failover and current scaling/routing controls in gaming session backend with sticky-state pressure."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: gaming session backend with sticky-state pressure, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while containing blast radius?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision while containing blast radius, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: identity token service during rollout + traffic shift. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in identity token service during rollout + traffic shift.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: identity token service during rollout + traffic shift, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under hotspot tenant skew?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision under hotspot tenant skew, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: log ingestion pipeline with skewed partitions. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in log ingestion pipeline with skewed partitions.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: log ingestion pipeline with skewed partitions, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during cold-start-sensitive periods?",
          "options": [
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision during cold-start-sensitive periods, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Generalize from after confirming diagnosis, what is the strongest next action during to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ride dispatch API with latency-sensitive routing. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in ride dispatch API with latency-sensitive routing.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ride dispatch API with latency-sensitive routing, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with rollback readiness requirements?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision with rollback readiness requirements, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: document collaboration backend with hot tenants. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in document collaboration backend with hot tenants."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: document collaboration backend with hot tenants, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while keeping failback safe?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision while keeping failback safe, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: pricing service with cache-miss storm. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in pricing service with cache-miss storm.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: pricing service with cache-miss storm, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with mixed ordered/unordered workloads?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision with mixed ordered/unordered workloads, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize from after confirming diagnosis, what is the strongest next action with mixed to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: support messaging system during control-plane drift. Current incident signal is cost cap conflict with strict latency SLO. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cost cap conflict with strict latency SLO and current scaling/routing controls in support messaging system during control-plane drift.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: support messaging system during control-plane drift, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under high queue-age variance?",
          "options": [
            "Execute this plan first: pre-scale standby regions and validate RTO/RPO with game-day drills.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision under high queue-age variance, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: catalog read API with stale replica lag. Current incident signal is queue age surge despite moderate average CPU. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between queue age surge despite moderate average CPU and current scaling/routing controls in catalog read API with stale replica lag.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: catalog read API with stale replica lag, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while protecting premium-tenant SLOs?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: define explicit degradation mode and admission controls when cap pressure is reached."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision while protecting premium-tenant SLOs, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: compliance scanner with long-running job backlog. Current incident signal is partition-level saturation hidden by fleet averages. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between partition-level saturation hidden by fleet averages and current scaling/routing controls in compliance scanner with long-running job backlog."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: compliance scanner with long-running job backlog, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during control-plane instability?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision during control-plane instability, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize from scaling Compute Scenarios to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: global commerce checkout during regional degradation. Current incident signal is cold path latency after scale-from-low baseline. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cold path latency after scale-from-low baseline and current scaling/routing controls in global commerce checkout during regional degradation.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: global commerce checkout during regional degradation, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with limited ops bandwidth?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision with limited ops bandwidth, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Common pitfall: planning on average transfer while peak bursts dominate."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: flash-sale inventory service with hotspot keys. Current incident signal is regional dependency cap hit during failover. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between regional dependency cap hit during failover and current scaling/routing controls in flash-sale inventory service with hotspot keys.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: flash-sale inventory service with hotspot keys, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Generalize from scenario review: flash-sale inventory service with hotspot keys to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action before next peak event?",
          "options": [
            "Execute this plan first: raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision before next peak event, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat platform with reconnect storm after AZ fault. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in chat platform with reconnect storm after AZ fault.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: chat platform with reconnect storm after AZ fault, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with lag-sensitive read paths?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision with lag-sensitive read paths, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticketing API under bot-driven burst traffic. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in ticketing API under bot-driven burst traffic."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ticketing API under bot-driven burst traffic, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during migration overlap?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision during migration overlap, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: fraud scoring service with uneven tenant load. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in fraud scoring service with uneven tenant load.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: fraud scoring service with uneven tenant load, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while constraining concurrency spikes?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision while constraining concurrency spikes, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "Generalize from after confirming diagnosis, what is the strongest next action while constraining to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ad bidding pipeline with strict p99 budget. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in ad bidding pipeline with strict p99 budget.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ad bidding pipeline with strict p99 budget, this choice most directly resolves the stated failure mode in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under partial regional outage?",
          "options": [
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision under partial regional outage, this mitigation aligns best with the scenario constraints in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: video metadata API during cross-region failover. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in video metadata API during cross-region failover.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: video metadata API during cross-region failover, this response is the most defensible call for Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with incident-to-steady-state transition?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For the follow-up change decision with incident-to-steady-state transition, this option best addresses the stated constraints in Scaling Compute Scenarios.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-061",
      "type": "multi-select",
      "question": "In end-to-end scaling incidents, which diagnostics are highest value first? (Select all that apply)",
      "options": [
        "Queue age and tail latency by partition/region",
        "Dependency saturation and error skew",
        "Only global mean CPU",
        "Traffic-shift/failover event timeline"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Integrated diagnosis needs granular signals and timeline correlation, not only global means.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-062",
      "type": "multi-select",
      "question": "Which controls reduce blast radius during emergency traffic shifts? (Select all that apply)",
      "options": [
        "Regional traffic caps",
        "Dependency-aware ramp sequencing",
        "Immediate full cutover by default",
        "Rollback thresholds with ownership"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Guarded shift and rollback controls limit cascade risk.",
      "detailedExplanation": "Generalize from controls reduce blast radius during emergency traffic shifts? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-063",
      "type": "multi-select",
      "question": "For hotspot + autoscaling interactions, which practices help? (Select all that apply)",
      "options": [
        "Fix key skew before broad scale-out",
        "Use shard-aware signals",
        "Rely on fleet averages only",
        "Apply fair scheduling/quotas"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Skew-aware controls prevent ineffective scale-out and unfair starvation.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-064",
      "type": "multi-select",
      "question": "Compute selection in scenario triage should consider which jointly? (Select all that apply)",
      "options": [
        "Runtime limits vs job profile",
        "Cold-start sensitivity",
        "Weekly meeting availability only",
        "Isolation and operability requirements"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Platform fit is multidimensional: runtime shape, latency sensitivity, and operational constraints.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-065",
      "type": "multi-select",
      "question": "Which signs indicate scale-in policy is unsafe during incidents? (Select all that apply)",
      "options": [
        "In-flight work loss during downscale",
        "Replica oscillation after short dips",
        "Stable queue age and no retries",
        "Frequent rollback to prior capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Unsafe scale-in shows churn, dropped work, and unstable rollback behavior.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-066",
      "type": "multi-select",
      "question": "For multi-region + sharding scenarios, which safeguards are useful? (Select all that apply)",
      "options": [
        "Regional isolation boundaries",
        "Per-partition retry budgets",
        "Single global mutable control without scoping",
        "Failover drills with RTO/RPO measurement"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Scoping, retry control, and tested failover guard against cascading failures.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-067",
      "type": "multi-select",
      "question": "Which interventions are valid under cost cap pressure with strict SLOs? (Select all that apply)",
      "options": [
        "Admission control for low-priority load",
        "Graceful degradation mode",
        "Remove all safety caps and hope",
        "Protect critical-path capacity floors"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "When caps and SLOs conflict, enforce priority/degradation policies explicitly.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-068",
      "type": "multi-select",
      "question": "What makes an integrated migration plan safer? (Select all that apply)",
      "options": [
        "Canary by traffic segment",
        "Baseline/post-change SLO+cost comparison",
        "Big-bang cutover only",
        "Clear fallback ownership/runbook"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Safe migration combines staged rollout, measurement, and owned fallback.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-069",
      "type": "multi-select",
      "question": "Which metrics should be watched together during incident stabilization? (Select all that apply)",
      "options": [
        "Tail latency",
        "Error budget burn rate",
        "Queue age/backlog",
        "Only deployment count"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Stabilization requires service quality, reliability, and work-congestion metrics together.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-070",
      "type": "multi-select",
      "question": "Which anti-patterns commonly appear in compute-scaling scenarios? (Select all that apply)",
      "options": [
        "Treating retries as free capacity",
        "Ignoring partition/regional skew",
        "Canarying policy changes",
        "Applying one metric to all workloads"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Oversimplified controls and blind scaling are recurrent failure modes.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-071",
      "type": "multi-select",
      "question": "For ordered-processing bottlenecks, which options are valid? (Select all that apply)",
      "options": [
        "Isolate strict-order path",
        "Relax order where domain allows",
        "Assume ordering has zero throughput cost",
        "Use idempotent handlers for safe retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ordering guarantees constrain parallelism and should be scoped intentionally.",
      "detailedExplanation": "Generalize from for ordered-processing bottlenecks, which options are valid? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-072",
      "type": "multi-select",
      "question": "What supports confident failback after incident containment? (Select all that apply)",
      "options": [
        "Gradual traffic restoration",
        "Consistency/integrity checks",
        "Immediate 100% restore always",
        "Rollback triggers if regression appears"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failback should be staged, validated, and reversible.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-073",
      "type": "multi-select",
      "question": "In mixed compute estates (VM/container/serverless), which governance helps? (Select all that apply)",
      "options": [
        "Platform selection rubric per workload",
        "Shared reliability guardrails",
        "Unbounded platform sprawl",
        "Periodic cost/perf review by service tier"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Mixed estates are sustainable with explicit selection rules and shared guardrails.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-074",
      "type": "multi-select",
      "question": "Which signs indicate control-plane risk is under-managed? (Select all that apply)",
      "options": [
        "Global policy blasts without canary",
        "No audited rollback path",
        "Regional staged rollout discipline",
        "Frequent config drift surprises"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Control-plane safety requires scoped rollout, auditability, and rollback readiness.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-075",
      "type": "multi-select",
      "question": "During reconnect storms, which controls matter most? (Select all that apply)",
      "options": [
        "Connection admission backpressure",
        "Jittered reconnect guidance",
        "Infinite immediate client retries",
        "Warm capacity buffer on connection gateways"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Reconnect storms need coordinated client and server-side damping.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-076",
      "type": "multi-select",
      "question": "Which are valid scenario-level success criteria after mitigation? (Select all that apply)",
      "options": [
        "SLO recovery sustained across peak window",
        "Reduced error-budget burn",
        "Higher mean CPU regardless user impact",
        "Cost returns near pre-incident bounds"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Success should be measured by user impact, reliability burn, and cost stability.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-077",
      "type": "multi-select",
      "question": "Which preparations improve incident response on compute scaling paths? (Select all that apply)",
      "options": [
        "Predefined degradation modes",
        "Owner-mapped runbooks",
        "Ad hoc undocumented actions only",
        "Regular game-day simulations"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Preparedness increases speed and correctness of response under pressure.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-078",
      "type": "numeric-input",
      "question": "Backlog is 960,000 jobs. Net drain after mitigation is 24,000 jobs/min. Minutes to clear backlog?",
      "answer": 40,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "960,000 / 24,000 = 40 minutes.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 960,000 and 24,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-079",
      "type": "numeric-input",
      "question": "Primary region loses 45% capacity. Remaining regions can absorb 18,000 rps and 12,000 rps. Maximum total failover traffic they can absorb?",
      "answer": 30000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "18,000 + 12,000 = 30,000 rps.",
      "detailedExplanation": "Generalize from primary region loses 45% capacity to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 45 and 18,000 rps should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-080",
      "type": "numeric-input",
      "question": "A hotspot shard handles 22% of 80,000 rps. How much rps hits that shard?",
      "answer": 17600,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.22 * 80,000 = 17,600 rps.",
      "detailedExplanation": "Generalize from hotspot shard handles 22% of 80,000 rps to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 22 and 80,000 rps appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-081",
      "type": "numeric-input",
      "question": "Queue age SLO is 90s. Current queue age is 225s. Percent over SLO?",
      "answer": 150,
      "unit": "%",
      "tolerance": 0.5,
      "explanation": "(225-90)/90 = 150% over target.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 90s and 225s should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-082",
      "type": "numeric-input",
      "question": "Autoscaling adds 8 workers/min. Starting at 32 workers, how many after 6 minutes without scale-in?",
      "answer": 80,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "32 + 8*6 = 80 workers.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 8 and 32 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-083",
      "type": "numeric-input",
      "question": "A retry policy adds 0.25 extra requests per original request. At 48,000 original rps, effective rps?",
      "answer": 60000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "48,000 * 1.25 = 60,000 rps.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Normalize units before computing so conversion mistakes do not propagate. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. If values like 0.25 and 48,000 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-084",
      "type": "numeric-input",
      "question": "Standby region warm pool is 140 instances. Promotion needs 196. Additional instances required?",
      "answer": 56,
      "unit": "instances",
      "tolerance": 0,
      "explanation": "196 - 140 = 56 instances.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 140 and 196 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-085",
      "type": "numeric-input",
      "question": "p99 improved from 1800ms to 720ms after mitigation. Percent reduction?",
      "answer": 60,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1800-720)/1800 = 60% reduction.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 1800ms and 720ms appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-086",
      "type": "numeric-input",
      "question": "A tier runs at 70% target utilization with 35,000 rps demand. Required raw capacity?",
      "answer": 50000,
      "unit": "rps",
      "tolerance": 0.02,
      "explanation": "35,000 / 0.7 = 50,000 rps.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 70 and 35,000 rps in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-087",
      "type": "numeric-input",
      "question": "Failover drill has 5 stages of 3 minutes each before full promotion. Total drill minutes?",
      "answer": 15,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "5 * 3 = 15 minutes.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 5 and 3 minutes in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-088",
      "type": "numeric-input",
      "question": "Per-partition cap is 4,500 rps. Hot partition receives 9,900 rps. Minimum equal split partitions needed?",
      "answer": 3,
      "unit": "partitions",
      "tolerance": 0,
      "explanation": "9,900 / 4,500 = 2.2, so at least 3 partitions.",
      "detailedExplanation": "Generalize from per-partition cap is 4,500 rps to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 4,500 rps and 9,900 rps in aligned units before deciding on an implementation approach. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-089",
      "type": "numeric-input",
      "question": "Degradation mode sheds 12% of 75,000 rps low-priority traffic. How much rps is shed?",
      "answer": 9000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.12 * 75,000 = 9,000 rps.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 12 and 75,000 rps should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-090",
      "type": "ordering",
      "question": "Order a robust integrated incident response loop.",
      "items": [
        "Scope bottleneck with granular signals",
        "Apply blast-radius containment controls",
        "Execute targeted mitigation",
        "Validate and codify guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnose, contain, mitigate, and institutionalize.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-091",
      "type": "ordering",
      "question": "Order by fastest to slowest mitigation impact (typical).",
      "items": [
        "Traffic/rate guardrails",
        "Autoscaling policy tuning",
        "Partition strategy adjustment",
        "Major platform migration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Immediate controls usually act faster than architectural shifts.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-092",
      "type": "ordering",
      "question": "Order by increasing observability granularity.",
      "items": [
        "Global averages",
        "Per-region metrics",
        "Per-service and per-shard metrics",
        "Per-shard plus top-key and queue-age outliers"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Finer granularity improves hotspot diagnosis precision.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-093",
      "type": "ordering",
      "question": "Order failover/failback safety from weakest to strongest.",
      "items": [
        "Untested manual steps",
        "Documented runbook only",
        "Runbook with periodic drills",
        "Runbook with drills and rollback triggers"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Tested and guarded workflows are safest.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-094",
      "type": "ordering",
      "question": "Order by increasing risk of retry amplification.",
      "items": [
        "Budgeted retries with jitter",
        "Bounded retries without jitter",
        "High retry limits during errors",
        "Immediate unbounded synchronized retries"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Loose, synchronized retry behavior amplifies incidents.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-095",
      "type": "ordering",
      "question": "Order platform-change rollout safety.",
      "items": [
        "Canary subset rollout",
        "Incremental traffic ramps",
        "Large batch cutover",
        "Immediate full cutover"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Smaller progressive steps reduce change risk.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-096",
      "type": "ordering",
      "question": "Order by increasing ordering guarantee strength. (Scaling Compute Scenarios context)",
      "items": [
        "Unordered processing",
        "Partition-local ordering",
        "Tenant-local ordering",
        "Global total ordering"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger ordering generally constrains parallelism more.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-097",
      "type": "ordering",
      "question": "Order by strongest evidence quality for mitigation success.",
      "items": [
        "Anecdotal operator impression",
        "Global mean metric improvement",
        "SLO + error-budget recovery across peak",
        "SLO/cost recovery plus recurrence guardrail validation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "High-confidence validation combines user impact, reliability, cost, and prevention signals.",
      "detailedExplanation": "Generalize from order by strongest evidence quality for mitigation success to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-098",
      "type": "ordering",
      "question": "Order escalation path for capacity-vs-cost conflict.",
      "items": [
        "Enable low-priority shedding",
        "Protect critical-path floor",
        "Tune autoscaling and routing to cap pressure",
        "Revisit capacity budget and architecture constraints"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start with tactical controls, then adjust system and planning assumptions.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-099",
      "type": "ordering",
      "question": "Order by increasing control-plane blast-radius safety.",
      "items": [
        "Global one-shot policy update",
        "Large regional batch update",
        "Regional canary update",
        "Regional canary with automated rollback"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer control-plane changes scope impact and add rollback automation.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-100",
      "type": "ordering",
      "question": "Order scenario-mitigation lifecycle.",
      "items": [
        "Detect pattern recurrence risk",
        "Define hypothesis + guardrails",
        "Implement and monitor in canary",
        "Promote and archive lessons"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Explicit hypotheses and guardrails improve repeatability and learning.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    }
  ]
}
