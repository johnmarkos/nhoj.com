{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 8,
  "chapterTitle": "Scaling Compute Scenarios",
  "chapterDescription": "Integrated scenario practice combining bottleneck triage, load balancing, autoscaling, hotspot mitigation, compute-platform choices, and multi-region failover trade-offs.",
  "problems": [
    {
      "id": "sc-scn-001",
      "type": "multiple-choice",
      "question": "Scenario: global commerce checkout during regional degradation. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? The issue started immediately after a traffic policy update.",
      "options": [
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For global commerce checkout during regional degradation, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "This prompt is really about \"scenario: global commerce checkout during regional degradation\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-002",
      "type": "multiple-choice",
      "question": "Scenario: flash-sale inventory service with hotspot keys. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Only one dependency tier reached saturation first.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For flash-sale inventory service with hotspot keys, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "If you keep \"scenario: flash-sale inventory service with hotspot keys\" in view, the correct answer separates faster. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-003",
      "type": "multiple-choice",
      "question": "Scenario: chat platform with reconnect storm after AZ fault. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Canary behavior diverged from full-fleet behavior.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For chat platform with reconnect storm after AZ fault, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "The core signal here is \"scenario: chat platform with reconnect storm after AZ fault\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-004",
      "type": "multiple-choice",
      "question": "Scenario: ticketing API under bot-driven burst traffic. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Global averages looked healthy while p99 regressed.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ticketing API under bot-driven burst traffic, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The key clue in this question is \"scenario: ticketing API under bot-driven burst traffic\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-005",
      "type": "multiple-choice",
      "question": "Scenario: fraud scoring service with uneven tenant load. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Retry amplification appeared in one tenant segment.",
      "options": [
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For fraud scoring service with uneven tenant load, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "Start from \"scenario: fraud scoring service with uneven tenant load\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-006",
      "type": "multiple-choice",
      "question": "Scenario: ad bidding pipeline with strict p99 budget. Primary symptom is ordered-processing constraints limiting parallelism. What is the strongest next move? Backlog growth remained localized after scale-out.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ad bidding pipeline with strict p99 budget, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"scenario: ad bidding pipeline with strict p99 budget\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-007",
      "type": "multiple-choice",
      "question": "Scenario: video metadata API during cross-region failover. Primary symptom is noisy-neighbor contention in shared nodes. What is the strongest next move? Regional failover timing exceeded the documented runbook.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For video metadata API during cross-region failover, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "Read this as a scenario about \"scenario: video metadata API during cross-region failover\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-008",
      "type": "multiple-choice",
      "question": "Scenario: notification system with delayed queue recovery. Primary symptom is control-plane change blast radius across regions. What is the strongest next move? Incident mitigations must preserve strict correctness boundaries.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For notification system with delayed queue recovery, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"scenario: notification system with delayed queue recovery\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-009",
      "type": "multiple-choice",
      "question": "Scenario: payment authorization path with dependency saturation. Primary symptom is insufficient standby capacity for evacuation. What is the strongest next move? Ops capacity is constrained and changes must be high leverage.",
      "options": [
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For payment authorization path with dependency saturation, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "This prompt is really about \"scenario: payment authorization path with dependency saturation\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-010",
      "type": "multiple-choice",
      "question": "Scenario: search frontend under cold-start regressions. Primary symptom is cost cap conflict with strict latency SLO. What is the strongest next move? The business event peak repeats in two hours.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For search frontend under cold-start regressions, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"scenario: search frontend under cold-start regressions\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-011",
      "type": "multiple-choice",
      "question": "Scenario: webhook ingestion tier with retry amplification. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? A prior rollback did not remove the hotspot pattern.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For webhook ingestion tier with retry amplification, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "The decision turns on \"scenario: webhook ingestion tier with retry amplification\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-012",
      "type": "multiple-choice",
      "question": "Scenario: gaming session backend with sticky-state pressure. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Tail latency and error-budget burn rose together.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For gaming session backend with sticky-state pressure, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"scenario: gaming session backend with sticky-state pressure\", then pressure-test the result against the options. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-013",
      "type": "multiple-choice",
      "question": "Scenario: identity token service during rollout + traffic shift. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Control-plane drift increased uncertainty in diagnostics.",
      "options": [
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For identity token service during rollout + traffic shift, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "The key clue in this question is \"scenario: identity token service during rollout + traffic shift\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-014",
      "type": "multiple-choice",
      "question": "Scenario: log ingestion pipeline with skewed partitions. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Capacity caps cannot be removed due to budget guardrails.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For log ingestion pipeline with skewed partitions, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The core signal here is \"scenario: log ingestion pipeline with skewed partitions\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-015",
      "type": "multiple-choice",
      "question": "Scenario: ride dispatch API with latency-sensitive routing. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Service owners require a reversible first step.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ride dispatch API with latency-sensitive routing, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "If you keep \"scenario: ride dispatch API with latency-sensitive routing\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-016",
      "type": "multiple-choice",
      "question": "Scenario: document collaboration backend with hot tenants. Primary symptom is ordered-processing constraints limiting parallelism. What is the strongest next move? Customer-facing impact is concentrated in one geography.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For document collaboration backend with hot tenants, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "This prompt is really about \"scenario: document collaboration backend with hot tenants\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-017",
      "type": "multiple-choice",
      "question": "Scenario: pricing service with cache-miss storm. Primary symptom is noisy-neighbor contention in shared nodes. What is the strongest next move? Hot partitions stayed hot even after worker expansion.",
      "options": [
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For pricing service with cache-miss storm, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "Use \"scenario: pricing service with cache-miss storm\" as your starting point, then verify tradeoffs carefully. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-018",
      "type": "multiple-choice",
      "question": "Scenario: support messaging system during control-plane drift. Primary symptom is control-plane change blast radius across regions. What is the strongest next move? Failback safety is a hard requirement in this incident.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For support messaging system during control-plane drift, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"scenario: support messaging system during control-plane drift\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-019",
      "type": "multiple-choice",
      "question": "Scenario: catalog read API with stale replica lag. Primary symptom is insufficient standby capacity for evacuation. What is the strongest next move? Admission controls are available but not yet tuned.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For catalog read API with stale replica lag, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "The decision turns on \"scenario: catalog read API with stale replica lag\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-020",
      "type": "multiple-choice",
      "question": "Scenario: compliance scanner with long-running job backlog. Primary symptom is cost cap conflict with strict latency SLO. What is the strongest next move? Team must avoid introducing new platform lock-in quickly.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For compliance scanner with long-running job backlog, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"scenario: compliance scanner with long-running job backlog\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-021",
      "type": "multiple-choice",
      "question": "Scenario: global commerce checkout during regional degradation. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? Standby region promotion is possible but dependency-limited.",
      "options": [
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For global commerce checkout during regional degradation, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "Read this as a scenario about \"scenario: global commerce checkout during regional degradation\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-022",
      "type": "multiple-choice",
      "question": "Scenario: flash-sale inventory service with hotspot keys. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Some workloads in the path require strict ordering.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For flash-sale inventory service with hotspot keys, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The key clue in this question is \"scenario: flash-sale inventory service with hotspot keys\". Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-023",
      "type": "multiple-choice",
      "question": "Scenario: chat platform with reconnect storm after AZ fault. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Connection-heavy clients are causing reconnect surges.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For chat platform with reconnect storm after AZ fault, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "Start from \"scenario: chat platform with reconnect storm after AZ fault\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-024",
      "type": "multiple-choice",
      "question": "Scenario: ticketing API under bot-driven burst traffic. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Rate-limiting policy currently treats all tenants equally.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ticketing API under bot-driven burst traffic, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "If you keep \"scenario: ticketing API under bot-driven burst traffic\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-025",
      "type": "multiple-choice",
      "question": "Scenario: fraud scoring service with uneven tenant load. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Previous incidents showed similar partition-skew signatures.",
      "options": [
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For fraud scoring service with uneven tenant load, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "The core signal here is \"scenario: fraud scoring service with uneven tenant load\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-026",
      "type": "multiple-choice",
      "question": "Scenario: ad bidding pipeline with strict p99 budget. Primary symptom is ordered-processing constraints limiting parallelism. What is the strongest next move? The fix must be validated within this shift handoff window.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, split ordered vs unordered workloads and isolate strict-order paths."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ad bidding pipeline with strict p99 budget, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"scenario: ad bidding pipeline with strict p99 budget\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-027",
      "type": "multiple-choice",
      "question": "Scenario: video metadata API during cross-region failover. Primary symptom is noisy-neighbor contention in shared nodes. What is the strongest next move? Metrics freshness lag makes fast feedback difficult.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, move sensitive workloads to dedicated pools with stronger placement/limits.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For video metadata API during cross-region failover, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "This prompt is really about \"scenario: video metadata API during cross-region failover\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-028",
      "type": "multiple-choice",
      "question": "Scenario: notification system with delayed queue recovery. Primary symptom is control-plane change blast radius across regions. What is the strongest next move? Current autoscaling policy has no scale-in stabilization.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, roll out control-plane changes by region canary with auto-rollback thresholds.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For notification system with delayed queue recovery, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"scenario: notification system with delayed queue recovery\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-029",
      "type": "multiple-choice",
      "question": "Scenario: payment authorization path with dependency saturation. Primary symptom is insufficient standby capacity for evacuation. What is the strongest next move? One region has weaker warm capacity than the others.",
      "options": [
        "In this scenario, pre-scale standby regions and validate RTO/RPO with game-day drills.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For payment authorization path with dependency saturation, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "Read this as a scenario about \"scenario: payment authorization path with dependency saturation\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-030",
      "type": "multiple-choice",
      "question": "Scenario: search frontend under cold-start regressions. Primary symptom is cost cap conflict with strict latency SLO. What is the strongest next move? Runbook ownership is clear but thresholds need tuning.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, define explicit degradation mode and admission controls when cap pressure is reached."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For search frontend under cold-start regressions, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"scenario: search frontend under cold-start regressions\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-031",
      "type": "multiple-choice",
      "question": "Scenario: webhook ingestion tier with retry amplification. Primary symptom is queue age surge despite moderate average CPU. What is the strongest next move? The mitigation should minimize blast radius to adjacent systems.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For webhook ingestion tier with retry amplification, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "The key clue in this question is \"scenario: webhook ingestion tier with retry amplification\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-032",
      "type": "multiple-choice",
      "question": "Scenario: gaming session backend with sticky-state pressure. Primary symptom is partition-level saturation hidden by fleet averages. What is the strongest next move? Workload mix changed after recent product feature launch.",
      "options": [
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes."
      ],
      "correct": 1,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For gaming session backend with sticky-state pressure, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"scenario: gaming session backend with sticky-state pressure\". Eliminate options that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-033",
      "type": "multiple-choice",
      "question": "Scenario: identity token service during rollout + traffic shift. Primary symptom is cold path latency after scale-from-low baseline. What is the strongest next move? Low-priority traffic can be degraded if explicitly controlled.",
      "options": [
        "In this scenario, raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics."
      ],
      "correct": 0,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For identity token service during rollout + traffic shift, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "The decision turns on \"scenario: identity token service during rollout + traffic shift\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 7519: JSON Web Token (JWT)",
          "url": "https://www.rfc-editor.org/rfc/rfc7519"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-034",
      "type": "multiple-choice",
      "question": "Scenario: log ingestion pipeline with skewed partitions. Primary symptom is regional dependency cap hit during failover. What is the strongest next move? Executive review requested explicit SLO/cost trade-off rationale.",
      "options": [
        "Scale every tier equally and ignore bottleneck decomposition.",
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, enforce regional traffic caps with dependency-aware failover sequencing."
      ],
      "correct": 3,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For log ingestion pipeline with skewed partitions, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "This prompt is really about \"scenario: log ingestion pipeline with skewed partitions\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-035",
      "type": "multiple-choice",
      "question": "Scenario: ride dispatch API with latency-sensitive routing. Primary symptom is retry storms amplifying hotspot pressure. What is the strongest next move? Post-incident prevention is required as part of the first fix.",
      "options": [
        "Increase retries globally and defer architecture changes.",
        "Treat global averages as sufficient and skip partition/regional diagnostics.",
        "In this scenario, apply retry budgets with jitter and partition-scoped backpressure controls.",
        "Scale every tier equally and ignore bottleneck decomposition."
      ],
      "correct": 2,
      "explanation": "Integrated scenario decisions should target the actual bottleneck and protect reliability/cost guardrails simultaneously. For ride dispatch API with latency-sensitive routing, this is the strongest fit in Scaling Compute Scenarios.",
      "detailedExplanation": "Use \"scenario: ride dispatch API with latency-sensitive routing\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat platform with reconnect storm after AZ fault. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in chat platform with reconnect storm after AZ fault.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: chat platform with reconnect storm after AZ fault, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The core signal here is \"scenario review: chat platform with reconnect storm after AZ fault\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under strict p99 SLO pressure?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action under strict p99 SLO pressure, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Use \"after confirming diagnosis, what is the strongest next action under strict p99 SLO\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"scaling Compute Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticketing API under bot-driven burst traffic. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in ticketing API under bot-driven burst traffic."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ticketing API under bot-driven burst traffic, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The key clue in this question is \"scenario review: ticketing API under bot-driven burst traffic\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while preventing retry amplification?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action while preventing retry amplification, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis, what is the strongest next action while preventing retry\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "If you keep \"scaling Compute Scenarios\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: fraud scoring service with uneven tenant load. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in fraud scoring service with uneven tenant load.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: fraud scoring service with uneven tenant load, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "This prompt is really about \"scenario review: fraud scoring service with uneven tenant load\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during staged regional evacuation?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action during staged regional evacuation, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "If you keep \"after confirming diagnosis, what is the strongest next action during staged regional\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"scaling Compute Scenarios\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ad bidding pipeline with strict p99 budget. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in ad bidding pipeline with strict p99 budget.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ad bidding pipeline with strict p99 budget, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "If you keep \"scenario review: ad bidding pipeline with strict p99 budget\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action without violating cost guardrails?",
          "options": [
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action without violating cost guardrails, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis, what is the strongest next action without violating cost\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"scaling Compute Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: video metadata API during cross-region failover. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in video metadata API during cross-region failover.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: video metadata API during cross-region failover, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "If you keep \"scenario review: video metadata API during cross-region failover\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while preserving correctness boundaries?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action while preserving correctness boundaries, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis, what is the strongest next action while preserving\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"scaling Compute Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: notification system with delayed queue recovery. Current incident signal is cost cap conflict with strict latency SLO. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cost cap conflict with strict latency SLO and current scaling/routing controls in notification system with delayed queue recovery."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: notification system with delayed queue recovery, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "This prompt is really about \"scenario review: notification system with delayed queue recovery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during rapid traffic growth?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: pre-scale standby regions and validate RTO/RPO with game-day drills.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action during rapid traffic growth, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "If you keep \"after confirming diagnosis, what is the strongest next action during rapid traffic\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: postponing scaling work until after constraint breach."
        }
      ],
      "detailedExplanation": "Start from \"scaling Compute Scenarios\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: payment authorization path with dependency saturation. Current incident signal is queue age surge despite moderate average CPU. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between queue age surge despite moderate average CPU and current scaling/routing controls in payment authorization path with dependency saturation.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: payment authorization path with dependency saturation, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Use \"scenario review: payment authorization path with dependency saturation\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with uncertain telemetry confidence?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: define explicit degradation mode and admission controls when cap pressure is reached.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action with uncertain telemetry confidence, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis, what is the strongest next action with uncertain telemetry\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"scaling Compute Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: search frontend under cold-start regressions. Current incident signal is partition-level saturation hidden by fleet averages. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between partition-level saturation hidden by fleet averages and current scaling/routing controls in search frontend under cold-start regressions.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: search frontend under cold-start regressions, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Read this as a scenario about \"scenario review: search frontend under cold-start regressions\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action before full rollout promotion?",
          "options": [
            "Execute this plan first: prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action before full rollout promotion, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis, what is the strongest next action before full rollout\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"scaling Compute Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: webhook ingestion tier with retry amplification. Current incident signal is cold path latency after scale-from-low baseline. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between cold path latency after scale-from-low baseline and current scaling/routing controls in webhook ingestion tier with retry amplification.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: webhook ingestion tier with retry amplification, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The decision turns on \"scenario review: webhook ingestion tier with retry amplification\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during dependency saturation?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action during dependency saturation, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Start from \"after confirming diagnosis, what is the strongest next action during dependency\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Use \"scaling Compute Scenarios\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: gaming session backend with sticky-state pressure. Current incident signal is regional dependency cap hit during failover. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between regional dependency cap hit during failover and current scaling/routing controls in gaming session backend with sticky-state pressure."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: gaming session backend with sticky-state pressure, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Start from \"scenario review: gaming session backend with sticky-state pressure\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while containing blast radius?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action while containing blast radius, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis, what is the strongest next action while containing blast\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"scaling Compute Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: identity token service during rollout + traffic shift. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in identity token service during rollout + traffic shift.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: identity token service during rollout + traffic shift, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The key clue in this question is \"scenario review: identity token service during rollout + traffic shift\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under hotspot tenant skew?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action under hotspot tenant skew, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis, what is the strongest next action under hotspot tenant skew\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"scaling Compute Scenarios\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: log ingestion pipeline with skewed partitions. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in log ingestion pipeline with skewed partitions.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: log ingestion pipeline with skewed partitions, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The core signal here is \"scenario review: log ingestion pipeline with skewed partitions\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during cold-start-sensitive periods?",
          "options": [
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action during cold-start-sensitive periods, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Use \"after confirming diagnosis, what is the strongest next action during\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"scaling Compute Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ride dispatch API with latency-sensitive routing. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in ride dispatch API with latency-sensitive routing.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ride dispatch API with latency-sensitive routing, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "If you keep \"scenario review: ride dispatch API with latency-sensitive routing\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with rollback readiness requirements?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action with rollback readiness requirements, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis, what is the strongest next action with rollback readiness\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"scaling Compute Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: document collaboration backend with hot tenants. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in document collaboration backend with hot tenants."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: document collaboration backend with hot tenants, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "This prompt is really about \"scenario review: document collaboration backend with hot tenants\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while keeping failback safe?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action while keeping failback safe, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "If you keep \"after confirming diagnosis, what is the strongest next action while keeping failback\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Start from \"scaling Compute Scenarios\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: pricing service with cache-miss storm. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in pricing service with cache-miss storm.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: pricing service with cache-miss storm, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The core signal here is \"scenario review: pricing service with cache-miss storm\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with mixed ordered/unordered workloads?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action with mixed ordered/unordered workloads, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Use \"after confirming diagnosis, what is the strongest next action with mixed\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"scaling Compute Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: support messaging system during control-plane drift. Current incident signal is cost cap conflict with strict latency SLO. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cost cap conflict with strict latency SLO and current scaling/routing controls in support messaging system during control-plane drift.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: support messaging system during control-plane drift, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The key clue in this question is \"scenario review: support messaging system during control-plane drift\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under high queue-age variance?",
          "options": [
            "Execute this plan first: pre-scale standby regions and validate RTO/RPO with game-day drills.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action under high queue-age variance, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis, what is the strongest next action under high queue-age\". Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "If you keep \"scaling Compute Scenarios\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: catalog read API with stale replica lag. Current incident signal is queue age surge despite moderate average CPU. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between queue age surge despite moderate average CPU and current scaling/routing controls in catalog read API with stale replica lag.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: catalog read API with stale replica lag, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Start from \"scenario review: catalog read API with stale replica lag\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while protecting premium-tenant SLOs?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: define explicit degradation mode and admission controls when cap pressure is reached."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action while protecting premium-tenant SLOs, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis, what is the strongest next action while protecting\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"scaling Compute Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: compliance scanner with long-running job backlog. Current incident signal is partition-level saturation hidden by fleet averages. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between partition-level saturation hidden by fleet averages and current scaling/routing controls in compliance scanner with long-running job backlog."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: compliance scanner with long-running job backlog, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The decision turns on \"scenario review: compliance scanner with long-running job backlog\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during control-plane instability?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: prioritize queue-age + tail-latency signals, then apply guarded autoscaling with scale-in stabilization.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action during control-plane instability, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Start from \"after confirming diagnosis, what is the strongest next action during control-plane\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Use \"scaling Compute Scenarios\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: global commerce checkout during regional degradation. Current incident signal is cold path latency after scale-from-low baseline. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between cold path latency after scale-from-low baseline and current scaling/routing controls in global commerce checkout during regional degradation.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: global commerce checkout during regional degradation, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Read this as a scenario about \"scenario review: global commerce checkout during regional degradation\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with limited ops bandwidth?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: fix partition skew (key strategy/fair scheduling) before adding broad fleet capacity.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action with limited ops bandwidth, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis, what is the strongest next action with limited ops bandwidth\". Solve this as chained reasoning where stage two must respect stage one assumptions. Convert bits/bytes carefully and include sustained peak assumptions in transfer planning. Common pitfall: planning on average transfer while peak bursts dominate."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"scaling Compute Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: flash-sale inventory service with hotspot keys. Current incident signal is regional dependency cap hit during failover. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between regional dependency cap hit during failover and current scaling/routing controls in flash-sale inventory service with hotspot keys.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: flash-sale inventory service with hotspot keys, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Use \"scenario review: flash-sale inventory service with hotspot keys\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action before next peak event?",
          "options": [
            "Execute this plan first: raise warm baseline and separate latency-critical paths from cold-start-prone handlers.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action before next peak event, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis, what is the strongest next action before next peak event\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "The decision turns on \"scaling Compute Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat platform with reconnect storm after AZ fault. Current incident signal is retry storms amplifying hotspot pressure. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between retry storms amplifying hotspot pressure and current scaling/routing controls in chat platform with reconnect storm after AZ fault.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: chat platform with reconnect storm after AZ fault, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "This prompt is really about \"scenario review: chat platform with reconnect storm after AZ fault\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with lag-sensitive read paths?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: enforce regional traffic caps with dependency-aware failover sequencing."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action with lag-sensitive read paths, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "If you keep \"after confirming diagnosis, what is the strongest next action with lag-sensitive read\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"scaling Compute Scenarios\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticketing API under bot-driven burst traffic. Current incident signal is ordered-processing constraints limiting parallelism. What is the primary diagnosis?",
          "options": [
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between ordered-processing constraints limiting parallelism and current scaling/routing controls in ticketing API under bot-driven burst traffic."
          ],
          "correct": 3,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ticketing API under bot-driven burst traffic, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "If you keep \"scenario review: ticketing API under bot-driven burst traffic\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action during migration overlap?",
          "options": [
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: apply retry budgets with jitter and partition-scoped backpressure controls.",
            "Apply uniform global throttling and postpone root-cause mitigation."
          ],
          "correct": 2,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action during migration overlap, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis, what is the strongest next action during migration overlap\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"scaling Compute Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: fraud scoring service with uneven tenant load. Current incident signal is noisy-neighbor contention in shared nodes. What is the primary diagnosis?",
          "options": [
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between noisy-neighbor contention in shared nodes and current scaling/routing controls in fraud scoring service with uneven tenant load.",
            "The issue is purely random and not diagnosable from workload signals."
          ],
          "correct": 2,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: fraud scoring service with uneven tenant load, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The core signal here is \"scenario review: fraud scoring service with uneven tenant load\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action while constraining concurrency spikes?",
          "options": [
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: split ordered vs unordered workloads and isolate strict-order paths.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits."
          ],
          "correct": 1,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action while constraining concurrency spikes, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Use \"after confirming diagnosis, what is the strongest next action while constraining\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"scaling Compute Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ad bidding pipeline with strict p99 budget. Current incident signal is control-plane change blast radius across regions. What is the primary diagnosis?",
          "options": [
            "Regional and partition-level metrics are unnecessary for triage.",
            "The dominant failure mode is a mismatch between control-plane change blast radius across regions and current scaling/routing controls in ad bidding pipeline with strict p99 budget.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type."
          ],
          "correct": 1,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: ad bidding pipeline with strict p99 budget, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "The key clue in this question is \"scenario review: ad bidding pipeline with strict p99 budget\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action under partial regional outage?",
          "options": [
            "Execute this plan first: move sensitive workloads to dedicated pools with stronger placement/limits.",
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention."
          ],
          "correct": 0,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action under partial regional outage, this is the strongest fit in Scaling Compute Scenarios. Keep mitigation tied to the stated constraints.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis, what is the strongest next action under partial regional\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"scaling Compute Scenarios\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: video metadata API during cross-region failover. Current incident signal is insufficient standby capacity for evacuation. What is the primary diagnosis?",
          "options": [
            "The dominant failure mode is a mismatch between insufficient standby capacity for evacuation and current scaling/routing controls in video metadata API during cross-region failover.",
            "The issue is purely random and not diagnosable from workload signals.",
            "Only vertical scaling can solve integrated incidents of this type.",
            "Regional and partition-level metrics are unnecessary for triage."
          ],
          "correct": 0,
          "explanation": "These symptoms indicate control mismatch with bottleneck physics, not a generic scale-all response. For Scenario review: video metadata API during cross-region failover, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "The key clue in this question is \"scenario review: video metadata API during cross-region failover\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next action with incident-to-steady-state transition?",
          "options": [
            "Apply uniform global throttling and postpone root-cause mitigation.",
            "Max out capacity caps immediately and remove safety limits.",
            "Defer changes until next quarter and rely on manual operator intervention.",
            "Execute this plan first: roll out control-plane changes by region canary with auto-rollback thresholds."
          ],
          "correct": 3,
          "explanation": "Start with the smallest high-leverage correction that directly addresses the observed bottleneck and guardrail risk. For After confirming diagnosis, what is the strongest next action with incident-to-steady-state transition, this is the strongest fit in Scaling Compute Scenarios.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis, what is the strongest next action with\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"scaling Compute Scenarios\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-061",
      "type": "multi-select",
      "question": "In end-to-end scaling incidents, which diagnostics are highest value first? (Select all that apply)",
      "options": [
        "Queue age and tail latency by partition/region",
        "Dependency saturation and error skew",
        "Only global mean CPU",
        "Traffic-shift/failover event timeline"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Integrated diagnosis needs granular signals and timeline correlation, not only global means.",
      "detailedExplanation": "The core signal here is \"in end-to-end scaling incidents, which diagnostics are highest value first? (Select all\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-062",
      "type": "multi-select",
      "question": "Which controls reduce blast radius during emergency traffic shifts? (Select all that apply)",
      "options": [
        "Regional traffic caps",
        "Dependency-aware ramp sequencing",
        "Immediate full cutover by default",
        "Rollback thresholds with ownership"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Guarded shift and rollback controls limit cascade risk.",
      "detailedExplanation": "Use \"controls reduce blast radius during emergency traffic shifts? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-063",
      "type": "multi-select",
      "question": "For hotspot + autoscaling interactions, which practices help? (Select all that apply)",
      "options": [
        "Fix key skew before broad scale-out",
        "Use shard-aware signals",
        "Rely on fleet averages only",
        "Apply fair scheduling/quotas"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Skew-aware controls prevent ineffective scale-out and unfair starvation.",
      "detailedExplanation": "This prompt is really about \"for hotspot + autoscaling interactions, which practices help? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-064",
      "type": "multi-select",
      "question": "Compute selection in scenario triage should consider which jointly? (Select all that apply)",
      "options": [
        "Runtime limits vs job profile",
        "Cold-start sensitivity",
        "Weekly meeting availability only",
        "Isolation and operability requirements"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Platform fit is multidimensional: runtime shape, latency sensitivity, and operational constraints.",
      "detailedExplanation": "The decision turns on \"compute selection in scenario triage should consider which jointly? (Select all that\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-065",
      "type": "multi-select",
      "question": "Which signs indicate scale-in policy is unsafe during incidents? (Select all that apply)",
      "options": [
        "In-flight work loss during downscale",
        "Replica oscillation after short dips",
        "Stable queue age and no retries",
        "Frequent rollback to prior capacity"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Unsafe scale-in shows churn, dropped work, and unstable rollback behavior.",
      "detailedExplanation": "Read this as a scenario about \"signs indicate scale-in policy is unsafe during incidents? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-066",
      "type": "multi-select",
      "question": "For multi-region + sharding scenarios, which safeguards are useful? (Select all that apply)",
      "options": [
        "Regional isolation boundaries",
        "Per-partition retry budgets",
        "Single global mutable control without scoping",
        "Failover drills with RTO/RPO measurement"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Scoping, retry control, and tested failover guard against cascading failures.",
      "detailedExplanation": "The key clue in this question is \"for multi-region + sharding scenarios, which safeguards are useful? (Select all that\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-067",
      "type": "multi-select",
      "question": "Which interventions are valid under cost cap pressure with strict SLOs? (Select all that apply)",
      "options": [
        "Admission control for low-priority load",
        "Graceful degradation mode",
        "Remove all safety caps and hope",
        "Protect critical-path capacity floors"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "When caps and SLOs conflict, enforce priority/degradation policies explicitly.",
      "detailedExplanation": "Start from \"interventions are valid under cost cap pressure with strict SLOs? (Select all that\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-068",
      "type": "multi-select",
      "question": "What makes an integrated migration plan safer? (Select all that apply)",
      "options": [
        "Canary by traffic segment",
        "Baseline/post-change SLO+cost comparison",
        "Big-bang cutover only",
        "Clear fallback ownership/runbook"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Safe migration combines staged rollout, measurement, and owned fallback.",
      "detailedExplanation": "If you keep \"makes an integrated migration plan safer? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-069",
      "type": "multi-select",
      "question": "Which metrics should be watched together during incident stabilization? (Select all that apply)",
      "options": [
        "Tail latency",
        "Error budget burn rate",
        "Queue age/backlog",
        "Only deployment count"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Stabilization requires service quality, reliability, and work-congestion metrics together.",
      "detailedExplanation": "The core signal here is \"metrics should be watched together during incident stabilization? (Select all that\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-070",
      "type": "multi-select",
      "question": "Which anti-patterns commonly appear in compute-scaling scenarios? (Select all that apply)",
      "options": [
        "Treating retries as free capacity",
        "Ignoring partition/regional skew",
        "Canarying policy changes",
        "Applying one metric to all workloads"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Oversimplified controls and blind scaling are recurrent failure modes.",
      "detailedExplanation": "This prompt is really about \"anti-patterns commonly appear in compute-scaling scenarios? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-071",
      "type": "multi-select",
      "question": "For ordered-processing bottlenecks, which options are valid? (Select all that apply)",
      "options": [
        "Isolate strict-order path",
        "Relax order where domain allows",
        "Assume ordering has zero throughput cost",
        "Use idempotent handlers for safe retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Ordering guarantees constrain parallelism and should be scoped intentionally.",
      "detailedExplanation": "Use \"for ordered-processing bottlenecks, which options are valid? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-072",
      "type": "multi-select",
      "question": "What supports confident failback after incident containment? (Select all that apply)",
      "options": [
        "Gradual traffic restoration",
        "Consistency/integrity checks",
        "Immediate 100% restore always",
        "Rollback triggers if regression appears"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failback should be staged, validated, and reversible.",
      "detailedExplanation": "The core signal here is \"supports confident failback after incident containment? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-073",
      "type": "multi-select",
      "question": "In mixed compute estates (VM/container/serverless), which governance helps? (Select all that apply)",
      "options": [
        "Platform selection rubric per workload",
        "Shared reliability guardrails",
        "Unbounded platform sprawl",
        "Periodic cost/perf review by service tier"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Mixed estates are sustainable with explicit selection rules and shared guardrails.",
      "detailedExplanation": "If you keep \"in mixed compute estates (VM/container/serverless), which governance helps? (Select all\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-074",
      "type": "multi-select",
      "question": "Which signs indicate control-plane risk is under-managed? (Select all that apply)",
      "options": [
        "Global policy blasts without canary",
        "No audited rollback path",
        "Regional staged rollout discipline",
        "Frequent config drift surprises"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Control-plane safety requires scoped rollout, auditability, and rollback readiness.",
      "detailedExplanation": "Start from \"signs indicate control-plane risk is under-managed? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-075",
      "type": "multi-select",
      "question": "During reconnect storms, which controls matter most? (Select all that apply)",
      "options": [
        "Connection admission backpressure",
        "Jittered reconnect guidance",
        "Infinite immediate client retries",
        "Warm capacity buffer on connection gateways"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Reconnect storms need coordinated client and server-side damping.",
      "detailedExplanation": "The key clue in this question is \"during reconnect storms, which controls matter most? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-076",
      "type": "multi-select",
      "question": "Which are valid scenario-level success criteria after mitigation? (Select all that apply)",
      "options": [
        "SLO recovery sustained across peak window",
        "Reduced error-budget burn",
        "Higher mean CPU regardless user impact",
        "Cost returns near pre-incident bounds"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Success should be measured by user impact, reliability burn, and cost stability.",
      "detailedExplanation": "Read this as a scenario about \"valid scenario-level success criteria after mitigation? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-077",
      "type": "multi-select",
      "question": "Which preparations improve incident response on compute scaling paths? (Select all that apply)",
      "options": [
        "Predefined degradation modes",
        "Owner-mapped runbooks",
        "Ad hoc undocumented actions only",
        "Regular game-day simulations"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Preparedness increases speed and correctness of response under pressure.",
      "detailedExplanation": "The decision turns on \"preparations improve incident response on compute scaling paths? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-078",
      "type": "numeric-input",
      "question": "Backlog is 960,000 jobs. Net drain after mitigation is 24,000 jobs/min. Minutes to clear backlog?",
      "answer": 40,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "960,000 / 24,000 = 40 minutes.",
      "detailedExplanation": "This prompt is really about \"backlog is 960,000 jobs\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 960,000 and 24,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-079",
      "type": "numeric-input",
      "question": "Primary region loses 45% capacity. Remaining regions can absorb 18,000 rps and 12,000 rps. Maximum total failover traffic they can absorb?",
      "answer": 30000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "18,000 + 12,000 = 30,000 rps.",
      "detailedExplanation": "Use \"primary region loses 45% capacity\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 45 and 18,000 rps should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-080",
      "type": "numeric-input",
      "question": "A hotspot shard handles 22% of 80,000 rps. How much rps hits that shard?",
      "answer": 17600,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.22 * 80,000 = 17,600 rps.",
      "detailedExplanation": "Use \"hotspot shard handles 22% of 80,000 rps\" as your starting point, then verify tradeoffs carefully. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 22 and 80,000 rps appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-081",
      "type": "numeric-input",
      "question": "Queue age SLO is 90s. Current queue age is 225s. Percent over SLO?",
      "answer": 150,
      "unit": "%",
      "tolerance": 0.5,
      "explanation": "(225-90)/90 = 150% over target.",
      "detailedExplanation": "This prompt is really about \"queue age SLO is 90s\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 90s and 225s should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-082",
      "type": "numeric-input",
      "question": "Autoscaling adds 8 workers/min. Starting at 32 workers, how many after 6 minutes without scale-in?",
      "answer": 80,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "32 + 8*6 = 80 workers.",
      "detailedExplanation": "If you keep \"autoscaling adds 8 workers/min\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 8 and 32 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-083",
      "type": "numeric-input",
      "question": "A retry policy adds 0.25 extra requests per original request. At 48,000 original rps, effective rps?",
      "answer": 60000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "48,000 * 1.25 = 60,000 rps.",
      "detailedExplanation": "The core signal here is \"retry policy adds 0\". Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 0.25 and 48,000 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-084",
      "type": "numeric-input",
      "question": "Standby region warm pool is 140 instances. Promotion needs 196. Additional instances required?",
      "answer": 56,
      "unit": "instances",
      "tolerance": 0,
      "explanation": "196 - 140 = 56 instances.",
      "detailedExplanation": "The key clue in this question is \"standby region warm pool is 140 instances\". Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 140 and 196 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-085",
      "type": "numeric-input",
      "question": "p99 improved from 1800ms to 720ms after mitigation. Percent reduction?",
      "answer": 60,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1800-720)/1800 = 60% reduction.",
      "detailedExplanation": "Start from \"p99 improved from 1800ms to 720ms after mitigation\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 1800ms and 720ms appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-086",
      "type": "numeric-input",
      "question": "A tier runs at 70% target utilization with 35,000 rps demand. Required raw capacity?",
      "answer": 50000,
      "unit": "rps",
      "tolerance": 0.02,
      "explanation": "35,000 / 0.7 = 50,000 rps.",
      "detailedExplanation": "The decision turns on \"tier runs at 70% target utilization with 35,000 rps demand\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 70 and 35,000 rps in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-087",
      "type": "numeric-input",
      "question": "Failover drill has 5 stages of 3 minutes each before full promotion. Total drill minutes?",
      "answer": 15,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "5 * 3 = 15 minutes.",
      "detailedExplanation": "Read this as a scenario about \"failover drill has 5 stages of 3 minutes each before full promotion\". Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 5 and 3 minutes in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-088",
      "type": "numeric-input",
      "question": "Per-partition cap is 4,500 rps. Hot partition receives 9,900 rps. Minimum equal split partitions needed?",
      "answer": 3,
      "unit": "partitions",
      "tolerance": 0,
      "explanation": "9,900 / 4,500 = 2.2, so at least 3 partitions.",
      "detailedExplanation": "Use \"per-partition cap is 4,500 rps\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 4,500 rps and 9,900 rps in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-089",
      "type": "numeric-input",
      "question": "Degradation mode sheds 12% of 75,000 rps low-priority traffic. How much rps is shed?",
      "answer": 9000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.12 * 75,000 = 9,000 rps.",
      "detailedExplanation": "This prompt is really about \"degradation mode sheds 12% of 75,000 rps low-priority traffic\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 12 and 75,000 rps should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-090",
      "type": "ordering",
      "question": "Order a robust integrated incident response loop.",
      "items": [
        "Scope bottleneck with granular signals",
        "Apply blast-radius containment controls",
        "Execute targeted mitigation",
        "Validate and codify guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Diagnose, contain, mitigate, and institutionalize.",
      "detailedExplanation": "Read this as a scenario about \"order a robust integrated incident response loop\". Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-091",
      "type": "ordering",
      "question": "Order by fastest to slowest mitigation impact (typical).",
      "items": [
        "Traffic/rate guardrails",
        "Autoscaling policy tuning",
        "Partition strategy adjustment",
        "Major platform migration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Immediate controls usually act faster than architectural shifts.",
      "detailedExplanation": "The decision turns on \"order by fastest to slowest mitigation impact (typical)\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-092",
      "type": "ordering",
      "question": "Order by increasing observability granularity.",
      "items": [
        "Global averages",
        "Per-region metrics",
        "Per-service and per-shard metrics",
        "Per-shard plus top-key and queue-age outliers"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Finer granularity improves hotspot diagnosis precision.",
      "detailedExplanation": "Start from \"order by increasing observability granularity\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-093",
      "type": "ordering",
      "question": "Order failover/failback safety from weakest to strongest.",
      "items": [
        "Untested manual steps",
        "Documented runbook only",
        "Runbook with periodic drills",
        "Runbook with drills and rollback triggers"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Tested and guarded workflows are safest.",
      "detailedExplanation": "The key clue in this question is \"order failover/failback safety from weakest to strongest\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-094",
      "type": "ordering",
      "question": "Order by increasing risk of retry amplification.",
      "items": [
        "Budgeted retries with jitter",
        "Bounded retries without jitter",
        "High retry limits during errors",
        "Immediate unbounded synchronized retries"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Loose, synchronized retry behavior amplifies incidents.",
      "detailedExplanation": "The core signal here is \"order by increasing risk of retry amplification\". Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-095",
      "type": "ordering",
      "question": "Order platform-change rollout safety.",
      "items": [
        "Canary subset rollout",
        "Incremental traffic ramps",
        "Large batch cutover",
        "Immediate full cutover"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Smaller progressive steps reduce change risk.",
      "detailedExplanation": "If you keep \"order platform-change rollout safety\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-096",
      "type": "ordering",
      "question": "Order by increasing ordering guarantee strength. (Scaling Compute Scenarios context)",
      "items": [
        "Unordered processing",
        "Partition-local ordering",
        "Tenant-local ordering",
        "Global total ordering"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger ordering generally constrains parallelism more.",
      "detailedExplanation": "This prompt is really about \"order by increasing ordering guarantee strength\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-097",
      "type": "ordering",
      "question": "Order by strongest evidence quality for mitigation success.",
      "items": [
        "Anecdotal operator impression",
        "Global mean metric improvement",
        "SLO + error-budget recovery across peak",
        "SLO/cost recovery plus recurrence guardrail validation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "High-confidence validation combines user impact, reliability, cost, and prevention signals.",
      "detailedExplanation": "Use \"order by strongest evidence quality for mitigation success\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-098",
      "type": "ordering",
      "question": "Order escalation path for capacity-vs-cost conflict.",
      "items": [
        "Enable low-priority shedding",
        "Protect critical-path floor",
        "Tune autoscaling and routing to cap pressure",
        "Revisit capacity budget and architecture constraints"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start with tactical controls, then adjust system and planning assumptions.",
      "detailedExplanation": "Read this as a scenario about \"order escalation path for capacity-vs-cost conflict\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-099",
      "type": "ordering",
      "question": "Order by increasing control-plane blast-radius safety.",
      "items": [
        "Global one-shot policy update",
        "Large regional batch update",
        "Regional canary update",
        "Regional canary with automated rollback"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer control-plane changes scope impact and add rollback automation.",
      "detailedExplanation": "The decision turns on \"order by increasing control-plane blast-radius safety\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "sc-scn-100",
      "type": "ordering",
      "question": "Order scenario-mitigation lifecycle.",
      "items": [
        "Detect pattern recurrence risk",
        "Define hypothesis + guardrails",
        "Implement and monitor in canary",
        "Promote and archive lessons"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Explicit hypotheses and guardrails improve repeatability and learning.",
      "detailedExplanation": "This prompt is really about \"order scenario-mitigation lifecycle\". Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "scaling-compute-scenarios"],
      "difficulty": "principal"
    }
  ]
}
