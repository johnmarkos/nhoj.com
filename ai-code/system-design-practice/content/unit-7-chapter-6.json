{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 6,
  "chapterTitle": "Multi-Region Compute Strategy",
  "chapterDescription": "Designing active-active and active-passive regional compute topologies with explicit failover, isolation, and control-plane safety trade-offs.",
  "problems": [
    {
      "id": "sc-mr-001",
      "type": "multiple-choice",
      "question": "A global checkout API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Error spikes were concentrated in APAC traffic.",
      "options": [
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Given the observed bottleneck and guardrails, multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A global checkout API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-002",
      "type": "multiple-choice",
      "question": "A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Customer impact increased during peak checkout window.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook."
      ],
      "correct": 3,
      "explanation": "From an incident-first perspective, multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-003",
      "type": "multiple-choice",
      "question": "A identity token service experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Previous runbook steps failed to reduce error rate.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Under the stated reliability and cost constraints, multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A identity token service experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-004",
      "type": "multiple-choice",
      "question": "A payments authorization tier experienced regional network partition and user-facing errors rose. Which next step is strongest? Secondary regions showed uneven saturation after reroute.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Prioritizing blast-radius reduction first, multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A payments authorization tier experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-005",
      "type": "multiple-choice",
      "question": "A search query frontend experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Replication lag alarms triggered before failover execution.",
      "options": [
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "With latency and correctness objectives explicit, multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A search query frontend experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Generalize from search query frontend experienced latency regression in one geography and user-facing to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-006",
      "type": "multiple-choice",
      "question": "A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose. Which next step is strongest? Only one region exhausted capacity during evacuation.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: pre-provision warm standby capacity in secondary regions."
      ],
      "correct": 3,
      "explanation": "Looking at rollback safety and operational load, multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-007",
      "type": "multiple-choice",
      "question": "A catalog read service experienced DNS failover delay and user-facing errors rose. Which next step is strongest? Global p95 remained stable while p99 degraded sharply.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: route by latency with strict regional isolation guardrails.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A catalog read service experienced DNS failover delay and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-008",
      "type": "multiple-choice",
      "question": "A order tracking API experienced state divergence across regions and user-facing errors rose. Which next step is strongest? Dependency quotas blocked full traffic promotion.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: define RPO/RTO-driven replication and failover thresholds.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A order tracking API experienced state divergence across regions and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-009",
      "type": "multiple-choice",
      "question": "A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose. Which next step is strongest? Control-plane rollback took longer than expected.",
      "options": [
        "Prioritize: separate global and regional control planes for resilience.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-010",
      "type": "multiple-choice",
      "question": "A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose. Which next step is strongest? Failback attempt caused another brief outage.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: run regular game-day failover validation with rollback criteria."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-011",
      "type": "multiple-choice",
      "question": "A video metadata API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Synthetic probes detected health drift before user reports.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A video metadata API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-012",
      "type": "multiple-choice",
      "question": "A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Regional metrics diverged from global dashboards.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-013",
      "type": "multiple-choice",
      "question": "A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Incident timeline showed delayed routing convergence.",
      "options": [
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-014",
      "type": "multiple-choice",
      "question": "A ticket booking API experienced regional network partition and user-facing errors rose. Which next step is strongest? Operational handoff gaps slowed mitigation decisions.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ticket booking API experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-015",
      "type": "multiple-choice",
      "question": "A feature flag delivery service experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Warm standby utilization exceeded planned thresholds.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A feature flag delivery service experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-016",
      "type": "multiple-choice",
      "question": "A webhook ingestion endpoint experienced zonal capacity shortfall and user-facing errors rose. Which next step is strongest? The same region repeatedly became the emergency sink.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: pre-provision warm standby capacity in secondary regions.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A webhook ingestion endpoint experienced zonal capacity shortfall and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize from webhook ingestion endpoint experienced zonal capacity shortfall and user-facing errors to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-017",
      "type": "multiple-choice",
      "question": "A inventory availability API experienced DNS failover delay and user-facing errors rose. Which next step is strongest? Latency gains conflicted with replication durability goals.",
      "options": [
        "Prioritize: route by latency with strict regional isolation guardrails.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A inventory availability API experienced DNS failover delay and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-018",
      "type": "multiple-choice",
      "question": "A pricing rules service experienced state divergence across regions and user-facing errors rose. Which next step is strongest? Traffic policy was stale after recent region expansion.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: define RPO/RTO-driven replication and failover thresholds."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A pricing rules service experienced state divergence across regions and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-019",
      "type": "multiple-choice",
      "question": "A document collaboration backend experienced traffic shift after undersea cable issue and user-facing errors rose. Which next step is strongest? One dependency failed open and amplified errors.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: separate global and regional control planes for resilience.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A document collaboration backend experienced traffic shift after undersea cable issue and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-020",
      "type": "multiple-choice",
      "question": "A support chat API experienced regional deploy rollback mismatch and user-facing errors rose. Which next step is strongest? Write-path safeguards were weaker than read-path safeguards.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: run regular game-day failover validation with rollback criteria.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A support chat API experienced regional deploy rollback mismatch and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-021",
      "type": "multiple-choice",
      "question": "A global checkout API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Regional alerts were noisy and masked true degradation.",
      "options": [
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A global checkout API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-022",
      "type": "multiple-choice",
      "question": "A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Client retry behavior amplified target-region load.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-023",
      "type": "multiple-choice",
      "question": "A identity token service experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Failover worked for reads but not for writes.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A identity token service experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-024",
      "type": "multiple-choice",
      "question": "A payments authorization tier experienced regional network partition and user-facing errors rose. Which next step is strongest? Promotion succeeded but observability confidence was low.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A payments authorization tier experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-025",
      "type": "multiple-choice",
      "question": "A search query frontend experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Regional health probes missed partial dependency failures.",
      "options": [
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A search query frontend experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-026",
      "type": "multiple-choice",
      "question": "A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose. Which next step is strongest? Cross-region links recovered slower than expected.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: pre-provision warm standby capacity in secondary regions."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-027",
      "type": "multiple-choice",
      "question": "A catalog read service experienced DNS failover delay and user-facing errors rose. Which next step is strongest? High-priority tenants were disproportionately impacted.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: route by latency with strict regional isolation guardrails.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A catalog read service experienced DNS failover delay and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Generalize from catalog read service experienced DNS failover delay and user-facing errors rose to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-028",
      "type": "multiple-choice",
      "question": "A order tracking API experienced state divergence across regions and user-facing errors rose. Which next step is strongest? Capacity models underestimated failover burst behavior.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: define RPO/RTO-driven replication and failover thresholds.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A order tracking API experienced state divergence across regions and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-029",
      "type": "multiple-choice",
      "question": "A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose. Which next step is strongest? A recent config rollout changed routing precedence.",
      "options": [
        "Prioritize: separate global and regional control planes for resilience.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-030",
      "type": "multiple-choice",
      "question": "A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose. Which next step is strongest? Postmortem highlighted weak blast-radius boundaries.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: run regular game-day failover validation with rollback criteria."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Generalize from gaming session coordinator experienced regional deploy rollback mismatch and to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-031",
      "type": "multiple-choice",
      "question": "A video metadata API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Traffic steering ignored one critical dependency signal.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A video metadata API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-032",
      "type": "multiple-choice",
      "question": "A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Runbook ambiguity caused conflicting operator actions.",
      "options": [
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-033",
      "type": "multiple-choice",
      "question": "A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Routing policy handled 5xx but not latency degradation.",
      "options": [
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-034",
      "type": "multiple-choice",
      "question": "A ticket booking API experienced regional network partition and user-facing errors rose. Which next step is strongest? The standby region had not been load-tested recently.",
      "options": [
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management.",
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ticket booking API experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-035",
      "type": "multiple-choice",
      "question": "A feature flag delivery service experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Canary policy behaved differently from full-fleet policy.",
      "options": [
        "Relax regional health-check sensitivity to reduce failover flapping during transient errors.",
        "Tune client retries and backoff while keeping the current failover topology unchanged.",
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Shift a larger default traffic share to the highest-capacity region to simplify failover management."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A feature flag delivery service experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a search query frontend, the team observes regional network partition. What is the most likely primary diagnosis?",
          "options": [
            "regional network partition exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "search query frontend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Generalize from during review of a search query frontend, the team observes regional network partition to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "After confirming diagnosis in search query frontend, which next change is strongest under strict RTO targets?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for search query frontend: Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a notification dispatch API, the team observes latency regression in one geography. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "latency regression in one geography exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "notification dispatch API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in notification dispatch API, which next change is strongest while limiting cross-region write conflicts?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for notification dispatch API: Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a catalog read service, the team observes zonal capacity shortfall. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "zonal capacity shortfall exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "catalog read service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in catalog read service, which next change is strongest with minimal blast-radius expansion?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for catalog read service: Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize from multi-Region Compute Strategy to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a order tracking API, the team observes DNS failover delay. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "DNS failover delay exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "order tracking API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis in order tracking API, which next change is strongest during sustained demand growth?",
          "options": [
            "Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for order tracking API: Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ad bidding edge service, the team observes state divergence across regions. What is the most likely primary diagnosis?",
          "options": [
            "state divergence across regions exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "ad bidding edge service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in ad bidding edge service, which next change is strongest without overloading downstream dependencies?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ad bidding edge service: Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a gaming session coordinator, the team observes traffic shift after undersea cable issue. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "traffic shift after undersea cable issue exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "gaming session coordinator shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in gaming session coordinator, which next change is strongest while preserving latency SLOs?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for gaming session coordinator: Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from multi-Region Compute Strategy to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a video metadata API, the team observes regional deploy rollback mismatch. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "regional deploy rollback mismatch exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "video metadata API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in video metadata API, which next change is strongest during partial control-plane degradation?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for video metadata API: Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Generalize from after confirming diagnosis in video metadata API, which next change is strongest during to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a fraud scoring gateway, the team observes single-region dependency outage. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "single-region dependency outage exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "fraud scoring gateway shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in fraud scoring gateway, which next change is strongest with replication lag constraints?",
          "options": [
            "Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for fraud scoring gateway: Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ride dispatch backend, the team observes cross-region replication lag spike. What is the most likely primary diagnosis?",
          "options": [
            "cross-region replication lag spike exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "ride dispatch backend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in ride dispatch backend, which next change is strongest before the next failback window?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ride dispatch backend: Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ticket booking API, the team observes control-plane misconfiguration. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "control-plane misconfiguration exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "ticket booking API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in ticket booking API, which next change is strongest under quota-limited secondary regions?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ticket booking API: Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a feature flag delivery service, the team observes regional network partition. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "regional network partition exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "feature flag delivery service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "After confirming diagnosis in feature flag delivery service, which next change is strongest with strong rollback guarantees?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for feature flag delivery service: Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a webhook ingestion endpoint, the team observes latency regression in one geography. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "latency regression in one geography exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "webhook ingestion endpoint shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Generalize from during review of a webhook ingestion endpoint, the team observes latency regression in to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes."
        },
        {
          "question": "After confirming diagnosis in webhook ingestion endpoint, which next change is strongest while reducing operator toil?",
          "options": [
            "Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for webhook ingestion endpoint: Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a inventory availability API, the team observes zonal capacity shortfall. What is the most likely primary diagnosis?",
          "options": [
            "zonal capacity shortfall exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "inventory availability API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis in inventory availability API, which next change is strongest during high-traffic business hours?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for inventory availability API: Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a pricing rules service, the team observes DNS failover delay. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "DNS failover delay exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "pricing rules service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis in pricing rules service, which next change is strongest with region-specific policy drift risk?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for pricing rules service: Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from multi-Region Compute Strategy to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a document collaboration backend, the team observes state divergence across regions. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "state divergence across regions exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "document collaboration backend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in document collaboration backend, which next change is strongest while preserving write-path correctness?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for document collaboration backend: Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a support chat API, the team observes traffic shift after undersea cable issue. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "traffic shift after undersea cable issue exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "support chat API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming diagnosis in support chat API, which next change is strongest during multi-AZ degradation?",
          "options": [
            "Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for support chat API: Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Generalize from after confirming diagnosis in support chat API, which next change is strongest during to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a global checkout API, the team observes regional deploy rollback mismatch. What is the most likely primary diagnosis?",
          "options": [
            "regional deploy rollback mismatch exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "global checkout API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in global checkout API, which next change is strongest with limited manual intervention?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for global checkout API: Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize from multi-Region Compute Strategy to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a real-time chat gateway, the team observes single-region dependency outage. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "single-region dependency outage exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "real-time chat gateway shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in real-time chat gateway, which next change is strongest under uncertain health telemetry?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for real-time chat gateway: Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a identity token service, the team observes cross-region replication lag spike. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "cross-region replication lag spike exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "identity token service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Generalize from during review of a identity token service, the team observes cross-region replication to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "After confirming diagnosis in identity token service, which next change is strongest while avoiding retry amplification?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for identity token service: Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a payments authorization tier, the team observes control-plane misconfiguration. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "control-plane misconfiguration exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "payments authorization tier shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in payments authorization tier, which next change is strongest with heterogeneous region capacity?",
          "options": [
            "Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for payments authorization tier: Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a search query frontend, the team observes regional network partition. What is the most likely primary diagnosis?",
          "options": [
            "regional network partition exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "search query frontend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes."
        },
        {
          "question": "After confirming diagnosis in search query frontend, which next change is strongest during staged evacuation?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for search query frontend: Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a notification dispatch API, the team observes latency regression in one geography. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "latency regression in one geography exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "notification dispatch API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming diagnosis in notification dispatch API, which next change is strongest while protecting premium-tenant SLOs?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for notification dispatch API: Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a catalog read service, the team observes zonal capacity shortfall. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "zonal capacity shortfall exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "catalog read service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in catalog read service, which next change is strongest under strict error-budget guardrails?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for catalog read service: Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a order tracking API, the team observes DNS failover delay. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "DNS failover delay exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "order tracking API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis in order tracking API, which next change is strongest with control-plane safety constraints?",
          "options": [
            "Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for order tracking API: Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Generalize from after confirming diagnosis in order tracking API, which next change is strongest with to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ad bidding edge service, the team observes state divergence across regions. What is the most likely primary diagnosis?",
          "options": [
            "state divergence across regions exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "ad bidding edge service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "After confirming diagnosis in ad bidding edge service, which next change is strongest before full traffic restoration?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ad bidding edge service: Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Generalize from after confirming diagnosis in ad bidding edge service, which next change is strongest to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-061",
      "type": "multi-select",
      "question": "Which factors should drive active-active vs active-passive selection? (Select all that apply)",
      "options": [
        "RPO/RTO requirements",
        "Write conflict tolerance",
        "Office timezone overlap only",
        "Operational failover complexity budget"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Recovery objectives, conflict handling, and ops complexity are primary decision factors.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-062",
      "type": "multi-select",
      "question": "Strong regional isolation controls include which? (Select all that apply)",
      "options": [
        "Per-region dependency boundaries",
        "Independent capacity headroom",
        "Single global failure domain for all writes",
        "Regional circuit breakers"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Isolation reduces blast radius and preserves partial service during regional incidents.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-063",
      "type": "multi-select",
      "question": "Useful failover readiness evidence includes which? (Select all that apply)",
      "options": [
        "Documented runbooks with owners",
        "Regular game-day exercises",
        "Never running drills to avoid risk",
        "Measured promotion time vs SLO"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Readiness is proven by repeated tests and measured outcomes.",
      "detailedExplanation": "Generalize from useful failover readiness evidence includes which? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-064",
      "type": "multi-select",
      "question": "Geo-routing policy inputs should usually include which? (Select all that apply)",
      "options": [
        "Latency",
        "Regional health state",
        "Random region selection only",
        "Capacity saturation signals"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Routing should combine performance and health/capacity safety signals.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-065",
      "type": "multi-select",
      "question": "Cross-region replication trade-offs often include which? (Select all that apply)",
      "options": [
        "Lag impacts stale-read risk",
        "Synchronous replication increases write latency",
        "Replication mode has no effect on durability",
        "Asynchronous replication can improve availability but raises RPO risk"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Replication choices directly affect latency, durability, and data-loss windows.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-066",
      "type": "multi-select",
      "question": "Control-plane resilience practices include which? (Select all that apply)",
      "options": [
        "Scoped rollout and canary changes",
        "Regional config blast-radius limits",
        "One-shot global config updates with no rollback",
        "Versioned policies with audit logs"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Treat control plane like critical software with staged rollout and rollback.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-067",
      "type": "multi-select",
      "question": "When failing over writes to a secondary region, what risks must be managed? (Select all that apply)",
      "options": [
        "Duplicate side effects from replay/retry",
        "Schema/version mismatch across regions",
        "Automatic elimination of consistency concerns",
        "Capacity saturation in target region"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failover writes require idempotency, compatibility, and capacity planning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-068",
      "type": "multi-select",
      "question": "Which signs indicate DNS-only failover may be insufficient? (Select all that apply)",
      "options": [
        "Client-side DNS caching delays traffic shift",
        "Need for request-level health decisions",
        "Immediate zero-latency cutover guaranteed",
        "Fine-grained route steering requirements"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "DNS is coarse-grained and can be too slow for tight RTO goals.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-069",
      "type": "multi-select",
      "question": "For active-active reads, which safeguards are useful? (Select all that apply)",
      "options": [
        "Staleness-aware read policies",
        "Monotonic read/session guarantees where needed",
        "Ignoring replication lag metrics",
        "Region preference fallback logic"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Read policies should explicitly account for lag and consistency expectations.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-070",
      "type": "multi-select",
      "question": "Which are valid reasons to keep a warm standby region? (Select all that apply)",
      "options": [
        "Faster failover for strict RTO targets",
        "Avoid full cold-start provisioning during incident",
        "Guarantee zero operational cost",
        "Limit capacity shock on surviving regions"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Warm standby trades cost for faster and safer incident response.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-071",
      "type": "multi-select",
      "question": "Which metrics help detect regional imbalance before outages? (Select all that apply)",
      "options": [
        "Per-region utilization and queue age",
        "Cross-region error rate skew",
        "Only global daily average QPS",
        "Failover trigger frequency"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Region-level metrics reveal asymmetry that fleet averages hide.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-072",
      "type": "multi-select",
      "question": "During region failback (returning traffic), which practices are strong? (Select all that apply)",
      "options": [
        "Gradual traffic ramp with guardrails",
        "Data/state consistency checks before full cutback",
        "Immediate 100% restoration regardless metrics",
        "Rollback thresholds on latency/errors"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failback should be staged and observable like failover.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-073",
      "type": "multi-select",
      "question": "Which design choices reduce blast radius from regional incidents? (Select all that apply)",
      "options": [
        "Regional dependency isolation",
        "Per-region rate limiting",
        "Single shared mutable global state with no partitioning",
        "Traffic caps during emergency reroute"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Isolation and caps prevent one regions failure from cascading globally.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-074",
      "type": "multi-select",
      "question": "For multi-region control planes, which risks should be anticipated? (Select all that apply)",
      "options": [
        "Config drift between regions",
        "Propagation delay for policy changes",
        "No need for audit/versioning in emergencies",
        "Conflicting operator actions under pressure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Operational consistency and governance remain critical during incidents.",
      "detailedExplanation": "Generalize from for multi-region control planes, which risks should be anticipated? (Select all that to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-075",
      "type": "multi-select",
      "question": "When to prefer active-passive over active-active? (Select all that apply)",
      "options": [
        "Write conflict cost is very high",
        "Strictly simpler operational model is required",
        "Need lowest possible steady-state latency from every region always",
        "RPO/RTO can be met with standby promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Active-passive is often chosen for simpler write semantics and operational control.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-076",
      "type": "multi-select",
      "question": "Which preparations improve successful regional evacuation during incidents? (Select all that apply)",
      "options": [
        "Precomputed traffic-shift playbooks",
        "Capacity rehearsal in secondary regions",
        "Assuming autoscaling alone handles any surge",
        "Dependency quota agreements ahead of incidents"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Evacuation succeeds when capacity and dependencies are prepared before failure.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-077",
      "type": "multi-select",
      "question": "Which anti-patterns commonly break multi-region reliability? (Select all that apply)",
      "options": [
        "Untested failover paths",
        "Control-plane global blasts",
        "Periodic game-day drills",
        "Inconsistent region-specific runbooks"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Reliability fails when failover and control-plane paths are untested or unsafe.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-078",
      "type": "numeric-input",
      "question": "Primary region handles 36,000 rps. During failover, two secondary regions split traffic 60/40. How much rps goes to the larger secondary region?",
      "answer": 21600,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.60 * 36,000 = 21,600 rps.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 36,000 rps and 60 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-079",
      "type": "numeric-input",
      "question": "A standby region can process 14,000 rps, and planned failover load is 11,200 rps. What percent headroom remains?",
      "answer": 20,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "(14,000 - 11,200) / 14,000 = 20%.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 14,000 rps and 11,200 rps appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-080",
      "type": "numeric-input",
      "question": "Cross-region replication lag is 9 seconds and write rate is 4,500 writes/sec. Approximately how many writes are potentially not replicated at failover?",
      "answer": 40500,
      "unit": "writes",
      "tolerance": 0.03,
      "explanation": "4,500 * 9 = 40,500 writes in lag window.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 9 seconds and 4,500 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-081",
      "type": "numeric-input",
      "question": "RTO target is 5 minutes. Measured failover sequence currently takes 8 minutes. By what percentage must failover time be reduced?",
      "answer": 37.5,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(8 - 5) / 8 = 37.5% reduction needed.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 5 minutes and 8 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-082",
      "type": "numeric-input",
      "question": "Global traffic is 72,000 rps across 3 active regions equally. One region fails and remaining two absorb evenly. New rps per surviving region?",
      "answer": 36000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "72,000 / 2 = 36,000 rps per remaining region.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 72,000 rps and 3 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-083",
      "type": "numeric-input",
      "question": "A failover drill shifts 25% traffic every 2 minutes. Starting at 0%, how many minutes to reach 100%?",
      "answer": 8,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Four increments of 25%; 4 * 2 = 8 minutes.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 25 and 2 minutes in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-084",
      "type": "numeric-input",
      "question": "Secondary region cold start requires 180 seconds for 120 instances to become ready. Total instance-seconds of warmup?",
      "answer": 21600,
      "unit": "instance-seconds",
      "tolerance": 0,
      "explanation": "120 * 180 = 21,600 instance-seconds.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 180 seconds and 120 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-085",
      "type": "numeric-input",
      "question": "A geo-routing policy sends 68% traffic to nearest region, 22% to second-best, 10% to overflow. At 50,000 rps, what is overflow rps?",
      "answer": 5000,
      "unit": "rps",
      "tolerance": 0.02,
      "explanation": "0.10 * 50,000 = 5,000 rps.",
      "detailedExplanation": "Generalize from geo-routing policy sends 68% traffic to nearest region, 22% to second-best, 10% to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 68 and 22 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-086",
      "type": "numeric-input",
      "question": "Regional error budget allows 0.05% failures over 12,000,000 requests/day. Maximum failed requests/day?",
      "answer": 6000,
      "unit": "requests",
      "tolerance": 0,
      "explanation": "0.0005 * 12,000,000 = 6,000.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Normalize units before computing so conversion mistakes do not propagate. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 0.05 and 12,000 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-087",
      "type": "numeric-input",
      "question": "Write latency is 35ms with async replication and 62ms with sync quorum replication. What percent increase is sync over async?",
      "answer": 77.14,
      "unit": "%",
      "tolerance": 0.5,
      "explanation": "(62 - 35) / 35 = 77.14% increase.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 35ms and 62ms should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-088",
      "type": "numeric-input",
      "question": "During partial outage, traffic cap limits rerouted load to 18,000 rps. Demand needing reroute is 24,000 rps. How much rps must be shed/degraded?",
      "answer": 6000,
      "unit": "rps",
      "tolerance": 0,
      "explanation": "24,000 - 18,000 = 6,000 rps.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep every transformation in one unit system and check order of magnitude at the end. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Keep quantities like 18,000 rps and 24,000 rps in aligned units before deciding on an implementation approach. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-089",
      "type": "numeric-input",
      "question": "A failback step decreases secondary traffic from 100% to 20% over 4 equal steps. How many percentage points per step?",
      "answer": 20,
      "unit": "percentage points",
      "tolerance": 0,
      "explanation": "(100 - 20) / 4 = 20 points per step.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 100 and 20 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-090",
      "type": "ordering",
      "question": "Order a robust regional failover workflow.",
      "items": [
        "Detect and scope regional degradation",
        "Apply guarded traffic shift",
        "Validate capacity/dependency health in target regions",
        "Stabilize and document post-incident learnings"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope first, shift safely, validate capacity, then institutionalize lessons.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-091",
      "type": "ordering",
      "question": "Order routing granularity from coarsest to finest.",
      "items": [
        "DNS region failover",
        "Anycast/BGP steering",
        "L7 geo-routing rules",
        "Request-level health and policy routing"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Controls become finer and more dynamic from DNS to request-level policy.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-092",
      "type": "ordering",
      "question": "Order replication modes by increasing write-latency impact (typical case).",
      "items": [
        "Async cross-region replication",
        "Semi-sync replication",
        "Sync quorum replication",
        "Global synchronous commit across distant regions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger synchronous guarantees generally increase write latency.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-093",
      "type": "ordering",
      "question": "Order preparedness maturity for multi-region incidents.",
      "items": [
        "No runbook",
        "Runbook without drills",
        "Runbook with periodic drills",
        "Runbook with drills and measured SLO compliance"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Preparedness matures through testing and measurable outcomes.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-094",
      "type": "ordering",
      "question": "Order by increasing regional blast-radius risk.",
      "items": [
        "Region-isolated dependencies and caps",
        "Shared dependencies with regional limits",
        "Shared global dependencies without strict isolation",
        "Single control-plane action affecting all regions instantly"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Shared global dependencies and broad control-plane blast increase risk.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-095",
      "type": "ordering",
      "question": "Order failback steps from safest to riskiest.",
      "items": [
        "Incremental traffic restore with rollback checks",
        "Batch restore with close monitoring",
        "Rapid restore in one large step",
        "Immediate full restore without checks"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failback should be gradual and observable to avoid secondary incidents.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-096",
      "type": "ordering",
      "question": "Order by strongest evidence of regional readiness.",
      "items": [
        "Architecture diagram only",
        "Tabletop discussion",
        "Controlled failover drill",
        "Regular drills plus measured RTO/RPO compliance"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Readiness confidence grows from static documentation to repeated measured drills.",
      "detailedExplanation": "Generalize from order by strongest evidence of regional readiness to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-097",
      "type": "ordering",
      "question": "Order incident response sequence for replication lag surge.",
      "items": [
        "Quantify lag and staleness impact",
        "Apply read/write policy guardrails",
        "Adjust routing/failover thresholds",
        "Run post-incident consistency audit"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Measure first, contain risk, tune controls, then audit consistency.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-098",
      "type": "ordering",
      "question": "Order by increasing complexity of multi-region write strategy.",
      "items": [
        "Single write region",
        "Active-passive write promotion",
        "Partitioned writes by home region",
        "Multi-master conflict resolution"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Write concurrency and conflict handling complexity grows across these models.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-099",
      "type": "ordering",
      "question": "Order control-plane change safety approach.",
      "items": [
        "Global immediate rollout",
        "Large batch rollout",
        "Regional canary rollout",
        "Regional canary with automatic rollback thresholds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer control-plane changes narrow blast radius and add rollback automation.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-100",
      "type": "ordering",
      "question": "Order regional traffic management sophistication.",
      "items": [
        "Static fixed region weights",
        "Health-based region exclusion",
        "Latency + health adaptive routing",
        "Latency + health + capacity + policy-aware routing"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Policy sophistication rises as more real-time constraints are incorporated.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    }
  ]
}
