{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 6,
  "chapterTitle": "Multi-Region Compute Strategy",
  "chapterDescription": "Designing active-active and active-passive regional compute topologies with explicit failover, isolation, and control-plane safety trade-offs.",
  "problems": [
    {
      "id": "sc-mr-001",
      "type": "multiple-choice",
      "question": "A global checkout API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Error spikes were concentrated in APAC traffic.",
      "options": [
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A global checkout API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The key clue in this question is \"global checkout API experienced single-region dependency outage and user-facing errors\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-002",
      "type": "multiple-choice",
      "question": "A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Customer impact increased during peak checkout window.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"real-time chat gateway experienced cross-region replication lag spike and user-facing\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-003",
      "type": "multiple-choice",
      "question": "A identity token service experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Previous runbook steps failed to reduce error rate.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A identity token service experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The decision turns on \"identity token service experienced control-plane misconfiguration and user-facing\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-004",
      "type": "multiple-choice",
      "question": "A payments authorization tier experienced regional network partition and user-facing errors rose. Which next step is strongest? Secondary regions showed uneven saturation after reroute.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A payments authorization tier experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "This prompt is really about \"payments authorization tier experienced regional network partition and user-facing\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-005",
      "type": "multiple-choice",
      "question": "A search query frontend experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Replication lag alarms triggered before failover execution.",
      "options": [
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A search query frontend experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Use \"search query frontend experienced latency regression in one geography and user-facing\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-006",
      "type": "multiple-choice",
      "question": "A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose. Which next step is strongest? Only one region exhausted capacity during evacuation.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: pre-provision warm standby capacity in secondary regions."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The core signal here is \"notification dispatch API experienced zonal capacity shortfall and user-facing errors\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-007",
      "type": "multiple-choice",
      "question": "A catalog read service experienced DNS failover delay and user-facing errors rose. Which next step is strongest? Global p95 remained stable while p99 degraded sharply.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: route by latency with strict regional isolation guardrails.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A catalog read service experienced DNS failover delay and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "If you keep \"catalog read service experienced DNS failover delay and user-facing errors rose\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-008",
      "type": "multiple-choice",
      "question": "A order tracking API experienced state divergence across regions and user-facing errors rose. Which next step is strongest? Dependency quotas blocked full traffic promotion.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: define RPO/RTO-driven replication and failover thresholds.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A order tracking API experienced state divergence across regions and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"order tracking API experienced state divergence across regions and user-facing errors\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-009",
      "type": "multiple-choice",
      "question": "A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose. Which next step is strongest? Control-plane rollback took longer than expected.",
      "options": [
        "Prioritize: separate global and regional control planes for resilience.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The key clue in this question is \"ad bidding edge service experienced traffic shift after undersea cable issue and\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-010",
      "type": "multiple-choice",
      "question": "A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose. Which next step is strongest? Failback attempt caused another brief outage.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: run regular game-day failover validation with rollback criteria."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"gaming session coordinator experienced regional deploy rollback mismatch and\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-011",
      "type": "multiple-choice",
      "question": "A video metadata API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Synthetic probes detected health drift before user reports.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A video metadata API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Read this as a scenario about \"video metadata API experienced single-region dependency outage and user-facing errors\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-012",
      "type": "multiple-choice",
      "question": "A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Regional metrics diverged from global dashboards.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The key clue in this question is \"fraud scoring gateway experienced cross-region replication lag spike and user-facing\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-013",
      "type": "multiple-choice",
      "question": "A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Incident timeline showed delayed routing convergence.",
      "options": [
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Start from \"ride dispatch backend experienced control-plane misconfiguration and user-facing errors\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-014",
      "type": "multiple-choice",
      "question": "A ticket booking API experienced regional network partition and user-facing errors rose. Which next step is strongest? Operational handoff gaps slowed mitigation decisions.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ticket booking API experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "If you keep \"ticket booking API experienced regional network partition and user-facing errors rose\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-015",
      "type": "multiple-choice",
      "question": "A feature flag delivery service experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Warm standby utilization exceeded planned thresholds.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A feature flag delivery service experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The core signal here is \"feature flag delivery service experienced latency regression in one geography and\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-016",
      "type": "multiple-choice",
      "question": "A webhook ingestion endpoint experienced zonal capacity shortfall and user-facing errors rose. Which next step is strongest? The same region repeatedly became the emergency sink.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: pre-provision warm standby capacity in secondary regions.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A webhook ingestion endpoint experienced zonal capacity shortfall and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"webhook ingestion endpoint experienced zonal capacity shortfall and user-facing errors\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-017",
      "type": "multiple-choice",
      "question": "A inventory availability API experienced DNS failover delay and user-facing errors rose. Which next step is strongest? Latency gains conflicted with replication durability goals.",
      "options": [
        "Prioritize: route by latency with strict regional isolation guardrails.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A inventory availability API experienced DNS failover delay and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "This prompt is really about \"inventory availability API experienced DNS failover delay and user-facing errors rose\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-018",
      "type": "multiple-choice",
      "question": "A pricing rules service experienced state divergence across regions and user-facing errors rose. Which next step is strongest? Traffic policy was stale after recent region expansion.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: define RPO/RTO-driven replication and failover thresholds."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A pricing rules service experienced state divergence across regions and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"pricing rules service experienced state divergence across regions and user-facing\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-019",
      "type": "multiple-choice",
      "question": "A document collaboration backend experienced traffic shift after undersea cable issue and user-facing errors rose. Which next step is strongest? One dependency failed open and amplified errors.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: separate global and regional control planes for resilience.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A document collaboration backend experienced traffic shift after undersea cable issue and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Read this as a scenario about \"document collaboration backend experienced traffic shift after undersea cable issue and\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-020",
      "type": "multiple-choice",
      "question": "A support chat API experienced regional deploy rollback mismatch and user-facing errors rose. Which next step is strongest? Write-path safeguards were weaker than read-path safeguards.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: run regular game-day failover validation with rollback criteria.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A support chat API experienced regional deploy rollback mismatch and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"support chat API experienced regional deploy rollback mismatch and user-facing errors\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-021",
      "type": "multiple-choice",
      "question": "A global checkout API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Regional alerts were noisy and masked true degradation.",
      "options": [
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A global checkout API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The decision turns on \"global checkout API experienced single-region dependency outage and user-facing errors\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-022",
      "type": "multiple-choice",
      "question": "A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Client retry behavior amplified target-region load.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A real-time chat gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"real-time chat gateway experienced cross-region replication lag spike and user-facing\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-023",
      "type": "multiple-choice",
      "question": "A identity token service experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Failover worked for reads but not for writes.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A identity token service experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The key clue in this question is \"identity token service experienced control-plane misconfiguration and user-facing\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-024",
      "type": "multiple-choice",
      "question": "A payments authorization tier experienced regional network partition and user-facing errors rose. Which next step is strongest? Promotion succeeded but observability confidence was low.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A payments authorization tier experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The core signal here is \"payments authorization tier experienced regional network partition and user-facing\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-025",
      "type": "multiple-choice",
      "question": "A search query frontend experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Regional health probes missed partial dependency failures.",
      "options": [
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A search query frontend experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "If you keep \"search query frontend experienced latency regression in one geography and user-facing\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-026",
      "type": "multiple-choice",
      "question": "A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose. Which next step is strongest? Cross-region links recovered slower than expected.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: pre-provision warm standby capacity in secondary regions."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A notification dispatch API experienced zonal capacity shortfall and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "This prompt is really about \"notification dispatch API experienced zonal capacity shortfall and user-facing errors\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-027",
      "type": "multiple-choice",
      "question": "A catalog read service experienced DNS failover delay and user-facing errors rose. Which next step is strongest? High-priority tenants were disproportionately impacted.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: route by latency with strict regional isolation guardrails.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A catalog read service experienced DNS failover delay and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Use \"catalog read service experienced DNS failover delay and user-facing errors rose\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-028",
      "type": "multiple-choice",
      "question": "A order tracking API experienced state divergence across regions and user-facing errors rose. Which next step is strongest? Capacity models underestimated failover burst behavior.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: define RPO/RTO-driven replication and failover thresholds.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A order tracking API experienced state divergence across regions and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"order tracking API experienced state divergence across regions and user-facing errors\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-029",
      "type": "multiple-choice",
      "question": "A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose. Which next step is strongest? A recent config rollout changed routing precedence.",
      "options": [
        "Prioritize: separate global and regional control planes for resilience.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ad bidding edge service experienced traffic shift after undersea cable issue and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The decision turns on \"ad bidding edge service experienced traffic shift after undersea cable issue and\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-030",
      "type": "multiple-choice",
      "question": "A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose. Which next step is strongest? Postmortem highlighted weak blast-radius boundaries.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: run regular game-day failover validation with rollback criteria."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A gaming session coordinator experienced regional deploy rollback mismatch and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"gaming session coordinator experienced regional deploy rollback mismatch and\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-031",
      "type": "multiple-choice",
      "question": "A video metadata API experienced single-region dependency outage and user-facing errors rose. Which next step is strongest? Traffic steering ignored one critical dependency signal.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: add region-aware failover policy with health-gated routing.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A video metadata API experienced single-region dependency outage and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "This prompt is really about \"video metadata API experienced single-region dependency outage and user-facing errors\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-032",
      "type": "multiple-choice",
      "question": "A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose. Which next step is strongest? Runbook ambiguity caused conflicting operator actions.",
      "options": [
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: introduce active-passive with tested promotion runbook.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives."
      ],
      "correct": 1,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A fraud scoring gateway experienced cross-region replication lag spike and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "If you keep \"fraud scoring gateway experienced cross-region replication lag spike and user-facing\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-033",
      "type": "multiple-choice",
      "question": "A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose. Which next step is strongest? Routing policy handled 5xx but not latency degradation.",
      "options": [
        "Prioritize: use active-active reads with write-home-region constraints.",
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged."
      ],
      "correct": 0,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ride dispatch backend experienced control-plane misconfiguration and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "The core signal here is \"ride dispatch backend experienced control-plane misconfiguration and user-facing errors\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-034",
      "type": "multiple-choice",
      "question": "A ticket booking API experienced regional network partition and user-facing errors rose. Which next step is strongest? The standby region had not been load-tested recently.",
      "options": [
        "Send all global traffic to the largest region permanently.",
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: enforce control-plane blast-radius limits per region."
      ],
      "correct": 3,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A ticket booking API experienced regional network partition and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The key clue in this question is \"ticket booking API experienced regional network partition and user-facing errors rose\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-035",
      "type": "multiple-choice",
      "question": "A feature flag delivery service experienced latency regression in one geography and user-facing errors rose. Which next step is strongest? Canary policy behaved differently from full-fleet policy.",
      "options": [
        "Disable regional health checks to avoid false positives.",
        "Increase client retries and keep current failover model unchanged.",
        "Prioritize: add per-region circuit breakers and traffic caps.",
        "Send all global traffic to the largest region permanently."
      ],
      "correct": 2,
      "explanation": "Multi-region strategy should explicitly trade off latency, availability, and consistency with tested failover behavior. For A feature flag delivery service experienced latency regression in one geography and user-facing errors rose, this is the strongest fit in Multi-Region Compute Strategy.",
      "detailedExplanation": "Start from \"feature flag delivery service experienced latency regression in one geography and\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a search query frontend, the team observes regional network partition. What is the most likely primary diagnosis?",
          "options": [
            "regional network partition exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "search query frontend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Use \"during review of a search query frontend, the team observes regional network partition\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "After confirming diagnosis in search query frontend, which next change is strongest under strict RTO targets?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for search query frontend: Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis in search query frontend, which next change is strongest\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"multi-Region Compute Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a notification dispatch API, the team observes latency regression in one geography. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "latency regression in one geography exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "notification dispatch API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Read this as a scenario about \"during review of a notification dispatch API, the team observes latency regression in\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in notification dispatch API, which next change is strongest while limiting cross-region write conflicts?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for notification dispatch API: Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis in notification dispatch API, which next change is strongest\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"multi-Region Compute Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a catalog read service, the team observes zonal capacity shortfall. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "zonal capacity shortfall exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "catalog read service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The decision turns on \"during review of a catalog read service, the team observes zonal capacity shortfall\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in catalog read service, which next change is strongest with minimal blast-radius expansion?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for catalog read service: Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Start from \"after confirming diagnosis in catalog read service, which next change is strongest with\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"multi-Region Compute Strategy\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a order tracking API, the team observes DNS failover delay. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "DNS failover delay exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "order tracking API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Start from \"during review of a order tracking API, the team observes DNS failover delay\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis in order tracking API, which next change is strongest during sustained demand growth?",
          "options": [
            "Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for order tracking API: Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis in order tracking API, which next change is strongest during\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"multi-Region Compute Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ad bidding edge service, the team observes state divergence across regions. What is the most likely primary diagnosis?",
          "options": [
            "state divergence across regions exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "ad bidding edge service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Start from \"during review of a ad bidding edge service, the team observes state divergence across\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in ad bidding edge service, which next change is strongest without overloading downstream dependencies?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ad bidding edge service: Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis in ad bidding edge service, which next change is strongest\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"multi-Region Compute Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a gaming session coordinator, the team observes traffic shift after undersea cable issue. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "traffic shift after undersea cable issue exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "gaming session coordinator shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The decision turns on \"during review of a gaming session coordinator, the team observes traffic shift after\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in gaming session coordinator, which next change is strongest while preserving latency SLOs?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for gaming session coordinator: Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Start from \"after confirming diagnosis in gaming session coordinator, which next change is\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"multi-Region Compute Strategy\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a video metadata API, the team observes regional deploy rollback mismatch. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "regional deploy rollback mismatch exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "video metadata API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The core signal here is \"during review of a video metadata API, the team observes regional deploy rollback\". Solve this as chained reasoning where stage two must respect stage one assumptions. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in video metadata API, which next change is strongest during partial control-plane degradation?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for video metadata API: Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Use \"after confirming diagnosis in video metadata API, which next change is strongest during\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"multi-Region Compute Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a fraud scoring gateway, the team observes single-region dependency outage. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "single-region dependency outage exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "fraud scoring gateway shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The key clue in this question is \"during review of a fraud scoring gateway, the team observes single-region dependency\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in fraud scoring gateway, which next change is strongest with replication lag constraints?",
          "options": [
            "Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for fraud scoring gateway: Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis in fraud scoring gateway, which next change is strongest\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"multi-Region Compute Strategy\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ride dispatch backend, the team observes cross-region replication lag spike. What is the most likely primary diagnosis?",
          "options": [
            "cross-region replication lag spike exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "ride dispatch backend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "This prompt is really about \"during review of a ride dispatch backend, the team observes cross-region replication\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in ride dispatch backend, which next change is strongest before the next failback window?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ride dispatch backend: Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "If you keep \"after confirming diagnosis in ride dispatch backend, which next change is strongest\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"multi-Region Compute Strategy\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ticket booking API, the team observes control-plane misconfiguration. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "control-plane misconfiguration exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "ticket booking API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "If you keep \"during review of a ticket booking API, the team observes control-plane misconfiguration\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in ticket booking API, which next change is strongest under quota-limited secondary regions?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ticket booking API: Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis in ticket booking API, which next change is strongest under\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"multi-Region Compute Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a feature flag delivery service, the team observes regional network partition. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "regional network partition exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "feature flag delivery service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Read this as a scenario about \"during review of a feature flag delivery service, the team observes regional network\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        },
        {
          "question": "After confirming diagnosis in feature flag delivery service, which next change is strongest with strong rollback guarantees?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for feature flag delivery service: Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis in feature flag delivery service, which next change is\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"multi-Region Compute Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a webhook ingestion endpoint, the team observes latency regression in one geography. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "latency regression in one geography exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "webhook ingestion endpoint shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Use \"during review of a webhook ingestion endpoint, the team observes latency regression in\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes."
        },
        {
          "question": "After confirming diagnosis in webhook ingestion endpoint, which next change is strongest while reducing operator toil?",
          "options": [
            "Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for webhook ingestion endpoint: Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis in webhook ingestion endpoint, which next change is\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"multi-Region Compute Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a inventory availability API, the team observes zonal capacity shortfall. What is the most likely primary diagnosis?",
          "options": [
            "zonal capacity shortfall exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "inventory availability API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Start from \"during review of a inventory availability API, the team observes zonal capacity\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis in inventory availability API, which next change is strongest during high-traffic business hours?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for inventory availability API: Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis in inventory availability API, which next change is\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "This prompt is really about \"multi-Region Compute Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a pricing rules service, the team observes DNS failover delay. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "DNS failover delay exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "pricing rules service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The decision turns on \"during review of a pricing rules service, the team observes DNS failover delay\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis in pricing rules service, which next change is strongest with region-specific policy drift risk?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for pricing rules service: Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Start from \"after confirming diagnosis in pricing rules service, which next change is strongest\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"multi-Region Compute Strategy\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a document collaboration backend, the team observes state divergence across regions. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "state divergence across regions exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "document collaboration backend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The key clue in this question is \"during review of a document collaboration backend, the team observes state divergence\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in document collaboration backend, which next change is strongest while preserving write-path correctness?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for document collaboration backend: Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis in document collaboration backend, which next change is\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "If you keep \"multi-Region Compute Strategy\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a support chat API, the team observes traffic shift after undersea cable issue. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "traffic shift after undersea cable issue exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "support chat API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The core signal here is \"during review of a support chat API, the team observes traffic shift after undersea\". Do not reset assumptions between stages; carry forward prior constraints directly. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming diagnosis in support chat API, which next change is strongest during multi-AZ degradation?",
          "options": [
            "Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for support chat API: Implement add region-aware failover policy with health-gated routing and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Use \"after confirming diagnosis in support chat API, which next change is strongest during\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The core signal here is \"multi-Region Compute Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a global checkout API, the team observes regional deploy rollback mismatch. What is the most likely primary diagnosis?",
          "options": [
            "regional deploy rollback mismatch exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "global checkout API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The decision turns on \"during review of a global checkout API, the team observes regional deploy rollback\". Do not reset assumptions between stages; carry forward prior constraints directly. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming diagnosis in global checkout API, which next change is strongest with limited manual intervention?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for global checkout API: Implement introduce active-passive with tested promotion runbook and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Start from \"after confirming diagnosis in global checkout API, which next change is strongest with\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Use \"multi-Region Compute Strategy\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a real-time chat gateway, the team observes single-region dependency outage. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "single-region dependency outage exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "real-time chat gateway shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Start from \"during review of a real-time chat gateway, the team observes single-region dependency\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in real-time chat gateway, which next change is strongest under uncertain health telemetry?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for real-time chat gateway: Implement use active-active reads with write-home-region constraints and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis in real-time chat gateway, which next change is strongest\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "This prompt is really about \"multi-Region Compute Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a identity token service, the team observes cross-region replication lag spike. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "cross-region replication lag spike exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "identity token service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Use \"during review of a identity token service, the team observes cross-region replication\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "After confirming diagnosis in identity token service, which next change is strongest while avoiding retry amplification?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for identity token service: Implement enforce control-plane blast-radius limits per region and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis in identity token service, which next change is strongest\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The decision turns on \"multi-Region Compute Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a payments authorization tier, the team observes control-plane misconfiguration. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "control-plane misconfiguration exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "payments authorization tier shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "Read this as a scenario about \"during review of a payments authorization tier, the team observes control-plane\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming diagnosis in payments authorization tier, which next change is strongest with heterogeneous region capacity?",
          "options": [
            "Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for payments authorization tier: Implement add per-region circuit breakers and traffic caps and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis in payments authorization tier, which next change is\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"multi-Region Compute Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a search query frontend, the team observes regional network partition. What is the most likely primary diagnosis?",
          "options": [
            "regional network partition exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "search query frontend shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "If you keep \"during review of a search query frontend, the team observes regional network partition\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes."
        },
        {
          "question": "After confirming diagnosis in search query frontend, which next change is strongest during staged evacuation?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for search query frontend: Implement pre-provision warm standby capacity in secondary regions and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis in search query frontend, which next change is strongest\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"multi-Region Compute Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a notification dispatch API, the team observes latency regression in one geography. What is the most likely primary diagnosis?",
          "options": [
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "latency regression in one geography exposed weak failover boundaries and regional isolation assumptions."
          ],
          "correct": 3,
          "explanation": "notification dispatch API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "This prompt is really about \"during review of a notification dispatch API, the team observes latency regression in\". Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming diagnosis in notification dispatch API, which next change is strongest while protecting premium-tenant SLOs?",
          "options": [
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity."
          ],
          "correct": 2,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for notification dispatch API: Implement route by latency with strict regional isolation guardrails and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "If you keep \"after confirming diagnosis in notification dispatch API, which next change is strongest\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Start from \"multi-Region Compute Strategy\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a catalog read service, the team observes zonal capacity shortfall. What is the most likely primary diagnosis?",
          "options": [
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks.",
            "zonal capacity shortfall exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly."
          ],
          "correct": 2,
          "explanation": "catalog read service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The key clue in this question is \"during review of a catalog read service, the team observes zonal capacity shortfall\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "After confirming diagnosis in catalog read service, which next change is strongest under strict error-budget guardrails?",
          "options": [
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest."
          ],
          "correct": 1,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for catalog read service: Implement define RPO/RTO-driven replication and failover thresholds and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis in catalog read service, which next change is strongest\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"multi-Region Compute Strategy\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a order tracking API, the team observes DNS failover delay. What is the most likely primary diagnosis?",
          "options": [
            "Regional incidents are best solved by removing health checks.",
            "DNS failover delay exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy."
          ],
          "correct": 1,
          "explanation": "order tracking API shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The core signal here is \"during review of a order tracking API, the team observes DNS failover delay\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis in order tracking API, which next change is strongest with control-plane safety constraints?",
          "options": [
            "Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only."
          ],
          "correct": 0,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for order tracking API: Implement separate global and regional control planes for resilience and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Use \"after confirming diagnosis in order tracking API, which next change is strongest with\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"multi-Region Compute Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "During review of a ad bidding edge service, the team observes state divergence across regions. What is the most likely primary diagnosis?",
          "options": [
            "state divergence across regions exposed weak failover boundaries and regional isolation assumptions.",
            "The only issue is that dashboards refresh too slowly.",
            "Global availability problems never involve routing policy.",
            "Regional incidents are best solved by removing health checks."
          ],
          "correct": 0,
          "explanation": "ad bidding edge service shows regional policy/isolation weaknesses that surfaced under failure conditions; the issue is architectural control behavior, not simple capacity math.",
          "detailedExplanation": "The core signal here is \"during review of a ad bidding edge service, the team observes state divergence across\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "After confirming diagnosis in ad bidding edge service, which next change is strongest before full traffic restoration?",
          "options": [
            "Scale one region vertically and avoid failover complexity.",
            "Route all write traffic to whichever region is currently cheapest.",
            "Leave architecture unchanged and rely on manual intervention only.",
            "Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics."
          ],
          "correct": 3,
          "explanation": "Prioritize the mitigation that directly addresses this regional failure mode for ad bidding edge service: Implement run regular game-day failover validation with rollback criteria and validate with failure drills and SLO impact metrics.",
          "detailedExplanation": "Use \"after confirming diagnosis in ad bidding edge service, which next change is strongest\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"multi-Region Compute Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-061",
      "type": "multi-select",
      "question": "Which factors should drive active-active vs active-passive selection? (Select all that apply)",
      "options": [
        "RPO/RTO requirements",
        "Write conflict tolerance",
        "Office timezone overlap only",
        "Operational failover complexity budget"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Recovery objectives, conflict handling, and ops complexity are primary decision factors.",
      "detailedExplanation": "If you keep \"factors should drive active-active vs active-passive selection? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-062",
      "type": "multi-select",
      "question": "Strong regional isolation controls include which? (Select all that apply)",
      "options": [
        "Per-region dependency boundaries",
        "Independent capacity headroom",
        "Single global failure domain for all writes",
        "Regional circuit breakers"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Isolation reduces blast radius and preserves partial service during regional incidents.",
      "detailedExplanation": "This prompt is really about \"strong regional isolation controls include which? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-063",
      "type": "multi-select",
      "question": "Useful failover readiness evidence includes which? (Select all that apply)",
      "options": [
        "Documented runbooks with owners",
        "Regular game-day exercises",
        "Never running drills to avoid risk",
        "Measured promotion time vs SLO"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Readiness is proven by repeated tests and measured outcomes.",
      "detailedExplanation": "Use \"useful failover readiness evidence includes which? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-064",
      "type": "multi-select",
      "question": "Geo-routing policy inputs should usually include which? (Select all that apply)",
      "options": [
        "Latency",
        "Regional health state",
        "Random region selection only",
        "Capacity saturation signals"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Routing should combine performance and health/capacity safety signals.",
      "detailedExplanation": "Read this as a scenario about \"geo-routing policy inputs should usually include which? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-065",
      "type": "multi-select",
      "question": "Cross-region replication trade-offs often include which? (Select all that apply)",
      "options": [
        "Lag impacts stale-read risk",
        "Synchronous replication increases write latency",
        "Replication mode has no effect on durability",
        "Asynchronous replication can improve availability but raises RPO risk"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Replication choices directly affect latency, durability, and data-loss windows.",
      "detailedExplanation": "The decision turns on \"cross-region replication trade-offs often include which? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-066",
      "type": "multi-select",
      "question": "Control-plane resilience practices include which? (Select all that apply)",
      "options": [
        "Scoped rollout and canary changes",
        "Regional config blast-radius limits",
        "One-shot global config updates with no rollback",
        "Versioned policies with audit logs"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Treat control plane like critical software with staged rollout and rollback.",
      "detailedExplanation": "Start from \"control-plane resilience practices include which? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-067",
      "type": "multi-select",
      "question": "When failing over writes to a secondary region, what risks must be managed? (Select all that apply)",
      "options": [
        "Duplicate side effects from replay/retry",
        "Schema/version mismatch across regions",
        "Automatic elimination of consistency concerns",
        "Capacity saturation in target region"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failover writes require idempotency, compatibility, and capacity planning.",
      "detailedExplanation": "The key clue in this question is \"failing over writes to a secondary region, what risks must be managed? (Select all that\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-068",
      "type": "multi-select",
      "question": "Which signs indicate DNS-only failover may be insufficient? (Select all that apply)",
      "options": [
        "Client-side DNS caching delays traffic shift",
        "Need for request-level health decisions",
        "Immediate zero-latency cutover guaranteed",
        "Fine-grained route steering requirements"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "DNS is coarse-grained and can be too slow for tight RTO goals.",
      "detailedExplanation": "The core signal here is \"signs indicate DNS-only failover may be insufficient? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-069",
      "type": "multi-select",
      "question": "For active-active reads, which safeguards are useful? (Select all that apply)",
      "options": [
        "Staleness-aware read policies",
        "Monotonic read/session guarantees where needed",
        "Ignoring replication lag metrics",
        "Region preference fallback logic"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Read policies should explicitly account for lag and consistency expectations.",
      "detailedExplanation": "If you keep \"for active-active reads, which safeguards are useful? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-070",
      "type": "multi-select",
      "question": "Which are valid reasons to keep a warm standby region? (Select all that apply)",
      "options": [
        "Faster failover for strict RTO targets",
        "Avoid full cold-start provisioning during incident",
        "Guarantee zero operational cost",
        "Limit capacity shock on surviving regions"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Warm standby trades cost for faster and safer incident response.",
      "detailedExplanation": "The key clue in this question is \"valid reasons to keep a warm standby region? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-071",
      "type": "multi-select",
      "question": "Which metrics help detect regional imbalance before outages? (Select all that apply)",
      "options": [
        "Per-region utilization and queue age",
        "Cross-region error rate skew",
        "Only global daily average QPS",
        "Failover trigger frequency"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Region-level metrics reveal asymmetry that fleet averages hide.",
      "detailedExplanation": "Start from \"metrics help detect regional imbalance before outages? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-072",
      "type": "multi-select",
      "question": "During region failback (returning traffic), which practices are strong? (Select all that apply)",
      "options": [
        "Gradual traffic ramp with guardrails",
        "Data/state consistency checks before full cutback",
        "Immediate 100% restoration regardless metrics",
        "Rollback thresholds on latency/errors"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failback should be staged and observable like failover.",
      "detailedExplanation": "The decision turns on \"during region failback (returning traffic), which practices are strong? (Select all\". Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-073",
      "type": "multi-select",
      "question": "Which design choices reduce blast radius from regional incidents? (Select all that apply)",
      "options": [
        "Regional dependency isolation",
        "Per-region rate limiting",
        "Single shared mutable global state with no partitioning",
        "Traffic caps during emergency reroute"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Isolation and caps prevent one regions failure from cascading globally.",
      "detailedExplanation": "Read this as a scenario about \"design choices reduce blast radius from regional incidents? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-074",
      "type": "multi-select",
      "question": "For multi-region control planes, which risks should be anticipated? (Select all that apply)",
      "options": [
        "Config drift between regions",
        "Propagation delay for policy changes",
        "No need for audit/versioning in emergencies",
        "Conflicting operator actions under pressure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Operational consistency and governance remain critical during incidents.",
      "detailedExplanation": "Use \"for multi-region control planes, which risks should be anticipated? (Select all that\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-075",
      "type": "multi-select",
      "question": "When to prefer active-passive over active-active? (Select all that apply)",
      "options": [
        "Write conflict cost is very high",
        "Strictly simpler operational model is required",
        "Need lowest possible steady-state latency from every region always",
        "RPO/RTO can be met with standby promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Active-passive is often chosen for simpler write semantics and operational control.",
      "detailedExplanation": "This prompt is really about \"to prefer active-passive over active-active? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-076",
      "type": "multi-select",
      "question": "Which preparations improve successful regional evacuation during incidents? (Select all that apply)",
      "options": [
        "Precomputed traffic-shift playbooks",
        "Capacity rehearsal in secondary regions",
        "Assuming autoscaling alone handles any surge",
        "Dependency quota agreements ahead of incidents"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Evacuation succeeds when capacity and dependencies are prepared before failure.",
      "detailedExplanation": "If you keep \"preparations improve successful regional evacuation during incidents? (Select all that\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-077",
      "type": "multi-select",
      "question": "Which anti-patterns commonly break multi-region reliability? (Select all that apply)",
      "options": [
        "Untested failover paths",
        "Control-plane global blasts",
        "Periodic game-day drills",
        "Inconsistent region-specific runbooks"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Reliability fails when failover and control-plane paths are untested or unsafe.",
      "detailedExplanation": "The core signal here is \"anti-patterns commonly break multi-region reliability? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-078",
      "type": "numeric-input",
      "question": "Primary region handles 36,000 rps. During failover, two secondary regions split traffic 60/40. How much rps goes to the larger secondary region?",
      "answer": 21600,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.60 * 36,000 = 21,600 rps.",
      "detailedExplanation": "The key clue in this question is \"primary region handles 36,000 rps\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 36,000 rps and 60 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-079",
      "type": "numeric-input",
      "question": "A standby region can process 14,000 rps, and planned failover load is 11,200 rps. What percent headroom remains?",
      "answer": 20,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "(14,000 - 11,200) / 14,000 = 20%.",
      "detailedExplanation": "Start from \"standby region can process 14,000 rps, and planned failover load is 11,200 rps\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 14,000 rps and 11,200 rps appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-080",
      "type": "numeric-input",
      "question": "Cross-region replication lag is 9 seconds and write rate is 4,500 writes/sec. Approximately how many writes are potentially not replicated at failover?",
      "answer": 40500,
      "unit": "writes",
      "tolerance": 0.03,
      "explanation": "4,500 * 9 = 40,500 writes in lag window.",
      "detailedExplanation": "Start from \"cross-region replication lag is 9 seconds and write rate is 4,500 writes/sec\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 9 seconds and 4,500 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-081",
      "type": "numeric-input",
      "question": "RTO target is 5 minutes. Measured failover sequence currently takes 8 minutes. By what percentage must failover time be reduced?",
      "answer": 37.5,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(8 - 5) / 8 = 37.5% reduction needed.",
      "detailedExplanation": "The key clue in this question is \"rTO target is 5 minutes\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 5 minutes and 8 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-082",
      "type": "numeric-input",
      "question": "Global traffic is 72,000 rps across 3 active regions equally. One region fails and remaining two absorb evenly. New rps per surviving region?",
      "answer": 36000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "72,000 / 2 = 36,000 rps per remaining region.",
      "detailedExplanation": "Read this as a scenario about \"global traffic is 72,000 rps across 3 active regions equally\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 72,000 rps and 3 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-083",
      "type": "numeric-input",
      "question": "A failover drill shifts 25% traffic every 2 minutes. Starting at 0%, how many minutes to reach 100%?",
      "answer": 8,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Four increments of 25%; 4 * 2 = 8 minutes.",
      "detailedExplanation": "The decision turns on \"failover drill shifts 25% traffic every 2 minutes\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 25 and 2 minutes in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-084",
      "type": "numeric-input",
      "question": "Secondary region cold start requires 180 seconds for 120 instances to become ready. Total instance-seconds of warmup?",
      "answer": 21600,
      "unit": "instance-seconds",
      "tolerance": 0,
      "explanation": "120 * 180 = 21,600 instance-seconds.",
      "detailedExplanation": "This prompt is really about \"secondary region cold start requires 180 seconds for 120 instances to become ready\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 180 seconds and 120 should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-085",
      "type": "numeric-input",
      "question": "A geo-routing policy sends 68% traffic to nearest region, 22% to second-best, 10% to overflow. At 50,000 rps, what is overflow rps?",
      "answer": 5000,
      "unit": "rps",
      "tolerance": 0.02,
      "explanation": "0.10 * 50,000 = 5,000 rps.",
      "detailedExplanation": "Use \"geo-routing policy sends 68% traffic to nearest region, 22% to second-best, 10% to\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 68 and 22 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-086",
      "type": "numeric-input",
      "question": "Regional error budget allows 0.05% failures over 12,000,000 requests/day. Maximum failed requests/day?",
      "answer": 6000,
      "unit": "requests",
      "tolerance": 0,
      "explanation": "0.0005 * 12,000,000 = 6,000.",
      "detailedExplanation": "The core signal here is \"regional error budget allows 0\". Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 0.05 and 12,000 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-087",
      "type": "numeric-input",
      "question": "Write latency is 35ms with async replication and 62ms with sync quorum replication. What percent increase is sync over async?",
      "answer": 77.14,
      "unit": "%",
      "tolerance": 0.5,
      "explanation": "(62 - 35) / 35 = 77.14% increase.",
      "detailedExplanation": "If you keep \"write latency is 35ms with async replication and 62ms with sync quorum replication\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 35ms and 62ms should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-088",
      "type": "numeric-input",
      "question": "During partial outage, traffic cap limits rerouted load to 18,000 rps. Demand needing reroute is 24,000 rps. How much rps must be shed/degraded?",
      "answer": 6000,
      "unit": "rps",
      "tolerance": 0,
      "explanation": "24,000 - 18,000 = 6,000 rps.",
      "detailedExplanation": "Start from \"during partial outage, traffic cap limits rerouted load to 18,000 rps\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Keep quantities like 18,000 rps and 24,000 rps in aligned units before selecting an answer. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-089",
      "type": "numeric-input",
      "question": "A failback step decreases secondary traffic from 100% to 20% over 4 equal steps. How many percentage points per step?",
      "answer": 20,
      "unit": "percentage points",
      "tolerance": 0,
      "explanation": "(100 - 20) / 4 = 20 points per step.",
      "detailedExplanation": "The key clue in this question is \"failback step decreases secondary traffic from 100% to 20% over 4 equal steps\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 100 and 20 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-090",
      "type": "ordering",
      "question": "Order a robust regional failover workflow.",
      "items": [
        "Detect and scope regional degradation",
        "Apply guarded traffic shift",
        "Validate capacity/dependency health in target regions",
        "Stabilize and document post-incident learnings"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope first, shift safely, validate capacity, then institutionalize lessons.",
      "detailedExplanation": "The decision turns on \"order a robust regional failover workflow\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-091",
      "type": "ordering",
      "question": "Order routing granularity from coarsest to finest.",
      "items": [
        "DNS region failover",
        "Anycast/BGP steering",
        "L7 geo-routing rules",
        "Request-level health and policy routing"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Controls become finer and more dynamic from DNS to request-level policy.",
      "detailedExplanation": "Read this as a scenario about \"order routing granularity from coarsest to finest\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-092",
      "type": "ordering",
      "question": "Order replication modes by increasing write-latency impact (typical case).",
      "items": [
        "Async cross-region replication",
        "Semi-sync replication",
        "Sync quorum replication",
        "Global synchronous commit across distant regions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stronger synchronous guarantees generally increase write latency.",
      "detailedExplanation": "The key clue in this question is \"order replication modes by increasing write-latency impact (typical case)\". Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-093",
      "type": "ordering",
      "question": "Order preparedness maturity for multi-region incidents.",
      "items": [
        "No runbook",
        "Runbook without drills",
        "Runbook with periodic drills",
        "Runbook with drills and measured SLO compliance"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Preparedness matures through testing and measurable outcomes.",
      "detailedExplanation": "Start from \"order preparedness maturity for multi-region incidents\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-094",
      "type": "ordering",
      "question": "Order by increasing regional blast-radius risk.",
      "items": [
        "Region-isolated dependencies and caps",
        "Shared dependencies with regional limits",
        "Shared global dependencies without strict isolation",
        "Single control-plane action affecting all regions instantly"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Shared global dependencies and broad control-plane blast increase risk.",
      "detailedExplanation": "If you keep \"order by increasing regional blast-radius risk\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-095",
      "type": "ordering",
      "question": "Order failback steps from safest to riskiest.",
      "items": [
        "Incremental traffic restore with rollback checks",
        "Batch restore with close monitoring",
        "Rapid restore in one large step",
        "Immediate full restore without checks"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Failback should be gradual and observable to avoid secondary incidents.",
      "detailedExplanation": "The core signal here is \"order failback steps from safest to riskiest\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-096",
      "type": "ordering",
      "question": "Order by strongest evidence of regional readiness.",
      "items": [
        "Architecture diagram only",
        "Tabletop discussion",
        "Controlled failover drill",
        "Regular drills plus measured RTO/RPO compliance"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Readiness confidence grows from static documentation to repeated measured drills.",
      "detailedExplanation": "Use \"order by strongest evidence of regional readiness\" as your starting point, then verify tradeoffs carefully. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-097",
      "type": "ordering",
      "question": "Order incident response sequence for replication lag surge.",
      "items": [
        "Quantify lag and staleness impact",
        "Apply read/write policy guardrails",
        "Adjust routing/failover thresholds",
        "Run post-incident consistency audit"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Measure first, contain risk, tune controls, then audit consistency.",
      "detailedExplanation": "This prompt is really about \"order incident response sequence for replication lag surge\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-098",
      "type": "ordering",
      "question": "Order by increasing complexity of multi-region write strategy.",
      "items": [
        "Single write region",
        "Active-passive write promotion",
        "Partitioned writes by home region",
        "Multi-master conflict resolution"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Write concurrency and conflict handling complexity grows across these models.",
      "detailedExplanation": "The decision turns on \"order by increasing complexity of multi-region write strategy\". Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-099",
      "type": "ordering",
      "question": "Order control-plane change safety approach.",
      "items": [
        "Global immediate rollout",
        "Large batch rollout",
        "Regional canary rollout",
        "Regional canary with automatic rollback thresholds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safer control-plane changes narrow blast radius and add rollback automation.",
      "detailedExplanation": "Read this as a scenario about \"order control-plane change safety approach\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    },
    {
      "id": "sc-mr-100",
      "type": "ordering",
      "question": "Order regional traffic management sophistication.",
      "items": [
        "Static fixed region weights",
        "Health-based region exclusion",
        "Latency + health adaptive routing",
        "Latency + health + capacity + policy-aware routing"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Policy sophistication rises as more real-time constraints are incorporated.",
      "detailedExplanation": "The key clue in this question is \"order regional traffic management sophistication\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "multi-region-compute-strategy"],
      "difficulty": "senior"
    }
  ]
}
