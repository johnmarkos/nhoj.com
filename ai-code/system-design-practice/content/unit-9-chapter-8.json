{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 8,
  "chapterTitle": "Reliability Scenarios",
  "chapterDescription": "Integrated incidents combining retry storms, partial outages, failover decisions, and degradation strategy under pressure.",
  "problems": [
    {
      "id": "rel-scn-001",
      "type": "multiple-choice",
      "question": "Case Alpha: global checkout incident bridge. Primary reliability risk is retry storm during partial dependency outage. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Stabilize first with admission and shedding, then restore critical paths in dependency order.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat global checkout incident bridge as a reliability-control decision, not an averages-only optimization. \"Stabilize first with admission and shedding, then restore critical paths in dependency order\" is correct since it mitigates retry storm during partial dependency outage while keeping containment local. The decision remains valid given: A rollback window is still available for the next 15 minutes.",
      "detailedExplanation": "Use \"global checkout incident bridge\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 15 minutes in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-002",
      "type": "multiple-choice",
      "question": "Case Beta: cross-region messaging outage response. Primary reliability risk is zone failure with stale failover metadata. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Use fault-domain aware triage to contain blast radius before broad failover.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Cross-region messaging outage response should be solved at the failure boundary named in Reliability Scenarios. \"Use fault-domain aware triage to contain blast radius before broad failover\" is strongest because it directly addresses zone failure with stale failover metadata and improves repeatability under stress. This aligns with the extra condition (Leadership asked for an action that lowers recurrence, not just symptoms).",
      "detailedExplanation": "The core signal here is \"cross-region messaging outage response\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-003",
      "type": "multiple-choice",
      "question": "Case Gamma: payments failover war room. Primary reliability risk is admission controls disabled under load spike. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Reliability Scenarios, payments failover war room fails mainly through admission controls disabled under load spike. The best choice is \"Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "If you keep \"payments failover war room\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-004",
      "type": "multiple-choice",
      "question": "Case Delta: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling. (Reliability Scenarios context)",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Promote only replicas meeting freshness constraints and document failback guards."
      ],
      "correct": 3,
      "explanation": "For search brownout incident review, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Promote only replicas meeting freshness constraints and document failback guards\" outperforms the alternatives because it targets degraded mode not activated for optional paths and preserves safe recovery behavior. It is also the most compatible with Recent game-day results showed hidden cross-zone coupling.",
      "detailedExplanation": "Start from \"search brownout incident review\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: identity control-plane outage. Primary reliability risk is replication lag ignored during promotion. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Activate graceful degradation early to preserve core user journeys.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat identity control-plane outage as a reliability-control decision, not an averages-only optimization. \"Activate graceful degradation early to preserve core user journeys\" is correct since it mitigates replication lag ignored during promotion while keeping containment local. The decision remains valid given: Customer impact is concentrated on invariant-critical transactions.",
      "detailedExplanation": "The key clue in this question is \"identity control-plane outage\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-006",
      "type": "multiple-choice",
      "question": "Case Zeta: stream processor recovery effort. Primary reliability risk is conflicting runbook decisions across teams. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Assign clear incident command roles and maintain a single decision timeline.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Stream processor recovery effort should be solved at the failure boundary named in Reliability Scenarios. \"Assign clear incident command roles and maintain a single decision timeline\" is strongest because it directly addresses conflicting runbook decisions across teams and improves repeatability under stress. This aligns with the extra condition (The previous mitigation improved averages but not tail behavior).",
      "detailedExplanation": "Read this as a scenario about \"stream processor recovery effort\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-007",
      "type": "multiple-choice",
      "question": "Case Eta: notification delivery incident command. Primary reliability risk is alert flood obscuring root-cause signal. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Sequence recovery by invariant criticality, not by team convenience.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Reliability Scenarios, notification delivery incident command fails mainly through alert flood obscuring root-cause signal. The best choice is \"Sequence recovery by invariant criticality, not by team convenience\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "The decision turns on \"notification delivery incident command\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-008",
      "type": "multiple-choice",
      "question": "Case Theta: catalog consistency outage triage. Primary reliability risk is brownout controls missing for non-core traffic. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Track SLO/error-budget impact during mitigation to guide risk trade-offs."
      ],
      "correct": 3,
      "explanation": "For catalog consistency outage triage, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Track SLO/error-budget impact during mitigation to guide risk trade-offs\" outperforms the alternatives because it targets brownout controls missing for non-core traffic and preserves safe recovery behavior. It is also the most compatible with Operations wants a reversible step before broader architecture changes.",
      "detailedExplanation": "This prompt is really about \"catalog consistency outage triage\". Eliminate approaches that hand-wave conflict resolution or quorum behavior. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-009",
      "type": "multiple-choice",
      "question": "Case Iota: multi-tenant platform degradation event. Primary reliability risk is recovery sequencing causes second incident. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Validate recovery with canary checks before full traffic restoration.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat multi-tenant platform degradation event as a reliability-control decision, not an averages-only optimization. \"Validate recovery with canary checks before full traffic restoration\" is correct since it mitigates recovery sequencing causes second incident while keeping containment local. The decision remains valid given: SLO burn rate accelerated after a config rollout this morning.",
      "detailedExplanation": "Use \"multi-tenant platform degradation event\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-010",
      "type": "multiple-choice",
      "question": "Case Kappa: mobile backend regional recovery. Primary reliability risk is error-budget burn unaccounted during mitigation. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Close with post-incident hardening items that prevent repeat cascade patterns.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Reliability Scenarios, mobile backend regional recovery fails mainly through error-budget burn unaccounted during mitigation. The best choice is \"Close with post-incident hardening items that prevent repeat cascade patterns\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "If you keep \"mobile backend regional recovery\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-011",
      "type": "multiple-choice",
      "question": "Case Lambda: global checkout incident bridge. Primary reliability risk is retry storm during partial dependency outage. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Stabilize first with admission and shedding, then restore critical paths in dependency order.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For global checkout incident bridge, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Stabilize first with admission and shedding, then restore critical paths in dependency order\" outperforms the alternatives because it targets retry storm during partial dependency outage and preserves safe recovery behavior. It is also the most compatible with The incident review highlighted missing boundary ownership.",
      "detailedExplanation": "The core signal here is \"global checkout incident bridge\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-012",
      "type": "multiple-choice",
      "question": "Case Mu: cross-region messaging outage response. Primary reliability risk is zone failure with stale failover metadata. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Use fault-domain aware triage to contain blast radius before broad failover."
      ],
      "correct": 3,
      "explanation": "Treat cross-region messaging outage response as a reliability-control decision, not an averages-only optimization. \"Use fault-domain aware triage to contain blast radius before broad failover\" is correct since it mitigates zone failure with stale failover metadata while keeping containment local. The decision remains valid given: Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "detailedExplanation": "Use \"cross-region messaging outage response\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-013",
      "type": "multiple-choice",
      "question": "Case Nu: payments failover war room. Primary reliability risk is admission controls disabled under load spike. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Payments failover war room should be solved at the failure boundary named in Reliability Scenarios. \"Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload\" is strongest because it directly addresses admission controls disabled under load spike and improves repeatability under stress. This aligns with the extra condition (A canary can be deployed immediately if the strategy is clear).",
      "detailedExplanation": "This prompt is really about \"payments failover war room\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-014",
      "type": "multiple-choice",
      "question": "Case Xi: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? Capacity remains available only in one neighboring zone. (Reliability Scenarios context)",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Promote only replicas meeting freshness constraints and document failback guards.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Reliability Scenarios, search brownout incident review fails mainly through degraded mode not activated for optional paths. The best choice is \"Promote only replicas meeting freshness constraints and document failback guards\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "The decision turns on \"search brownout incident review\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-015",
      "type": "multiple-choice",
      "question": "Case Omicron: identity control-plane outage. Primary reliability risk is replication lag ignored during promotion. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Activate graceful degradation early to preserve core user journeys.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For identity control-plane outage, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Activate graceful degradation early to preserve core user journeys\" outperforms the alternatives because it targets replication lag ignored during promotion and preserves safe recovery behavior. It is also the most compatible with Client retries are already elevated and could amplify mistakes.",
      "detailedExplanation": "Read this as a scenario about \"identity control-plane outage\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-016",
      "type": "multiple-choice",
      "question": "Case Pi: stream processor recovery effort. Primary reliability risk is conflicting runbook decisions across teams. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Assign clear incident command roles and maintain a single decision timeline."
      ],
      "correct": 3,
      "explanation": "Treat stream processor recovery effort as a reliability-control decision, not an averages-only optimization. \"Assign clear incident command roles and maintain a single decision timeline\" is correct since it mitigates conflicting runbook decisions across teams while keeping containment local. The decision remains valid given: The team must preserve core write correctness under mitigation.",
      "detailedExplanation": "The key clue in this question is \"stream processor recovery effort\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-017",
      "type": "multiple-choice",
      "question": "Case Rho: notification delivery incident command. Primary reliability risk is alert flood obscuring root-cause signal. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Sequence recovery by invariant criticality, not by team convenience.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Notification delivery incident command should be solved at the failure boundary named in Reliability Scenarios. \"Sequence recovery by invariant criticality, not by team convenience\" is strongest because it directly addresses alert flood obscuring root-cause signal and improves repeatability under stress. This aligns with the extra condition (Recent staffing changes require simpler operational controls).",
      "detailedExplanation": "Start from \"notification delivery incident command\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-018",
      "type": "multiple-choice",
      "question": "Case Sigma: catalog consistency outage triage. Primary reliability risk is brownout controls missing for non-core traffic. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Track SLO/error-budget impact during mitigation to guide risk trade-offs.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Reliability Scenarios, catalog consistency outage triage fails mainly through brownout controls missing for non-core traffic. The best choice is \"Track SLO/error-budget impact during mitigation to guide risk trade-offs\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Cross-region latency variance increased during the event.",
      "detailedExplanation": "If you keep \"catalog consistency outage triage\" in view, the correct answer separates faster. Eliminate options that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-019",
      "type": "multiple-choice",
      "question": "Case Tau: multi-tenant platform degradation event. Primary reliability risk is recovery sequencing causes second incident. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Validate recovery with canary checks before full traffic restoration.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For multi-tenant platform degradation event, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Validate recovery with canary checks before full traffic restoration\" outperforms the alternatives because it targets recovery sequencing causes second incident and preserves safe recovery behavior. It is also the most compatible with This path mixes latency-sensitive and correctness-sensitive requests.",
      "detailedExplanation": "The core signal here is \"multi-tenant platform degradation event\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: mobile backend regional recovery. Primary reliability risk is error-budget burn unaccounted during mitigation. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Close with post-incident hardening items that prevent repeat cascade patterns."
      ],
      "correct": 3,
      "explanation": "Mobile backend regional recovery should be solved at the failure boundary named in Reliability Scenarios. \"Close with post-incident hardening items that prevent repeat cascade patterns\" is strongest because it directly addresses error-budget burn unaccounted during mitigation and improves repeatability under stress. This aligns with the extra condition (The service has one hidden shared component with no backup path).",
      "detailedExplanation": "The core signal here is \"mobile backend regional recovery\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-021",
      "type": "multiple-choice",
      "question": "Case Phi: global checkout incident bridge. Primary reliability risk is retry storm during partial dependency outage. Which next move is strongest? The product team accepts degraded reads but not incorrect writes. (Reliability Scenarios context)",
      "options": [
        "Stabilize first with admission and shedding, then restore critical paths in dependency order.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Reliability Scenarios, global checkout incident bridge fails mainly through retry storm during partial dependency outage. The best choice is \"Stabilize first with admission and shedding, then restore critical paths in dependency order\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "If you keep \"global checkout incident bridge\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-022",
      "type": "multiple-choice",
      "question": "Case Chi: cross-region messaging outage response. Primary reliability risk is zone failure with stale failover metadata. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Use fault-domain aware triage to contain blast radius before broad failover.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For cross-region messaging outage response, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Use fault-domain aware triage to contain blast radius before broad failover\" outperforms the alternatives because it targets zone failure with stale failover metadata and preserves safe recovery behavior. It is also the most compatible with Change approval favors narrowly scoped policies over global flips.",
      "detailedExplanation": "This prompt is really about \"cross-region messaging outage response\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-023",
      "type": "multiple-choice",
      "question": "Case Psi: payments failover war room. Primary reliability risk is admission controls disabled under load spike. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat payments failover war room as a reliability-control decision, not an averages-only optimization. \"Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload\" is correct since it mitigates admission controls disabled under load spike while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "Use \"payments failover war room\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-024",
      "type": "multiple-choice",
      "question": "Case Omega: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics. (Reliability Scenarios context)",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Promote only replicas meeting freshness constraints and document failback guards."
      ],
      "correct": 3,
      "explanation": "Search brownout incident review should be solved at the failure boundary named in Reliability Scenarios. \"Promote only replicas meeting freshness constraints and document failback guards\" is strongest because it directly addresses degraded mode not activated for optional paths and improves repeatability under stress. This aligns with the extra condition (On-call needs mitigation that is observable by explicit metrics).",
      "detailedExplanation": "Read this as a scenario about \"search brownout incident review\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-025",
      "type": "multiple-choice",
      "question": "Case Atlas: identity control-plane outage. Primary reliability risk is replication lag ignored during promotion. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Activate graceful degradation early to preserve core user journeys.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Reliability Scenarios, identity control-plane outage fails mainly through replication lag ignored during promotion. The best choice is \"Activate graceful degradation early to preserve core user journeys\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "The decision turns on \"identity control-plane outage\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-026",
      "type": "multiple-choice",
      "question": "Case Nova: stream processor recovery effort. Primary reliability risk is conflicting runbook decisions across teams. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Assign clear incident command roles and maintain a single decision timeline.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For stream processor recovery effort, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Assign clear incident command roles and maintain a single decision timeline\" outperforms the alternatives because it targets conflicting runbook decisions across teams and preserves safe recovery behavior. It is also the most compatible with Business impact is highest in the top 5% of critical flows.",
      "detailedExplanation": "Start from \"stream processor recovery effort\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-027",
      "type": "multiple-choice",
      "question": "Case Orion: notification delivery incident command. Primary reliability risk is alert flood obscuring root-cause signal. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Sequence recovery by invariant criticality, not by team convenience.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat notification delivery incident command as a reliability-control decision, not an averages-only optimization. \"Sequence recovery by invariant criticality, not by team convenience\" is correct since it mitigates alert flood obscuring root-cause signal while keeping containment local. The decision remains valid given: Regional failover is possible but expensive if used prematurely.",
      "detailedExplanation": "The key clue in this question is \"notification delivery incident command\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-028",
      "type": "multiple-choice",
      "question": "Case Vega: catalog consistency outage triage. Primary reliability risk is brownout controls missing for non-core traffic. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Track SLO/error-budget impact during mitigation to guide risk trade-offs."
      ],
      "correct": 3,
      "explanation": "Catalog consistency outage triage should be solved at the failure boundary named in Reliability Scenarios. \"Track SLO/error-budget impact during mitigation to guide risk trade-offs\" is strongest because it directly addresses brownout controls missing for non-core traffic and improves repeatability under stress. This aligns with the extra condition (A hot tenant currently consumes disproportionate worker capacity).",
      "detailedExplanation": "The core signal here is \"catalog consistency outage triage\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-029",
      "type": "multiple-choice",
      "question": "Case Helios: multi-tenant platform degradation event. Primary reliability risk is recovery sequencing causes second incident. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Validate recovery with canary checks before full traffic restoration.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Reliability Scenarios, multi-tenant platform degradation event fails mainly through recovery sequencing causes second incident. The best choice is \"Validate recovery with canary checks before full traffic restoration\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "If you keep \"multi-tenant platform degradation event\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-030",
      "type": "multiple-choice",
      "question": "Case Aurora: mobile backend regional recovery. Primary reliability risk is error-budget burn unaccounted during mitigation. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Close with post-incident hardening items that prevent repeat cascade patterns.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat mobile backend regional recovery as a reliability-control decision, not an averages-only optimization. \"Close with post-incident hardening items that prevent repeat cascade patterns\" is correct since it mitigates error-budget burn unaccounted during mitigation while keeping containment local. The decision remains valid given: Queue age is rising even though average CPU appears normal.",
      "detailedExplanation": "The key clue in this question is \"mobile backend regional recovery\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: global checkout incident bridge. Primary reliability risk is retry storm during partial dependency outage. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Stabilize first with admission and shedding, then restore critical paths in dependency order.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Global checkout incident bridge should be solved at the failure boundary named in Reliability Scenarios. \"Stabilize first with admission and shedding, then restore critical paths in dependency order\" is strongest because it directly addresses retry storm during partial dependency outage and improves repeatability under stress. This aligns with the extra condition (A control-plane API is healthy but data-plane errors are increasing).",
      "detailedExplanation": "Start from \"global checkout incident bridge\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-032",
      "type": "multiple-choice",
      "question": "Case Pulse: cross-region messaging outage response. Primary reliability risk is zone failure with stale failover metadata. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Use fault-domain aware triage to contain blast radius before broad failover."
      ],
      "correct": 3,
      "explanation": "In Reliability Scenarios, cross-region messaging outage response fails mainly through zone failure with stale failover metadata. The best choice is \"Use fault-domain aware triage to contain blast radius before broad failover\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "The decision turns on \"cross-region messaging outage response\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-033",
      "type": "multiple-choice",
      "question": "Case Forge: payments failover war room. Primary reliability risk is admission controls disabled under load spike. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For payments failover war room, prefer the option that prevents reoccurrence in Reliability Scenarios. \"Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload\" outperforms the alternatives because it targets admission controls disabled under load spike and preserves safe recovery behavior. It is also the most compatible with Legal/compliance constraints require explicit behavior in degraded mode.",
      "detailedExplanation": "Read this as a scenario about \"payments failover war room\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-034",
      "type": "multiple-choice",
      "question": "Case Harbor: search brownout incident review. Primary reliability risk is degraded mode not activated for optional paths. Which next move is strongest? Past incidents show this failure mode recurs every quarter. (Reliability Scenarios context)",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Promote only replicas meeting freshness constraints and document failback guards.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat search brownout incident review as a reliability-control decision, not an averages-only optimization. \"Promote only replicas meeting freshness constraints and document failback guards\" is correct since it mitigates degraded mode not activated for optional paths while keeping containment local. The decision remains valid given: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "Use \"search brownout incident review\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-035",
      "type": "multiple-choice",
      "question": "Case Vector: identity control-plane outage. Primary reliability risk is replication lag ignored during promotion. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Activate graceful degradation early to preserve core user journeys.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Identity control-plane outage should be solved at the failure boundary named in Reliability Scenarios. \"Activate graceful degradation early to preserve core user journeys\" is strongest because it directly addresses replication lag ignored during promotion and improves repeatability under stress. This aligns with the extra condition (User trust impact is tied to visible inconsistency, not only downtime).",
      "detailedExplanation": "This prompt is really about \"identity control-plane outage\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for global checkout incident bridge: signal points to degraded mode not activated for optional paths. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for global checkout incident bridge is mismatched to degraded mode not activated for optional paths, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Global checkout incident bridge is a two-step reliability decision. At stage 1, \"The design for global checkout incident bridge is mismatched to degraded mode not activated for optional paths, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around degraded mode not activated for optional paths.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for global checkout incident bridge: signal points to degraded mode\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for global checkout incident bridge:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Assign clear incident command roles and maintain a single decision timeline.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Reliability Scenarios, the best answer is \"Assign clear incident command roles and maintain a single decision timeline\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"reliability Scenarios\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for cross-region messaging outage response: signal points to replication lag ignored during promotion. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for cross-region messaging outage response is mismatched to replication lag ignored during promotion, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for cross-region messaging outage response is mismatched to replication lag ignored during promotion, creating repeat reliability incidents\" best matches cross-region messaging outage response by targeting replication lag ignored during promotion and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for cross-region messaging outage response: signal points to\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for cross-region messaging outage\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Sequence recovery by invariant criticality, not by team convenience.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "For \"incident diagnosis for cross-region messaging outage\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Sequence recovery by invariant criticality, not by team convenience\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for payments failover war room: signal points to conflicting runbook decisions across teams. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for payments failover war room is mismatched to conflicting runbook decisions across teams, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Reliability Scenarios: for payments failover war room, \"The design for payments failover war room is mismatched to conflicting runbook decisions across teams, creating repeat reliability incidents\" is correct because it addresses conflicting runbook decisions across teams and improves controllability.",
          "detailedExplanation": "If you keep \"incident diagnosis for payments failover war room: signal points to conflicting runbook\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for payments failover war room:\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Track SLO/error-budget impact during mitigation to guide risk trade-offs."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Track SLO/error-budget impact during mitigation to guide risk trade-offs\" best matches In the \"incident diagnosis for payments failover war room:\" scenario, what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"reliability Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search brownout incident review: signal points to alert flood obscuring root-cause signal. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for search brownout incident review is mismatched to alert flood obscuring root-cause signal, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Reliability Scenarios, the best answer is \"The design for search brownout incident review is mismatched to alert flood obscuring root-cause signal, creating repeat reliability incidents\". It is the option most directly aligned to alert flood obscuring root-cause signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for search brownout incident review: signal points to alert flood\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident diagnosis for search brownout incident review:\", what should change first before wider rollout?",
          "options": [
            "Validate recovery with canary checks before full traffic restoration.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Reliability Scenarios: for After diagnosing \"incident diagnosis for search brownout incident review:\", what should change first before wider rollout, \"Validate recovery with canary checks before full traffic restoration\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"reliability Scenarios\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity control-plane outage: signal points to brownout controls missing for non-core traffic. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for identity control-plane outage is mismatched to brownout controls missing for non-core traffic, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for identity control-plane outage is mismatched to brownout controls missing for non-core traffic, creating repeat reliability incidents\" best matches identity control-plane outage by targeting brownout controls missing for non-core traffic and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for identity control-plane outage: signal points to brownout\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for identity control-plane outage:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Close with post-incident hardening items that prevent repeat cascade patterns.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "After diagnosing \"incident diagnosis for identity control-plane outage:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Close with post-incident hardening items that prevent repeat cascade patterns\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Start from \"reliability Scenarios\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for stream processor recovery effort: signal points to recovery sequencing causes second incident. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for stream processor recovery effort is mismatched to recovery sequencing causes second incident, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Reliability Scenarios: for stream processor recovery effort, \"The design for stream processor recovery effort is mismatched to recovery sequencing causes second incident, creating repeat reliability incidents\" is correct because it addresses recovery sequencing causes second incident and improves controllability.",
          "detailedExplanation": "If you keep \"incident diagnosis for stream processor recovery effort: signal points to recovery\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for stream processor recovery\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Stabilize first with admission and shedding, then restore critical paths in dependency order.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Stabilize first with admission and shedding, then restore critical paths in dependency order\" best matches In the \"incident diagnosis for stream processor recovery\" scenario, what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"reliability Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification delivery incident command: signal points to error-budget burn unaccounted during mitigation. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for notification delivery incident command is mismatched to error-budget burn unaccounted during mitigation, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Reliability Scenarios, the best answer is \"The design for notification delivery incident command is mismatched to error-budget burn unaccounted during mitigation, creating repeat reliability incidents\". It is the option most directly aligned to error-budget burn unaccounted during mitigation while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for notification delivery incident command: signal points to\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for notification delivery incident\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Use fault-domain aware triage to contain blast radius before broad failover."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Reliability Scenarios: for Now that \"incident diagnosis for notification delivery incident\" is diagnosed, which immediate adjustment best addresses the risk, \"Use fault-domain aware triage to contain blast radius before broad failover\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog consistency outage triage: signal points to retry storm during partial dependency outage. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for catalog consistency outage triage is mismatched to retry storm during partial dependency outage, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Catalog consistency outage triage is a two-step reliability decision. At stage 1, \"The design for catalog consistency outage triage is mismatched to retry storm during partial dependency outage, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around retry storm during partial dependency outage.",
          "detailedExplanation": "Use \"incident diagnosis for catalog consistency outage triage: signal points to retry storm\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for catalog consistency outage\", which immediate adjustment best addresses the risk?",
          "options": [
            "Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Reliability Scenarios, the best answer is \"Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for multi-tenant platform degradation event: signal points to zone failure with stale failover metadata. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for multi-tenant platform degradation event is mismatched to zone failure with stale failover metadata, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for multi-tenant platform degradation event is mismatched to zone failure with stale failover metadata, creating repeat reliability incidents\" best matches multi-tenant platform degradation event by targeting zone failure with stale failover metadata and lowering repeat risk.",
          "detailedExplanation": "Start from \"incident diagnosis for multi-tenant platform degradation event: signal points to zone\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for multi-tenant platform\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Promote only replicas meeting freshness constraints and document failback guards.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for multi-tenant platform\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Promote only replicas meeting freshness constraints and document failback guards\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "This prompt is really about \"reliability Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for mobile backend regional recovery: signal points to admission controls disabled under load spike. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for mobile backend regional recovery is mismatched to admission controls disabled under load spike, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Reliability Scenarios: for mobile backend regional recovery, \"The design for mobile backend regional recovery is mismatched to admission controls disabled under load spike, creating repeat reliability incidents\" is correct because it addresses admission controls disabled under load spike and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for mobile backend regional recovery: signal points to admission\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for mobile backend regional\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Activate graceful degradation early to preserve core user journeys.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Activate graceful degradation early to preserve core user journeys\" best matches Given the diagnosis in \"incident diagnosis for mobile backend regional\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"reliability Scenarios\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for global checkout incident bridge: signal points to degraded mode not activated for optional paths. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for global checkout incident bridge is mismatched to degraded mode not activated for optional paths, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Reliability Scenarios, the best answer is \"The design for global checkout incident bridge is mismatched to degraded mode not activated for optional paths, creating repeat reliability incidents\". It is the option most directly aligned to degraded mode not activated for optional paths while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for global checkout incident bridge: signal points to degraded mode\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for global checkout incident bridge:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Assign clear incident command roles and maintain a single decision timeline."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Reliability Scenarios: for \"incident diagnosis for global checkout incident bridge:\", which next change should be prioritized first, \"Assign clear incident command roles and maintain a single decision timeline\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"reliability Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for cross-region messaging outage response: signal points to replication lag ignored during promotion. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for cross-region messaging outage response is mismatched to replication lag ignored during promotion, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Cross-region messaging outage response is a two-step reliability decision. At stage 1, \"The design for cross-region messaging outage response is mismatched to replication lag ignored during promotion, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around replication lag ignored during promotion.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for cross-region messaging outage response: signal points to\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for cross-region messaging outage\", which next step is strongest under current constraints?",
          "options": [
            "Sequence recovery by invariant criticality, not by team convenience.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Reliability Scenarios, the best answer is \"Sequence recovery by invariant criticality, not by team convenience\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"reliability Scenarios\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for payments failover war room: signal points to conflicting runbook decisions across teams. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for payments failover war room is mismatched to conflicting runbook decisions across teams, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for payments failover war room is mismatched to conflicting runbook decisions across teams, creating repeat reliability incidents\" best matches payments failover war room by targeting conflicting runbook decisions across teams and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for payments failover war room: signal points to conflicting runbook\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident diagnosis for payments failover war room:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Track SLO/error-budget impact during mitigation to guide risk trade-offs.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "After diagnosing \"incident diagnosis for payments failover war room:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Track SLO/error-budget impact during mitigation to guide risk trade-offs\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"reliability Scenarios\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search brownout incident review: signal points to alert flood obscuring root-cause signal. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for search brownout incident review is mismatched to alert flood obscuring root-cause signal, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Reliability Scenarios: for search brownout incident review, \"The design for search brownout incident review is mismatched to alert flood obscuring root-cause signal, creating repeat reliability incidents\" is correct because it addresses alert flood obscuring root-cause signal and improves controllability.",
          "detailedExplanation": "If you keep \"incident diagnosis for search brownout incident review: signal points to alert flood\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for search brownout incident review:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Validate recovery with canary checks before full traffic restoration.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Validate recovery with canary checks before full traffic restoration\" best matches In the \"incident diagnosis for search brownout incident review:\" scenario, which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"reliability Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity control-plane outage: signal points to brownout controls missing for non-core traffic. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for identity control-plane outage is mismatched to brownout controls missing for non-core traffic, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Identity control-plane outage is a two-step reliability decision. At stage 1, \"The design for identity control-plane outage is mismatched to brownout controls missing for non-core traffic, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around brownout controls missing for non-core traffic.",
          "detailedExplanation": "Use \"incident diagnosis for identity control-plane outage: signal points to brownout\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for identity control-plane outage:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Close with post-incident hardening items that prevent repeat cascade patterns."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Reliability Scenarios, the best answer is \"Close with post-incident hardening items that prevent repeat cascade patterns\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"reliability Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for stream processor recovery effort: signal points to recovery sequencing causes second incident. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for stream processor recovery effort is mismatched to recovery sequencing causes second incident, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for stream processor recovery effort is mismatched to recovery sequencing causes second incident, creating repeat reliability incidents\" best matches stream processor recovery effort by targeting recovery sequencing causes second incident and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for stream processor recovery effort: signal points to recovery\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident diagnosis for stream processor recovery\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Stabilize first with admission and shedding, then restore critical paths in dependency order.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Now that \"incident diagnosis for stream processor recovery\" is diagnosed, which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Stabilize first with admission and shedding, then restore critical paths in dependency order\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification delivery incident command: signal points to error-budget burn unaccounted during mitigation. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for notification delivery incident command is mismatched to error-budget burn unaccounted during mitigation, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Reliability Scenarios: for notification delivery incident command, \"The design for notification delivery incident command is mismatched to error-budget burn unaccounted during mitigation, creating repeat reliability incidents\" is correct because it addresses error-budget burn unaccounted during mitigation and improves controllability.",
          "detailedExplanation": "If you keep \"incident diagnosis for notification delivery incident command: signal points to\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for notification delivery incident\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Use fault-domain aware triage to contain blast radius before broad failover.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use fault-domain aware triage to contain blast radius before broad failover\" best matches In the \"incident diagnosis for notification delivery incident\" scenario, what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"reliability Scenarios\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for catalog consistency outage triage: signal points to retry storm during partial dependency outage. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for catalog consistency outage triage is mismatched to retry storm during partial dependency outage, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Reliability Scenarios, the best answer is \"The design for catalog consistency outage triage is mismatched to retry storm during partial dependency outage, creating repeat reliability incidents\". It is the option most directly aligned to retry storm during partial dependency outage while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for catalog consistency outage triage: signal points to retry storm\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for catalog consistency outage\" scenario, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Reliability Scenarios: for In the \"incident diagnosis for catalog consistency outage\" scenario, what should change first before wider rollout, \"Coordinate timeout/retry policy changes with breaker thresholds to avoid new overload\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Start from \"reliability Scenarios\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for multi-tenant platform degradation event: signal points to zone failure with stale failover metadata. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for multi-tenant platform degradation event is mismatched to zone failure with stale failover metadata, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Multi-tenant platform degradation event is a two-step reliability decision. At stage 1, \"The design for multi-tenant platform degradation event is mismatched to zone failure with stale failover metadata, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around zone failure with stale failover metadata.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for multi-tenant platform degradation event: signal points to zone\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for multi-tenant platform\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Promote only replicas meeting freshness constraints and document failback guards."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Reliability Scenarios, the best answer is \"Promote only replicas meeting freshness constraints and document failback guards\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"reliability Scenarios\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for mobile backend regional recovery: signal points to admission controls disabled under load spike. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for mobile backend regional recovery is mismatched to admission controls disabled under load spike, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for mobile backend regional recovery is mismatched to admission controls disabled under load spike, creating repeat reliability incidents\" best matches mobile backend regional recovery by targeting admission controls disabled under load spike and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for mobile backend regional recovery: signal points to admission\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for mobile backend regional\", which next step is strongest under current constraints?",
          "options": [
            "Activate graceful degradation early to preserve core user journeys.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "With root cause identified for \"incident diagnosis for mobile backend regional\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Activate graceful degradation early to preserve core user journeys\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for global checkout incident bridge: signal points to degraded mode not activated for optional paths. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for global checkout incident bridge is mismatched to degraded mode not activated for optional paths, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Reliability Scenarios: for global checkout incident bridge, \"The design for global checkout incident bridge is mismatched to degraded mode not activated for optional paths, creating repeat reliability incidents\" is correct because it addresses degraded mode not activated for optional paths and improves controllability.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for global checkout incident bridge: signal points to degraded mode\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for global checkout incident bridge:\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Assign clear incident command roles and maintain a single decision timeline.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Assign clear incident command roles and maintain a single decision timeline\" best matches For \"incident diagnosis for global checkout incident bridge:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"reliability Scenarios\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for cross-region messaging outage response: signal points to replication lag ignored during promotion. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for cross-region messaging outage response is mismatched to replication lag ignored during promotion, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Reliability Scenarios, the best answer is \"The design for cross-region messaging outage response is mismatched to replication lag ignored during promotion, creating repeat reliability incidents\". It is the option most directly aligned to replication lag ignored during promotion while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"incident diagnosis for cross-region messaging outage response: signal points to\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for cross-region messaging outage\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Sequence recovery by invariant criticality, not by team convenience.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Reliability Scenarios: for Given the diagnosis in \"incident diagnosis for cross-region messaging outage\", what should change first before wider rollout, \"Sequence recovery by invariant criticality, not by team convenience\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "This prompt is really about \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for payments failover war room: signal points to conflicting runbook decisions across teams. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for payments failover war room is mismatched to conflicting runbook decisions across teams, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Payments failover war room is a two-step reliability decision. At stage 1, \"The design for payments failover war room is mismatched to conflicting runbook decisions across teams, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around conflicting runbook decisions across teams.",
          "detailedExplanation": "Use \"incident diagnosis for payments failover war room: signal points to conflicting runbook\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for payments failover war room:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Track SLO/error-budget impact during mitigation to guide risk trade-offs."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Reliability Scenarios, the best answer is \"Track SLO/error-budget impact during mitigation to guide risk trade-offs\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The decision turns on \"reliability Scenarios\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search brownout incident review: signal points to alert flood obscuring root-cause signal. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for search brownout incident review is mismatched to alert flood obscuring root-cause signal, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for search brownout incident review is mismatched to alert flood obscuring root-cause signal, creating repeat reliability incidents\" best matches search brownout incident review by targeting alert flood obscuring root-cause signal and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for search brownout incident review: signal points to alert flood\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for search brownout incident review:\", what is the highest-leverage change to make now?",
          "options": [
            "Validate recovery with canary checks before full traffic restoration.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Using the diagnosis from \"incident diagnosis for search brownout incident review:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Validate recovery with canary checks before full traffic restoration\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for identity control-plane outage: signal points to brownout controls missing for non-core traffic. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for identity control-plane outage is mismatched to brownout controls missing for non-core traffic, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Reliability Scenarios, the best answer is \"The design for identity control-plane outage is mismatched to brownout controls missing for non-core traffic, creating repeat reliability incidents\". It is the option most directly aligned to brownout controls missing for non-core traffic while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for identity control-plane outage: signal points to brownout\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for identity control-plane outage:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Close with post-incident hardening items that prevent repeat cascade patterns.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Reliability Scenarios: for Using the diagnosis from \"incident diagnosis for identity control-plane outage:\", what is the highest-leverage change to make now, \"Close with post-incident hardening items that prevent repeat cascade patterns\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"reliability Scenarios\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-061",
      "type": "multi-select",
      "question": "Identify every correct choice: which indicators most directly reveal cross-domain blast radius.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Reliability Scenarios, Identify every correct choice: which indicators most directly reveal cross-domain blast radius needs layered controls, not one silver bullet. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The decision turns on \"indicators most directly reveal cross-domain blast radius? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-062",
      "type": "multi-select",
      "question": "Identify every correct choice: which controls reduce hidden single points of failure.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Identify every correct choice: which controls reduce hidden single points of failure, the highest-signal answer is a bundle of controls. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Start from \"controls reduce hidden single points of failure? (Select all that apply)\", then pressure-test the result against the options. Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-063",
      "type": "multi-select",
      "question": "Identify every correct choice: during partial failures, which practices improve diagnosis quality.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Reliability Scenarios: The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The key clue in this question is \"during partial failures, which practices improve diagnosis quality? (Select all that\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-064",
      "type": "multi-select",
      "question": "Identify every correct choice: what belongs in a useful dependency failure taxonomy.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Identify every correct choice: what belongs in a useful dependency failure taxonomy is intentionally multi-dimensional in Reliability Scenarios. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The core signal here is \"belongs in a useful dependency failure taxonomy? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-065",
      "type": "multi-select",
      "question": "Identify every correct choice: which patterns limit correlated failures across zones.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Reliability Scenarios, Identify every correct choice: which patterns limit correlated failures across zones needs layered controls, not one silver bullet. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "If you keep \"patterns limit correlated failures across zones? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-066",
      "type": "multi-select",
      "question": "Identify every correct choice: which runbook elements increase incident execution reliability.",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Identify every correct choice: which runbook elements increase incident execution reliability, the highest-signal answer is a bundle of controls. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "This prompt is really about \"runbook elements increase incident execution reliability? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-067",
      "type": "multi-select",
      "question": "Identify every correct choice: which signals should trigger graceful isolation first.",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Reliability Scenarios: The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Use \"signals should trigger graceful isolation first? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-068",
      "type": "multi-select",
      "question": "Identify every correct choice: which architectural choices help contain tenant-induced overload.",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Identify every correct choice: which architectural choices help contain tenant-induced overload is intentionally multi-dimensional in Reliability Scenarios. The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Read this as a scenario about \"architectural choices help contain tenant-induced overload? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-069",
      "type": "multi-select",
      "question": "Identify every correct choice: for reliability policies, which items should be explicit per endpoint.",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Reliability Scenarios, Identify every correct choice: for reliability policies, which items should be explicit per endpoint needs layered controls, not one silver bullet. The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The decision turns on \"for reliability policies, which items should be explicit per endpoint? (Select all that\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-070",
      "type": "multi-select",
      "question": "Identify every correct choice: which anti-patterns commonly enlarge outage blast radius.",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Reliability Scenarios: The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Use \"anti-patterns commonly enlarge outage blast radius? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-071",
      "type": "multi-select",
      "question": "Identify every correct choice: what improves confidence in failover assumptions.",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Identify every correct choice: what improves confidence in failover assumptions is intentionally multi-dimensional in Reliability Scenarios. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "This prompt is really about \"improves confidence in failover assumptions? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-072",
      "type": "multi-select",
      "question": "Identify every correct choice: which data is essential when classifying partial vs fail-stop incidents.",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Reliability Scenarios, Identify every correct choice: which data is essential when classifying partial vs fail-stop incidents needs layered controls, not one silver bullet. The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "If you keep \"data is essential when classifying partial vs fail-stop incidents? (Select all that\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-073",
      "type": "multi-select",
      "question": "Identify every correct choice: which controls improve safety when control-plane health is uncertain.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Identify every correct choice: which controls improve safety when control-plane health is uncertain, the highest-signal answer is a bundle of controls. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The core signal here is \"controls improve safety when control-plane health is uncertain? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-074",
      "type": "multi-select",
      "question": "Identify every correct choice: for critical writes, which guardrails reduce corruption risk under faults.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Reliability Scenarios: The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The key clue in this question is \"for critical writes, which guardrails reduce corruption risk under faults? (Select all\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-075",
      "type": "multi-select",
      "question": "Identify every correct choice: which recurring reviews keep reliability boundaries accurate over time.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Identify every correct choice: which recurring reviews keep reliability boundaries accurate over time is intentionally multi-dimensional in Reliability Scenarios. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Start from \"recurring reviews keep reliability boundaries accurate over time? (Select all that\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-076",
      "type": "multi-select",
      "question": "Identify every correct choice: which decisions help teams align on reliability trade-offs during incidents.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Reliability Scenarios, Identify every correct choice: which decisions help teams align on reliability trade-offs during incidents needs layered controls, not one silver bullet. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The decision turns on \"decisions help teams align on reliability trade-offs during incidents? (Select all that\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-077",
      "type": "multi-select",
      "question": "Identify every correct choice: what evidence best shows a mitigation reduced recurrence risk.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Identify every correct choice: what evidence best shows a mitigation reduced recurrence risk, the highest-signal answer is a bundle of controls. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Read this as a scenario about \"evidence best shows a mitigation reduced recurrence risk? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-078",
      "type": "numeric-input",
      "question": "From a service processes 4,200,000 requests/day and 0.22% violate reliability SLO, how many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for From a service processes 4,200,000 requests/day and 0 gives 9240 requests. In interview pacing, hitting this value within +/-3% is the pass condition.",
      "detailedExplanation": "Use \"service processes 4,200,000 requests/day and 0\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 4,200 and 000 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-079",
      "type": "numeric-input",
      "question": "From incident queue receives 1,800 items/min and drains 2,050 items/min, net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "Reliability Scenarios expects quick quantitative triage: From incident queue receives 1,800 items/min and drains 2,050 items/min, net drain rate evaluates to 250 items/min. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "This prompt is really about \"incident queue receives 1,800 items/min and drains 2,050 items/min\". Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 1,800 and 2,050 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-080",
      "type": "numeric-input",
      "question": "From retry policy adds 0.35 extra attempts per request at 60,000 req/sec, effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for From retry policy adds 0: 81000 attempts/sec. Answers within +/-2% show correct directional reasoning for Reliability Scenarios.",
      "detailedExplanation": "This prompt is really about \"retry policy adds 0\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 0.35 and 60,000 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-081",
      "type": "numeric-input",
      "question": "From failover takes 18 seconds and happens 21 times/day, total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for From failover takes 18 seconds and happens 21 times/day, total failover seconds/day gives 378 seconds. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Use \"failover takes 18 seconds and happens 21 times/day\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 18 seconds and 21 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-082",
      "type": "numeric-input",
      "question": "From target p99 latency is 700ms; observed p99 is 980ms, percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Reliability Scenarios expects quick quantitative triage: From target p99 latency is 700ms; observed p99 is 980ms, percent over target evaluates to 40 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "The core signal here is \"target p99 latency is 700ms\". Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 700ms and 980ms in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-083",
      "type": "numeric-input",
      "question": "How would you answer this: if 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For How would you answer this: if 31% of 120,000 requests/min are critical-path, how many critical requests/min, the computed target in Reliability Scenarios is 37200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "If you keep \"if 31% of 120,000 requests/min are critical-path, how many critical requests/min\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 31 and 120,000 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-084",
      "type": "numeric-input",
      "question": "From error rate drops from 1.2% to 0.3%, percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for From error rate drops from 1: 75 %. Answers within +/-30% show correct directional reasoning for Reliability Scenarios.",
      "detailedExplanation": "Start from \"error rate drops from 1\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 1.2 and 0.3 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-085",
      "type": "numeric-input",
      "question": "From a 7-node quorum system requires majority writes, minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for From a 7-node quorum system requires majority writes, minimum acknowledgements required gives 4 acks. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "The key clue in this question is \"7-node quorum system requires majority writes\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 7 and 4 should be normalized first so downstream reasoning stays consistent. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-086",
      "type": "numeric-input",
      "question": "From backlog is 48,000 tasks and net drain is 320 tasks/min, minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Reliability Scenarios expects quick quantitative triage: From backlog is 48,000 tasks and net drain is 320 tasks/min, minutes to clear backlog evaluates to 150 minutes. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "Read this as a scenario about \"backlog is 48,000 tasks and net drain is 320 tasks/min\". Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 48,000 and 320 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-087",
      "type": "numeric-input",
      "question": "From a system with 14 zones has 2 unavailable, what percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For From a system with 14 zones has 2 unavailable, what percent remain available, the computed target in Reliability Scenarios is 85.71 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "The decision turns on \"system with 14 zones has 2 unavailable\". Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 14 and 2 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-088",
      "type": "numeric-input",
      "question": "From mTTR improved from 45 min to 30 min, percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for From mTTR improved from 45 min to 30 min, percent reduction: 33.33 %. Answers within +/-30% show correct directional reasoning for Reliability Scenarios.",
      "detailedExplanation": "This prompt is really about \"mTTR improved from 45 min to 30 min\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 45 min and 30 min in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-089",
      "type": "numeric-input",
      "question": "How would you answer this: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for How would you answer this: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day gives 225000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "Use \"if 9% of 2,500,000 daily operations need manual recovery checks, checks/day\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 9 and 2,500 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle. Focus on reliability scenarios tradeoffs.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Reliability Scenarios should start with Detect and scope affected fault domains and end with Validate recovery and harden recurrence defenses. Order a reliability response lifecycle rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "If you keep \"order a reliability response lifecycle\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-091",
      "type": "ordering",
      "question": "For reliability scenarios, order from lowest to highest reliability risk.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For reliability scenarios, order from lowest to highest reliability risk, the correct ordering runs from Isolated dependency with fallback and budget to Implicit dependency with no failure policy. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The core signal here is \"order from lowest to highest reliability risk\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-092",
      "type": "ordering",
      "question": "Within reliability scenarios, order failover safety steps.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Reliability Scenarios emphasizes safe recovery order. Beginning at Verify candidate health and freshness and finishing at Run failback readiness checks before restoration keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Use \"order failover safety steps\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-093",
      "type": "ordering",
      "question": "In this reliability scenarios context, order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: No admission limits must happen before Priority-aware shedding plus per-domain concurrency bounds. That ordering matches incident-safe flow in Reliability Scenarios.",
      "detailedExplanation": "This prompt is really about \"order by increasing overload-protection strength\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-094",
      "type": "ordering",
      "question": "Considering reliability scenarios, order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Reliability Scenarios should start with Select recovery point by RPO target and end with Promote and re-enable writes with monitoring. Considering reliability scenarios, order data recovery execution rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The decision turns on \"order data recovery execution\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-095",
      "type": "ordering",
      "question": "From a reliability scenarios viewpoint, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a reliability scenarios viewpoint, order reliability operations loop, the correct ordering runs from Define SLIs tied to user impact to Review incidents and close corrective actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Read this as a scenario about \"order reliability operations loop\". Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-096",
      "type": "ordering",
      "question": "Order by growing blast radius impact. (Reliability Scenarios context)",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Reliability Scenarios emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The key clue in this question is \"order by increasing blast radius\". Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-097",
      "type": "ordering",
      "question": "Order retry-policy maturity. Use a reliability scenarios perspective.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Fixed immediate retries must happen before Jittered backoff with retry budgets and telemetry. That ordering matches incident-safe flow in Reliability Scenarios.",
      "detailedExplanation": "Start from \"order retry-policy maturity\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-098",
      "type": "ordering",
      "question": "Order degradation sophistication. Focus on reliability scenarios tradeoffs.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Reliability Scenarios should start with Undocumented ad hoc fallback and end with Automated policy-driven degradation with user semantics. Order degradation sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "If you keep \"order degradation sophistication\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-099",
      "type": "ordering",
      "question": "For reliability scenarios, order incident command rigor.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For reliability scenarios, order incident command rigor, the correct ordering runs from Ad hoc responders with no roles to Role-defined operations plus decision log and action tracking. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The core signal here is \"order incident command rigor\". Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    },
    {
      "id": "rel-scn-100",
      "type": "ordering",
      "question": "Within reliability scenarios, order reliability validation confidence.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Reliability Scenarios emphasizes safe recovery order. Beginning at Single success in staging and finishing at Sustained recovery plus recurrence drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Use \"order reliability validation confidence\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["reliability", "reliability-scenarios"],
      "difficulty": "principal"
    }
  ]
}
