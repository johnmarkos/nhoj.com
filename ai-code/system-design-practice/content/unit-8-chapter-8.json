{
  "unit": 8,
  "unitTitle": "Consistency & Coordination",
  "chapter": 8,
  "chapterTitle": "Consistency & Coordination Scenarios",
  "chapterDescription": "Integrated scenarios combining consistency models, quorum paths, ordering, transactions, coordination, consensus, and conflict convergence decisions.",
  "problems": [
    {
      "id": "cc-scn-001",
      "type": "multiple-choice",
      "question": "Scenario: global checkout under partition + inventory races. Dominant risk is stale reads on invariant-critical path. Which next move is strongest? Recent incident timeline confirms this is reproducible under load.",
      "options": [
        "Route invariant-critical reads to leader/quorum and keep tolerant reads bounded-stale with explicit semantics.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-002",
      "type": "multiple-choice",
      "question": "Scenario: multi-region profile sync with stale follower reads. Dominant risk is causal order violations between dependent events. Which next move is strongest? Business impact is high and correctness-first for critical entities.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Introduce causal/version metadata checks and deterministic merge policy for concurrent updates."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-003",
      "type": "multiple-choice",
      "question": "Scenario: chat timeline with out-of-order replay. Dominant risk is split-brain/stale-leader write acceptance. Which next move is strongest? A reversible first mitigation is required by operations.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Enforce fencing/term validation at write sinks to reject stale leaders/owners.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-004",
      "type": "multiple-choice",
      "question": "Scenario: ticket booking with duplicate callback retries. Dominant risk is unsafe distributed transaction side effects. Which next move is strongest? Current telemetry now exposes conflict and lag signals.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Split critical transaction boundary from side effects using outbox/saga with idempotent handlers.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-005",
      "type": "multiple-choice",
      "question": "Scenario: payment ledger with leader churn during peak. Dominant risk is merge policy causing silent data loss. Which next move is strongest? The team must avoid blanket strongest consistency everywhere.",
      "options": [
        "Replace global LWW with domain-specific conflict policy plus escalation for irreconcilable fields.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-006",
      "type": "multiple-choice",
      "question": "Scenario: feature-flag rollout with split-brain control plane. Dominant risk is quorum/membership reconfiguration risk. Which next move is strongest? Client retries and replays are common in this traffic pattern.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Stage membership/quorum changes with joint-consensus safety and rollback checkpoints."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-007",
      "type": "multiple-choice",
      "question": "Scenario: offline-first document edits reconciling post-outage. Dominant risk is lock liveness collapse under contention. Which next move is strongest? Cross-region latency variance is non-trivial in this topology.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Apply jittered retries, scoped lock granularity, and fairness controls to restore liveness safely.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-008",
      "type": "multiple-choice",
      "question": "Scenario: tenant migration with membership reconfiguration. Dominant risk is replay/duplicate delivery without idempotent guards. Which next move is strongest? User-facing trust is degraded by silent inconsistencies.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Add dedupe keys + monotonic version guards so replay cannot overwrite newer state.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-009",
      "type": "multiple-choice",
      "question": "Scenario: shipment status stream with delayed compensation events. Dominant risk is compensation ordering ambiguity. Which next move is strongest? Current runbooks do not map guarantees per endpoint clearly.",
      "options": [
        "Define compensation ordering rules and prevent late compensations from corrupting current state.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-010",
      "type": "multiple-choice",
      "question": "Scenario: quota enforcement service with lock contention and lag. Dominant risk is consistency SLO breach with cost/latency pressure. Which next move is strongest? Leadership requested explicit trade-off rationale postmortem.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Use endpoint-tiered consistency policy with explicit degraded mode when SLO/cost constraints conflict."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-011",
      "type": "multiple-choice",
      "question": "Scenario: identity claim replication during regional failover. Dominant risk is stale reads on invariant-critical path. Which next move is strongest? The path mixes critical and non-critical data classes.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Route invariant-critical reads to leader/quorum and keep tolerant reads bounded-stale with explicit semantics.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-012",
      "type": "multiple-choice",
      "question": "Scenario: notification state convergence after retry storm. Dominant risk is causal order violations between dependent events. Which next move is strongest? Partial outages have previously caused cascading retries.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Introduce causal/version metadata checks and deterministic merge policy for concurrent updates.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-013",
      "type": "multiple-choice",
      "question": "Scenario: fraud case workflow with conflicting updates. Dominant risk is split-brain/stale-leader write acceptance. Which next move is strongest? Some workflows can degrade, but invariants cannot.",
      "options": [
        "Enforce fencing/term validation at write sinks to reject stale leaders/owners.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-014",
      "type": "multiple-choice",
      "question": "Scenario: catalog publish with quorum degradation. Dominant risk is unsafe distributed transaction side effects. Which next move is strongest? The fix should reduce recurrence, not just immediate symptoms.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Split critical transaction boundary from side effects using outbox/saga with idempotent handlers."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-015",
      "type": "multiple-choice",
      "question": "Scenario: support ticket merge path under stale-owner writes. Dominant risk is merge policy causing silent data loss. Which next move is strongest? Canary rollout with rollback gates is mandatory.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Replace global LWW with domain-specific conflict policy plus escalation for irreconcilable fields.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-016",
      "type": "multiple-choice",
      "question": "Scenario: ad campaign config sync under election storms. Dominant risk is quorum/membership reconfiguration risk. Which next move is strongest? Downstream consumers currently assume stronger guarantees than provided.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Stage membership/quorum changes with joint-consensus safety and rollback checkpoints.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-017",
      "type": "multiple-choice",
      "question": "Scenario: order state machine with partial commit visibility. Dominant risk is lock liveness collapse under contention. Which next move is strongest? Failover drills exposed timing/ordering edge cases.",
      "options": [
        "Apply jittered retries, scoped lock granularity, and fairness controls to restore liveness safely.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-018",
      "type": "multiple-choice",
      "question": "Scenario: device sync API with clock-skewed writes. Dominant risk is replay/duplicate delivery without idempotent guards. Which next move is strongest? The system includes both synchronous and async paths.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Add dedupe keys + monotonic version guards so replay cannot overwrite newer state."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-019",
      "type": "multiple-choice",
      "question": "Scenario: subscription billing close with deadlock spikes. Dominant risk is compensation ordering ambiguity. Which next move is strongest? Storage and queue semantics differ across components.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Define compensation ordering rules and prevent late compensations from corrupting current state.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-020",
      "type": "multiple-choice",
      "question": "Scenario: moderation queue ownership with fencing gaps. Dominant risk is consistency SLO breach with cost/latency pressure. Which next move is strongest? Coordination and merge policies are currently under-documented.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Use endpoint-tiered consistency policy with explicit degraded mode when SLO/cost constraints conflict.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-021",
      "type": "multiple-choice",
      "question": "Scenario: global checkout under partition + inventory races. Dominant risk is stale reads on invariant-critical path. Which next move is strongest? Recent schema changes increased conflict surface area.",
      "options": [
        "Route invariant-critical reads to leader/quorum and keep tolerant reads bounded-stale with explicit semantics.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-022",
      "type": "multiple-choice",
      "question": "Scenario: multi-region profile sync with stale follower reads. Dominant risk is causal order violations between dependent events. Which next move is strongest? A stale-owner write recently caused customer-visible regressions.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Introduce causal/version metadata checks and deterministic merge policy for concurrent updates."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-023",
      "type": "multiple-choice",
      "question": "Scenario: chat timeline with out-of-order replay. Dominant risk is split-brain/stale-leader write acceptance. Which next move is strongest? Support teams need explainable resolution behavior.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Enforce fencing/term validation at write sinks to reject stale leaders/owners.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-024",
      "type": "multiple-choice",
      "question": "Scenario: ticket booking with duplicate callback retries. Dominant risk is unsafe distributed transaction side effects. Which next move is strongest? Throughput must remain acceptable after mitigation.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Split critical transaction boundary from side effects using outbox/saga with idempotent handlers.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-025",
      "type": "multiple-choice",
      "question": "Scenario: payment ledger with leader churn during peak. Dominant risk is merge policy causing silent data loss. Which next move is strongest? Operator tooling needs clearer safety signals after rollout.",
      "options": [
        "Replace global LWW with domain-specific conflict policy plus escalation for irreconcilable fields.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-026",
      "type": "multiple-choice",
      "question": "Scenario: feature-flag rollout with split-brain control plane. Dominant risk is quorum/membership reconfiguration risk. Which next move is strongest? The issue spans quorum, ordering, and merge boundaries.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Stage membership/quorum changes with joint-consensus safety and rollback checkpoints."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-027",
      "type": "multiple-choice",
      "question": "Scenario: offline-first document edits reconciling post-outage. Dominant risk is lock liveness collapse under contention. Which next move is strongest? Transient partitions are expected in this environment.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Apply jittered retries, scoped lock granularity, and fairness controls to restore liveness safely.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-028",
      "type": "multiple-choice",
      "question": "Scenario: tenant migration with membership reconfiguration. Dominant risk is replay/duplicate delivery without idempotent guards. Which next move is strongest? Replay backlog growth correlates with anomaly spikes.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Add dedupe keys + monotonic version guards so replay cannot overwrite newer state.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-029",
      "type": "multiple-choice",
      "question": "Scenario: shipment status stream with delayed compensation events. Dominant risk is compensation ordering ambiguity. Which next move is strongest? Critical-path latency budget remains strict.",
      "options": [
        "Define compensation ordering rules and prevent late compensations from corrupting current state.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-030",
      "type": "multiple-choice",
      "question": "Scenario: quota enforcement service with lock contention and lag. Dominant risk is consistency SLO breach with cost/latency pressure. Which next move is strongest? Compensating workflows are present but not uniformly safe.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Use endpoint-tiered consistency policy with explicit degraded mode when SLO/cost constraints conflict."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-031",
      "type": "multiple-choice",
      "question": "Scenario: identity claim replication during regional failover. Dominant risk is stale reads on invariant-critical path. Which next move is strongest? Ownership boundaries between services are blurry today.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Route invariant-critical reads to leader/quorum and keep tolerant reads bounded-stale with explicit semantics.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-032",
      "type": "multiple-choice",
      "question": "Scenario: notification state convergence after retry storm. Dominant risk is causal order violations between dependent events. Which next move is strongest? Consistency SLOs now include freshness and conflict age.",
      "options": [
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Introduce causal/version metadata checks and deterministic merge policy for concurrent updates.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation."
      ],
      "correct": 1,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-033",
      "type": "multiple-choice",
      "question": "Scenario: fraud case workflow with conflicting updates. Dominant risk is split-brain/stale-leader write acceptance. Which next move is strongest? Post-incident actions require deterministic validation criteria.",
      "options": [
        "Enforce fencing/term validation at write sinks to reject stale leaders/owners.",
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy."
      ],
      "correct": 0,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-034",
      "type": "multiple-choice",
      "question": "Scenario: catalog publish with quorum degradation. Dominant risk is unsafe distributed transaction side effects. Which next move is strongest? The design must handle both steady-state and failure mode.",
      "options": [
        "Scale globally and ignore bottleneck-specific consistency failure mode.",
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Split critical transaction boundary from side effects using outbox/saga with idempotent handlers."
      ],
      "correct": 3,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-035",
      "type": "multiple-choice",
      "question": "Scenario: support ticket merge path under stale-owner writes. Dominant risk is merge policy causing silent data loss. Which next move is strongest? This scenario is used as a template for similar systems.",
      "options": [
        "Force one universal strongest setting without endpoint differentiation.",
        "Rely on retries/manual reconciliation as primary correctness strategy.",
        "Replace global LWW with domain-specific conflict policy plus escalation for irreconcilable fields.",
        "Scale globally and ignore bottleneck-specific consistency failure mode."
      ],
      "correct": 2,
      "explanation": "Integrated scenarios require bottleneck-first consistency controls that preserve invariants while managing liveness/cost trade-offs."
    },
    {
      "id": "cc-scn-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat timeline with out-of-order replay. Signal points to unsafe distributed transaction side effects. What is the primary diagnosis?",
          "options": [
            "The current design in chat timeline with out-of-order replay mismatches unsafe distributed transaction side effects, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths."
          ],
          "correct": 0,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change while preserving critical-path correctness?",
          "options": [
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Replace global LWW with domain-specific conflict policy plus escalation for irreconcilable fields."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticket booking with duplicate callback retries. Signal points to merge policy causing silent data loss. What is the primary diagnosis?",
          "options": [
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in ticket booking with duplicate callback retries mismatches merge policy causing silent data loss, causing recurring consistency/co-ordination failures."
          ],
          "correct": 3,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change during active failover?",
          "options": [
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Stage membership/quorum changes with joint-consensus safety and rollback checkpoints.",
            "Disable safety checks temporarily to speed incident recovery."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: payment ledger with leader churn during peak. Signal points to quorum/membership reconfiguration risk. What is the primary diagnosis?",
          "options": [
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in payment ledger with leader churn during peak mismatches quorum/membership reconfiguration risk, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists."
          ],
          "correct": 2,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with bounded latency budget?",
          "options": [
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Apply jittered retries, scoped lock granularity, and fairness controls to restore liveness safely.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: feature-flag rollout with split-brain control plane. Signal points to lock liveness collapse under contention. What is the primary diagnosis?",
          "options": [
            "One policy is always optimal for all entities and paths.",
            "The current design in feature-flag rollout with split-brain control plane mismatches lock liveness collapse under contention, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps."
          ],
          "correct": 1,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change under replay-heavy recovery?",
          "options": [
            "Add dedupe keys + monotonic version guards so replay cannot overwrite newer state.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: offline-first document edits reconciling post-outage. Signal points to replay/duplicate delivery without idempotent guards. What is the primary diagnosis?",
          "options": [
            "The current design in offline-first document edits reconciling post-outage mismatches replay/duplicate delivery without idempotent guards, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths."
          ],
          "correct": 0,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change while minimizing split-brain risk?",
          "options": [
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Define compensation ordering rules and prevent late compensations from corrupting current state."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: tenant migration with membership reconfiguration. Signal points to compensation ordering ambiguity. What is the primary diagnosis?",
          "options": [
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in tenant migration with membership reconfiguration mismatches compensation ordering ambiguity, causing recurring consistency/co-ordination failures."
          ],
          "correct": 3,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with explicit degraded semantics?",
          "options": [
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Use endpoint-tiered consistency policy with explicit degraded mode when SLO/cost constraints conflict.",
            "Disable safety checks temporarily to speed incident recovery."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: shipment status stream with delayed compensation events. Signal points to consistency SLO breach with cost/latency pressure. What is the primary diagnosis?",
          "options": [
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in shipment status stream with delayed compensation events mismatches consistency SLO breach with cost/latency pressure, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists."
          ],
          "correct": 2,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change under high concurrency contention?",
          "options": [
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Route invariant-critical reads to leader/quorum and keep tolerant reads bounded-stale with explicit semantics.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: quota enforcement service with lock contention and lag. Signal points to stale reads on invariant-critical path. What is the primary diagnosis?",
          "options": [
            "One policy is always optimal for all entities and paths.",
            "The current design in quota enforcement service with lock contention and lag mismatches stale reads on invariant-critical path, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps."
          ],
          "correct": 1,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change before wide rollout?",
          "options": [
            "Introduce causal/version metadata checks and deterministic merge policy for concurrent updates.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: identity claim replication during regional failover. Signal points to causal order violations between dependent events. What is the primary diagnosis?",
          "options": [
            "The current design in identity claim replication during regional failover mismatches causal order violations between dependent events, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths."
          ],
          "correct": 0,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with deterministic rollback options?",
          "options": [
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Enforce fencing/term validation at write sinks to reject stale leaders/owners."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: notification state convergence after retry storm. Signal points to split-brain/stale-leader write acceptance. What is the primary diagnosis?",
          "options": [
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in notification state convergence after retry storm mismatches split-brain/stale-leader write acceptance, causing recurring consistency/co-ordination failures."
          ],
          "correct": 3,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change while reducing silent data loss?",
          "options": [
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Split critical transaction boundary from side effects using outbox/saga with idempotent handlers.",
            "Disable safety checks temporarily to speed incident recovery."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: fraud case workflow with conflicting updates. Signal points to unsafe distributed transaction side effects. What is the primary diagnosis?",
          "options": [
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in fraud case workflow with conflicting updates mismatches unsafe distributed transaction side effects, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists."
          ],
          "correct": 2,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change under mixed criticality workloads?",
          "options": [
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Replace global LWW with domain-specific conflict policy plus escalation for irreconcilable fields.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: catalog publish with quorum degradation. Signal points to merge policy causing silent data loss. What is the primary diagnosis?",
          "options": [
            "One policy is always optimal for all entities and paths.",
            "The current design in catalog publish with quorum degradation mismatches merge policy causing silent data loss, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps."
          ],
          "correct": 1,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change during membership churn?",
          "options": [
            "Stage membership/quorum changes with joint-consensus safety and rollback checkpoints.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: support ticket merge path under stale-owner writes. Signal points to quorum/membership reconfiguration risk. What is the primary diagnosis?",
          "options": [
            "The current design in support ticket merge path under stale-owner writes mismatches quorum/membership reconfiguration risk, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths."
          ],
          "correct": 0,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with stale-read pressure on key endpoints?",
          "options": [
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Apply jittered retries, scoped lock granularity, and fairness controls to restore liveness safely."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ad campaign config sync under election storms. Signal points to lock liveness collapse under contention. What is the primary diagnosis?",
          "options": [
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in ad campaign config sync under election storms mismatches lock liveness collapse under contention, causing recurring consistency/co-ordination failures."
          ],
          "correct": 3,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change under compensation-order uncertainty?",
          "options": [
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Add dedupe keys + monotonic version guards so replay cannot overwrite newer state.",
            "Disable safety checks temporarily to speed incident recovery."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: order state machine with partial commit visibility. Signal points to replay/duplicate delivery without idempotent guards. What is the primary diagnosis?",
          "options": [
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in order state machine with partial commit visibility mismatches replay/duplicate delivery without idempotent guards, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists."
          ],
          "correct": 2,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with strong audit explainability?",
          "options": [
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Define compensation ordering rules and prevent late compensations from corrupting current state.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: device sync API with clock-skewed writes. Signal points to compensation ordering ambiguity. What is the primary diagnosis?",
          "options": [
            "One policy is always optimal for all entities and paths.",
            "The current design in device sync API with clock-skewed writes mismatches compensation ordering ambiguity, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps."
          ],
          "correct": 1,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change while preserving throughput guardrails?",
          "options": [
            "Use endpoint-tiered consistency policy with explicit degraded mode when SLO/cost constraints conflict.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: subscription billing close with deadlock spikes. Signal points to consistency SLO breach with cost/latency pressure. What is the primary diagnosis?",
          "options": [
            "The current design in subscription billing close with deadlock spikes mismatches consistency SLO breach with cost/latency pressure, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths."
          ],
          "correct": 0,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change during partial control-plane outage?",
          "options": [
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Route invariant-critical reads to leader/quorum and keep tolerant reads bounded-stale with explicit semantics."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: moderation queue ownership with fencing gaps. Signal points to stale reads on invariant-critical path. What is the primary diagnosis?",
          "options": [
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in moderation queue ownership with fencing gaps mismatches stale reads on invariant-critical path, causing recurring consistency/co-ordination failures."
          ],
          "correct": 3,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with constrained operator bandwidth?",
          "options": [
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Introduce causal/version metadata checks and deterministic merge policy for concurrent updates.",
            "Disable safety checks temporarily to speed incident recovery."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: global checkout under partition + inventory races. Signal points to causal order violations between dependent events. What is the primary diagnosis?",
          "options": [
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in global checkout under partition + inventory races mismatches causal order violations between dependent events, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists."
          ],
          "correct": 2,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change under cross-region RTT variance?",
          "options": [
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Enforce fencing/term validation at write sinks to reject stale leaders/owners.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: multi-region profile sync with stale follower reads. Signal points to split-brain/stale-leader write acceptance. What is the primary diagnosis?",
          "options": [
            "One policy is always optimal for all entities and paths.",
            "The current design in multi-region profile sync with stale follower reads mismatches split-brain/stale-leader write acceptance, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps."
          ],
          "correct": 1,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change while protecting user-visible consistency?",
          "options": [
            "Split critical transaction boundary from side effects using outbox/saga with idempotent handlers.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: chat timeline with out-of-order replay. Signal points to unsafe distributed transaction side effects. What is the primary diagnosis?",
          "options": [
            "The current design in chat timeline with out-of-order replay mismatches unsafe distributed transaction side effects, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths."
          ],
          "correct": 0,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change during schema evolution overlap?",
          "options": [
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Replace global LWW with domain-specific conflict policy plus escalation for irreconcilable fields."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: ticket booking with duplicate callback retries. Signal points to merge policy causing silent data loss. What is the primary diagnosis?",
          "options": [
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in ticket booking with duplicate callback retries mismatches merge policy causing silent data loss, causing recurring consistency/co-ordination failures."
          ],
          "correct": 3,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with elevated retry amplification?",
          "options": [
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Stage membership/quorum changes with joint-consensus safety and rollback checkpoints.",
            "Disable safety checks temporarily to speed incident recovery."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: payment ledger with leader churn during peak. Signal points to quorum/membership reconfiguration risk. What is the primary diagnosis?",
          "options": [
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths.",
            "The current design in payment ledger with leader churn during peak mismatches quorum/membership reconfiguration risk, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists."
          ],
          "correct": 2,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change under lock-service instability?",
          "options": [
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Apply jittered retries, scoped lock granularity, and fairness controls to restore liveness safely.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: feature-flag rollout with split-brain control plane. Signal points to lock liveness collapse under contention. What is the primary diagnosis?",
          "options": [
            "One policy is always optimal for all entities and paths.",
            "The current design in feature-flag rollout with split-brain control plane mismatches lock liveness collapse under contention, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps."
          ],
          "correct": 1,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change while keeping conflict backlog bounded?",
          "options": [
            "Add dedupe keys + monotonic version guards so replay cannot overwrite newer state.",
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Scenario review: offline-first document edits reconciling post-outage. Signal points to replay/duplicate delivery without idempotent guards. What is the primary diagnosis?",
          "options": [
            "The current design in offline-first document edits reconciling post-outage mismatches replay/duplicate delivery without idempotent guards, causing recurring consistency/co-ordination failures.",
            "Global averages prove no consistency issue exists.",
            "Retries alone can safely resolve distributed correctness gaps.",
            "One policy is always optimal for all entities and paths."
          ],
          "correct": 0,
          "explanation": "The failure is a policy/boundary mismatch between guarantees required and guarantees actually enforced."
        },
        {
          "question": "After confirming diagnosis, what is the strongest next change with end-to-end incident validation?",
          "options": [
            "Disable safety checks temporarily to speed incident recovery.",
            "Accept occasional invariant violations and clean up later.",
            "Delay all architectural fixes and rely on manual paging playbooks.",
            "Define compensation ordering rules and prevent late compensations from corrupting current state."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage correction that directly closes the observed correctness gap."
        }
      ]
    },
    {
      "id": "cc-scn-061",
      "type": "multi-select",
      "question": "In integrated consistency incidents, first diagnostic priorities include which? (Select all that apply)",
      "options": [
        "Endpoint guarantee vs invariant mapping",
        "Lag/ordering/term telemetry correlation",
        "Only global CPU averages",
        "Replay/conflict-age signals"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Initial diagnosis should map business invariants to concrete consistency telemetry."
    },
    {
      "id": "cc-scn-062",
      "type": "multi-select",
      "question": "Which controls reduce silent data-loss risk? (Select all that apply)",
      "options": [
        "Domain-specific merge semantics",
        "Version/causal guards on writes",
        "Blind LWW for all fields",
        "Escalation path for unresolved high-risk conflicts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Silent loss drops when merge policy is explicit and guarded."
    },
    {
      "id": "cc-scn-063",
      "type": "multi-select",
      "question": "Safe failover in consistency-sensitive systems should include which? (Select all that apply)",
      "options": [
        "Stale-leader write rejection",
        "Critical-path read routing policy",
        "Disable term checks under pressure",
        "Clear degraded semantics for non-critical paths"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Failover must preserve safety while clearly defining degraded behavior."
    },
    {
      "id": "cc-scn-064",
      "type": "multi-select",
      "question": "When combining transactions with async side effects, strong patterns include which? (Select all that apply)",
      "options": [
        "Transactional outbox",
        "Idempotent consumers/handlers",
        "Dual-write without recovery plan",
        "Compensation semantics for partial failures"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Outbox+idempotency+compensation reduce distributed commit ambiguity."
    },
    {
      "id": "cc-scn-065",
      "type": "multi-select",
      "question": "For replay-heavy recovery windows, useful safeguards include which? (Select all that apply)",
      "options": [
        "Dedupe keys",
        "Monotonic version checks",
        "Timestamp-only overwrite",
        "Backpressure on conflict queues"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Replay windows require dedupe/version safety and controlled processing."
    },
    {
      "id": "cc-scn-066",
      "type": "multi-select",
      "question": "Consensus/membership changes during incidents should follow which? (Select all that apply)",
      "options": [
        "Stepwise reconfiguration",
        "Quorum validation each step",
        "Bulk unsafe voter replacement",
        "Rollback checkpoints"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Stepwise, validated reconfiguration avoids cascading control-plane failures."
    },
    {
      "id": "cc-scn-067",
      "type": "multi-select",
      "question": "Conflict backlog triage should prioritize which? (Select all that apply)",
      "options": [
        "High-value invariant-critical entities",
        "Oldest unresolved conflicts",
        "Only newest conflicts",
        "Customer-impacting fields first"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Prioritize by risk, age, and customer impact."
    },
    {
      "id": "cc-scn-068",
      "type": "multi-select",
      "question": "Consistency SLOs in scenario operations often include which? (Select all that apply)",
      "options": [
        "Freshness bound violations",
        "Conflict unresolved age",
        "Only request throughput",
        "Stale-read incident rate"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "SLOs should directly track consistency-quality outcomes."
    },
    {
      "id": "cc-scn-069",
      "type": "multi-select",
      "question": "When cost pressure conflicts with strict consistency, valid responses include which? (Select all that apply)",
      "options": [
        "Tiered endpoint guarantees",
        "Graceful degradation for tolerant paths",
        "Drop invariant checks universally",
        "Explicit policy docs + alerts on cap pressure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Tiering and degradation policies manage cost while preserving critical correctness."
    },
    {
      "id": "cc-scn-070",
      "type": "multi-select",
      "question": "Anti-patterns in integrated scenarios include which? (Select all that apply)",
      "options": [
        "Treating retries as correctness control",
        "Ignoring stale-owner write risk",
        "Canarying guarantee changes",
        "Undocumented endpoint consistency semantics"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Retries and implicit assumptions are frequent root causes of recurring incidents."
    },
    {
      "id": "cc-scn-071",
      "type": "multi-select",
      "question": "Which are valid reasons to combine strict + eventual paths in one system? (Select all that apply)",
      "options": [
        "Mixed criticality data model",
        "Latency budget diversity by endpoint",
        "All entities need identical guarantees",
        "Side effects can converge asynchronously"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Hybrid guarantees are often necessary for practicality and performance."
    },
    {
      "id": "cc-scn-072",
      "type": "multi-select",
      "question": "During post-incident hardening, useful actions include which? (Select all that apply)",
      "options": [
        "Guarantee contract updates",
        "Regression simulations/game-days",
        "Removing telemetry to reduce overhead",
        "Runbook ownership and rollback tests"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Hardening requires updated contracts, drills, and operational ownership."
    },
    {
      "id": "cc-scn-073",
      "type": "multi-select",
      "question": "For lock + consensus + merge interactions, which checks help prevent cascades? (Select all that apply)",
      "options": [
        "Fencing token validation",
        "Term-aware write rejection",
        "Assume lock ownership implies latest term",
        "Conflict queue saturation alerts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Cross-layer checks are needed to prevent stale ownership from corrupting state."
    },
    {
      "id": "cc-scn-074",
      "type": "multi-select",
      "question": "When causal order confidence drops, safe behavior can include which? (Select all that apply)",
      "options": [
        "Delay dependent actions briefly",
        "Escalate to stronger validation path",
        "Blindly apply late events",
        "Surface pending state to users"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Explicit pending/validation behavior is safer than silent misordering."
    },
    {
      "id": "cc-scn-075",
      "type": "multi-select",
      "question": "Scenario success criteria should include which? (Select all that apply)",
      "options": [
        "Invariant violations near zero on critical paths",
        "Conflict backlog returns to steady bounds",
        "Only reduced CPU usage",
        "Consistency SLO recovery through peak"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Success should be correctness-first, with sustainable backlog and SLO recovery."
    },
    {
      "id": "cc-scn-076",
      "type": "multi-select",
      "question": "Which preparations improve resilience before next incident? (Select all that apply)",
      "options": [
        "Predefined degradation toggles",
        "Canary-safe policy rollout templates",
        "Manual ad hoc decisions only",
        "Ownership map for critical guarantees"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Preparedness combines technical toggles with clear ownership and rollout templates."
    },
    {
      "id": "cc-scn-077",
      "type": "multi-select",
      "question": "For cross-team systems, governance needs which? (Select all that apply)",
      "options": [
        "Shared consistency vocabulary",
        "Per-endpoint guarantee registry",
        "Implicit tribal knowledge only",
        "Escalation policy for unresolved conflicts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Cross-team correctness needs explicit shared contracts and escalation rules."
    },
    {
      "id": "cc-scn-078",
      "type": "numeric-input",
      "question": "A critical path handles 4,500,000 ops/day with stale-read incident rate 0.18%. Incidents/day?",
      "answer": 8100,
      "unit": "operations",
      "tolerance": 0.03,
      "explanation": "0.0018*4,500,000=8,100."
    },
    {
      "id": "cc-scn-079",
      "type": "numeric-input",
      "question": "Conflict backlog is 18,000 items; resolver throughput is 240/min with incoming 90/min. Minutes to clear?",
      "answer": 120,
      "unit": "minutes",
      "tolerance": 0.03,
      "explanation": "Net drain 150/min; 18,000/150=120."
    },
    {
      "id": "cc-scn-080",
      "type": "numeric-input",
      "question": "Replay duplicates add 0.22 extra deliveries per event at 65,000 events/sec. Effective deliveries/sec?",
      "answer": 79300,
      "unit": "deliveries/sec",
      "tolerance": 0.02,
      "explanation": "65,000*1.22=79,300."
    },
    {
      "id": "cc-scn-081",
      "type": "numeric-input",
      "question": "Leader failover takes 14s and happens 26 times/day. Total failover seconds/day?",
      "answer": 364,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "14*26=364s."
    },
    {
      "id": "cc-scn-082",
      "type": "numeric-input",
      "question": "Bounded staleness target is 800ms; observed p99 lag is 1,120ms. Percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1,120-800)/800=40%."
    },
    {
      "id": "cc-scn-083",
      "type": "numeric-input",
      "question": "If 27% of 110,000 req/min need strict reads, strict reads/min?",
      "answer": 29700,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "0.27*110,000=29,700."
    },
    {
      "id": "cc-scn-084",
      "type": "numeric-input",
      "question": "Duplicate singleton job rate drops from 0.9% to 0.15%. Percent reduction?",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(0.9-0.15)/0.9=83.33%."
    },
    {
      "id": "cc-scn-085",
      "type": "numeric-input",
      "question": "Compensation queue receives 1,500 events/min and processes 1,620/min. Net drain rate?",
      "answer": 120,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "1,620-1,500=120."
    },
    {
      "id": "cc-scn-086",
      "type": "numeric-input",
      "question": "A 9-voter cluster needs majority commit. Minimum acknowledgements required?",
      "answer": 5,
      "unit": "acknowledgements",
      "tolerance": 0,
      "explanation": "Majority of 9 is 5."
    },
    {
      "id": "cc-scn-087",
      "type": "numeric-input",
      "question": "Consistency incident MTTR improved from 50 min to 32 min. Percent reduction?",
      "answer": 36,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(50-32)/50=36%."
    },
    {
      "id": "cc-scn-088",
      "type": "numeric-input",
      "question": "Unresolved high-risk conflicts are 4,800/day; manual queue clears 3,600/day. Net growth/day?",
      "answer": 1200,
      "unit": "conflicts/day",
      "tolerance": 0,
      "explanation": "4,800-3,600=1,200."
    },
    {
      "id": "cc-scn-089",
      "type": "numeric-input",
      "question": "If 12% of 2,200,000 daily writes require compensation checks, checks/day?",
      "answer": 264000,
      "unit": "writes",
      "tolerance": 0.02,
      "explanation": "0.12*2,200,000=264,000."
    },
    {
      "id": "cc-scn-090",
      "type": "ordering",
      "question": "Order an integrated consistency-incident response flow.",
      "items": [
        "Map affected invariants/endpoints",
        "Contain with safe policy toggles",
        "Apply targeted root-cause fix",
        "Validate with SLO + recurrence guards"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope, contain, fix, then harden."
    },
    {
      "id": "cc-scn-091",
      "type": "ordering",
      "question": "Order by increasing correctness risk.",
      "items": [
        "Strict critical-path policy + eventual side effects",
        "Tiered endpoint guarantees",
        "Weak global eventual policy",
        "Ad hoc retries/manual reconciliation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Risk rises when explicit guarantees and controls are removed."
    },
    {
      "id": "cc-scn-092",
      "type": "ordering",
      "question": "Order rollout safety for guarantee changes.",
      "items": [
        "Canary critical subset",
        "Shadow compare old/new behavior",
        "Progressive expansion",
        "Finalize docs/runbooks + alerts"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safe rollout needs staged expansion with comparative telemetry."
    },
    {
      "id": "cc-scn-093",
      "type": "ordering",
      "question": "Order by increasing stale-writer risk.",
      "items": [
        "Fencing + term checks at sink",
        "Term checks only",
        "Leader cache hints only",
        "No ownership validation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stale-writer risk increases as sink validation weakens."
    },
    {
      "id": "cc-scn-094",
      "type": "ordering",
      "question": "Order conflict-resolution maturity.",
      "items": [
        "Global LWW",
        "Entity-tiered merge rules",
        "Entity-tiered + causal metadata",
        "Entity-tiered + causal + escalation workflow"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Maturity grows with metadata and escalation controls."
    },
    {
      "id": "cc-scn-095",
      "type": "ordering",
      "question": "Order by strongest evidence of mitigation success.",
      "items": [
        "Lower average latency",
        "Lower conflict count only",
        "Lower conflict + stale-read incidents",
        "Sustained SLO recovery + bounded backlog + low loss incidents"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "High-confidence success combines correctness and operational durability."
    },
    {
      "id": "cc-scn-096",
      "type": "ordering",
      "question": "Order membership operation safety.",
      "items": [
        "Bulk voter swap",
        "Stepwise reconfiguration",
        "Stepwise + rollback points",
        "Stepwise + rollback + post-step quorum validation"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safety improves with granular validation and rollback controls."
    },
    {
      "id": "cc-scn-097",
      "type": "ordering",
      "question": "Order by increasing replay safety.",
      "items": [
        "Timestamp overwrite only",
        "Idempotency key checks",
        "Idempotency + version monotonicity",
        "Idempotency + version + causal dependency checks"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Replay safety strengthens with layered protections."
    },
    {
      "id": "cc-scn-098",
      "type": "ordering",
      "question": "Order degraded-mode sophistication.",
      "items": [
        "Implicit degradation",
        "Manual undocumented toggles",
        "Documented degradation matrix",
        "Documented matrix + automated guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sophistication increases with explicit policy and automation."
    },
    {
      "id": "cc-scn-099",
      "type": "ordering",
      "question": "Order by increasing operational complexity.",
      "items": [
        "Single consistency mode",
        "Tiered endpoint policy",
        "Tiered + consensus/lock integration",
        "Tiered + integrated policy + cross-team governance"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Complexity grows with integration depth and governance scope."
    },
    {
      "id": "cc-scn-100",
      "type": "ordering",
      "question": "Order scenario remediation lifecycle.",
      "items": [
        "Hypothesis from telemetry",
        "Targeted canary mitigation",
        "Post-fix stress/failure drills",
        "Institutionalize contracts and ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Durable remediation requires both validation and organizational hardening."
    }
  ]
}
