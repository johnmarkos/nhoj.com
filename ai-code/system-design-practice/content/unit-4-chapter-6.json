{
  "unit": 4,
  "unitTitle": "Storage Selection",
  "chapter": 6,
  "chapterTitle": "Object Storage",
  "chapterDescription": "S3-style blob storage for files, media, backups, and data lakes.",
  "problems": [
    {
      "id": "obj-001",
      "type": "multiple-choice",
      "question": "What is object storage?",
      "options": [
        "Storing JavaScript objects",
        "A flat storage system for blobs of data (files, images, videos) accessed by unique keys",
        "A relational database for objects",
        "Object-oriented programming storage"
      ],
      "correct": 1,
      "explanation": "Object storage stores data as objects (blobs) with unique keys. No hierarchy like file systems (although keys can mimic paths). Designed for: large unstructured data, massive scale, high durability. Examples: S3, GCS, Azure Blob Storage.",
      "detailedExplanation": "Generalize from object storage to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-002",
      "type": "multi-select",
      "question": "Which are object storage services?",
      "options": [
        "Amazon S3",
        "Google Cloud Storage",
        "Azure Blob Storage",
        "PostgreSQL"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "S3, GCS, and Azure Blob Storage are object storage services. PostgreSQL is a relational database. Object storage is for blobs; databases are for structured data with queries.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-003",
      "type": "multiple-choice",
      "question": "What is a bucket in S3/GCS?",
      "options": [
        "A physical storage device",
        "A container for objects with a globally unique name",
        "A backup folder",
        "A database table"
      ],
      "correct": 1,
      "explanation": "A bucket is a container for objects. Bucket names are globally unique (no one else can use your bucket name). Objects live in buckets, addressed as: s3://bucket-name/object-key. Buckets have region and access control settings.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-004",
      "type": "multiple-choice",
      "question": "What is an object key in S3?",
      "options": [
        "An encryption key",
        "The unique identifier/path for an object within a bucket",
        "A primary key",
        "An API key"
      ],
      "correct": 1,
      "explanation": "The object key is the unique name within a bucket: 'images/photo.jpg', 'backups/2024/data.zip'. Keys look like paths but S3 is flat — '/' is just part of the name. The full address is: bucket + key.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 2024 should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-005",
      "type": "two-stage",
      "stages": [
        {
          "question": "S3 has no actual folder hierarchy. When you see 'photos/2024/vacation/image.jpg' in S3, what is this?",
          "options": [
            "Three nested folders containing a file",
            "One object with key 'photos/2024/vacation/image.jpg' — the slashes are just characters in the key",
            "Four separate objects",
            "A symbolic link"
          ],
          "correct": 1,
          "explanation": "S3 is flat. 'photos/2024/vacation/image.jpg' is a single object key. The slashes are conventions that the console shows as 'folders', but there's no folder metadata. You can have 'a/b/c.jpg' without 'a/' or 'a/b/' existing.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. If values like 2024 appear, convert them into one unit basis before comparison. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "How do you list 'all images in the vacation folder'?",
          "options": [
            "List the vacation directory",
            "Use prefix filter: list objects with prefix 'photos/2024/vacation/'",
            "Query the folder table",
            "It's not possible"
          ],
          "correct": 1,
          "explanation": "Use prefix: ListObjects with prefix='photos/2024/vacation/' returns all objects starting with that prefix. This is how 'folder' listing works. Delimiter='/' gives 'folder-like' grouping. But it's prefix matching, not real folders.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. If values like 2024 appear, convert them into one unit basis before comparison. Common pitfall: underestimating replication and retention overhead."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-006",
      "type": "multiple-choice",
      "question": "What durability does S3 Standard claim?",
      "options": ["99%", "99.9%", "99.999999999% (eleven 9s)", "100%"],
      "correct": 2,
      "explanation": "S3 Standard claims 99.999999999% (11 nines) durability. This means expected loss of 0.000000001% of objects per year. Achieved by replicating across multiple devices and facilities. For 10 million objects, expect to lose ~0.0001 objects per year.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 99.999999999 and 11 in aligned units before deciding on an implementation approach. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-007",
      "type": "multiple-choice",
      "question": "What is the difference between durability and availability?",
      "options": [
        "They're the same",
        "Durability: data won't be lost. Availability: data is accessible when requested",
        "Durability is for writes; availability is for reads",
        "Availability is more important"
      ],
      "correct": 1,
      "explanation": "Durability = data safety (won't be lost). Availability = data accessibility (can read it now). S3 Standard: 11 nines durability, 99.99% availability. Highly durable but might have brief unavailability. Both matter but for different reasons.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 11 and 99.99 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-008",
      "type": "multi-select",
      "question": "What are common use cases for object storage?",
      "options": [
        "Static file hosting (images, CSS, JS)",
        "Backup and archive",
        "Data lakes for analytics",
        "Low-latency transactional database"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Object storage use cases: static files (web assets, images), backups (cheap, durable), data lakes (store raw data, process with Spark/Athena). Not for transactional databases — too much latency, no ACID.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-009",
      "type": "multiple-choice",
      "question": "What is S3 Standard-IA (Infrequent Access)?",
      "options": [
        "Immediate access storage",
        "A storage class for less frequently accessed data — lower storage cost, higher retrieval cost",
        "Indexed access storage",
        "A backup service"
      ],
      "correct": 1,
      "explanation": "Standard-IA is for data accessed less often (monthly, quarterly). Lower storage cost than Standard, but higher cost per retrieval. Same durability (11 nines). Good for: backups accessed during recovery, compliance archives.",
      "detailedExplanation": "Generalize from s3 Standard-IA (Infrequent Access) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 11 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-010",
      "type": "ordering",
      "question": "Rank S3 storage classes from lowest to highest storage cost (per GB/month):",
      "items": [
        "S3 Standard",
        "S3 Glacier Deep Archive",
        "S3 Standard-IA",
        "S3 Glacier Instant Retrieval"
      ],
      "correctOrder": [1, 3, 2, 0],
      "explanation": "Glacier Deep Archive is cheapest (long-term archive). Glacier Instant Retrieval next. Standard-IA is cheaper than Standard but more than Glacier. Standard is most expensive but most accessible. Trade-off: cost vs. access speed.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-011",
      "type": "multiple-choice",
      "question": "What is S3 Glacier?",
      "options": [
        "A cold storage service",
        "Low-cost archive storage with slower retrieval times (minutes to hours)",
        "A CDN service",
        "A glacier monitoring service"
      ],
      "correct": 1,
      "explanation": "Glacier is S3's archive tier. Very low storage cost, but retrieval takes time: Expedited (1-5 min), Standard (3-5 hours), Bulk (5-12 hours). For: compliance archives, rarely accessed backups, long-term storage. Glacier Deep Archive is even cheaper/slower.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 1 and 5 min should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-012",
      "type": "two-stage",
      "stages": [
        {
          "question": "You store 100TB of logs. Logs are queried in first 30 days, then kept for compliance for 7 years. How to optimize cost?",
          "options": [
            "Keep in S3 Standard forever",
            "Lifecycle policy: Standard (30 days) → Glacier (7 years) → Delete",
            "Delete after 30 days",
            "Use EBS volumes"
          ],
          "correct": 1,
          "explanation": "Lifecycle policy automates transitions. Keep in Standard for active use, move to Glacier for long-term compliance (much cheaper). After retention period, delete automatically. This optimizes cost across the data lifecycle.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 100TB and 30 days appear, convert them into one unit basis before comparison. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "What if you need to occasionally query old logs during an investigation?",
          "options": [
            "Keep everything in Standard",
            "Use Glacier Instant Retrieval or accept retrieval delay from standard Glacier",
            "Logs can't be queried from Glacier",
            "Store a copy in both"
          ],
          "correct": 1,
          "explanation": "Options: (1) Glacier Instant Retrieval — millisecond access, slightly higher cost than Deep Archive. (2) Standard Glacier — wait hours for retrieval when needed (rare investigations). (3) Keep searchable summaries in Elasticsearch, raw logs in Glacier.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints."
        }
      ],
      "detailedExplanation": "Generalize from object Storage to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-013",
      "type": "multiple-choice",
      "question": "What is a lifecycle policy in S3?",
      "options": [
        "Insurance for data",
        "Rules that automatically transition or delete objects based on age or other criteria",
        "A backup schedule",
        "A data classification scheme"
      ],
      "correct": 1,
      "explanation": "Lifecycle policies automate object management: transition to cheaper storage classes after X days, delete after Y days. Apply to buckets or prefixes. Essential for cost optimization — don't pay Standard prices for 5-year-old data.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-014",
      "type": "multi-select",
      "question": "What actions can S3 lifecycle policies perform?",
      "options": [
        "Transition to different storage classes",
        "Delete objects after a period",
        "Delete incomplete multipart uploads",
        "Automatically compress objects"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Lifecycle can: transition (Standard → IA → Glacier), expire (delete after N days), and clean up incomplete multipart uploads. It doesn't automatically compress — that's done at upload time or by processing.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-015",
      "type": "multiple-choice",
      "question": "What is S3 versioning?",
      "options": [
        "Different S3 API versions",
        "Keeping multiple versions of an object, preserving previous versions on update or delete",
        "Version control like Git",
        "Software versioning"
      ],
      "correct": 1,
      "explanation": "With versioning enabled, S3 keeps all versions of an object. Overwrites create new versions (old ones preserved). Deletes add a delete marker (previous versions remain). Essential for: recovering from accidental deletes, audit trails.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-016",
      "type": "two-stage",
      "stages": [
        {
          "question": "Versioning is enabled. You upload 'report.pdf', then upload a new 'report.pdf'. What happens?",
          "options": [
            "Old version is deleted",
            "New version becomes current; old version is preserved with a version ID",
            "Upload fails — name conflict",
            "Both have the same version"
          ],
          "correct": 1,
          "explanation": "Both versions exist. The new one is 'current' (returned by default GET). The old one has a version ID and can be retrieved explicitly: GET report.pdf?versionId=xxx. Storage costs include all versions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes."
        },
        {
          "question": "You 'delete' report.pdf. What happens?",
          "options": [
            "All versions are permanently deleted",
            "A delete marker is added; all versions still exist and can be recovered",
            "Only the current version is deleted",
            "Delete is blocked"
          ],
          "correct": 1,
          "explanation": "Delete adds a delete marker (a special version saying 'this is deleted'). GET returns 404. But all previous versions remain — you can restore by deleting the delete marker or getting a specific version. To permanently delete, delete each version.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Interface decisions should be justified by contract stability and client impact over time. Keep quantities like 404 in aligned units before deciding on an implementation approach. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-017",
      "type": "multiple-choice",
      "question": "What is S3 Object Lock?",
      "options": [
        "Locking objects for concurrent access",
        "A WORM (Write Once Read Many) feature preventing deletion or modification for a retention period",
        "Encryption at rest",
        "Access control"
      ],
      "correct": 1,
      "explanation": "Object Lock provides WORM storage. Once written, objects can't be deleted or modified for the retention period. Compliance mode: even root can't delete. Used for: regulatory compliance, ransomware protection. Requires versioning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-018",
      "type": "multi-select",
      "question": "What are S3 Object Lock modes?",
      "options": [
        "Governance mode — can be overridden with special permissions",
        "Compliance mode — no one can delete, including root",
        "Legal hold — indefinite lock until removed",
        "Encryption mode — encrypts locked objects"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Governance: locked but privileged users can override. Compliance: truly immutable (no override). Legal hold: indefinite lock regardless of retention period, for litigation. These are access modes, not encryption (that's separate).",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-019",
      "type": "multiple-choice",
      "question": "What is a presigned URL in S3?",
      "options": [
        "A permanently signed URL",
        "A time-limited URL that grants temporary access to an object without AWS credentials",
        "A URL that's signed before upload",
        "A premium URL service"
      ],
      "correct": 1,
      "explanation": "Presigned URLs grant temporary access. Generate with your credentials, share the URL. The recipient can GET/PUT without AWS credentials, until expiration. Use for: temporary downloads, allowing uploads without exposing credentials.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer the approach that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-020",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your app needs to let users upload avatars directly to S3 (bypassing your server). How?",
          "options": [
            "Give users your AWS credentials",
            "Generate a presigned PUT URL, give it to the user, they upload directly to S3",
            "Have users send files through your API",
            "Make the bucket public"
          ],
          "correct": 1,
          "explanation": "Presigned URL for PUT: your backend generates a URL (with expiration, size limits), gives to client. Client uploads directly to S3 using the presigned URL. Your server never handles the file. Efficient and scalable.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements."
        },
        {
          "question": "Security concern: presigned URL is leaked. Mitigation?",
          "options": [
            "Nothing can be done",
            "Use short expiration times, limit to specific object keys, consider additional authentication",
            "Don't use presigned URLs",
            "Encrypt the URL"
          ],
          "correct": 1,
          "explanation": "Mitigations: short expiration (minutes, not hours), specific object key (user can only write to their designated key), check file after upload (validate content), use POST policies for additional constraints. Defense in depth.",
          "detailedExplanation": "Generalize from security concern: presigned URL is leaked to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-021",
      "type": "multiple-choice",
      "question": "What is S3 Transfer Acceleration?",
      "options": [
        "Faster CPU for S3",
        "Using CloudFront edge locations to speed up transfers over long distances",
        "Parallel uploads",
        "A CDN replacement"
      ],
      "correct": 1,
      "explanation": "Transfer Acceleration routes uploads/downloads through CloudFront edge locations. Data travels to the nearest edge over the internet, then over AWS's fast backbone to S3. Helps when uploading from far away. Has additional cost.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-022",
      "type": "multiple-choice",
      "question": "What is multipart upload in S3?",
      "options": [
        "Uploading to multiple buckets",
        "Uploading large objects in parts for reliability and parallelism",
        "Uploading multiple objects at once",
        "A multi-user upload"
      ],
      "correct": 1,
      "explanation": "Multipart upload splits large objects into parts (5MB - 5GB each). Upload parts in parallel, retry failed parts without restarting. Required for objects > 5GB. Benefits: faster, resumable, better network utilization.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 5MB and 5GB in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-023",
      "type": "multi-select",
      "question": "What are benefits of multipart upload?",
      "options": [
        "Parallel uploads for speed",
        "Resume from failure (retry individual parts)",
        "Required for objects over 5GB",
        "Lower storage cost"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Multipart benefits: parallel (faster), resumable (don't restart from scratch), handles large objects (up to 5TB). Storage cost is the same — multipart is about the upload process, not storage.",
      "detailedExplanation": "Generalize from benefits of multipart upload to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 5TB in aligned units before deciding on an implementation approach. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-024",
      "type": "two-stage",
      "stages": [
        {
          "question": "A multipart upload is started but never completed (parts uploaded, but no 'complete' call). What happens?",
          "options": [
            "Parts are automatically deleted",
            "Parts remain and you're charged for storage until you abort or complete",
            "S3 automatically completes it",
            "The bucket is locked"
          ],
          "correct": 1,
          "explanation": "Incomplete multipart uploads consume storage (and cost money). Parts stay until you explicitly abort or complete. This can accumulate surprisingly. Use lifecycle policies to abort incomplete uploads after a period (e.g., 7 days).",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 7 days appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints."
        },
        {
          "question": "How do you prevent accumulating incomplete multipart upload costs?",
          "options": [
            "Don't use multipart upload",
            "Lifecycle rule to abort incomplete multipart uploads after X days",
            "Manually check every week",
            "Disable multipart in the bucket"
          ],
          "correct": 1,
          "explanation": "Lifecycle rule: 'AbortIncompleteMultipartUpload' after 7 days (or your preferred period). Automatically cleans up orphaned parts. Set and forget. This is a best practice for all buckets using multipart uploads.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Numbers such as 7 days should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-025",
      "type": "multiple-choice",
      "question": "What is S3 Select?",
      "options": [
        "Selecting which S3 region to use",
        "Querying data inside an object using SQL, retrieving only matching data",
        "Selecting objects to delete",
        "A premium S3 tier"
      ],
      "correct": 1,
      "explanation": "S3 Select runs SQL queries on objects (CSV, JSON, Parquet). Instead of downloading entire object, query and get matching rows. Reduces data transfer costs and latency. Example: SELECT * FROM S3Object WHERE status='active'.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-026",
      "type": "multiple-choice",
      "question": "What is Amazon Athena?",
      "options": [
        "A wisdom service",
        "A serverless query service to analyze S3 data using SQL",
        "A storage service",
        "A compute service"
      ],
      "correct": 1,
      "explanation": "Athena is serverless SQL for S3. Define schema over S3 data, query with SQL. Pay per query (data scanned). Good for: ad-hoc analytics, data lake queries. Uses Presto/Trino under the hood. Much more powerful than S3 Select.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the schema/index decision that minimizes query and write amplification for this workload. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-027",
      "type": "multi-select",
      "question": "What is the difference between S3 Select and Athena?",
      "options": [
        "S3 Select queries single objects; Athena queries across many objects",
        "S3 Select is simpler/lighter; Athena supports complex SQL including JOINs",
        "They're the same service",
        "Athena uses a table abstraction; S3 Select is object-level"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "S3 Select: single object, simple filter, lightweight. Athena: full SQL over table (many objects), complex queries, JOINs, aggregations. Use S3 Select for simple 'just get matching rows from this file'. Athena for analytics.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-028",
      "type": "two-stage",
      "stages": [
        {
          "question": "You have 1TB of CSV logs in S3. You need to find all errors from the last hour. Which tool?",
          "options": [
            "Download all 1TB and grep locally",
            "Use Athena with a query filtering by timestamp and status='error'",
            "Use S3 Select on each file",
            "Convert to a database first"
          ],
          "correct": 1,
          "explanation": "Athena: define a table over the logs, query with SQL: SELECT * FROM logs WHERE timestamp > X AND status = 'error'. Only scans relevant data (if partitioned well). Pay for data scanned. Much faster than downloading everything.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Modeling quality is measured by query fit, cardinality behavior, and operational cost. If values like 1TB appear, convert them into one unit basis before comparison. Common pitfall: schema optimized for entities instead of queries."
        },
        {
          "question": "How can you reduce Athena query costs for this log analysis?",
          "options": [
            "Use smaller files",
            "Partition data by date (Athena skips irrelevant partitions) and use columnar format (Parquet)",
            "Query less often",
            "Use a cheaper region"
          ],
          "correct": 1,
          "explanation": "Partition by date: query for last hour only scans that partition. Columnar format (Parquet): only reads columns you SELECT. Compression reduces data size. These can reduce scanned data (and cost) by 10-100x. Structure data for query patterns.",
          "detailedExplanation": "Generalize from you reduce Athena query costs for this log analysis to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 10 and 100x in aligned units before deciding on an implementation approach. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-029",
      "type": "multiple-choice",
      "question": "What is a data lake?",
      "options": [
        "A lake monitoring system",
        "A centralized repository storing raw data at any scale, often in object storage",
        "A database",
        "A water management database"
      ],
      "correct": 1,
      "explanation": "A data lake stores raw data in native format (structured, semi-structured, unstructured) at scale. Object storage (S3) is common. Analysis happens in place: Athena, Spark, ML. Contrasts with data warehouse (processed, structured). Lake = raw + scale.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-030",
      "type": "multi-select",
      "question": "Why is object storage often used for data lakes?",
      "options": [
        "Virtually unlimited scale",
        "Low cost per GB",
        "Decoupled storage and compute",
        "Built-in ACID transactions"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Data lake on object storage: unlimited scale (petabytes easy), cheap storage, decoupled compute (use Spark, Athena, etc. on demand). Object storage lacks ACID — that's addressed by lakehouse formats (Delta Lake, Iceberg) which add transactions.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-031",
      "type": "multiple-choice",
      "question": "What is the 'lakehouse' architecture?",
      "options": [
        "A house by a lake",
        "Combining data lake (raw, cheap storage) with data warehouse features (transactions, schema enforcement)",
        "A new type of database",
        "A storage vendor"
      ],
      "correct": 1,
      "explanation": "Lakehouse = data lake + warehouse features. Technologies like Delta Lake, Apache Iceberg, Apache Hudi add: ACID transactions, schema evolution, time travel to data lake storage. Get warehouse-like reliability with lake-like scale and cost.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-032",
      "type": "multiple-choice",
      "question": "What is Delta Lake?",
      "options": [
        "A change data capture tool",
        "An open-source storage layer adding ACID transactions to data lakes (Parquet + transaction log)",
        "A new file format",
        "An AWS service"
      ],
      "correct": 1,
      "explanation": "Delta Lake adds ACID transactions to Parquet data on S3/data lakes. It uses a transaction log to track changes. Features: ACID, time travel (query historical versions), schema enforcement. Open-source, works with Spark.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-033",
      "type": "two-stage",
      "stages": [
        {
          "question": "You write data to S3 in Parquet format. Concurrent writers might corrupt data. How does Delta Lake help?",
          "options": [
            "It doesn't — Parquet is always safe",
            "Transaction log ensures atomic commits; concurrent writes are serialized",
            "Delta Lake prevents writing",
            "Uses database locks"
          ],
          "correct": 1,
          "explanation": "Delta Lake's transaction log records each change atomically. Concurrent writers: optimistic concurrency — try to write, check for conflicts, retry if needed. Readers see consistent snapshots. This enables ACID on object storage.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "You realize yesterday's ETL job wrote bad data. With Delta Lake, how do you fix it?",
          "options": [
            "Restore from backup",
            "Time travel: query or restore to a previous version",
            "Rerun the entire pipeline",
            "It's not possible"
          ],
          "correct": 1,
          "explanation": "Delta Lake time travel: each change is versioned. Query historical data: SELECT * FROM table VERSION AS OF 5. Or restore: RESTORE table TO VERSION AS OF 5. Fix bad writes without reprocessing everything. Version history is configurable.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. If values like 5 appear, convert them into one unit basis before comparison. Common pitfall: interface design coupled too tightly to internal implementation."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-034",
      "type": "multiple-choice",
      "question": "What is Apache Iceberg?",
      "options": [
        "A cold storage format",
        "An open table format for data lakes with ACID, schema evolution, and partition evolution",
        "An AWS service",
        "A visualization tool"
      ],
      "correct": 1,
      "explanation": "Iceberg is a table format for data lakes (like Delta Lake, Hudi). Features: ACID transactions, schema evolution, partition evolution (change partitioning without rewriting), time travel. Open-source, vendor-neutral, supported by AWS, Snowflake, Dremio.",
      "detailedExplanation": "Generalize from apache Iceberg to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-035",
      "type": "multi-select",
      "question": "What problems do lakehouse formats (Delta Lake, Iceberg) solve?",
      "options": [
        "ACID transactions on object storage",
        "Schema enforcement and evolution",
        "Time travel / versioning",
        "Making data lakes faster than databases for OLTP"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Lakehouse formats add: ACID (consistent writes), schema enforcement (data quality), time travel (audit, rollback). They don't make lakes faster than databases for OLTP — lakes are still optimized for analytics workloads, not transactions.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-036",
      "type": "multiple-choice",
      "question": "What is S3 event notification?",
      "options": [
        "Email about S3 outages",
        "Triggering events (Lambda, SQS, SNS) when objects are created, deleted, or modified",
        "S3 usage reports",
        "Notification about billing"
      ],
      "correct": 1,
      "explanation": "Event notifications trigger on S3 actions: ObjectCreated, ObjectRemoved. Send to Lambda, SQS, or SNS. Use cases: process uploads (resize images), update search index, trigger workflows. Event-driven architectures with S3 as source.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Users upload images to S3. You need to generate thumbnails automatically. Architecture?",
          "options": [
            "Poll S3 for new images",
            "S3 event triggers Lambda, Lambda creates thumbnail and saves to S3",
            "Manual batch job nightly",
            "Generate thumbnails in the browser"
          ],
          "correct": 1,
          "explanation": "Event-driven: S3 ObjectCreated event → Lambda function. Lambda reads image, creates thumbnail, writes to S3 (same or different bucket). Serverless, automatic, scales with upload volume. No polling needed.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes."
        },
        {
          "question": "What if thumbnail generation fails for some images?",
          "options": [
            "They're lost forever",
            "Configure dead letter queue (DLQ) to capture failed events for retry/investigation",
            "S3 automatically retries",
            "Lambda never fails"
          ],
          "correct": 1,
          "explanation": "Configure Lambda with a DLQ (SQS). Failed invocations send the event to DLQ. You can retry, investigate, or alert. Also: Lambda retries (async) before DLQ, built-in retry logic. Always plan for failures in async workflows.",
          "detailedExplanation": "Generalize from if thumbnail generation fails for some images to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-038",
      "type": "multiple-choice",
      "question": "What is Amazon S3 Cross-Region Replication (CRR)?",
      "options": [
        "Copying files manually",
        "Automatic asynchronous replication of objects to a bucket in another region",
        "CDN caching",
        "Database replication"
      ],
      "correct": 1,
      "explanation": "CRR automatically replicates objects to another region. Use cases: disaster recovery (data in multiple regions), compliance (data residency), lower latency (replicate closer to users). Replication is asynchronous; uses versioning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-039",
      "type": "multi-select",
      "question": "What are use cases for S3 replication?",
      "options": [
        "Disaster recovery (copy to another region)",
        "Compliance (data in specific regions)",
        "Aggregating logs from multiple accounts",
        "Real-time synchronous access"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Replication use cases: DR (survive region failure), compliance (meet data residency), log aggregation (replicate from many sources to central). Not real-time synchronous — replication is async, typically seconds to minutes lag.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-040",
      "type": "multiple-choice",
      "question": "What is S3 Batch Operations?",
      "options": [
        "Uploading files in batches",
        "A service to perform operations on billions of objects at scale (copy, tag, invoke Lambda)",
        "Batch delete only",
        "A scheduling service"
      ],
      "correct": 1,
      "explanation": "Batch Operations runs large-scale operations: copy objects, add tags, invoke Lambda on each object, restore from Glacier. Provide a manifest (list of objects), specify operation, run job. Essential for bulk migrations, mass updates.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard storage decisions that optimize one dimension while violating another core requirement. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to add a 'department=finance' tag to 500 million S3 objects. Approach?",
          "options": [
            "Script that loops through all objects",
            "S3 Batch Operations with PUT tagging operation",
            "Manual tagging in console",
            "It's not possible at this scale"
          ],
          "correct": 1,
          "explanation": "Batch Operations: create a job specifying the operation (PUT tagging), provide manifest of objects, run. S3 handles execution at scale, reports progress and failures. Much more reliable than a custom script for 500M objects.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. If values like 500 and 500M appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints."
        },
        {
          "question": "How do you generate a manifest of 500 million objects?",
          "options": [
            "Type them manually",
            "Use S3 Inventory report or ListObjects",
            "Batch Operations generates it automatically",
            "Export from database"
          ],
          "correct": 1,
          "explanation": "S3 Inventory: scheduled report of bucket contents (CSV/Parquet/ORC). Run weekly/daily. Use as manifest for Batch Operations. For smaller scales, ListObjects to generate manifest. Inventory is better for very large buckets.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Numbers such as 500 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-042",
      "type": "multiple-choice",
      "question": "What is S3 Inventory?",
      "options": [
        "Tracking product inventory",
        "A scheduled report listing objects in a bucket with metadata",
        "A cost report",
        "An access log"
      ],
      "correct": 1,
      "explanation": "S3 Inventory generates lists of objects in your bucket: key, size, last modified, storage class, encryption status, etc. Daily or weekly. Output to S3 (CSV, Parquet, ORC). Useful for: auditing, generating manifests, understanding bucket contents.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-043",
      "type": "multiple-choice",
      "question": "What is object tagging in S3?",
      "options": [
        "Naming objects",
        "Key-value labels on objects for organization, access control, and lifecycle policies",
        "Hashtags for social sharing",
        "Version tags"
      ],
      "correct": 1,
      "explanation": "Tags are key-value pairs on objects: {project: 'X', owner: 'finance'}. Use for: cost allocation (filter costs by tag), access control (IAM policies based on tags), lifecycle policies (transition/delete tagged objects). Up to 10 tags per object.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 10 in aligned units before deciding on an implementation approach. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-044",
      "type": "multi-select",
      "question": "What can you use S3 tags for?",
      "options": [
        "Cost allocation (AWS Cost Explorer filters by tag)",
        "Access control (IAM policies can require tags)",
        "Lifecycle policies (transition based on tags)",
        "Full-text search of object contents"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Tags enable: cost allocation (attribute costs to projects), access control (policies requiring specific tags), lifecycle (transition or delete based on tags). Not for content search — tags are metadata, not content. Use S3 Select or external search for content.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "You have S3 buckets for multiple projects. Finance needs to know costs per project. How?",
          "options": [
            "Separate AWS accounts per project",
            "Tag objects with project name, use Cost Explorer to filter by tag",
            "Manual tracking",
            "Calculate from object sizes"
          ],
          "correct": 1,
          "explanation": "Tag objects (project: 'project-A', project: 'project-B'). Enable tag-based cost allocation in Billing. Cost Explorer shows costs filtered by tag. Tag at upload time via application or Batch Operations for existing data.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements."
        },
        {
          "question": "Access control: only project-A team should access project-A objects. How?",
          "options": [
            "Separate buckets",
            "IAM policies that require tag project=project-A for project-A users",
            "Trust project-B team not to look",
            "Tags can't control access"
          ],
          "correct": 1,
          "explanation": "IAM condition: {StringEquals: {s3:ExistingObjectTag/project: 'project-A'}}. Project-A users' policy allows access only to objects with matching tag. This is ABAC (Attribute-Based Access Control). Works with tags on objects.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead."
        }
      ],
      "detailedExplanation": "Generalize from object Storage to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-046",
      "type": "multiple-choice",
      "question": "What is S3 encryption at rest?",
      "options": [
        "Encrypting data while uploading",
        "Encrypting objects stored in S3 so they're protected on disk",
        "HTTPS for access",
        "Encrypting bucket names"
      ],
      "correct": 1,
      "explanation": "Encryption at rest protects data on S3's storage. Options: SSE-S3 (S3-managed keys), SSE-KMS (AWS KMS keys with auditing), SSE-C (customer-provided keys). S3 encrypts on write, decrypts on read. Now enabled by default for new buckets.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-047",
      "type": "ordering",
      "question": "Rank S3 encryption options from simplest to most control:",
      "items": [
        "SSE-S3 (S3-managed keys)",
        "SSE-KMS (KMS-managed keys)",
        "SSE-C (customer-provided keys)",
        "Client-side encryption"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "SSE-S3: simplest, AWS manages everything. SSE-KMS: more control, audit via CloudTrail, key policies. SSE-C: you manage keys (more work, more control). Client-side: you encrypt before upload (full control, S3 never sees plaintext).",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-048",
      "type": "multi-select",
      "question": "What are benefits of SSE-KMS over SSE-S3?",
      "options": [
        "Key usage auditing in CloudTrail",
        "Separate key permissions (who can use which keys)",
        "Automatic key rotation",
        "Lower cost"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "SSE-KMS benefits: audit trail (every key use logged), granular access (IAM on keys), automatic rotation (enable for CMKs). SSE-S3 is simpler but less control. SSE-KMS has slightly higher cost (KMS API calls).",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-049",
      "type": "multiple-choice",
      "question": "What is a bucket policy in S3?",
      "options": [
        "A company policy document stored in S3",
        "A JSON document defining access permissions for a bucket and its objects",
        "A lifecycle policy",
        "A replication policy"
      ],
      "correct": 1,
      "explanation": "Bucket policies are JSON access policies attached to buckets. Define who can do what (Principal, Action, Resource, Condition). Used for: cross-account access, public access, enforcing encryption. Evaluated alongside IAM policies.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "You want to allow another AWS account to upload to your S3 bucket. How?",
          "options": [
            "Give them your credentials",
            "Bucket policy granting s3:PutObject to their account principal",
            "Make the bucket public",
            "It's not possible cross-account"
          ],
          "correct": 1,
          "explanation": "Bucket policy: {Effect: Allow, Principal: {AWS: arn:aws:iam::ACCOUNT-ID:root}, Action: s3:PutObject, Resource: arn:aws:s3:::bucket/*}. Grant specific actions to specific accounts. Never share credentials.",
          "detailedExplanation": "Generalize from you want to allow another AWS account to upload to your S3 bucket to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "By default, who owns objects uploaded by another account?",
          "options": [
            "Your account (bucket owner)",
            "The uploading account (object owner)",
            "Shared ownership",
            "AWS owns them"
          ],
          "correct": 1,
          "explanation": "By default, the uploading account owns the object (you can't access it without ACL). For bucket owner to have control, use: bucket-owner-full-control ACL on upload, or enable Bucket Owner Enforced (disables ACLs, bucket owner always owns).",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-051",
      "type": "multiple-choice",
      "question": "What is S3 Block Public Access?",
      "options": [
        "Blocking public IP addresses",
        "A set of controls to prevent buckets and objects from being made public",
        "Blocking certain file types",
        "Firewall for S3"
      ],
      "correct": 1,
      "explanation": "Block Public Access settings prevent accidental public exposure. Can be set at account or bucket level. Even if a bucket policy or ACL grants public access, Block Public Access overrides it. Enabled by default for new buckets. Critical for security.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-052",
      "type": "multi-select",
      "question": "What does S3 Block Public Access control?",
      "options": [
        "Block public access via new bucket policies",
        "Block public access via any bucket policies",
        "Block public access via ACLs",
        "Block all internet access (even presigned URLs)"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Block Public Access can: block new public policies, block any public policies, block public ACLs, ignore public ACLs. It doesn't block presigned URLs (those are authenticated) or private access. It prevents accidental public exposure.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-053",
      "type": "multiple-choice",
      "question": "What is S3 Access Points?",
      "options": [
        "Physical data centers",
        "Named endpoints with dedicated access policies, simplifying access management for shared datasets",
        "WiFi access points",
        "API endpoints"
      ],
      "correct": 1,
      "explanation": "Access Points are named entry points to a bucket with their own policy. Instead of one complex bucket policy, create access points for different use cases: analytics-ap, etl-ap, each with tailored permissions. Simplifies management at scale.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "A bucket is shared by 5 teams, each needing different access. Managing one bucket policy is complex. Better approach?",
          "options": [
            "5 separate buckets",
            "5 access points, each with its own policy scoped to that team's needs",
            "One complex policy with many statements",
            "Give everyone full access"
          ],
          "correct": 1,
          "explanation": "Access points: create team-a-ap, team-b-ap, etc. Each has its own policy (e.g., team-a-ap allows team-a prefix only). Teams use their access point. Simpler than one monolithic policy, easier to audit, separate concerns.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 5 in aligned units before deciding on an implementation approach. Common pitfall: ignoring durability/recovery requirements."
        },
        {
          "question": "What if a team should only access specific prefixes?",
          "options": [
            "Access points can't restrict prefixes",
            "Access point policy limits to specific prefixes; or use S3 Object Lambda to filter",
            "Use bucket policy instead",
            "Create separate buckets"
          ],
          "correct": 1,
          "explanation": "Access point policies can scope to prefixes: {Resource: arn:aws:s3:us-east-1:123456789012:accesspoint/my-ap/object/team-a/*}. Or for more complex filtering, S3 Object Lambda can transform responses.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Keep quantities like 1 and 123456789012 in aligned units before deciding on an implementation approach. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-055",
      "type": "multiple-choice",
      "question": "What is Google Cloud Storage?",
      "options": [
        "A file sync service",
        "Google's object storage service, similar to S3",
        "A database service",
        "Google Drive API"
      ],
      "correct": 1,
      "explanation": "Google Cloud Storage (GCS) is Google's object storage: buckets, objects, storage classes (Standard, Nearline, Coldline, Archive). Similar concepts to S3. Used for: cloud apps, data analytics (with BigQuery), backups. Integrates with GCP services.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-056",
      "type": "multi-select",
      "question": "What are GCS storage classes?",
      "options": [
        "Standard (frequent access)",
        "Nearline (monthly access)",
        "Coldline (quarterly access)",
        "Archive (yearly access)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "GCS classes parallel S3: Standard (hot), Nearline (~Standard-IA, 30-day min), Coldline (~Glacier Instant, 90-day min), Archive (~Glacier Deep Archive, 365-day min). Choose based on access frequency for cost optimization.",
      "detailedExplanation": "Generalize from gCS storage classes to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 30 and 90 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-057",
      "type": "multiple-choice",
      "question": "What is Azure Blob Storage?",
      "options": [
        "A file share service",
        "Microsoft Azure's object storage for unstructured data",
        "A database service",
        "Block storage for VMs"
      ],
      "correct": 1,
      "explanation": "Azure Blob Storage is Azure's object storage. Concepts: Storage Account > Container > Blob. Tiers: Hot, Cool, Archive. Used for: static files, backups, data lakes. Azure Data Lake Storage Gen2 is Blob Storage with hierarchical namespace.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard storage decisions that optimize one dimension while violating another core requirement. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-058",
      "type": "ordering",
      "question": "Match storage concepts: S3 bucket = GCS bucket = Azure ___",
      "items": ["Storage Account", "Container", "Blob", "Resource Group"],
      "correctOrder": [1, 0, 2, 3],
      "explanation": "S3 Bucket = GCS Bucket ≈ Azure Container (though Azure has Storage Account above Container). S3 Object = GCS Object = Azure Blob. Azure's hierarchy: Resource Group > Storage Account > Container > Blob.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Build the rank from biggest differences first, then refine with adjacent checks. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-059",
      "type": "multiple-choice",
      "question": "What is MinIO?",
      "options": [
        "A mini version of S3",
        "An open-source, S3-compatible object storage server",
        "A cloud service",
        "A file compression tool"
      ],
      "correct": 1,
      "explanation": "MinIO is open-source, S3-compatible object storage. Run it on your own infrastructure. Use cases: on-premise S3, air-gapped environments, avoiding cloud vendor lock-in. High performance, Kubernetes-native. Popular for private cloud and edge.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your company requires all data to stay on-premise (no public cloud). You want S3-compatible storage. Option?",
          "options": [
            "Use S3 anyway",
            "Deploy MinIO or similar S3-compatible storage on-premise",
            "On-premise can't have object storage",
            "Use a NAS"
          ],
          "correct": 1,
          "explanation": "MinIO provides S3 API on your hardware. Your apps use S3 SDKs; MinIO handles storage. Other options: Ceph (with RGW), OpenStack Swift. For cloud exit strategy or hybrid, S3-compatible storage enables portability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes."
        },
        {
          "question": "You want to use the same code for on-premise MinIO and cloud S3. What makes this possible?",
          "options": [
            "Rewrite code for each",
            "S3-compatible APIs — same SDKs and API calls work on both",
            "A special translation layer",
            "It's not possible"
          ],
          "correct": 1,
          "explanation": "S3 compatibility means MinIO implements the S3 API. Use AWS SDK, change the endpoint URL to point to MinIO instead of s3.amazonaws.com. Same code works. This is the power of standardized APIs.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-061",
      "type": "multiple-choice",
      "question": "What is S3 Object Lambda?",
      "options": [
        "Lambda functions that store data in S3",
        "A feature to transform data as it's read from S3 (e.g., redact PII, resize images)",
        "Lambda written in Object-oriented style",
        "S3 event triggers"
      ],
      "correct": 1,
      "explanation": "S3 Object Lambda runs a Lambda function on GET requests, transforming data on-the-fly. Use cases: redact PII for non-privileged users, resize images per request, convert formats. Store one copy, serve many variants dynamically.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-062",
      "type": "multi-select",
      "question": "What are use cases for S3 Object Lambda?",
      "options": [
        "Redacting sensitive data (PII) for certain users",
        "Dynamically resizing images",
        "Converting data formats (XML to JSON)",
        "Faster uploads"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Object Lambda use cases: redaction (return data without PII), resize (return image at requested size), format conversion (return JSON from XML source). It's for read transformations, not upload processing (use triggers for that).",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Evaluate each candidate approach independently under the same constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-063",
      "type": "two-stage",
      "stages": [
        {
          "question": "You store customer records in S3. Some users should see full records, others should see records with SSN redacted. Options?",
          "options": [
            "Store two copies of every record",
            "S3 Object Lambda to redact SSN for non-privileged users dynamically",
            "Let application layer handle it",
            "Don't store SSN"
          ],
          "correct": 1,
          "explanation": "Object Lambda: store one copy. Create an access point with Object Lambda that redacts SSN. Non-privileged users access via that access point; privileged users access directly. One source of truth, different views.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "Performance consideration of Object Lambda?",
          "options": [
            "No impact",
            "Added latency for Lambda execution; cold starts possible",
            "Faster than regular S3",
            "Only works for small objects"
          ],
          "correct": 1,
          "explanation": "Object Lambda adds Lambda execution time to GET latency. Cold starts can add more latency. For hot paths, consider caching the transformed result. Object Lambda is powerful but has latency implications. Balance use case needs.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-064",
      "type": "multiple-choice",
      "question": "What is CloudFront integration with S3?",
      "options": [
        "A backup service",
        "Using CloudFront (CDN) to cache and serve S3 content globally with low latency",
        "Encrypting S3 data",
        "Monitoring S3"
      ],
      "correct": 1,
      "explanation": "CloudFront + S3: content is cached at edge locations worldwide. Users fetch from nearest edge, not from S3 region. Benefits: lower latency, reduced S3 costs (fewer requests to origin), DDoS protection. Common for static websites, media.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-065",
      "type": "multi-select",
      "question": "Benefits of putting CloudFront in front of S3?",
      "options": [
        "Lower latency for global users",
        "Reduced S3 request costs (fewer origin requests)",
        "DDoS protection",
        "Object versioning"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "CloudFront benefits: global edge caching (low latency), fewer S3 requests (CloudFront caches), DDoS protection (AWS Shield). Versioning is an S3 feature, not related to CloudFront.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-066",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your S3 bucket serves static files. You want to keep the bucket private but serve files via CloudFront. How?",
          "options": [
            "Make bucket public",
            "Use Origin Access Control (OAC) — CloudFront can access S3, but direct S3 access is blocked",
            "Give CloudFront IAM credentials",
            "It's not possible"
          ],
          "correct": 1,
          "explanation": "Origin Access Control (formerly Origin Access Identity): CloudFront gets special access to S3, bucket policy allows only CloudFront. Direct S3 URLs are blocked. Users must go through CloudFront. Secure and enforces CDN use.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "With OAC, what bucket policy is needed?",
          "options": [
            "Public access policy",
            "Policy allowing CloudFront service principal to GetObject",
            "No policy needed",
            "IAM role for the bucket"
          ],
          "correct": 1,
          "explanation": "Bucket policy: {Effect: Allow, Principal: {Service: cloudfront.amazonaws.com}, Action: s3:GetObject, Resource: bucket/*, Condition: {StringEquals: {AWS:SourceArn: distribution-arn}}}. Only that CloudFront distribution can access.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-067",
      "type": "multiple-choice",
      "question": "What is S3 Intelligent-Tiering?",
      "options": [
        "Manual tier selection",
        "Automatic cost optimization that moves objects between tiers based on access patterns",
        "Intelligent sorting of objects",
        "AI-powered storage"
      ],
      "correct": 1,
      "explanation": "Intelligent-Tiering automatically moves objects between access tiers based on usage. Frequently accessed → hot tier; not accessed for 30 days → infrequent; 90 days → archive. No retrieval fees, small monitoring fee. Good when access patterns are unpredictable.",
      "detailedExplanation": "Generalize from s3 Intelligent-Tiering to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 30 days and 90 days should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-068",
      "type": "multi-select",
      "question": "When is S3 Intelligent-Tiering a good choice?",
      "options": [
        "Unknown or changing access patterns",
        "Want automatic cost optimization without lifecycle management",
        "Data accessed very predictably (always hot or always cold)",
        "Avoid retrieval fees on infrequently accessed data"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Intelligent-Tiering works well for: unpredictable access (auto-optimizes), hands-off approach (no manual lifecycle), avoiding retrieval fees (no fees, just monitoring). For predictable patterns, manual tiering might be cheaper.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-069",
      "type": "multiple-choice",
      "question": "What is the maximum object size in S3?",
      "options": ["100 MB", "5 GB", "5 TB", "Unlimited"],
      "correct": 2,
      "explanation": "Maximum object size is 5 TB. Single PUT is limited to 5 GB; larger objects require multipart upload. No limit on number of objects. Individual object can be up to 5 TB, which covers most use cases.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Numbers such as 5 TB and 5 GB should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-070",
      "type": "ordering",
      "question": "Rank these storage options from cheapest to most expensive per GB (approximate):",
      "items": [
        "S3 Standard",
        "S3 Glacier Deep Archive",
        "EBS (SSD)",
        "EC2 instance storage"
      ],
      "correctOrder": [1, 0, 3, 2],
      "explanation": "Glacier Deep Archive is cheapest (~$0.001/GB). S3 Standard is more (~$0.023/GB). Instance storage comes with EC2 (ephemeral). EBS SSD is most expensive (~$0.10/GB). Object storage is much cheaper than block storage for bulk data.",
      "detailedExplanation": "Generalize from rank these storage options from cheapest to most expensive per GB (approximate): to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Place obvious extremes first, then sort the middle by pairwise comparison. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Keep quantities like 0.001 and 0.023 in aligned units before deciding on an implementation approach. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-071",
      "type": "multiple-choice",
      "question": "How does S3 pricing work?",
      "options": [
        "Flat monthly fee",
        "Pay for storage (GB-month), requests (GET/PUT), and data transfer out",
        "Per object fee",
        "Free with AWS account"
      ],
      "correct": 1,
      "explanation": "S3 pricing: storage (per GB-month, varies by class), requests (per 1000 GET/PUT, varies by class), data transfer (outbound to internet, intra-region is free). No upfront costs. Understand all components for cost estimation.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. If values like 1000 appear, convert them into one unit basis before comparison. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-072",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your S3 bill is higher than expected. Storage cost is small, but request costs are huge. What might be happening?",
          "options": [
            "Too much data",
            "Excessive small object requests — many GETs/PUTs on tiny objects",
            "Wrong region",
            "Encryption overhead"
          ],
          "correct": 1,
          "explanation": "Request costs add up: millions of GET requests on small objects can exceed storage cost. Check: is your app making excessive requests? Are there retry storms? Is there unexpected traffic (scraping)? Analyze request patterns.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "How to reduce request costs?",
          "options": [
            "Store larger objects",
            "Cache frequently accessed objects (CloudFront or app cache), batch operations",
            "Use cheaper storage class",
            "Request costs can't be reduced"
          ],
          "correct": 1,
          "explanation": "Reduce requests: CloudFront caching (fewer origin requests), application caching, batch reads (list then filter vs. many HEAD requests), reduce unnecessary operations. Storage class doesn't affect request cost (though Glacier has different pricing).",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-073",
      "type": "multiple-choice",
      "question": "What is S3 Requester Pays?",
      "options": [
        "AWS pays for S3",
        "A bucket configuration where the requester pays for request and data transfer costs",
        "Customers request payment",
        "A billing feature"
      ],
      "correct": 1,
      "explanation": "Requester Pays: normally bucket owner pays for requests and transfer. With Requester Pays enabled, the requester's AWS account is charged. Use case: sharing large datasets — recipients pay for their access. Requester must be authenticated.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-074",
      "type": "multi-select",
      "question": "What are common S3 anti-patterns?",
      "options": [
        "Using S3 as a primary database",
        "Many small objects when larger aggregations would work",
        "Not using lifecycle policies for aging data",
        "Storing media files"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Anti-patterns: S3 as database (no ACID, no queries), excessive small objects (request cost, performance), no lifecycle (paying Standard for archive data). Storing media is a perfect use case — not an anti-pattern.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-075",
      "type": "multiple-choice",
      "question": "What is S3 consistency model (current)?",
      "options": [
        "Eventually consistent for all operations",
        "Strong read-after-write consistency for all operations",
        "Consistent for writes, eventual for reads",
        "No consistency guarantees"
      ],
      "correct": 1,
      "explanation": "Since December 2020, S3 provides strong read-after-write consistency. PUT a new object → immediate GET returns it. Overwrite → GET returns new version. DELETE → GET returns 404. This was a major improvement over the previous eventual consistency model.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. If values like 2020 and 404 appear, convert them into one unit basis before comparison. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-076",
      "type": "two-stage",
      "stages": [
        {
          "question": "Before 2020, S3 was eventually consistent for overwrites. What problem did that cause?",
          "options": [
            "No problem",
            "Read after write might return old version; applications needed to handle stale reads",
            "Writes would fail",
            "Data corruption"
          ],
          "correct": 1,
          "explanation": "Eventually consistent: write new version, immediately read, might get old version. Applications needed: retries, caching, accepting stale reads. This caused bugs and complexity. Strong consistency (now default) eliminates this.",
          "detailedExplanation": "Generalize from before 2020, S3 was eventually consistent for overwrites to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Numbers such as 2020 should be normalized first so downstream reasoning stays consistent. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "With current strong consistency, do you still need to worry about stale reads?",
          "options": [
            "Yes — still eventually consistent",
            "No — strong consistency means reads reflect latest writes",
            "Only for deletes",
            "Depends on storage class"
          ],
          "correct": 1,
          "explanation": "Current S3 is strongly consistent. After successful write, reads return the new data. After successful delete, reads return 404. This applies to all storage classes. Applications can rely on consistent reads. (Replication across regions is still async.)",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 404 should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-077",
      "type": "multiple-choice",
      "question": "What is S3 Access Analyzer?",
      "options": [
        "Query analyzer for S3 Select",
        "A tool that identifies S3 resources accessible from outside your account",
        "Storage usage analyzer",
        "Cost analyzer"
      ],
      "correct": 1,
      "explanation": "IAM Access Analyzer (for S3) reviews bucket policies and ACLs to find resources accessible externally: public buckets, cross-account access. Helps identify unintended exposure. Part of security posture management.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that fits durability and cost requirements for the workload shape. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-078",
      "type": "multi-select",
      "question": "What security features should be enabled for production S3 buckets?",
      "options": [
        "Block Public Access",
        "Encryption at rest (SSE)",
        "Server access logging or CloudTrail",
        "Public read for all buckets"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Production best practices: Block Public Access (prevent accidental exposure), encryption at rest (data protection), logging (audit trail). Never enable public read unless specifically needed (static websites, public datasets) — and even then, use CloudFront.",
      "detailedExplanation": "Generalize from security features should be enabled for production S3 buckets to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-079",
      "type": "multiple-choice",
      "question": "What is S3 Server Access Logging?",
      "options": [
        "Logging in to S3",
        "Detailed logs of requests made to a bucket, written to another bucket",
        "Error logging",
        "Application logging"
      ],
      "correct": 1,
      "explanation": "Server access logging records all requests to a bucket: requester, object, time, action, response. Logs written to a target bucket. Use for: audit, security analysis, access pattern understanding. Alternative: CloudTrail for S3 data events (more expensive but richer).",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-080",
      "type": "two-stage",
      "stages": [
        {
          "question": "You suspect unauthorized access to your S3 bucket. How do you investigate?",
          "options": [
            "Ask AWS",
            "Review server access logs or CloudTrail S3 data events for the bucket",
            "It's impossible to know",
            "Check CloudWatch metrics"
          ],
          "correct": 1,
          "explanation": "Access logs show every request: IP, user agent, auth info, action, result. CloudTrail S3 data events provide similar info with AWS identity context. Look for: unusual IPs, high volume, unexpected principals, access to sensitive keys.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints."
        },
        {
          "question": "You find requests from an unknown IAM user in another account. What happened?",
          "options": [
            "Data breach",
            "Check bucket policy — cross-account access may be intentionally granted",
            "AWS internal access",
            "Log corruption"
          ],
          "correct": 1,
          "explanation": "First check: is cross-account access intended? Bucket policies can grant access to other accounts. If not intended, identify and remove the policy grant. If unexpected access, investigate further (compromised credentials, policy mistake).",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-081",
      "type": "multiple-choice",
      "question": "What is S3 Storage Lens?",
      "options": [
        "A magnifying tool",
        "A dashboard providing visibility into storage usage, cost, and optimization opportunities across S3",
        "An optical storage format",
        "A lens for photos in S3"
      ],
      "correct": 1,
      "explanation": "Storage Lens provides organization-wide visibility into S3: usage trends, cost optimization recommendations, activity metrics. Dashboard shows: total storage, growth rate, request patterns, cost breakdowns. Free tier (basic) and advanced tier (additional metrics).",
      "detailedExplanation": "Generalize from s3 Storage Lens to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-082",
      "type": "multi-select",
      "question": "What insights does S3 Storage Lens provide?",
      "options": [
        "Storage usage trends over time",
        "Cost optimization recommendations",
        "Activity metrics (request rates)",
        "Object content analysis"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Storage Lens shows: usage trends (growth, distribution), cost optimization (suggest lifecycle policies, storage class changes), activity (request patterns, errors). It doesn't analyze object content — that's Macie or your own processing.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-083",
      "type": "multiple-choice",
      "question": "What is Amazon Macie?",
      "options": [
        "A data analysis service",
        "A security service using ML to discover, classify, and protect sensitive data in S3",
        "A storage service",
        "A database"
      ],
      "correct": 1,
      "explanation": "Macie scans S3 for sensitive data: PII, credentials, financial info. Uses ML to classify content. Alerts on: public buckets with sensitive data, policy violations. Helps with compliance (GDPR, HIPAA) and security.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that ignore durability, retention, or access-pattern constraints. Separate raw data, redundancy overhead, and retention horizon when reasoning about storage choices. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-084",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to ensure no S3 buckets contain unencrypted PII. How?",
          "options": [
            "Manually review all objects",
            "Enable Macie to scan buckets and alert on sensitive data findings",
            "Trust developers not to store PII",
            "It's not possible at scale"
          ],
          "correct": 1,
          "explanation": "Macie automatically scans S3 content, identifies PII (SSN, credit cards, etc.), and generates findings. Configure: which buckets, how often, what to detect. Findings go to Security Hub or custom alerts. Continuous compliance monitoring.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "Macie finds PII in a public bucket. Remediation?",
          "options": [
            "Delete the bucket",
            "Remove public access, move/encrypt sensitive data, investigate how this happened",
            "Ignore the finding",
            "Make the bucket more public"
          ],
          "correct": 1,
          "explanation": "Immediately: remove public access (Block Public Access). Then: assess exposure (who accessed?), remediate data (encrypt, delete, move), investigate root cause (how did public + PII happen?), prevent recurrence (policies, guardrails).",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-085",
      "type": "multiple-choice",
      "question": "What is the difference between S3 and EBS?",
      "options": [
        "They're the same",
        "S3 is object storage (files via HTTP API); EBS is block storage (volumes mounted to EC2)",
        "EBS is for backup; S3 is for databases",
        "S3 is faster"
      ],
      "correct": 1,
      "explanation": "S3: object storage, accessed via HTTP/REST, virtually unlimited, pay per use. EBS: block storage volumes attached to EC2, like a hard drive, fixed size, better for databases/file systems. Different use cases and access patterns.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-086",
      "type": "multi-select",
      "question": "When to use S3 vs. EBS?",
      "options": [
        "S3 for static files, backups, data lakes",
        "EBS for database storage, file systems needing mount",
        "S3 when multiple applications need concurrent access",
        "EBS for web content delivery"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "S3: static files (cheap, durable, scalable), shared access (multiple apps via API). EBS: databases (low latency, file system), single EC2 mount. Web content delivery often uses S3 + CloudFront, not EBS.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-087",
      "type": "multiple-choice",
      "question": "What is S3 for hosting a static website?",
      "options": [
        "Not possible",
        "S3 bucket configured for static website hosting — serves HTML, CSS, JS directly",
        "Requires EC2",
        "Only for images"
      ],
      "correct": 1,
      "explanation": "S3 can host static websites: enable static website hosting, upload HTML/CSS/JS, S3 serves directly. No servers to manage. For HTTPS and custom domain, add CloudFront. Perfect for: static sites, SPAs, documentation.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-088",
      "type": "two-stage",
      "stages": [
        {
          "question": "You want to host a React SPA on S3 with a custom domain and HTTPS. What's the architecture?",
          "options": [
            "S3 alone with custom domain",
            "S3 (static hosting) + CloudFront (CDN, HTTPS) + Route 53 (DNS)",
            "EC2 with React",
            "Lambda serving React"
          ],
          "correct": 1,
          "explanation": "Standard architecture: S3 holds files, CloudFront serves with HTTPS (free cert via ACM), Route 53 points custom domain to CloudFront. Serverless, scalable, cheap. S3 alone doesn't support HTTPS with custom domains.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 53 should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints."
        },
        {
          "question": "React SPA uses client-side routing (/about, /users). Accessing /about directly returns 404. Fix?",
          "options": [
            "Create /about.html file",
            "Configure CloudFront or S3 to serve index.html for all routes (error page configuration)",
            "Use server-side rendering",
            "It's not fixable with S3"
          ],
          "correct": 1,
          "explanation": "SPA routing: all routes should serve index.html, React handles routing client-side. Configure: S3 website error document = index.html, or CloudFront custom error response (404 → /index.html, 200). Either approach enables client-side routing.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Numbers such as 404 and 200 should be normalized first so downstream reasoning stays consistent. Common pitfall: schema optimized for entities instead of queries."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-089",
      "type": "multiple-choice",
      "question": "What is S3 Express One Zone?",
      "options": [
        "A delivery service",
        "High-performance S3 storage class with single-AZ, low-latency access",
        "A fast region",
        "One-time zone access"
      ],
      "correct": 1,
      "explanation": "S3 Express One Zone (2023): single-AZ, low-latency (single-digit ms), high throughput. For: ML training, analytics, real-time apps needing speed over multi-AZ durability. Trade-off: single AZ (less durability than standard S3).",
      "detailedExplanation": "Generalize from s3 Express One Zone to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the approach that fits durability and cost requirements for the workload shape. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 2023 should be normalized first so downstream reasoning stays consistent. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-090",
      "type": "ordering",
      "question": "Rank these storage options from lowest to highest latency for random access:",
      "items": [
        "EC2 instance store (local NVMe)",
        "S3 Standard",
        "EBS SSD (gp3)",
        "S3 Glacier"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "Instance store: local, sub-ms. EBS: network-attached, low ms. S3: HTTP API, higher latency (10-100ms typical). Glacier: retrieval takes minutes to hours. For low-latency random access, use block storage, not object storage.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Interface decisions should be justified by contract stability and client impact over time. Numbers such as 10 and 100ms should be normalized first so downstream reasoning stays consistent. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-091",
      "type": "multiple-choice",
      "question": "What is the shared responsibility model for S3 security?",
      "options": [
        "AWS handles everything",
        "Customer handles everything",
        "AWS secures the infrastructure; customer secures their data and access controls",
        "Security is optional"
      ],
      "correct": 2,
      "explanation": "Shared responsibility: AWS secures the cloud (physical, infrastructure, durability). Customer secures in the cloud: data encryption, access policies, who can access, logging. AWS provides tools; customer must use them correctly.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Discard storage decisions that optimize one dimension while violating another core requirement. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-092",
      "type": "two-stage",
      "stages": [
        {
          "question": "An S3 bucket is accidentally made public and data is exposed. Who is responsible?",
          "options": [
            "AWS — they should prevent this",
            "Customer — access configuration is customer responsibility",
            "Shared responsibility",
            "No one"
          ],
          "correct": 1,
          "explanation": "Customer responsibility: bucket policies, ACLs, Block Public Access are customer-configured. AWS provides tools (Block Public Access enabled by default now), but if customer disables and creates public policy, that's on the customer.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead."
        },
        {
          "question": "How can organizations prevent accidental public buckets?",
          "options": [
            "Hope developers don't make mistakes",
            "Account-level Block Public Access, SCPs, automated compliance checks",
            "Disable S3",
            "Only use on-premise storage"
          ],
          "correct": 1,
          "explanation": "Preventive controls: account-level Block Public Access (overrides bucket settings), SCPs (prevent DisableBlockPublicAccess), automated checks (Config rules, Macie, Access Analyzer). Defense in depth — multiple layers of prevention.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements."
        }
      ],
      "detailedExplanation": "Generalize from object Storage to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-093",
      "type": "multiple-choice",
      "question": "What is S3 Object Ownership?",
      "options": [
        "Tracking who created objects",
        "A setting controlling whether ACLs are used and who owns objects in the bucket",
        "Legal ownership of data",
        "File metadata"
      ],
      "correct": 1,
      "explanation": "Object Ownership controls: who owns objects (bucket owner vs. object writer), whether ACLs are enabled. 'Bucket Owner Enforced' disables ACLs entirely — bucket owner owns all objects, policies control access. Simplifies cross-account scenarios.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-094",
      "type": "multi-select",
      "question": "What are best practices for S3 bucket naming?",
      "options": [
        "Use globally unique names (required)",
        "Use descriptive names (environment, purpose)",
        "Avoid PII in bucket names",
        "Include AWS account ID in name for uniqueness"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "Bucket naming: globally unique (required), descriptive (mybucket-prod-logs), no PII (bucket names can appear in URLs), include account ID or random suffix for uniqueness. Good naming helps with management and security.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-095",
      "type": "multiple-choice",
      "question": "What is the eventual consistency model for S3 bucket listings?",
      "options": [
        "Always consistent",
        "Eventually consistent — listing may not immediately reflect recent creates or deletes",
        "Never consistent",
        "Depends on storage class"
      ],
      "correct": 1,
      "explanation": "While object operations are now strongly consistent, bucket listings (ListObjects) may have slight delays for very recent changes. In practice, this rarely matters. Object reads (GET, HEAD) are strongly consistent.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard choices that violate required invariants during concurrent or failed states. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-096",
      "type": "two-stage",
      "stages": [
        {
          "question": "You're designing a system that stores user uploads, needs global access, and must comply with GDPR (data residency). Architecture?",
          "options": [
            "One global S3 bucket",
            "Regional buckets (EU for EU users), with replication only where permitted",
            "Store in US only",
            "Don't use S3"
          ],
          "correct": 1,
          "explanation": "GDPR requires EU user data in EU. Architecture: EU bucket for EU users, possibly other regional buckets. Replicate within EU if needed. Don't replicate EU data to US unless compliant. Access controls enforce data residency.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: ignoring durability/recovery requirements."
        },
        {
          "question": "How do you route users to the right regional bucket?",
          "options": [
            "One bucket serves all",
            "Application layer routes based on user region; or use S3 Multi-Region Access Points",
            "Users choose their bucket",
            "DNS routing alone"
          ],
          "correct": 1,
          "explanation": "Options: application routes to correct bucket based on user, or use S3 Multi-Region Access Points (routes to nearest bucket, supports failover). For compliance, ensure the routing respects data residency — not just latency-based.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-097",
      "type": "multiple-choice",
      "question": "What is S3 Multi-Region Access Points?",
      "options": [
        "Multiple S3 regions",
        "A single endpoint that routes to buckets in multiple regions based on latency or failover",
        "Multi-account access",
        "A replication feature"
      ],
      "correct": 1,
      "explanation": "Multi-Region Access Points provide one global endpoint that routes to the closest (or available) regional bucket. Combine with replication for active-active or failover scenarios. Simplifies multi-region architectures.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-098",
      "type": "multi-select",
      "question": "What are use cases for Multi-Region Access Points?",
      "options": [
        "Global applications needing low latency",
        "Disaster recovery (failover to another region)",
        "Single-region applications",
        "Active-active replication with automatic routing"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Multi-Region Access Points: global apps (route to nearest), DR (failover routing), active-active (replicate + route to any). Single-region apps don't need multi-region access points — they'd use a regular bucket in one region.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-099",
      "type": "ordering",
      "question": "Rank these from most to least important when choosing object storage for a new application:",
      "items": [
        "Durability (data safety)",
        "Cost",
        "Latency for hot data",
        "Feature richness"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "Data safety is paramount — losing data is unacceptable. Latency matters for user experience (hot data). Cost matters at scale. Feature richness is nice to have. Priorities may vary — a backup system might prioritize cost over latency.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Place obvious extremes first, then sort the middle by pairwise comparison. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    },
    {
      "id": "obj-100",
      "type": "multiple-choice",
      "question": "What's the key takeaway about when to use object storage?",
      "options": [
        "Replace all databases with S3",
        "Use for unstructured data (files, media, backups, data lakes) at scale; not for transactional workloads",
        "Only for backups",
        "Only for large files"
      ],
      "correct": 1,
      "explanation": "Object storage is for: files/blobs (images, videos, documents), backups, data lakes, static content. Not for: transactional databases, low-latency structured queries. It's cheap, durable, scalable — for the right use cases.",
      "detailedExplanation": "Generalize from what's the key takeaway about when to use object storage to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer the approach that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["storage-selection", "object-storage"],
      "difficulty": "senior"
    }
  ]
}
