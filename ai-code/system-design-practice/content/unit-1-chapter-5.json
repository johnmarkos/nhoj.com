{
  "unit": 1,
  "unitTitle": "Estimation",
  "chapter": 5,
  "chapterTitle": "QPS & Load",
  "chapterDescription": "Request rates, server capacity, load balancing, throughput calculations",
  "problems": [
    {
      "id": "qps-001",
      "type": "multiple-choice",
      "question": "Your service has 10 million DAU. Each user makes 50 requests per day on average. What's your average QPS?",
      "options": ["~580 QPS", "~5,800 QPS", "~58,000 QPS", "~580,000 QPS"],
      "correct": 1,
      "explanation": "10M × 50 = 500M requests/day. 500M ÷ 86,400 sec = 5,787 QPS ≈ 5,800 QPS.",
      "detailedExplanation": "Read this as a scenario about \"your service has 10 million DAU\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 10 and 50 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-002",
      "type": "multiple-choice",
      "question": "A server handles 500 requests/second. Each request takes 20ms average. How many concurrent requests?",
      "options": ["~10", "~100", "~1,000", "~10,000"],
      "correct": 0,
      "explanation": "Little's Law: L = λ × W. Concurrent = 500 req/s × 0.02s = 10 concurrent requests.",
      "detailedExplanation": "The key clue in this question is \"server handles 500 requests/second\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 500 and 20ms should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-003",
      "type": "multiple-choice",
      "question": "Your peak traffic is 5× your average. Average is 10,000 QPS. How many servers if each handles 2,000 QPS?",
      "options": ["5 servers", "13 servers", "25 servers", "50 servers"],
      "correct": 2,
      "explanation": "Peak = 5 × 10,000 = 50,000 QPS. Servers needed = 50,000 ÷ 2,000 = 25 servers.",
      "detailedExplanation": "Start from \"your peak traffic is 5× your average\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 5 and 10,000 QPS in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-004",
      "type": "multiple-choice",
      "question": "A database connection takes 5ms to establish. Your app needs 1,000 connections/second. Why use connection pooling?",
      "options": [
        "Save 5 seconds/second of CPU",
        "Save 5 seconds/second of latency",
        "Avoid TCP port exhaustion",
        "All of the above"
      ],
      "correct": 3,
      "explanation": "Connection pooling saves 5ms × 1,000 = 5 seconds of latency per second, reduces CPU overhead, and prevents port exhaustion from rapid connect/disconnect.",
      "detailedExplanation": "If you keep \"database connection takes 5ms to establish\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 5ms and 1,000 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-005",
      "type": "multiple-choice",
      "question": "Your API has P50=10ms, P99=100ms. At 1,000 QPS, how many requests/second exceed 100ms?",
      "options": ["~1", "~10", "~100", "~500"],
      "correct": 1,
      "explanation": "P99 means 99% are under 100ms, so 1% exceed it. 1% of 1,000 = 10 requests/second.",
      "detailedExplanation": "The core signal here is \"your API has P50=10ms, P99=100ms\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 10ms and 100ms in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-006",
      "type": "multiple-choice",
      "question": "A rate limiter allows 100 requests per minute per user. 10,000 users are active. Max theoretical QPS?",
      "options": ["~167 QPS", "~1,670 QPS", "~16,700 QPS", "~167,000 QPS"],
      "correct": 2,
      "explanation": "10,000 users × 100 req/min = 1M req/min = 16,667 req/sec ≈ 16,700 QPS.",
      "detailedExplanation": "Use \"rate limiter allows 100 requests per minute per user\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 100 and 10,000 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-007",
      "type": "multiple-choice",
      "question": "Your load balancer distributes traffic to 8 servers. One server fails. Load increase per remaining server?",
      "options": ["~12.5%", "~14.3%", "~16.7%", "~25%"],
      "correct": 1,
      "explanation": "Before: each handles 1/8 = 12.5%. After: each handles 1/7 = 14.3%. Increase = 14.3% - 12.5% = 1.8% absolute, or 14.3% relative increase.",
      "detailedExplanation": "This prompt is really about \"your load balancer distributes traffic to 8 servers\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 8 and 1 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-008",
      "type": "multiple-choice",
      "question": "A cache has 90% hit rate. Backend handles 1,000 QPS at capacity. Total QPS the system can handle?",
      "options": ["~1,000 QPS", "~9,000 QPS", "~10,000 QPS", "~100,000 QPS"],
      "correct": 2,
      "explanation": "10% of requests hit backend. If backend max is 1,000 QPS, that's 10% of total. Total = 1,000 ÷ 0.1 = 10,000 QPS.",
      "detailedExplanation": "The decision turns on \"cache has 90% hit rate\". Reject options that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 90 and 1,000 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-009",
      "type": "multiple-choice",
      "question": "Each API request triggers 3 database queries. API is at 5,000 QPS. Database QPS?",
      "options": ["~1,700 QPS", "~5,000 QPS", "~15,000 QPS", "~25,000 QPS"],
      "correct": 2,
      "explanation": "5,000 API calls × 3 DB queries = 15,000 database QPS.",
      "detailedExplanation": "Read this as a scenario about \"each API request triggers 3 database queries\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Keep quantities like 3 and 5,000 QPS in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-010",
      "type": "multiple-choice",
      "question": "Your server has 100 worker threads. Each request takes 50ms. Maximum throughput?",
      "options": ["~200 QPS", "~2,000 QPS", "~20,000 QPS", "~200,000 QPS"],
      "correct": 1,
      "explanation": "Each thread handles 1/0.05 = 20 requests/sec. 100 threads × 20 = 2,000 QPS max.",
      "detailedExplanation": "Start from \"your server has 100 worker threads\", then pressure-test the result against the options. Prefer the choice that fits durability and cost requirements for the workload shape. Capacity answers are more defensible when growth and replication are modeled explicitly. Numbers such as 100 and 50ms should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-011",
      "type": "multiple-choice",
      "question": "A microservice has 200ms timeout. Downstream service P99 is 150ms. What happens at 99.9th percentile?",
      "options": [
        "Request succeeds",
        "Request times out",
        "Depends on retry config",
        "Cannot determine"
      ],
      "correct": 2,
      "explanation": "P99 is 150ms but P99.9 could be higher than 200ms. Without knowing P99.9 or the latency distribution, we can't determine if it times out.",
      "detailedExplanation": "The key clue in this question is \"microservice has 200ms timeout\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 200ms and 150ms in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-012",
      "type": "multiple-choice",
      "question": "Your service auto-scales at 70% CPU. Each server handles 1,000 QPS at 70% CPU. Traffic grows from 5,000 to 15,000 QPS. Servers after scaling?",
      "options": [
        "5 → 15 servers",
        "5 → 21 servers",
        "7 → 15 servers",
        "7 → 21 servers"
      ],
      "correct": 0,
      "explanation": "At 70% CPU = 1,000 QPS per server. 5,000 QPS needs 5 servers. 15,000 QPS needs 15 servers.",
      "detailedExplanation": "Read this as a scenario about \"your service auto-scales at 70% CPU\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 70 and 1,000 QPS in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-013",
      "type": "multiple-choice",
      "question": "A queue processes 100 messages/second. Messages arrive at 120/second. Queue depth after 1 minute?",
      "options": [
        "~200 messages",
        "~1,200 messages",
        "~6,000 messages",
        "~7,200 messages"
      ],
      "correct": 1,
      "explanation": "Backlog rate = 120 - 100 = 20 messages/second. After 60 seconds: 20 × 60 = 1,200 messages queued.",
      "detailedExplanation": "The decision turns on \"queue processes 100 messages/second\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 100 and 120 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-014",
      "type": "multiple-choice",
      "question": "Database has 100 max connections. Each query takes 10ms. Max query throughput?",
      "options": [
        "~1,000 QPS",
        "~10,000 QPS",
        "~100,000 QPS",
        "~1,000,000 QPS"
      ],
      "correct": 1,
      "explanation": "Each connection handles 1/0.01 = 100 queries/sec. 100 connections × 100 = 10,000 QPS.",
      "detailedExplanation": "This prompt is really about \"database has 100 max connections\". Eliminate options that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Keep quantities like 100 and 10ms in aligned units before selecting an answer. Common pitfall: ignoring durability/recovery requirements.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-015",
      "type": "multiple-choice",
      "question": "Your CDN absorbs 95% of traffic. Origin handles 500 QPS max. Peak traffic the CDN can support?",
      "options": ["~500 QPS", "~5,000 QPS", "~10,000 QPS", "~50,000 QPS"],
      "correct": 2,
      "explanation": "5% hits origin. If origin max is 500, total = 500 ÷ 0.05 = 10,000 QPS.",
      "detailedExplanation": "Use \"your CDN absorbs 95% of traffic\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 95 and 500 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-016",
      "type": "multiple-choice",
      "question": "An endpoint has 100ms latency. You add 2 sequential downstream calls of 30ms each. New latency?",
      "options": ["~130ms", "~160ms", "~200ms", "~300ms"],
      "correct": 1,
      "explanation": "100ms + 30ms + 30ms = 160ms (sequential calls add their latencies).",
      "detailedExplanation": "The core signal here is \"endpoint has 100ms latency\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. If values like 100ms and 2 appear, convert them into one unit basis before comparison. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-017",
      "type": "multiple-choice",
      "question": "You parallelize 2 calls of 50ms each instead of running them sequentially. Time saved?",
      "options": ["0ms", "~25ms", "~50ms", "~100ms"],
      "correct": 2,
      "explanation": "Sequential: 50 + 50 = 100ms. Parallel: max(50, 50) = 50ms. Saved: 50ms.",
      "detailedExplanation": "If you keep \"you parallelize 2 calls of 50ms each instead of running them sequentially\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 2 and 50ms appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-018",
      "type": "multiple-choice",
      "question": "A service handles 10,000 QPS. You add a circuit breaker that trips at 50% error rate. At 60% errors, effective QPS?",
      "options": [
        "0 QPS (circuit open)",
        "~4,000 QPS",
        "~6,000 QPS",
        "~10,000 QPS"
      ],
      "correct": 0,
      "explanation": "Circuit breaker trips at 50% errors. At 60%, the circuit is open, rejecting all requests (0 effective QPS until it recovers).",
      "detailedExplanation": "Start from \"service handles 10,000 QPS\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 10,000 QPS and 50 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-019",
      "type": "multiple-choice",
      "question": "Your Redis cache handles 100,000 ops/sec. Each API request needs 5 cache operations. Max API QPS limited by cache?",
      "options": ["~5,000 QPS", "~20,000 QPS", "~100,000 QPS", "~500,000 QPS"],
      "correct": 1,
      "explanation": "100,000 cache ops ÷ 5 ops per request = 20,000 API QPS.",
      "detailedExplanation": "The key clue in this question is \"your Redis cache handles 100,000 ops/sec\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. If values like 100,000 and 5 appear, convert them into one unit basis before comparison. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-020",
      "type": "multiple-choice",
      "question": "Health checks run every 5 seconds. 50 services, 10 instances each. Health check QPS?",
      "options": ["~10 QPS", "~100 QPS", "~500 QPS", "~5,000 QPS"],
      "correct": 1,
      "explanation": "50 services × 10 instances = 500 health checks per 5 seconds = 100 QPS.",
      "detailedExplanation": "Use \"health checks run every 5 seconds\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 5 seconds and 50 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-021",
      "type": "multiple-choice",
      "question": "Your API Gateway has 10ms overhead per request. Backend latency is 40ms. Total latency and throughput impact?",
      "options": [
        "50ms latency, 20% throughput loss",
        "50ms latency, no throughput impact",
        "40ms latency, 10ms async",
        "Depends on gateway architecture"
      ],
      "correct": 0,
      "explanation": "Gateway adds 10ms to the request path. Total = 40 + 10 = 50ms. Throughput per worker drops from 1/0.04 to 1/0.05 = 20% reduction.",
      "detailedExplanation": "This prompt is really about \"your API Gateway has 10ms overhead per request\". Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. If values like 10ms and 40ms appear, convert them into one unit basis before comparison. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-022",
      "type": "multiple-choice",
      "question": "A retry policy retries failed requests 3 times with 1-second backoff. Base failure rate is 1%. Worst-case QPS amplification?",
      "options": ["~1%", "~3%", "~4%", "~1.03×"],
      "correct": 2,
      "explanation": "1% fail × up to 3 retries = 3% extra requests. Total = 100% + 3% = 103%, or ~4% amplification (1.03× is the same as ~3% increase).",
      "detailedExplanation": "If you keep \"retry policy retries failed requests 3 times with 1-second backoff\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 3 and 1 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-023",
      "type": "multiple-choice",
      "question": "You shard by user_id across 16 database shards. Hot user generates 10% of all queries. That shard's load vs. even distribution?",
      "options": [
        "~1.6× average",
        "~2.5× average",
        "~10× average",
        "~16× average"
      ],
      "correct": 1,
      "explanation": "Even distribution = 100%/16 = 6.25% per shard. Hot shard gets 10% (hot user) + 90%/16 (fair share of rest) = 10% + 5.6% = 15.6%. Ratio = 15.6/6.25 = 2.5×.",
      "detailedExplanation": "The core signal here is \"you shard by user_id across 16 database shards\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Keep quantities like 16 and 10 in aligned units before selecting an answer. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-024",
      "type": "multiple-choice",
      "question": "Your batch job processes 1 million records. Single-threaded takes 10 hours. With 100 workers, theoretical time?",
      "options": [
        "~6 minutes",
        "~1 hour",
        "~6 hours",
        "Depends on coordination overhead"
      ],
      "correct": 0,
      "explanation": "Ideal speedup: 10 hours ÷ 100 = 0.1 hours = 6 minutes. (Real-world has coordination overhead.)",
      "detailedExplanation": "The key clue in this question is \"your batch job processes 1 million records\". Discard choices that violate required invariants during concurrent or failed states. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 1 and 10 hours should be normalized first so downstream reasoning stays consistent. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-025",
      "type": "multiple-choice",
      "question": "Token bucket rate limiter: 100 tokens/second, bucket size 500. After idle period, how many burst requests allowed?",
      "options": [
        "100 requests",
        "500 requests",
        "600 requests",
        "Unlimited until 100/s sustained"
      ],
      "correct": 1,
      "explanation": "Bucket fills to max capacity (500) when idle. Burst capacity = bucket size = 500 requests.",
      "detailedExplanation": "Start from \"token bucket rate limiter: 100 tokens/second, bucket size 500\", then pressure-test the result against the options. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 100 and 500 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-026",
      "type": "multiple-choice",
      "question": "Leaky bucket rate limiter: 100 requests/second outflow. Burst of 500 requests arrives. Time to drain?",
      "options": [
        "~0.5 seconds",
        "~5 seconds",
        "~50 seconds",
        "Depends on bucket size"
      ],
      "correct": 1,
      "explanation": "500 requests ÷ 100/second = 5 seconds to drain (assuming bucket is large enough to hold 500).",
      "detailedExplanation": "The decision turns on \"leaky bucket rate limiter: 100 requests/second outflow\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 100 and 500 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-027",
      "type": "multiple-choice",
      "question": "WebSocket server holds 100,000 connections. Each connection receives 1 message/minute. Message processing QPS?",
      "options": ["~167 QPS", "~1,667 QPS", "~16,667 QPS", "~100,000 QPS"],
      "correct": 1,
      "explanation": "100,000 connections × 1 msg/min ÷ 60 sec = 1,667 messages/second.",
      "detailedExplanation": "Read this as a scenario about \"webSocket server holds 100,000 connections\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 100,000 and 1 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-028",
      "type": "multiple-choice",
      "question": "Your service has 99.9% availability. It handles 1 million requests/day. Expected failed requests per day?",
      "options": ["~100", "~1,000", "~10,000", "~100,000"],
      "correct": 1,
      "explanation": "0.1% failure rate × 1M = 1,000 failed requests per day.",
      "detailedExplanation": "Use \"your service has 99\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 99.9 and 1 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-029",
      "type": "multiple-choice",
      "question": "A GraphQL query fans out to 5 backend services in parallel. Each service has P95=50ms. Query P95?",
      "options": [
        "~50ms",
        "~100ms",
        "~250ms",
        "Higher than any individual P95"
      ],
      "correct": 3,
      "explanation": "When calling N services in parallel, the total P95 is worse than any individual P95 because you wait for the slowest. With 5 services, probability all are under P95 is 0.95^5 = 77%, so query P95 is higher.",
      "detailedExplanation": "This prompt is really about \"graphQL query fans out to 5 backend services in parallel\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 5 and 50ms in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-030",
      "type": "multiple-choice",
      "question": "Database read replicas handle 80% of reads. Primary handles writes + 20% of reads. Primary write QPS is 1,000. Read:write ratio is 10:1. Primary total QPS?",
      "options": ["~1,000 QPS", "~3,000 QPS", "~10,000 QPS", "~11,000 QPS"],
      "correct": 1,
      "explanation": "Writes: 1,000. Total reads: 1,000 × 10 = 10,000. Primary reads: 20% of 10,000 = 2,000. Primary total: 1,000 + 2,000 = 3,000 QPS.",
      "detailedExplanation": "Read this as a scenario about \"database read replicas handle 80% of reads\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 80 and 20 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-031",
      "type": "multiple-choice",
      "question": "You implement request coalescing for identical requests. 100 duplicate requests in a 10ms window become 1 backend call. Reduction factor?",
      "options": ["~10×", "~100×", "Depends on dedup rate", "~1,000×"],
      "correct": 1,
      "explanation": "100 requests become 1. Reduction = 100×.",
      "detailedExplanation": "The decision turns on \"you implement request coalescing for identical requests\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 100 and 10ms appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-032",
      "type": "multiple-choice",
      "question": "Your API serves 50,000 QPS. Each request costs $0.00001 in compute. Annual API cost?",
      "options": ["~$1,500", "~$15,000", "~$150,000", "~$1.5M"],
      "correct": 1,
      "explanation": "50,000 QPS × 86,400 sec/day × 365 days × $0.00001 = $15,768 ≈ $15,000/year.",
      "detailedExplanation": "Start from \"your API serves 50,000 QPS\", then pressure-test the result against the options. Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. If values like 50,000 QPS and 0.00001 appear, convert them into one unit basis before comparison. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-033",
      "type": "multiple-choice",
      "question": "Thundering herd: cache key expires, 1,000 concurrent requests hit empty cache. Without protection, backend receives?",
      "options": [
        "1 request",
        "~100 requests",
        "~1,000 requests",
        "Depends on cache implementation"
      ],
      "correct": 2,
      "explanation": "Without protection (like request coalescing or locking), all 1,000 requests see cache miss and hit backend simultaneously.",
      "detailedExplanation": "The key clue in this question is \"thundering herd: cache key expires, 1,000 concurrent requests hit empty cache\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. If values like 1,000 appear, convert them into one unit basis before comparison. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-034",
      "type": "multiple-choice",
      "question": "Your service mesh adds sidecar proxies. Each request passes through 2 sidecars (client + server). Sidecar latency is 1ms each. Impact on P50 if base P50 is 20ms?",
      "options": ["20ms (no change)", "21ms", "22ms", "24ms"],
      "correct": 2,
      "explanation": "2 sidecars × 1ms = 2ms added. New P50 = 20 + 2 = 22ms.",
      "detailedExplanation": "The core signal here is \"your service mesh adds sidecar proxies\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 2 and 1ms in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-035",
      "type": "multiple-choice",
      "question": "Event-driven architecture: producer emits 10,000 events/second. 5 consumers each process all events. Total event processing rate?",
      "options": [
        "~10,000/sec",
        "~50,000/sec",
        "~2,000/sec",
        "Depends on partitioning"
      ],
      "correct": 1,
      "explanation": "Fan-out: each consumer processes all 10,000 events. Total = 5 × 10,000 = 50,000 event processings/second.",
      "detailedExplanation": "If you keep \"event-driven architecture: producer emits 10,000 events/second\" in view, the correct answer separates faster. Reject designs that improve throughput while weakening reliability guarantees. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 10,000 and 5 in aligned units before selecting an answer. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-036",
      "type": "multiple-choice",
      "question": "Kafka partition has 1 consumer. Consumer processes at 1,000 messages/second. Producer writes 1,500/second. Consumer lag after 1 hour?",
      "options": [
        "~500 messages",
        "~30,000 messages",
        "~1.8 million messages",
        "~3.6 million messages"
      ],
      "correct": 2,
      "explanation": "Lag rate = 1,500 - 1,000 = 500/second. After 1 hour: 500 × 3,600 = 1.8 million messages behind.",
      "detailedExplanation": "This prompt is really about \"kafka partition has 1 consumer\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Keep quantities like 1 and 1,000 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-037",
      "type": "multiple-choice",
      "question": "Auto-scaling cooldown is 5 minutes. Traffic spikes take 2 minutes to detect. Minimum time from traffic spike to scaled capacity?",
      "options": [
        "~2 minutes",
        "~5 minutes",
        "~7 minutes",
        "Depends on instance boot time"
      ],
      "correct": 3,
      "explanation": "Time = detection (2 min) + instance launch (varies) + health check. Cooldown doesn't affect scale-up, only prevents rapid scale-down. Total depends on how long instances take to boot and become healthy.",
      "detailedExplanation": "Use \"auto-scaling cooldown is 5 minutes\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 5 minutes and 2 minutes in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-038",
      "type": "multiple-choice",
      "question": "Your database has 50ms average query time. You add an index that reduces it to 10ms. Throughput improvement?",
      "options": [
        "~5× faster queries",
        "~5× more throughput",
        "Both",
        "Neither—depends on whether CPU or IO bound"
      ],
      "correct": 2,
      "explanation": "Queries are 5× faster (50ms → 10ms). With fixed resources (connections, CPU), throughput also increases ~5× since each query uses resources for 1/5 the time.",
      "detailedExplanation": "Read this as a scenario about \"your database has 50ms average query time\". Discard modeling choices that look clean but perform poorly for the target queries. Choose data shape based on workload paths, not on normalization dogma alone. If values like 50ms and 10ms appear, convert them into one unit basis before comparison. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-039",
      "type": "multiple-choice",
      "question": "Connection pool has 10 connections. Query takes 100ms. Target throughput is 200 QPS. Pool size needed?",
      "options": [
        "10 connections",
        "20 connections",
        "100 connections",
        "200 connections"
      ],
      "correct": 1,
      "explanation": "Each connection handles 10 QPS (1/0.1s). For 200 QPS: 200 ÷ 10 = 20 connections needed.",
      "detailedExplanation": "The decision turns on \"connection pool has 10 connections\". Eliminate options that ignore durability, retention, or access-pattern constraints. Storage decisions should align durability expectations with access and cost behavior. If values like 10 and 100ms appear, convert them into one unit basis before comparison. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-040",
      "type": "multiple-choice",
      "question": "gRPC streaming: client opens 1 stream, sends 1,000 messages/second. Compared to 1,000 unary calls/second, connection overhead?",
      "options": [
        "Same overhead",
        "~1,000× less connection overhead",
        "~10× less overhead",
        "Depends on message size"
      ],
      "correct": 1,
      "explanation": "Streaming: 1 connection setup for 1,000 messages. Unary: potentially 1,000 connection setups (though HTTP/2 multiplexes). Streaming eliminates per-request connection overhead ≈ 1,000× reduction.",
      "detailedExplanation": "If you keep \"gRPC streaming: client opens 1 stream, sends 1,000 messages/second\" in view, the correct answer separates faster. Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Numbers such as 1 and 1,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-041",
      "type": "multiple-choice",
      "question": "DNS TTL is 60 seconds. You deploy new servers. Worst-case time for all clients to reach new servers?",
      "options": [
        "~60 seconds",
        "~120 seconds",
        "~180 seconds",
        "Depends on client caching behavior"
      ],
      "correct": 3,
      "explanation": "DNS TTL is a hint, not enforced. Browsers, OS, and intermediate resolvers may cache longer. Some clients ignore TTL entirely. Worst case is unpredictable.",
      "detailedExplanation": "The core signal here is \"dNS TTL is 60 seconds\". Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 60 seconds should be normalized first so downstream reasoning stays consistent. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-042",
      "type": "multiple-choice",
      "question": "Read-through cache: miss penalty is 50ms (DB query). Hit latency is 1ms. At 95% hit rate, average latency?",
      "options": ["~1ms", "~3.5ms", "~25ms", "~47.5ms"],
      "correct": 1,
      "explanation": "Avg = (0.95 × 1ms) + (0.05 × 50ms) = 0.95 + 2.5 = 3.45ms ≈ 3.5ms.",
      "detailedExplanation": "Use \"read-through cache: miss penalty is 50ms (DB query)\" as your starting point, then verify tradeoffs carefully. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. If values like 50ms and 1ms appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-043",
      "type": "multiple-choice",
      "question": "Server processes requests in FIFO order. Queue has 100 requests, each takes 10ms. Wait time for new request?",
      "options": ["~10ms", "~100ms", "~1 second", "~10 seconds"],
      "correct": 2,
      "explanation": "New request waits for 100 requests × 10ms = 1,000ms = 1 second.",
      "detailedExplanation": "This prompt is really about \"server processes requests in FIFO order\". Eliminate options that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 100 and 10ms should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-044",
      "type": "multiple-choice",
      "question": "You implement priority queues: high (10%), medium (30%), low (60%). High-priority latency vs. single queue?",
      "options": [
        "Same latency",
        "~10× lower latency",
        "Lower, but depends on implementation",
        "Higher due to priority checking overhead"
      ],
      "correct": 2,
      "explanation": "High-priority requests skip ahead of 90% of the queue, but actual improvement depends on queue depth, arrival rates, and priority algorithm overhead.",
      "detailedExplanation": "The decision turns on \"you implement priority queues: high (10%), medium (30%), low (60%)\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 10 and 30 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-045",
      "type": "multiple-choice",
      "question": "Exponential backoff: initial delay 100ms, multiplier 2×, max 5 retries. Total time if all retries fail?",
      "options": [
        "~1.5 seconds",
        "~3.1 seconds",
        "~6.3 seconds",
        "~12.6 seconds"
      ],
      "correct": 1,
      "explanation": "Delays: 100 + 200 + 400 + 800 + 1600 = 3,100ms = 3.1 seconds total backoff time.",
      "detailedExplanation": "Read this as a scenario about \"exponential backoff: initial delay 100ms, multiplier 2×, max 5 retries\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 100ms and 2 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-046",
      "type": "multiple-choice",
      "question": "Your SLO is P99 < 200ms. Current P99 is 180ms. How much headroom as a percentage?",
      "options": ["~10%", "~11%", "~20%", "~90%"],
      "correct": 1,
      "explanation": "Headroom = (200 - 180) / 180 = 20/180 = 11.1%. (Or 20/200 = 10% of SLO budget remaining.)",
      "detailedExplanation": "The key clue in this question is \"your SLO is P99 < 200ms\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 200ms and 180ms in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-047",
      "type": "multiple-choice",
      "question": "You add request hedging: send duplicate request after 50ms if no response. At 10,000 QPS with 5% requests > 50ms, hedged QPS?",
      "options": ["~10,000 QPS", "~10,500 QPS", "~15,000 QPS", "~20,000 QPS"],
      "correct": 1,
      "explanation": "5% of requests trigger hedge = 500 extra requests. Total = 10,000 + 500 = 10,500 QPS.",
      "detailedExplanation": "Start from \"you add request hedging: send duplicate request after 50ms if no response\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 50ms and 10,000 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-048",
      "type": "multiple-choice",
      "question": "Microservice A calls B, B calls C. A timeout is 500ms, B timeout is 400ms, C timeout is 300ms. What's wrong?",
      "options": [
        "Nothing, timeouts cascade correctly",
        "A's timeout should be > B + C",
        "C's timeout should be lowest",
        "B's timeout should equal C's"
      ],
      "correct": 0,
      "explanation": "A (500ms) > B (400ms) > C (300ms) is correct cascading. Each service has time for its downstream plus processing. This allows proper timeout propagation.",
      "detailedExplanation": "If you keep \"microservice A calls B, B calls C\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 500ms and 400ms in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-049",
      "type": "multiple-choice",
      "question": "Your service has 4 nines availability (99.99%). Max downtime per month?",
      "options": ["~4.3 minutes", "~43 minutes", "~4.3 hours", "~43 hours"],
      "correct": 0,
      "explanation": "99.99% uptime = 0.01% downtime. 30 days × 24 hours × 60 min × 0.0001 = 4.32 minutes/month.",
      "detailedExplanation": "The core signal here is \"your service has 4 nines availability (99\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 4 and 99.99 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-050",
      "type": "multiple-choice",
      "question": "Consistent hashing ring with 100 virtual nodes per server. 1 of 10 servers fails. How much traffic rehashes?",
      "options": ["~1%", "~10%", "~50%", "~100%"],
      "correct": 1,
      "explanation": "With consistent hashing, only the failed server's traffic rehashes ≈ 1/10 = 10% of total traffic moves to other servers.",
      "detailedExplanation": "This prompt is really about \"consistent hashing ring with 100 virtual nodes per server\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 100 and 1 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-051",
      "type": "multiple-choice",
      "question": "Blue-green deployment: 100% traffic on blue, 0% on green. You shift 10% to green. Green errors spike. Blast radius?",
      "options": [
        "~10% of users affected",
        "~100% of users affected",
        "~0% (errors caught in green)",
        "Depends on error type"
      ],
      "correct": 0,
      "explanation": "Only 10% of traffic goes to green, so only 10% of users are affected by green's errors.",
      "detailedExplanation": "Use \"blue-green deployment: 100% traffic on blue, 0% on green\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 100 and 0 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-052",
      "type": "multiple-choice",
      "question": "Request has 100ms budget. It makes 3 sequential calls. Even split gives each call how much time?",
      "options": [
        "~33ms each",
        "~100ms each",
        "Less than 33ms (need buffer)",
        "Depends on call criticality"
      ],
      "correct": 2,
      "explanation": "100ms ÷ 3 = 33ms, but you need buffer for processing between calls. Each call should get less than 33ms to leave room for overhead.",
      "detailedExplanation": "The core signal here is \"request has 100ms budget\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 100ms and 3 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-053",
      "type": "multiple-choice",
      "question": "Your API has 1,000 QPS. You add authentication that rejects 20% as unauthorized. Backend sees?",
      "options": [
        "~800 QPS",
        "~1,000 QPS",
        "~1,200 QPS",
        "Depends on where auth happens"
      ],
      "correct": 3,
      "explanation": "If auth happens at API gateway, backend sees 800 QPS. If auth happens at backend, it sees 1,000 QPS. Location matters.",
      "detailedExplanation": "If you keep \"your API has 1,000 QPS\" in view, the correct answer separates faster. Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Numbers such as 1,000 QPS and 20 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-054",
      "type": "multiple-choice",
      "question": "Write-behind cache batches 100 writes, flushes every second. Incoming write rate is 500/second. Flush frequency?",
      "options": [
        "Every 200ms",
        "Every second",
        "Every 5 seconds",
        "Depends on batch size or time, whichever first"
      ],
      "correct": 0,
      "explanation": "At 500 writes/second, batch of 100 fills in 200ms. Batch flushes when full (every 200ms) rather than waiting for 1-second timer.",
      "detailedExplanation": "Start from \"write-behind cache batches 100 writes, flushes every second\", then pressure-test the result against the options. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Keep quantities like 100 and 500 in aligned units before selecting an answer. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-055",
      "type": "multiple-choice",
      "question": "Load shedding kicks in at 80% capacity. Current load is 10,000 QPS. Capacity is 12,000 QPS. Requests shed?",
      "options": [
        "0 (under threshold)",
        "~400 QPS",
        "~2,000 QPS",
        "~10,000 QPS"
      ],
      "correct": 1,
      "explanation": "80% of 12,000 = 9,600 QPS threshold. Current 10,000 > 9,600, so shedding activates. Shed = 10,000 - 9,600 = 400 QPS.",
      "detailedExplanation": "The key clue in this question is \"load shedding kicks in at 80% capacity\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 80 and 10,000 QPS in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-056",
      "type": "multiple-choice",
      "question": "Service mesh rate limit: 1,000 req/min per service. 10 instances of client service. Effective limit to backend?",
      "options": [
        "~1,000 req/min total",
        "~10,000 req/min total",
        "~100 req/min per instance",
        "Depends on rate limit scope"
      ],
      "correct": 3,
      "explanation": "Rate limit could be per-source-instance, per-source-service, or global. Without knowing the scope, we can't determine the effective limit.",
      "detailedExplanation": "Read this as a scenario about \"service mesh rate limit: 1,000 req/min per service\". Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Keep quantities like 1,000 and 10 in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-057",
      "type": "multiple-choice",
      "question": "Amdahl's Law: 90% of code is parallelizable. Max speedup with infinite processors?",
      "options": ["~9×", "~10×", "~90×", "Infinite"],
      "correct": 1,
      "explanation": "Speedup = 1 / (1 - P) where P is parallel fraction. 1 / (1 - 0.9) = 1 / 0.1 = 10× maximum.",
      "detailedExplanation": "The decision turns on \"amdahl's Law: 90% of code is parallelizable\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 90 and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-058",
      "type": "multiple-choice",
      "question": "Request tracing shows: API (10ms) → Auth (20ms) → DB (30ms) → Cache write (5ms). Critical path?",
      "options": ["~10ms", "~30ms", "~65ms", "Depends on parallelism"],
      "correct": 3,
      "explanation": "If calls are sequential, total = 65ms. If some are parallel (e.g., cache write async), critical path is shorter. Need to know the execution graph.",
      "detailedExplanation": "This prompt is really about \"request tracing shows: API (10ms) → Auth (20ms) → DB (30ms) → Cache write (5ms)\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Keep quantities like 10ms and 20ms in aligned units before selecting an answer. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-059",
      "type": "multiple-choice",
      "question": "Bulkhead pattern: 10 threads for Service A, 10 for Service B. A is slow (100ms), B is fast (10ms). A's impact on B?",
      "options": [
        "B is unaffected",
        "B slows down 10×",
        "B loses half its throughput",
        "Both degrade equally"
      ],
      "correct": 0,
      "explanation": "Bulkhead isolates resources. A's threads are separate from B's. A being slow doesn't consume B's threads, so B is unaffected.",
      "detailedExplanation": "Use \"bulkhead pattern: 10 threads for Service A, 10 for Service B\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 10 and 100ms in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-060",
      "type": "multiple-choice",
      "question": "Your CDN has 50 PoPs. Request routing adds 5ms. Without CDN, origin latency is 200ms. CDN latency improvement?",
      "options": [
        "~5ms worse (routing overhead)",
        "~195ms better (local PoP)",
        "Depends on cache hit rate",
        "Depends on user location"
      ],
      "correct": 2,
      "explanation": "If cache hits, latency ≈ 5ms (routing only). If cache misses, latency = 5ms + 200ms = 205ms (worse than direct). Improvement depends on hit rate.",
      "detailedExplanation": "The key clue in this question is \"your CDN has 50 PoPs\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 50 and 5ms should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-061",
      "type": "multiple-choice",
      "question": "gRPC keepalive ping every 30 seconds. 100,000 connections. Keepalive QPS?",
      "options": ["~33 QPS", "~333 QPS", "~3,333 QPS", "~33,333 QPS"],
      "correct": 2,
      "explanation": "100,000 connections ÷ 30 seconds = 3,333 pings/second.",
      "detailedExplanation": "Start from \"gRPC keepalive ping every 30 seconds\", then pressure-test the result against the options. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 30 seconds and 100,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-062",
      "type": "multiple-choice",
      "question": "Database connection overhead is 50ms. Query is 5ms. Connection pooling speedup for single query?",
      "options": [
        "~1.1× faster",
        "~10× faster",
        "~11× faster",
        "No speedup for single query"
      ],
      "correct": 2,
      "explanation": "Without pool: 50ms + 5ms = 55ms. With pool: 5ms (connection already established). Speedup = 55/5 = 11×.",
      "detailedExplanation": "The decision turns on \"database connection overhead is 50ms\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 50ms and 5ms should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-063",
      "type": "multiple-choice",
      "question": "Your service has 3 dependencies, each with 99% availability (independent failures). Service availability?",
      "options": ["~97%", "~99%", "~99.9%", "~99.97%"],
      "correct": 0,
      "explanation": "All must be up: 0.99 × 0.99 × 0.99 = 0.97 = 97%.",
      "detailedExplanation": "Read this as a scenario about \"your service has 3 dependencies, each with 99% availability (independent failures)\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 3 and 99 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-064",
      "type": "multiple-choice",
      "question": "Request queuing time is 50ms. Processing time is 50ms. Server utilization?",
      "options": ["~25%", "~50%", "~75%", "~100%"],
      "correct": 1,
      "explanation": "Response time = queue + processing. If queue = processing, system is at 50% utilization (queuing theory: at 100% utilization, queue time → infinity).",
      "detailedExplanation": "Use \"request queuing time is 50ms\" as your starting point, then verify tradeoffs carefully. Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 50ms and 50 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-065",
      "type": "multiple-choice",
      "question": "You increase server count from 10 to 15. Traffic is unchanged. Per-server load change?",
      "options": [
        "~33% decrease",
        "~50% decrease",
        "~33% increase",
        "~50% increase"
      ],
      "correct": 0,
      "explanation": "Load per server: 1/10 → 1/15. Change = (1/15 - 1/10) / (1/10) = (0.067 - 0.1) / 0.1 = -33%.",
      "detailedExplanation": "This prompt is really about \"you increase server count from 10 to 15\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 10 and 15 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-066",
      "type": "multiple-choice",
      "question": "Canary deployment: 5% traffic to canary. Canary latency is 2× baseline. Overall latency impact?",
      "options": ["~2× slower", "~5% slower", "~10% slower", "Negligible"],
      "correct": 1,
      "explanation": "5% of requests take 2× time. Average = 95% × 1 + 5% × 2 = 0.95 + 0.10 = 1.05 = 5% slower overall.",
      "detailedExplanation": "If you keep \"canary deployment: 5% traffic to canary\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 5 and 2 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-067",
      "type": "multiple-choice",
      "question": "TCP connection takes 3 round trips to establish (SYN, SYN-ACK, ACK + request). RTT is 50ms. Connection overhead?",
      "options": ["~50ms", "~100ms", "~150ms", "~200ms"],
      "correct": 2,
      "explanation": "3 round trips × 50ms RTT = 150ms connection overhead.",
      "detailedExplanation": "The core signal here is \"tCP connection takes 3 round trips to establish (SYN, SYN-ACK, ACK + request)\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 3 and 50ms should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-068",
      "type": "multiple-choice",
      "question": "Your API returns paginated results, 100 items per page. Client needs all 10,000 items. Sequential requests at 50ms each. Total time?",
      "options": ["~0.5 seconds", "~5 seconds", "~50 seconds", "~500 seconds"],
      "correct": 1,
      "explanation": "10,000 ÷ 100 = 100 pages. 100 × 50ms = 5,000ms = 5 seconds.",
      "detailedExplanation": "The key clue in this question is \"your API returns paginated results, 100 items per page\". Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Keep quantities like 100 and 10,000 in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-069",
      "type": "multiple-choice",
      "question": "Semaphore limits concurrent requests to downstream service to 50. Each request takes 100ms. Max throughput to downstream?",
      "options": [
        "~50 QPS",
        "~500 QPS",
        "~5,000 QPS",
        "Unlimited (semaphore doesn't limit throughput)"
      ],
      "correct": 1,
      "explanation": "50 concurrent × 10 requests/second per slot (1/0.1s) = 500 QPS max.",
      "detailedExplanation": "Start from \"semaphore limits concurrent requests to downstream service to 50\", then pressure-test the result against the options. Eliminate options that ignore durability, retention, or access-pattern constraints. Capacity answers are more defensible when growth and replication are modeled explicitly. Keep quantities like 50 and 100ms in aligned units before selecting an answer. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-070",
      "type": "multiple-choice",
      "question": "Your read replica lags primary by 100ms. Read-after-write consistency required. Options?",
      "options": [
        "Read from primary",
        "Wait 100ms then read replica",
        "Use session affinity to primary",
        "All are valid approaches"
      ],
      "correct": 3,
      "explanation": "All are valid: read from primary (guaranteed fresh), wait for lag (eventually consistent), or session affinity (user's writes always visible to them).",
      "detailedExplanation": "The core signal here is \"your read replica lags primary by 100ms\". Discard choices that violate required invariants during concurrent or failed states. Strong answers connect quorum/coordination settings to concrete correctness goals. If values like 100ms appear, convert them into one unit basis before comparison. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-071",
      "type": "multiple-choice",
      "question": "Global load balancer routes by latency. US users: 50ms to US region, 200ms to EU. EU users: 200ms to US, 50ms to EU. Cross-region traffic?",
      "options": [
        "0% (optimal routing)",
        "~50%",
        "~100%",
        "Depends on load balancing algorithm"
      ],
      "correct": 0,
      "explanation": "Latency-based routing sends US users to US (50ms < 200ms) and EU users to EU (50ms < 200ms). No cross-region traffic needed.",
      "detailedExplanation": "If you keep \"global load balancer routes by latency\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 50ms and 200ms in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-072",
      "type": "multiple-choice",
      "question": "Write-ahead log (WAL) flush every 10ms. Crash occurs. Maximum data loss?",
      "options": [
        "0 (WAL protects all data)",
        "~10ms of writes",
        "~20ms of writes",
        "Depends on flush timing"
      ],
      "correct": 1,
      "explanation": "WAL flushes every 10ms. Crash loses unflushed writes since last flush = up to 10ms of writes.",
      "detailedExplanation": "This prompt is really about \"write-ahead log (WAL) flush every 10ms\". Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 10ms in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-073",
      "type": "multiple-choice",
      "question": "Your API has 100ms SLO. Database adds 40ms, network adds 20ms. Processing budget?",
      "options": ["~40ms", "~60ms", "~100ms", "~160ms"],
      "correct": 0,
      "explanation": "100ms SLO - 40ms DB - 20ms network = 40ms for processing.",
      "detailedExplanation": "Use \"your API has 100ms SLO\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 100ms and 40ms appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-074",
      "type": "multiple-choice",
      "question": "Fan-in pattern: 10 upstream services each send 100 req/sec to your service. Your inbound QPS?",
      "options": [
        "~100 QPS",
        "~1,000 QPS",
        "~10,000 QPS",
        "Depends on request timing"
      ],
      "correct": 1,
      "explanation": "Fan-in: 10 services × 100 req/sec = 1,000 QPS total inbound.",
      "detailedExplanation": "Read this as a scenario about \"fan-in pattern: 10 upstream services each send 100 req/sec to your service\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 10 and 100 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-075",
      "type": "multiple-choice",
      "question": "Fan-out pattern: your service sends to 10 downstream services per request. Inbound is 100 QPS. Outbound QPS?",
      "options": [
        "~100 QPS",
        "~1,000 QPS",
        "~10,000 QPS",
        "Depends on parallelism"
      ],
      "correct": 1,
      "explanation": "Fan-out: 100 inbound × 10 downstream calls = 1,000 QPS outbound.",
      "detailedExplanation": "The decision turns on \"fan-out pattern: your service sends to 10 downstream services per request\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 10 and 100 QPS appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-076",
      "type": "multiple-choice",
      "question": "Server GC pauses 100ms every 10 seconds. Impact on P99 latency?",
      "options": [
        "P99 increases by ~1ms",
        "P99 increases by ~10ms",
        "P99 ≥ 100ms",
        "Depends on request duration"
      ],
      "correct": 2,
      "explanation": "GC pause affects 100ms/10s = 1% of time. If P99 includes GC pause, P99 ≥ 100ms (the pause duration).",
      "detailedExplanation": "Start from \"server GC pauses 100ms every 10 seconds\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 100ms and 10 seconds should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-077",
      "type": "multiple-choice",
      "question": "Async processing: request enqueues job, returns immediately (5ms). Job takes 5 seconds. User-perceived latency?",
      "options": [
        "~5ms",
        "~5 seconds",
        "~5.005 seconds",
        "Depends on user's next action"
      ],
      "correct": 0,
      "explanation": "User sees 5ms response (job is async). The 5-second processing happens in background.",
      "detailedExplanation": "The key clue in this question is \"async processing: request enqueues job, returns immediately (5ms)\". Reject designs that improve throughput while weakening reliability guarantees. Throughput is only one part; replay behavior and consumer lag handling matter equally. Numbers such as 5ms and 5 seconds should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-078",
      "type": "multiple-choice",
      "question": "CQRS: reads go to read model, writes go to write model. Write model publishes events. Event delay is 500ms. Read-after-write behavior?",
      "options": [
        "Always consistent",
        "~500ms eventual consistency",
        "Inconsistent forever",
        "Depends on where read is routed"
      ],
      "correct": 1,
      "explanation": "Write publishes event, read model updates in ~500ms. Read-after-write may see stale data for up to 500ms.",
      "detailedExplanation": "The core signal here is \"cQRS: reads go to read model, writes go to write model\". Reject options that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 500ms in aligned units before selecting an answer. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-079",
      "type": "multiple-choice",
      "question": "Your service handles 10,000 QPS. You deploy a change that increases latency from 50ms to 100ms. Thread pool is 500. What happens?",
      "options": [
        "Latency doubles, throughput unchanged",
        "Latency doubles, throughput halves",
        "Service becomes overloaded",
        "Depends on queue size"
      ],
      "correct": 2,
      "explanation": "Old capacity: 500 threads × 20 req/s = 10,000 QPS. New capacity: 500 × 10 = 5,000 QPS. Traffic (10,000) > capacity (5,000) = overload, queuing, and degradation.",
      "detailedExplanation": "If you keep \"your service handles 10,000 QPS\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 10,000 QPS and 50ms appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-080",
      "type": "multiple-choice",
      "question": "Circuit breaker: closed (normal), open (failing), half-open (testing). Open duration is 30 seconds. Recovery time after errors stop?",
      "options": [
        "Immediate",
        "~30 seconds minimum",
        "~60 seconds",
        "Depends on half-open success rate"
      ],
      "correct": 1,
      "explanation": "Circuit stays open for 30 seconds before entering half-open state. Minimum 30 seconds before any recovery begins.",
      "detailedExplanation": "The decision turns on \"circuit breaker: closed (normal), open (failing), half-open (testing)\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 30 seconds should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-081",
      "type": "multiple-choice",
      "question": "Request batching: 10 requests batched into 1 backend call. Individual request latency was 20ms. Batched request latency?",
      "options": [
        "~2ms per request",
        "~20ms per request",
        "~200ms per request",
        "Depends on batch processing time"
      ],
      "correct": 3,
      "explanation": "Batch latency depends on: batch wait time + backend processing for batch + response distribution. Could be faster (amortized) or slower (wait for batch to fill).",
      "detailedExplanation": "Read this as a scenario about \"request batching: 10 requests batched into 1 backend call\". Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 10 and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-082",
      "type": "multiple-choice",
      "question": "You add distributed tracing. Trace context is 100 bytes per request. At 100,000 QPS, trace overhead bandwidth?",
      "options": ["~1 MB/s", "~10 MB/s", "~100 MB/s", "~1 GB/s"],
      "correct": 1,
      "explanation": "100 bytes × 100,000 req/s = 10 MB/s bandwidth for trace context.",
      "detailedExplanation": "The key clue in this question is \"you add distributed tracing\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 100 and 100,000 QPS in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-083",
      "type": "multiple-choice",
      "question": "Service discovery refreshes every 30 seconds. New server deployed. Max time until all clients discover it?",
      "options": [
        "~30 seconds",
        "~60 seconds",
        "~90 seconds",
        "Depends on health check timing"
      ],
      "correct": 0,
      "explanation": "Clients refresh every 30 seconds. New server appears on next refresh = max 30 seconds.",
      "detailedExplanation": "Start from \"service discovery refreshes every 30 seconds\", then pressure-test the result against the options. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 30 seconds appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-084",
      "type": "multiple-choice",
      "question": "Your database supports 10,000 QPS. You add a caching layer with 80% hit rate. New effective capacity?",
      "options": ["~12,500 QPS", "~50,000 QPS", "~80,000 QPS", "~100,000 QPS"],
      "correct": 1,
      "explanation": "20% of requests hit DB. If DB max is 10,000, that's 20% of total. Total capacity = 10,000 ÷ 0.2 = 50,000 QPS.",
      "detailedExplanation": "If you keep \"your database supports 10,000 QPS\" in view, the correct answer separates faster. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 10,000 QPS and 80 in aligned units before selecting an answer. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-085",
      "type": "multiple-choice",
      "question": "Synchronous microservice call: 50ms. Converting to async (message queue): enqueue 5ms, dequeue+process 50ms. Latency comparison for caller?",
      "options": [
        "Same latency",
        "~5ms latency (async)",
        "~55ms latency",
        "~105ms latency"
      ],
      "correct": 1,
      "explanation": "Caller only waits for enqueue (5ms), not processing. Caller latency drops from 50ms to 5ms.",
      "detailedExplanation": "The core signal here is \"synchronous microservice call: 50ms\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. If values like 50ms and 5ms appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-086",
      "type": "multiple-choice",
      "question": "Request coalescing window is 5ms. Identical requests arrive at 0ms, 3ms, 7ms. How many backend calls?",
      "options": ["1 call", "2 calls", "3 calls", "Depends on implementation"],
      "correct": 1,
      "explanation": "0ms and 3ms are within 5ms window → 1 call. 7ms starts new window → 1 call. Total: 2 backend calls.",
      "detailedExplanation": "Use \"request coalescing window is 5ms\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 5ms and 0ms appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-087",
      "type": "multiple-choice",
      "question": "Your service P50=10ms, P99=100ms. Which has more impact on user experience at scale?",
      "options": [
        "P50 (affects most users)",
        "P99 (affects power users)",
        "Equal impact",
        "Depends on business context"
      ],
      "correct": 3,
      "explanation": "Impact depends on context. P50 affects 50% of requests. P99 affects 1% but may be critical paths, power users, or cascade into retries. Business context matters.",
      "detailedExplanation": "This prompt is really about \"your service P50=10ms, P99=100ms\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 10ms and 100ms in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-088",
      "type": "multiple-choice",
      "question": "Database transaction takes 50ms. Lock contention under high load. At 1,000 TPS, concurrent transactions holding locks?",
      "options": ["~5", "~50", "~500", "~5,000"],
      "correct": 1,
      "explanation": "Little's Law: concurrent = rate × duration. 1,000 TPS × 0.05s = 50 concurrent transactions.",
      "detailedExplanation": "The decision turns on \"database transaction takes 50ms\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 50ms and 1,000 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-089",
      "type": "multiple-choice",
      "question": "Horizontal scaling: 1 server handles 1,000 QPS at 70% CPU. To handle 10,000 QPS with 50% headroom (capacity = 2× load), servers needed?",
      "options": ["10 servers", "14 servers", "20 servers", "28 servers"],
      "correct": 2,
      "explanation": "50% headroom means capacity = 2× needed load. Need 20,000 QPS capacity for 10,000 QPS load. At 1,000 QPS per server: 20,000 ÷ 1,000 = 20 servers.",
      "detailedExplanation": "Read this as a scenario about \"horizontal scaling: 1 server handles 1,000 QPS at 70% CPU\". Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 1 and 1,000 QPS appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-090",
      "type": "multiple-choice",
      "question": "You add request validation (2ms). Previously: API (10ms) → DB (40ms). New total latency?",
      "options": ["~42ms", "~50ms", "~52ms", "~54ms"],
      "correct": 2,
      "explanation": "Validation (2ms) + API (10ms) + DB (40ms) = 52ms.",
      "detailedExplanation": "Start from \"you add request validation (2ms)\", then pressure-test the result against the options. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Keep quantities like 2ms and 10ms in aligned units before selecting an answer. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-091",
      "type": "multiple-choice",
      "question": "Idempotency check adds 5ms per request (cache lookup). At 10,000 QPS, additional latency budget consumed?",
      "options": [
        "5ms per request",
        "50 seconds per second",
        "~50,000ms per second",
        "5ms per request, 0.5% of typical budget"
      ],
      "correct": 0,
      "explanation": "Each request adds 5ms latency. At 10,000 QPS, that's 50,000ms of total latency added per second, but each individual request only sees 5ms increase.",
      "detailedExplanation": "The key clue in this question is \"idempotency check adds 5ms per request (cache lookup)\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 5ms and 10,000 QPS in aligned units before selecting an answer. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-092",
      "type": "multiple-choice",
      "question": "Service mesh sidecar crashes. Main application container is healthy. What happens to traffic?",
      "options": [
        "Traffic flows normally",
        "Traffic fails (no proxy)",
        "Traffic bypasses sidecar",
        "Depends on mesh configuration"
      ],
      "correct": 3,
      "explanation": "Behavior depends on configuration. Some meshes fail closed (no traffic), others fail open (bypass proxy), some have automatic recovery. Check your mesh settings.",
      "detailedExplanation": "Read this as a scenario about \"service mesh sidecar crashes\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-093",
      "type": "multiple-choice",
      "question": "Request retry policy: 3 retries, jitter up to 100ms. Two clients retry simultaneously. Collision probability?",
      "options": ["~100%", "~50%", "~10%", "Very low (jitter spreads retries)"],
      "correct": 3,
      "explanation": "Jitter randomizes retry timing across 100ms window. Probability of exact collision is very low (retries spread across time).",
      "detailedExplanation": "The decision turns on \"request retry policy: 3 retries, jitter up to 100ms\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 3 and 100ms should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-094",
      "type": "multiple-choice",
      "question": "Load balancer health check: 3 consecutive failures to mark unhealthy, 5 second interval. Minimum detection time?",
      "options": ["~5 seconds", "~10 seconds", "~15 seconds", "~20 seconds"],
      "correct": 1,
      "explanation": "Need 3 consecutive failures. First check fails at t=0, second at t=5, third at t=10. Marked unhealthy at 10 seconds.",
      "detailedExplanation": "This prompt is really about \"load balancer health check: 3 consecutive failures to mark unhealthy, 5 second interval\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 3 and 5 second appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-095",
      "type": "multiple-choice",
      "question": "Your API serves both mobile (slow networks, high latency tolerance) and web (fast networks, low latency expectation). Same SLO for both?",
      "options": [
        "Yes, simplifies monitoring",
        "No, different SLOs per client type",
        "Yes, design for worst case",
        "No, but same backend limits apply"
      ],
      "correct": 1,
      "explanation": "Different clients have different expectations. Mobile users tolerate higher latency than web users. SLOs should reflect user expectations per platform.",
      "detailedExplanation": "Use \"your API serves both mobile (slow networks, high latency tolerance) and web (fast\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-096",
      "type": "multiple-choice",
      "question": "Pre-warming: load 1 million cache entries at startup. Each entry takes 1ms to load. Startup time impact?",
      "options": ["~1 second", "~17 minutes", "~1.7 hours", "~17 hours"],
      "correct": 1,
      "explanation": "1M entries × 1ms = 1,000,000ms = 1,000 seconds = 16.7 minutes.",
      "detailedExplanation": "The core signal here is \"pre-warming: load 1 million cache entries at startup\". Reject options that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 1 and 1ms in aligned units before selecting an answer. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-097",
      "type": "multiple-choice",
      "question": "A/B test: 50% traffic to variant A (100ms latency), 50% to variant B (50ms latency). Median user latency?",
      "options": ["~50ms", "~75ms", "~100ms", "Either 50ms or 100ms (bimodal)"],
      "correct": 3,
      "explanation": "50% see 50ms, 50% see 100ms. Median is the middle value. With bimodal distribution, median depends on exact split—could be either 50ms or 100ms, distribution is bimodal not normal.",
      "detailedExplanation": "If you keep \"a/B test: 50% traffic to variant A (100ms latency), 50% to variant B (50ms latency)\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 50 and 100ms appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-098",
      "type": "multiple-choice",
      "question": "Daily traffic pattern: 10× spike at noon vs 3am. You provision for peak. Night-time utilization?",
      "options": ["~10%", "~30%", "~50%", "~90%"],
      "correct": 0,
      "explanation": "Peak is 10× trough. If provisioned for peak, trough utilization = 1/10 = 10%.",
      "detailedExplanation": "Start from \"daily traffic pattern: 10× spike at noon vs 3am\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 10 and 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-099",
      "type": "multiple-choice",
      "question": "Database max connections: 100. Connection pool per app server: 10. Max app servers?",
      "options": [
        "10 servers",
        "100 servers",
        "1,000 servers",
        "Depends on connection pooler"
      ],
      "correct": 0,
      "explanation": "100 total connections ÷ 10 per server = 10 app servers max before exhausting DB connections.",
      "detailedExplanation": "The key clue in this question is \"database max connections: 100\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 100 and 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-100",
      "type": "multiple-choice",
      "question": "Your SLO is 99.9% requests under 200ms. Current P99.9 is 190ms. Error budget remaining?",
      "options": ["~5%", "~10%", "~50%", "~95%"],
      "correct": 0,
      "explanation": "P99.9 of 190ms vs SLO of 200ms. Headroom = (200-190)/200 = 5% of latency budget remaining before SLO breach.",
      "detailedExplanation": "Read this as a scenario about \"your SLO is 99\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 99.9 and 200ms in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n001",
      "type": "numeric-input",
      "question": "100 million DAU, 20 requests per user per day. Average QPS?",
      "answer": 23148,
      "tolerance": 0.1,
      "explanation": "100M × 20 = 2B requests/day ÷ 86,400 = 23,148 QPS.",
      "detailedExplanation": "Start from \"100 million DAU, 20 requests per user per day\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 100 and 20 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n002",
      "type": "numeric-input",
      "question": "Peak traffic is 5× average. Average QPS is 1,000. Peak QPS?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "1,000 × 5 = 5,000 QPS at peak.",
      "detailedExplanation": "The decision turns on \"peak traffic is 5× average\". Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 5 and 1,000 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n003",
      "type": "numeric-input",
      "question": "A server handles 2,000 QPS. How many servers for 50,000 QPS with 50% headroom?",
      "answer": 38,
      "tolerance": 0.15,
      "explanation": "50,000 × 1.5 (headroom) = 75,000 capacity needed. 75,000 ÷ 2,000 = 37.5, round up to 38.",
      "detailedExplanation": "Read this as a scenario about \"server handles 2,000 QPS\". Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 2,000 QPS and 50,000 QPS appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n004",
      "type": "numeric-input",
      "question": "Request takes 50ms. Single-threaded max QPS?",
      "answer": 20,
      "tolerance": 0.1,
      "explanation": "1000ms ÷ 50ms = 20 requests per second.",
      "detailedExplanation": "Use \"request takes 50ms\" as your starting point, then verify tradeoffs carefully. Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 50ms and 1000ms appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n005",
      "type": "numeric-input",
      "question": "10 threads, 100ms per request. Max QPS for this service?",
      "answer": 100,
      "tolerance": 0.1,
      "explanation": "Each thread handles 10 QPS. 10 threads × 10 = 100 QPS.",
      "detailedExplanation": "This prompt is really about \"10 threads, 100ms per request\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 10 and 100ms in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n006",
      "type": "numeric-input",
      "question": "5,000 QPS for 8 hours during business day, 500 QPS for 16 off-hours. Daily requests in millions?",
      "answer": 173,
      "unit": "million",
      "tolerance": 0.1,
      "explanation": "5000 × 8 × 3600 + 500 × 16 × 3600 = 144M + 28.8M = 172.8M.",
      "detailedExplanation": "If you keep \"5,000 QPS for 8 hours during business day, 500 QPS for 16 off-hours\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 5,000 QPS and 8 hours appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n007",
      "type": "numeric-input",
      "question": "Rate limit: 100 requests per minute per user. Max requests per second per user?",
      "answer": 1.67,
      "tolerance": 0.15,
      "explanation": "100 ÷ 60 = 1.67 requests per second.",
      "detailedExplanation": "The core signal here is \"rate limit: 100 requests per minute per user\". Keep every transformation in one unit system and check order of magnitude at the end. Interface decisions should be justified by contract stability and client impact over time. Keep quantities like 100 and 60 in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n008",
      "type": "numeric-input",
      "question": "Database handles 5,000 reads/sec and 500 writes/sec. Read:write ratio?",
      "answer": 10,
      "tolerance": 0.1,
      "explanation": "5,000 ÷ 500 = 10:1 read:write ratio.",
      "detailedExplanation": "The key clue in this question is \"database handles 5,000 reads/sec and 500 writes/sec\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 5,000 and 500 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n009",
      "type": "numeric-input",
      "question": "Cache hit rate is 95%. 10,000 QPS total. Database QPS (cache misses)?",
      "answer": 500,
      "tolerance": 0.1,
      "explanation": "5% miss rate. 10,000 × 0.05 = 500 QPS to database.",
      "detailedExplanation": "Start from \"cache hit rate is 95%\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Numbers such as 95 and 10,000 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n010",
      "type": "numeric-input",
      "question": "1 million concurrent WebSocket connections, each sends 1 message per 10 seconds. Messages per second?",
      "answer": 100000,
      "tolerance": 0.1,
      "explanation": "1M ÷ 10 = 100,000 messages per second.",
      "detailedExplanation": "The core signal here is \"1 million concurrent WebSocket connections, each sends 1 message per 10 seconds\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 1 and 10 seconds in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n011",
      "type": "numeric-input",
      "question": "API gateway handles 100,000 QPS. 60% goes to service A. Service A's QPS?",
      "answer": 60000,
      "tolerance": 0.1,
      "explanation": "100,000 × 0.60 = 60,000 QPS.",
      "detailedExplanation": "If you keep \"aPI gateway handles 100,000 QPS\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Interface decisions should be justified by contract stability and client impact over time. Numbers such as 100,000 QPS and 60 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n012",
      "type": "numeric-input",
      "question": "Search index handles 1,000 QPS per shard. Need 15,000 QPS capacity. Minimum shards?",
      "answer": 15,
      "tolerance": 0.1,
      "explanation": "15,000 ÷ 1,000 = 15 shards.",
      "detailedExplanation": "This prompt is really about \"search index handles 1,000 QPS per shard\". Keep every transformation in one unit system and check order of magnitude at the end. Schema and index choices should follow access patterns and write/read amplification constraints. Keep quantities like 1,000 QPS and 15,000 QPS in aligned units before selecting an answer. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n013",
      "type": "numeric-input",
      "question": "Load balancer distributes to 5 servers. Total 25,000 QPS. QPS per server?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "25,000 ÷ 5 = 5,000 QPS per server.",
      "detailedExplanation": "Use \"load balancer distributes to 5 servers\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 5 and 25,000 QPS in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n014",
      "type": "numeric-input",
      "question": "Retry rate is 5%. Original QPS is 10,000. Total QPS including retries?",
      "answer": 10500,
      "tolerance": 0.1,
      "explanation": "10,000 + (10,000 × 0.05) = 10,500 QPS.",
      "detailedExplanation": "Read this as a scenario about \"retry rate is 5%\". Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 5 and 10,000 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n015",
      "type": "numeric-input",
      "question": "A/B test: 10% traffic to variant B. Total 50,000 QPS. Variant B QPS?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "50,000 × 0.10 = 5,000 QPS.",
      "detailedExplanation": "The decision turns on \"a/B test: 10% traffic to variant B\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 10 and 50,000 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n016",
      "type": "numeric-input",
      "question": "Connection pool has 100 connections, each handles 50 QPS. Max throughput?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "100 × 50 = 5,000 QPS max.",
      "detailedExplanation": "Start from \"connection pool has 100 connections, each handles 50 QPS\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Storage decisions should align durability expectations with access and cost behavior. Numbers such as 100 and 50 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: choosing tier by cost only and missing access constraints.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n017",
      "type": "numeric-input",
      "question": "Kafka partition handles 10,000 messages/sec. Need 80,000 msg/sec. Minimum partitions?",
      "answer": 8,
      "tolerance": 0.1,
      "explanation": "80,000 ÷ 10,000 = 8 partitions.",
      "detailedExplanation": "The key clue in this question is \"kafka partition handles 10,000 messages/sec\". Keep every transformation in one unit system and check order of magnitude at the end. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 10,000 and 80,000 in aligned units before selecting an answer. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-n018",
      "type": "numeric-input",
      "question": "Redis handles 100,000 ops/sec. Your app needs 25 ops per request at 2,000 QPS. Redis utilization %?",
      "answer": 50,
      "unit": "%",
      "tolerance": 0.1,
      "explanation": "2,000 × 25 = 50,000 ops/sec. 50,000 ÷ 100,000 = 50%.",
      "detailedExplanation": "The core signal here is \"redis handles 100,000 ops/sec\". Keep every transformation in one unit system and check order of magnitude at the end. Cache design quality is mostly about correctness boundaries, not only hit rate. Keep quantities like 100,000 and 25 in aligned units before selecting an answer. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o001",
      "type": "ordering",
      "question": "Rank by typical QPS capacity (lowest to highest).",
      "items": [
        "Single PostgreSQL",
        "Redis instance",
        "Single web server",
        "Kafka broker"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "PostgreSQL (~5K) < Web server (~10K) < Redis (~100K) < Kafka (~1M msg/sec).",
      "detailedExplanation": "If you keep \"rank by typical QPS capacity (lowest to highest)\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. Treat freshness policy and invalidation paths as first-class constraints. If values like 5K and 10K appear, convert them into one unit basis before comparison. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o002",
      "type": "ordering",
      "question": "Rank these user bases by expected QPS (lowest to highest), assuming 10 req/user/day.",
      "items": ["100K DAU", "10M DAU", "1M DAU", "100M DAU"],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "100K (12 QPS) < 1M (116 QPS) < 10M (1,157 QPS) < 100M (11,574 QPS).",
      "detailedExplanation": "This prompt is really about \"rank these user bases by expected QPS (lowest to highest), assuming 10 req/user/day\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 10 and 100K appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o003",
      "type": "ordering",
      "question": "Rank by operations per request (typically lowest to highest).",
      "items": [
        "Static file serve",
        "User login",
        "News feed render",
        "Search query"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Static (1 op) < Login (2-3 ops) < Search (5-10 ops) < Feed (10-100 ops).",
      "detailedExplanation": "Use \"rank by operations per request (typically lowest to highest)\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o004",
      "type": "ordering",
      "question": "Rank by QPS multiplier effect (lowest to highest).",
      "items": [
        "1 retry on failure",
        "Fan-out to 10 services",
        "95% cache hit rate",
        "3 database queries per request"
      ],
      "correctOrder": [2, 0, 3, 1],
      "explanation": "Cache reduces load. 1 retry adds ~5%. 3 queries = 3×. Fan-out = 10×.",
      "detailedExplanation": "Read this as a scenario about \"rank by QPS multiplier effect (lowest to highest)\". Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 1 and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o005",
      "type": "ordering",
      "question": "Rank these scaling strategies by QPS increase potential (lowest to highest).",
      "items": [
        "Add more RAM",
        "Add read replicas",
        "Add caching layer",
        "Horizontal sharding"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "RAM (marginal) < Replicas (2-5×) < Caching (10-100×) < Sharding (unlimited).",
      "detailedExplanation": "The decision turns on \"rank these scaling strategies by QPS increase potential (lowest to highest)\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 2 and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o006",
      "type": "ordering",
      "question": "Rank by latency impact on max QPS (lowest to highest latency = highest QPS).",
      "items": [
        "50ms response",
        "200ms response",
        "10ms response",
        "500ms response"
      ],
      "correctOrder": [3, 1, 0, 2],
      "explanation": "Lower latency = higher QPS. 500ms (2 QPS) < 200ms (5) < 50ms (20) < 10ms (100).",
      "detailedExplanation": "Start from \"rank by latency impact on max QPS (lowest to highest latency = highest QPS)\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 500ms and 2 QPS in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o007",
      "type": "ordering",
      "question": "Rank traffic patterns by peak:average ratio (lowest to highest).",
      "items": [
        "Enterprise SaaS",
        "Social media",
        "E-commerce flash sale",
        "Banking"
      ],
      "correctOrder": [0, 3, 1, 2],
      "explanation": "Enterprise (~1.5×) < Banking (~2×) < Social (~3-5×) < Flash sale (~10-100×).",
      "detailedExplanation": "The key clue in this question is \"rank traffic patterns by peak:average ratio (lowest to highest)\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 1.5 and 2 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-o008",
      "type": "ordering",
      "question": "Rank by requests generated per action (lowest to highest).",
      "items": [
        "Page view",
        "User signup",
        "Checkout flow",
        "Real-time dashboard"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Page view (1-3) < Signup (5-10) < Checkout (10-20) < Dashboard (continuous polling).",
      "detailedExplanation": "The core signal here is \"rank by requests generated per action (lowest to highest)\". Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 1 and 3 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m001",
      "type": "multi-select",
      "question": "Which equal ~1,000 QPS?",
      "options": [
        "86.4 million requests/day",
        "2.6 billion requests/month",
        "3.6 million requests/hour",
        "60,000 requests/minute"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All convert to ~1,000 QPS: 86.4M/86.4K, 2.6B/2.6M, 3.6M/3.6K, 60K/60.",
      "detailedExplanation": "Use \"equal ~1,000 QPS\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 1,000 QPS and 86.4M appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m002",
      "type": "multi-select",
      "question": "Which increase effective QPS capacity?",
      "options": [
        "Adding caching",
        "Reducing response payload",
        "Adding more threads",
        "Reducing database queries per request"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Caching reduces backend load, more threads handle more concurrent requests, fewer DB queries reduce latency. Payload size affects bandwidth, not QPS directly.",
      "detailedExplanation": "The core signal here is \"increase effective QPS capacity\". Treat every option as a separate true/false test under the same constraints. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m003",
      "type": "multi-select",
      "question": "At 10,000 QPS with 100ms latency, which are true? (Little's Law)",
      "options": [
        "1,000 concurrent requests",
        "100 concurrent requests",
        "10 requests in flight per ms",
        "Need 1,000 threads for sync processing"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "L = λW = 10,000 × 0.1 = 1,000 concurrent. 1000/100ms = 10/ms. Sync needs 1 thread per concurrent request.",
      "detailedExplanation": "If you keep \"at 10,000 QPS with 100ms latency, which are true? (Little's Law)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 10,000 QPS and 100ms should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m004",
      "type": "multi-select",
      "question": "Which scenarios result in QPS amplification?",
      "options": [
        "Client retries",
        "Fan-out queries",
        "Response caching",
        "Batch processing"
      ],
      "correctIndices": [0, 1],
      "explanation": "Retries and fan-out multiply requests. Caching and batching reduce QPS.",
      "detailedExplanation": "Start from \"scenarios result in QPS amplification\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m005",
      "type": "multi-select",
      "question": "A service handles 5,000 QPS. Which are valid scaling options for 20,000 QPS?",
      "options": [
        "4 instances behind load balancer",
        "Increase thread pool 4×",
        "Add caching with 75% hit rate",
        "4 database shards"
      ],
      "correctIndices": [0, 2],
      "explanation": "4 instances = 20K QPS ✓. 4× threads helps but rarely 4× QPS. 75% cache = 4× effective capacity ✓. DB shards help DB, not service QPS directly.",
      "detailedExplanation": "The key clue in this question is \"service handles 5,000 QPS\". Treat every option as a separate true/false test under the same constraints. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Keep quantities like 5,000 QPS and 20,000 QPS in aligned units before selecting an answer. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m006",
      "type": "multi-select",
      "question": "Which affect peak-to-average QPS ratio?",
      "options": [
        "Geographic distribution of users",
        "Time-based usage patterns",
        "Viral content events",
        "Database replication lag"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Geography (time zones), time patterns (business hours), and viral events affect peak:avg. Replication lag doesn't affect traffic patterns.",
      "detailedExplanation": "Read this as a scenario about \"affect peak-to-average QPS ratio\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m007",
      "type": "multi-select",
      "question": "Server handles 1,000 QPS at 50% CPU. Which are likely true?",
      "options": [
        "Can probably handle 1,500 QPS",
        "Theoretical max around 2,000 QPS",
        "Should add capacity before 80% CPU",
        "CPU is the bottleneck"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "50% CPU suggests ~2× headroom. Best practice: scale before 80%. Can't determine if CPU is THE bottleneck without more info.",
      "detailedExplanation": "The decision turns on \"server handles 1,000 QPS at 50% CPU\". Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 1,000 QPS and 50 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-m008",
      "type": "multi-select",
      "question": "Rate limiting at 100 req/min/user. With 10,000 concurrent users, which are true?",
      "options": [
        "Max 1 million req/min total",
        "Max ~16,667 QPS system-wide",
        "Each user gets ~1.67 QPS",
        "Bursty users may hit limits even at lower average"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "10K × 100 = 1M/min max. 1M/60 = 16,667 QPS. 100/60 = 1.67/user. Bursts can exceed sustained average.",
      "detailedExplanation": "This prompt is really about \"rate limiting at 100 req/min/user\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 100 and 10,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-t001",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your app has 50 million DAU, each making 30 requests/day. What's your average QPS?",
          "options": [
            "~1,700 QPS",
            "~17,000 QPS",
            "~170,000 QPS",
            "~1.7 million QPS"
          ],
          "correct": 1,
          "explanation": "50M × 30 = 1.5B/day ÷ 86,400 = 17,361 QPS.",
          "detailedExplanation": "The core signal here is \"your app has 50 million DAU, each making 30 requests/day\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 50 and 30 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks."
        },
        {
          "question": "If peak is 3× average, and each server handles 5,000 QPS, how many servers?",
          "options": [
            "~4 servers",
            "~11 servers",
            "~21 servers",
            "~52 servers"
          ],
          "correct": 1,
          "explanation": "Peak = 52,000 QPS. 52,000 ÷ 5,000 = 10.4, round up to 11.",
          "detailedExplanation": "Use \"if peak is 3× average, and each server handles 5,000 QPS, how many servers\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 3 and 5,000 QPS appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "explanation": "Capacity planning from DAU to server count.",
      "detailedExplanation": "The core signal here is \"capacity planning from DAU to server count\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-t002",
      "type": "two-stage",
      "stages": [
        {
          "question": "Each request makes 3 database queries. At 10,000 app QPS, database QPS?",
          "options": [
            "~3,333 QPS",
            "~10,000 QPS",
            "~30,000 QPS",
            "~300,000 QPS"
          ],
          "correct": 2,
          "explanation": "10,000 × 3 = 30,000 database QPS.",
          "detailedExplanation": "The decision turns on \"each request makes 3 database queries\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 3 and 10,000 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "Database handles 10,000 QPS per replica. How many read replicas needed?",
          "options": ["1 replica", "3 replicas", "5 replicas", "10 replicas"],
          "correct": 1,
          "explanation": "30,000 ÷ 10,000 = 3 replicas minimum.",
          "detailedExplanation": "Start from \"database handles 10,000 QPS per replica\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Consistency decisions should be explicit about which conflicts are acceptable and why. Keep quantities like 10,000 QPS and 30,000 in aligned units before selecting an answer. Common pitfall: ignoring conflict resolution behavior."
        }
      ],
      "explanation": "Query amplification affects database scaling.",
      "detailedExplanation": "Use \"query amplification affects database scaling\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-t003",
      "type": "two-stage",
      "stages": [
        {
          "question": "Request latency is 200ms. What's the max QPS per thread?",
          "options": ["~2 QPS", "~5 QPS", "~20 QPS", "~50 QPS"],
          "correct": 1,
          "explanation": "1000ms ÷ 200ms = 5 QPS per thread.",
          "detailedExplanation": "Start from \"request latency is 200ms\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 200ms and 1000ms in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "You need 500 QPS. How many threads in your thread pool?",
          "options": ["25 threads", "50 threads", "100 threads", "500 threads"],
          "correct": 2,
          "explanation": "500 QPS ÷ 5 QPS/thread = 100 threads.",
          "detailedExplanation": "The decision turns on \"you need 500 QPS\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 500 QPS and 5 QPS appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "explanation": "Thread pool sizing from latency and throughput requirements.",
      "detailedExplanation": "This prompt is really about \"thread pool sizing from latency and throughput requirements\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Storage decisions should align durability expectations with access and cost behavior. Common pitfall: underestimating replication and retention overhead.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    },
    {
      "id": "qps-t004",
      "type": "two-stage",
      "stages": [
        {
          "question": "Cache hit rate is 90%. Backend handles 1,000 QPS max. What total QPS can you serve?",
          "options": [
            "~1,000 QPS",
            "~9,000 QPS",
            "~10,000 QPS",
            "~100,000 QPS"
          ],
          "correct": 2,
          "explanation": "10% misses hit backend. 1,000 = 10% of X. X = 10,000 QPS total.",
          "detailedExplanation": "Use \"cache hit rate is 90%\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Treat freshness policy and invalidation paths as first-class constraints. Keep quantities like 90 and 1,000 QPS in aligned units before selecting an answer. Common pitfall: stale data despite high hit rates."
        },
        {
          "question": "To serve 50,000 QPS total, what cache hit rate do you need?",
          "options": ["95%", "96%", "98%", "99%"],
          "correct": 2,
          "explanation": "Backend max 1,000 QPS = 2% of 50,000. Need 98% hit rate.",
          "detailedExplanation": "The core signal here is \"to serve 50,000 QPS total, what cache hit rate do you need\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Cache design quality is mostly about correctness boundaries, not only hit rate. Numbers such as 50,000 QPS and 1,000 QPS should be normalized first so downstream reasoning stays consistent. Common pitfall: stale data despite high hit rates."
        }
      ],
      "explanation": "Cache hit rate determines effective capacity.",
      "detailedExplanation": "The decision turns on \"cache hit rate determines effective capacity\". Do not reset assumptions between stages; carry forward prior constraints directly. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["estimation", "qps-and-load"],
      "difficulty": "senior"
    }
  ]
}
