{
  "unit": 1,
  "unitTitle": "Estimation",
  "chapter": 5,
  "chapterTitle": "QPS & Load",
  "chapterDescription": "Request rates, server capacity, load balancing, throughput calculations",
  "problems": [
    {
      "id": "qps-001",
      "type": "multiple-choice",
      "question": "Your service has 10 million DAU. Each user makes 50 requests per day on average. What's your average QPS?",
      "options": ["~580 QPS", "~5,800 QPS", "~58,000 QPS", "~580,000 QPS"],
      "correct": 1,
      "explanation": "10M × 50 = 500M requests/day. 500M ÷ 86,400 sec = 5,787 QPS ≈ 5,800 QPS.",
      "detailedExplanation": "10M × 50 = 500M requests/day. 500M ÷ 86,400 sec = 5,787 QPS ≈ 5,800 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-002",
      "type": "multiple-choice",
      "question": "A server handles 500 requests/second. Each request takes 20ms average. How many concurrent requests?",
      "options": ["~10", "~100", "~1,000", "~10,000"],
      "correct": 0,
      "explanation": "Little's Law: L = λ × W. Concurrent = 500 req/s × 0.02s = 10 concurrent requests.",
      "detailedExplanation": "Little's Law: L = λ × W. Concurrent = 500 req/s × 0.02s = 10 concurrent requests. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-003",
      "type": "multiple-choice",
      "question": "Your peak traffic is 5× your average. Average is 10,000 QPS. How many servers if each handles 2,000 QPS?",
      "options": ["5 servers", "13 servers", "25 servers", "50 servers"],
      "correct": 2,
      "explanation": "Peak = 5 × 10,000 = 50,000 QPS. Servers needed = 50,000 ÷ 2,000 = 25 servers.",
      "detailedExplanation": "Peak = 5 × 10,000 = 50,000 QPS. Servers needed = 50,000 ÷ 2,000 = 25 servers. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-004",
      "type": "multiple-choice",
      "question": "A database connection takes 5ms to establish. Your app needs 1,000 connections/second. Why use connection pooling?",
      "options": [
        "Save 5 seconds/second of CPU",
        "Save 5 seconds/second of latency",
        "Avoid TCP port exhaustion",
        "All of the above"
      ],
      "correct": 3,
      "explanation": "Connection pooling saves 5ms × 1,000 = 5 seconds of latency per second, reduces CPU overhead, and prevents port exhaustion from rapid connect/disconnect.",
      "detailedExplanation": "Connection pooling saves 5ms × 1,000 = 5 seconds of latency per second, reduces CPU overhead, and prevents port exhaustion from rapid connect/disconnect. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-005",
      "type": "multiple-choice",
      "question": "Your API has P50=10ms, P99=100ms. At 1,000 QPS, how many requests/second exceed 100ms?",
      "options": ["~1", "~10", "~100", "~500"],
      "correct": 1,
      "explanation": "P99 means 99% are under 100ms, so 1% exceed it. 1% of 1,000 = 10 requests/second.",
      "detailedExplanation": "P99 means 99% are under 100ms, so 1% exceed it. 1% of 1,000 = 10 requests/second. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-006",
      "type": "multiple-choice",
      "question": "A rate limiter allows 100 requests per minute per user. 10,000 users are active. Max theoretical QPS?",
      "options": ["~167 QPS", "~1,670 QPS", "~16,700 QPS", "~167,000 QPS"],
      "correct": 2,
      "explanation": "10,000 users × 100 req/min = 1M req/min = 16,667 req/sec ≈ 16,700 QPS.",
      "detailedExplanation": "10,000 users × 100 req/min = 1M req/min = 16,667 req/sec ≈ 16,700 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        }
      ]
    },
    {
      "id": "qps-007",
      "type": "multiple-choice",
      "question": "Your load balancer distributes traffic to 8 servers. One server fails. Load increase per remaining server?",
      "options": ["~12.5%", "~14.3%", "~16.7%", "~25%"],
      "correct": 1,
      "explanation": "Before: each handles 1/8 = 12.5%. After: each handles 1/7 = 14.3%. Increase = 14.3% - 12.5% = 1.8% absolute, or 14.3% relative increase.",
      "detailedExplanation": "Before: each handles 1/8 = 12.5%. After: each handles 1/7 = 14.3%. Increase = 14.3% - 12.5% = 1.8% absolute, or 14.3% relative increase. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-008",
      "type": "multiple-choice",
      "question": "A cache has 90% hit rate. Backend handles 1,000 QPS at capacity. Total QPS the system can handle?",
      "options": ["~1,000 QPS", "~9,000 QPS", "~10,000 QPS", "~100,000 QPS"],
      "correct": 2,
      "explanation": "10% of requests hit backend. If backend max is 1,000 QPS, that's 10% of total. Total = 1,000 ÷ 0.1 = 10,000 QPS.",
      "detailedExplanation": "10% of requests hit backend. If backend max is 1,000 QPS, that's 10% of total. Total = 1,000 ÷ 0.1 = 10,000 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-009",
      "type": "multiple-choice",
      "question": "Each API request triggers 3 database queries. API is at 5,000 QPS. Database QPS?",
      "options": ["~1,700 QPS", "~5,000 QPS", "~15,000 QPS", "~25,000 QPS"],
      "correct": 2,
      "explanation": "5,000 API calls × 3 DB queries = 15,000 database QPS.",
      "detailedExplanation": "5,000 API calls × 3 DB queries = 15,000 database QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-010",
      "type": "multiple-choice",
      "question": "Your server has 100 worker threads. Each request takes 50ms. Maximum throughput?",
      "options": ["~200 QPS", "~2,000 QPS", "~20,000 QPS", "~200,000 QPS"],
      "correct": 1,
      "explanation": "Each thread handles 1/0.05 = 20 requests/sec. 100 threads × 20 = 2,000 QPS max.",
      "detailedExplanation": "Each thread handles 1/0.05 = 20 requests/sec. 100 threads × 20 = 2,000 QPS max. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-011",
      "type": "multiple-choice",
      "question": "A microservice has 200ms timeout. Downstream service P99 is 150ms. What happens at 99.9th percentile?",
      "options": [
        "Request succeeds",
        "Request times out",
        "Depends on retry config",
        "Cannot determine"
      ],
      "correct": 2,
      "explanation": "P99 is 150ms but P99.9 could be higher than 200ms. Without knowing P99.9 or the latency distribution, we can't determine if it times out.",
      "detailedExplanation": "P99 is 150ms but P99.9 could be higher than 200ms. Without knowing P99.9 or the latency distribution, we can't determine if it times out. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-012",
      "type": "multiple-choice",
      "question": "Your service auto-scales at 70% CPU. Each server handles 1,000 QPS at 70% CPU. Traffic grows from 5,000 to 15,000 QPS. Servers after scaling?",
      "options": [
        "5 → 15 servers",
        "5 → 21 servers",
        "7 → 15 servers",
        "7 → 21 servers"
      ],
      "correct": 0,
      "explanation": "At 70% CPU = 1,000 QPS per server. 5,000 QPS needs 5 servers. 15,000 QPS needs 15 servers.",
      "detailedExplanation": "At 70% CPU = 1,000 QPS per server. 5,000 QPS needs 5 servers. 15,000 QPS needs 15 servers. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-013",
      "type": "multiple-choice",
      "question": "A queue processes 100 messages/second. Messages arrive at 120/second. Queue depth after 1 minute?",
      "options": [
        "~200 messages",
        "~1,200 messages",
        "~6,000 messages",
        "~7,200 messages"
      ],
      "correct": 1,
      "explanation": "Backlog rate = 120 - 100 = 20 messages/second. After 60 seconds: 20 × 60 = 1,200 messages queued.",
      "detailedExplanation": "Backlog rate = 120 - 100 = 20 messages/second. After 60 seconds: 20 × 60 = 1,200 messages queued. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-014",
      "type": "multiple-choice",
      "question": "Database has 100 max connections. Each query takes 10ms. Max query throughput?",
      "options": [
        "~1,000 QPS",
        "~10,000 QPS",
        "~100,000 QPS",
        "~1,000,000 QPS"
      ],
      "correct": 1,
      "explanation": "Each connection handles 1/0.01 = 100 queries/sec. 100 connections × 100 = 10,000 QPS.",
      "detailedExplanation": "Each connection handles 1/0.01 = 100 queries/sec. 100 connections × 100 = 10,000 QPS. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-015",
      "type": "multiple-choice",
      "question": "Your CDN absorbs 95% of traffic. Origin handles 500 QPS max. Peak traffic the CDN can support?",
      "options": ["~500 QPS", "~5,000 QPS", "~10,000 QPS", "~50,000 QPS"],
      "correct": 2,
      "explanation": "5% hits origin. If origin max is 500, total = 500 ÷ 0.05 = 10,000 QPS.",
      "detailedExplanation": "5% hits origin. If origin max is 500, total = 500 ÷ 0.05 = 10,000 QPS. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "qps-016",
      "type": "multiple-choice",
      "question": "An endpoint has 100ms latency. You add 2 sequential downstream calls of 30ms each. New latency?",
      "options": ["~130ms", "~160ms", "~200ms", "~300ms"],
      "correct": 1,
      "explanation": "100ms + 30ms + 30ms = 160ms (sequential calls add their latencies).",
      "detailedExplanation": "100ms + 30ms + 30ms = 160ms (sequential calls add their latencies). Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-017",
      "type": "multiple-choice",
      "question": "You parallelize 2 calls of 50ms each instead of running them sequentially. Time saved?",
      "options": ["0ms", "~25ms", "~50ms", "~100ms"],
      "correct": 2,
      "explanation": "Sequential: 50 + 50 = 100ms. Parallel: max(50, 50) = 50ms. Saved: 50ms.",
      "detailedExplanation": "Sequential: 50 + 50 = 100ms. Parallel: max(50, 50) = 50ms. Saved: 50ms. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-018",
      "type": "multiple-choice",
      "question": "A service handles 10,000 QPS. You add a circuit breaker that trips at 50% error rate. At 60% errors, effective QPS?",
      "options": [
        "0 QPS (circuit open)",
        "~4,000 QPS",
        "~6,000 QPS",
        "~10,000 QPS"
      ],
      "correct": 0,
      "explanation": "Circuit breaker trips at 50% errors. At 60%, the circuit is open, rejecting all requests (0 effective QPS until it recovers).",
      "detailedExplanation": "Circuit breaker trips at 50% errors. At 60%, the circuit is open, rejecting all requests (0 effective QPS until it recovers). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-019",
      "type": "multiple-choice",
      "question": "Your Redis cache handles 100,000 ops/sec. Each API request needs 5 cache operations. Max API QPS limited by cache?",
      "options": ["~5,000 QPS", "~20,000 QPS", "~100,000 QPS", "~500,000 QPS"],
      "correct": 1,
      "explanation": "100,000 cache ops ÷ 5 ops per request = 20,000 API QPS.",
      "detailedExplanation": "100,000 cache ops ÷ 5 ops per request = 20,000 API QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-020",
      "type": "multiple-choice",
      "question": "Health checks run every 5 seconds. 50 services, 10 instances each. Health check QPS?",
      "options": ["~10 QPS", "~100 QPS", "~500 QPS", "~5,000 QPS"],
      "correct": 1,
      "explanation": "50 services × 10 instances = 500 health checks per 5 seconds = 100 QPS.",
      "detailedExplanation": "50 services × 10 instances = 500 health checks per 5 seconds = 100 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-021",
      "type": "multiple-choice",
      "question": "Your API Gateway has 10ms overhead per request. Backend latency is 40ms. Total latency and throughput impact?",
      "options": [
        "50ms latency, 20% throughput loss",
        "50ms latency, no throughput impact",
        "40ms latency, 10ms async",
        "Depends on gateway architecture"
      ],
      "correct": 0,
      "explanation": "Gateway adds 10ms to the request path. Total = 40 + 10 = 50ms. Throughput per worker drops from 1/0.04 to 1/0.05 = 20% reduction.",
      "detailedExplanation": "Gateway adds 10ms to the request path. Total = 40 + 10 = 50ms. Throughput per worker drops from 1/0.04 to 1/0.05 = 20% reduction. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-022",
      "type": "multiple-choice",
      "question": "A retry policy retries failed requests 3 times with 1-second backoff. Base failure rate is 1%. Worst-case QPS amplification?",
      "options": ["~1%", "~3%", "~4%", "~1.03×"],
      "correct": 2,
      "explanation": "1% fail × up to 3 retries = 3% extra requests. Total = 100% + 3% = 103%, or ~4% amplification (1.03× is the same as ~3% increase).",
      "detailedExplanation": "1% fail × up to 3 retries = 3% extra requests. Total = 100% + 3% = 103%, or ~4% amplification (1.03× is the same as ~3% increase). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-023",
      "type": "multiple-choice",
      "question": "You shard by user_id across 16 database shards. Hot user generates 10% of all queries. That shard's load vs. even distribution?",
      "options": [
        "~1.6× average",
        "~2.5× average",
        "~10× average",
        "~16× average"
      ],
      "correct": 1,
      "explanation": "Even distribution = 100%/16 = 6.25% per shard. Hot shard gets 10% (hot user) + 90%/16 (fair share of rest) = 10% + 5.6% = 15.6%. Ratio = 15.6/6.25 = 2.5×.",
      "detailedExplanation": "Even distribution = 100%/16 = 6.25% per shard. Hot shard gets 10% (hot user) + 90%/16 (fair share of rest) = 10% + 5.6% = 15.6%. Ratio = 15.6/6.25 = 2.5×. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-024",
      "type": "multiple-choice",
      "question": "Your batch job processes 1 million records. Single-threaded takes 10 hours. With 100 workers, theoretical time?",
      "options": [
        "~6 minutes",
        "~1 hour",
        "~6 hours",
        "Depends on coordination overhead"
      ],
      "correct": 0,
      "explanation": "Ideal speedup: 10 hours ÷ 100 = 0.1 hours = 6 minutes. (Real-world has coordination overhead.)",
      "detailedExplanation": "Ideal speedup: 10 hours ÷ 100 = 0.1 hours = 6 minutes. (Real-world has coordination overhead.). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-025",
      "type": "multiple-choice",
      "question": "Token bucket rate limiter: 100 tokens/second, bucket size 500. After idle period, how many burst requests allowed?",
      "options": [
        "100 requests",
        "500 requests",
        "600 requests",
        "Unlimited until 100/s sustained"
      ],
      "correct": 1,
      "explanation": "Bucket fills to max capacity (500) when idle. Burst capacity = bucket size = 500 requests.",
      "detailedExplanation": "Bucket fills to max capacity (500) when idle. Burst capacity = bucket size = 500 requests. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-026",
      "type": "multiple-choice",
      "question": "Leaky bucket rate limiter: 100 requests/second outflow. Burst of 500 requests arrives. Time to drain?",
      "options": [
        "~0.5 seconds",
        "~5 seconds",
        "~50 seconds",
        "Depends on bucket size"
      ],
      "correct": 1,
      "explanation": "500 requests ÷ 100/second = 5 seconds to drain (assuming bucket is large enough to hold 500).",
      "detailedExplanation": "500 requests ÷ 100/second = 5 seconds to drain (assuming bucket is large enough to hold 500). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-027",
      "type": "multiple-choice",
      "question": "WebSocket server holds 100,000 connections. Each connection receives 1 message/minute. Message processing QPS?",
      "options": ["~167 QPS", "~1,667 QPS", "~16,667 QPS", "~100,000 QPS"],
      "correct": 1,
      "explanation": "100,000 connections × 1 msg/min ÷ 60 sec = 1,667 messages/second.",
      "detailedExplanation": "100,000 connections × 1 msg/min ÷ 60 sec = 1,667 messages/second. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-028",
      "type": "multiple-choice",
      "question": "Your service has 99.9% availability. It handles 1 million requests/day. Expected failed requests per day?",
      "options": ["~100", "~1,000", "~10,000", "~100,000"],
      "correct": 1,
      "explanation": "0.1% failure rate × 1M = 1,000 failed requests per day.",
      "detailedExplanation": "0.1% failure rate × 1M = 1,000 failed requests per day. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-029",
      "type": "multiple-choice",
      "question": "A GraphQL query fans out to 5 backend services in parallel. Each service has P95=50ms. Query P95?",
      "options": [
        "~50ms",
        "~100ms",
        "~250ms",
        "Higher than any individual P95"
      ],
      "correct": 3,
      "explanation": "When calling N services in parallel, the total P95 is worse than any individual P95 because you wait for the slowest. With 5 services, probability all are under P95 is 0.95^5 = 77%, so query P95 is higher.",
      "detailedExplanation": "When calling N services in parallel, the total P95 is worse than any individual P95 because you wait for the slowest. With 5 services, probability all are under P95 is 0.95^5 = 77%, so query P95 is higher. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-030",
      "type": "multiple-choice",
      "question": "Database read replicas handle 80% of reads. Primary handles writes + 20% of reads. Primary write QPS is 1,000. Read:write ratio is 10:1. Primary total QPS?",
      "options": ["~1,000 QPS", "~3,000 QPS", "~10,000 QPS", "~11,000 QPS"],
      "correct": 1,
      "explanation": "Writes: 1,000. Total reads: 1,000 × 10 = 10,000. Primary reads: 20% of 10,000 = 2,000. Primary total: 1,000 + 2,000 = 3,000 QPS.",
      "detailedExplanation": "Writes: 1,000. Total reads: 1,000 × 10 = 10,000. Primary reads: 20% of 10,000 = 2,000. Primary total: 1,000 + 2,000 = 3,000 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-031",
      "type": "multiple-choice",
      "question": "You implement request coalescing for identical requests. 100 duplicate requests in a 10ms window become 1 backend call. Reduction factor?",
      "options": ["~10×", "~100×", "Depends on dedup rate", "~1,000×"],
      "correct": 1,
      "explanation": "100 requests become 1. Reduction = 100×.",
      "detailedExplanation": "100 requests become 1. Reduction = 100×. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-032",
      "type": "multiple-choice",
      "question": "Your API serves 50,000 QPS. Each request costs $0.00001 in compute. Annual API cost?",
      "options": ["~$1,500", "~$15,000", "~$150,000", "~$1.5M"],
      "correct": 1,
      "explanation": "50,000 QPS × 86,400 sec/day × 365 days × $0.00001 = $15,768 ≈ $15,000/year.",
      "detailedExplanation": "50,000 QPS × 86,400 sec/day × 365 days × $0.00001 = $15,768 ≈ $15,000/year. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-033",
      "type": "multiple-choice",
      "question": "Thundering herd: cache key expires, 1,000 concurrent requests hit empty cache. Without protection, backend receives?",
      "options": [
        "1 request",
        "~100 requests",
        "~1,000 requests",
        "Depends on cache implementation"
      ],
      "correct": 2,
      "explanation": "Without protection (like request coalescing or locking), all 1,000 requests see cache miss and hit backend simultaneously.",
      "detailedExplanation": "Without protection (like request coalescing or locking), all 1,000 requests see cache miss and hit backend simultaneously. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-034",
      "type": "multiple-choice",
      "question": "Your service mesh adds sidecar proxies. Each request passes through 2 sidecars (client + server). Sidecar latency is 1ms each. Impact on P50 if base P50 is 20ms?",
      "options": ["20ms (no change)", "21ms", "22ms", "24ms"],
      "correct": 2,
      "explanation": "2 sidecars × 1ms = 2ms added. New P50 = 20 + 2 = 22ms.",
      "detailedExplanation": "2 sidecars × 1ms = 2ms added. New P50 = 20 + 2 = 22ms. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-035",
      "type": "multiple-choice",
      "question": "Event-driven architecture: producer emits 10,000 events/second. 5 consumers each process all events. Total event processing rate?",
      "options": [
        "~10,000/sec",
        "~50,000/sec",
        "~2,000/sec",
        "Depends on partitioning"
      ],
      "correct": 1,
      "explanation": "Fan-out: each consumer processes all 10,000 events. Total = 5 × 10,000 = 50,000 event processings/second.",
      "detailedExplanation": "Fan-out: each consumer processes all 10,000 events. Total = 5 × 10,000 = 50,000 event processings/second. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-036",
      "type": "multiple-choice",
      "question": "Kafka partition has 1 consumer. Consumer processes at 1,000 messages/second. Producer writes 1,500/second. Consumer lag after 1 hour?",
      "options": [
        "~500 messages",
        "~30,000 messages",
        "~1.8 million messages",
        "~3.6 million messages"
      ],
      "correct": 2,
      "explanation": "Lag rate = 1,500 - 1,000 = 500/second. After 1 hour: 500 × 3,600 = 1.8 million messages behind.",
      "detailedExplanation": "Lag rate = 1,500 - 1,000 = 500/second. After 1 hour: 500 × 3,600 = 1.8 million messages behind. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-037",
      "type": "multiple-choice",
      "question": "Auto-scaling cooldown is 5 minutes. Traffic spikes take 2 minutes to detect. Minimum time from traffic spike to scaled capacity?",
      "options": [
        "~2 minutes",
        "~5 minutes",
        "~7 minutes",
        "Depends on instance boot time"
      ],
      "correct": 3,
      "explanation": "Time = detection (2 min) + instance launch (varies) + health check. Cooldown doesn't affect scale-up, only prevents rapid scale-down. Total depends on how long instances take to boot and become healthy.",
      "detailedExplanation": "Time = detection (2 min) + instance launch (varies) + health check. Cooldown doesn't affect scale-up, only prevents rapid scale-down. Total depends on how long instances take to boot and become healthy. A good sanity check compares your estimate to one or two known anchor numbers. If the ratio is wildly off, call out which assumption is likely wrong and recompute quickly.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-038",
      "type": "multiple-choice",
      "question": "Your database has 50ms average query time. You add an index that reduces it to 10ms. Throughput improvement?",
      "options": [
        "~5× faster queries",
        "~5× more throughput",
        "Both",
        "Neither—depends on whether CPU or IO bound"
      ],
      "correct": 2,
      "explanation": "Queries are 5× faster (50ms → 10ms). With fixed resources (connections, CPU), throughput also increases ~5× since each query uses resources for 1/5 the time.",
      "detailedExplanation": "Queries are 5× faster (50ms → 10ms). With fixed resources (connections, CPU), throughput also increases ~5× since each query uses resources for 1/5 the time. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-039",
      "type": "multiple-choice",
      "question": "Connection pool has 10 connections. Query takes 100ms. Target throughput is 200 QPS. Pool size needed?",
      "options": [
        "10 connections",
        "20 connections",
        "100 connections",
        "200 connections"
      ],
      "correct": 1,
      "explanation": "Each connection handles 10 QPS (1/0.1s). For 200 QPS: 200 ÷ 10 = 20 connections needed.",
      "detailedExplanation": "Each connection handles 10 QPS (1/0.1s). For 200 QPS: 200 ÷ 10 = 20 connections needed. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-040",
      "type": "multiple-choice",
      "question": "gRPC streaming: client opens 1 stream, sends 1,000 messages/second. Compared to 1,000 unary calls/second, connection overhead?",
      "options": [
        "Same overhead",
        "~1,000× less connection overhead",
        "~10× less overhead",
        "Depends on message size"
      ],
      "correct": 1,
      "explanation": "Streaming: 1 connection setup for 1,000 messages. Unary: potentially 1,000 connection setups (though HTTP/2 multiplexes). Streaming eliminates per-request connection overhead ≈ 1,000× reduction.",
      "detailedExplanation": "Streaming: 1 connection setup for 1,000 messages. Unary: potentially 1,000 connection setups (though HTTP/2 multiplexes). Streaming eliminates per-request connection overhead ≈ 1,000× reduction. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "qps-041",
      "type": "multiple-choice",
      "question": "DNS TTL is 60 seconds. You deploy new servers. Worst-case time for all clients to reach new servers?",
      "options": [
        "~60 seconds",
        "~120 seconds",
        "~180 seconds",
        "Depends on client caching behavior"
      ],
      "correct": 3,
      "explanation": "DNS TTL is a hint, not enforced. Browsers, OS, and intermediate resolvers may cache longer. Some clients ignore TTL entirely. Worst case is unpredictable.",
      "detailedExplanation": "DNS TTL is a hint, not enforced. Browsers, OS, and intermediate resolvers may cache longer. Some clients ignore TTL entirely. Worst case is unpredictable. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-042",
      "type": "multiple-choice",
      "question": "Read-through cache: miss penalty is 50ms (DB query). Hit latency is 1ms. At 95% hit rate, average latency?",
      "options": ["~1ms", "~3.5ms", "~25ms", "~47.5ms"],
      "correct": 1,
      "explanation": "Avg = (0.95 × 1ms) + (0.05 × 50ms) = 0.95 + 2.5 = 3.45ms ≈ 3.5ms.",
      "detailedExplanation": "Avg = (0.95 × 1ms) + (0.05 × 50ms) = 0.95 + 2.5 = 3.45ms ≈ 3.5ms. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-043",
      "type": "multiple-choice",
      "question": "Server processes requests in FIFO order. Queue has 100 requests, each takes 10ms. Wait time for new request?",
      "options": ["~10ms", "~100ms", "~1 second", "~10 seconds"],
      "correct": 2,
      "explanation": "New request waits for 100 requests × 10ms = 1,000ms = 1 second.",
      "detailedExplanation": "New request waits for 100 requests × 10ms = 1,000ms = 1 second. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-044",
      "type": "multiple-choice",
      "question": "You implement priority queues: high (10%), medium (30%), low (60%). High-priority latency vs. single queue?",
      "options": [
        "Same latency",
        "~10× lower latency",
        "Lower, but depends on implementation",
        "Higher due to priority checking overhead"
      ],
      "correct": 2,
      "explanation": "High-priority requests skip ahead of 90% of the queue, but actual improvement depends on queue depth, arrival rates, and priority algorithm overhead.",
      "detailedExplanation": "High-priority requests skip ahead of 90% of the queue, but actual improvement depends on queue depth, arrival rates, and priority algorithm overhead. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-045",
      "type": "multiple-choice",
      "question": "Exponential backoff: initial delay 100ms, multiplier 2×, max 5 retries. Total time if all retries fail?",
      "options": [
        "~1.5 seconds",
        "~3.1 seconds",
        "~6.3 seconds",
        "~12.6 seconds"
      ],
      "correct": 1,
      "explanation": "Delays: 100 + 200 + 400 + 800 + 1600 = 3,100ms = 3.1 seconds total backoff time.",
      "detailedExplanation": "Delays: 100 + 200 + 400 + 800 + 1600 = 3,100ms = 3.1 seconds total backoff time. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-046",
      "type": "multiple-choice",
      "question": "Your SLO is P99 < 200ms. Current P99 is 180ms. How much headroom as a percentage?",
      "options": ["~10%", "~11%", "~20%", "~90%"],
      "correct": 1,
      "explanation": "Headroom = (200 - 180) / 180 = 20/180 = 11.1%. (Or 20/200 = 10% of SLO budget remaining.)",
      "detailedExplanation": "Headroom = (200 - 180) / 180 = 20/180 = 11.1%. (Or 20/200 = 10% of SLO budget remaining.). Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-047",
      "type": "multiple-choice",
      "question": "You add request hedging: send duplicate request after 50ms if no response. At 10,000 QPS with 5% requests > 50ms, hedged QPS?",
      "options": ["~10,000 QPS", "~10,500 QPS", "~15,000 QPS", "~20,000 QPS"],
      "correct": 1,
      "explanation": "5% of requests trigger hedge = 500 extra requests. Total = 10,000 + 500 = 10,500 QPS.",
      "detailedExplanation": "5% of requests trigger hedge = 500 extra requests. Total = 10,000 + 500 = 10,500 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-048",
      "type": "multiple-choice",
      "question": "Microservice A calls B, B calls C. A timeout is 500ms, B timeout is 400ms, C timeout is 300ms. What's wrong?",
      "options": [
        "Nothing, timeouts cascade correctly",
        "A's timeout should be > B + C",
        "C's timeout should be lowest",
        "B's timeout should equal C's"
      ],
      "correct": 0,
      "explanation": "A (500ms) > B (400ms) > C (300ms) is correct cascading. Each service has time for its downstream plus processing. This allows proper timeout propagation.",
      "detailedExplanation": "A (500ms) > B (400ms) > C (300ms) is correct cascading. Each service has time for its downstream plus processing. This allows proper timeout propagation. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-049",
      "type": "multiple-choice",
      "question": "Your service has 4 nines availability (99.99%). Max downtime per month?",
      "options": ["~4.3 minutes", "~43 minutes", "~4.3 hours", "~43 hours"],
      "correct": 0,
      "explanation": "99.99% uptime = 0.01% downtime. 30 days × 24 hours × 60 min × 0.0001 = 4.32 minutes/month.",
      "detailedExplanation": "99.99% uptime = 0.01% downtime. 30 days × 24 hours × 60 min × 0.0001 = 4.32 minutes/month. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-050",
      "type": "multiple-choice",
      "question": "Consistent hashing ring with 100 virtual nodes per server. 1 of 10 servers fails. How much traffic rehashes?",
      "options": ["~1%", "~10%", "~50%", "~100%"],
      "correct": 1,
      "explanation": "With consistent hashing, only the failed server's traffic rehashes ≈ 1/10 = 10% of total traffic moves to other servers.",
      "detailedExplanation": "With consistent hashing, only the failed server's traffic rehashes ≈ 1/10 = 10% of total traffic moves to other servers. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-051",
      "type": "multiple-choice",
      "question": "Blue-green deployment: 100% traffic on blue, 0% on green. You shift 10% to green. Green errors spike. Blast radius?",
      "options": [
        "~10% of users affected",
        "~100% of users affected",
        "~0% (errors caught in green)",
        "Depends on error type"
      ],
      "correct": 0,
      "explanation": "Only 10% of traffic goes to green, so only 10% of users are affected by green's errors.",
      "detailedExplanation": "Only 10% of traffic goes to green, so only 10% of users are affected by green's errors. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-052",
      "type": "multiple-choice",
      "question": "Request has 100ms budget. It makes 3 sequential calls. Even split gives each call how much time?",
      "options": [
        "~33ms each",
        "~100ms each",
        "Less than 33ms (need buffer)",
        "Depends on call criticality"
      ],
      "correct": 2,
      "explanation": "100ms ÷ 3 = 33ms, but you need buffer for processing between calls. Each call should get less than 33ms to leave room for overhead.",
      "detailedExplanation": "100ms ÷ 3 = 33ms, but you need buffer for processing between calls. Each call should get less than 33ms to leave room for overhead. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-053",
      "type": "multiple-choice",
      "question": "Your API has 1,000 QPS. You add authentication that rejects 20% as unauthorized. Backend sees?",
      "options": [
        "~800 QPS",
        "~1,000 QPS",
        "~1,200 QPS",
        "Depends on where auth happens"
      ],
      "correct": 3,
      "explanation": "If auth happens at API gateway, backend sees 800 QPS. If auth happens at backend, it sees 1,000 QPS. Location matters.",
      "detailedExplanation": "If auth happens at API gateway, backend sees 800 QPS. If auth happens at backend, it sees 1,000 QPS. Location matters. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-054",
      "type": "multiple-choice",
      "question": "Write-behind cache batches 100 writes, flushes every second. Incoming write rate is 500/second. Flush frequency?",
      "options": [
        "Every 200ms",
        "Every second",
        "Every 5 seconds",
        "Depends on batch size or time, whichever first"
      ],
      "correct": 0,
      "explanation": "At 500 writes/second, batch of 100 fills in 200ms. Batch flushes when full (every 200ms) rather than waiting for 1-second timer.",
      "detailedExplanation": "At 500 writes/second, batch of 100 fills in 200ms. Batch flushes when full (every 200ms) rather than waiting for 1-second timer. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-055",
      "type": "multiple-choice",
      "question": "Load shedding kicks in at 80% capacity. Current load is 10,000 QPS. Capacity is 12,000 QPS. Requests shed?",
      "options": [
        "0 (under threshold)",
        "~400 QPS",
        "~2,000 QPS",
        "~10,000 QPS"
      ],
      "correct": 1,
      "explanation": "80% of 12,000 = 9,600 QPS threshold. Current 10,000 > 9,600, so shedding activates. Shed = 10,000 - 9,600 = 400 QPS.",
      "detailedExplanation": "80% of 12,000 = 9,600 QPS threshold. Current 10,000 > 9,600, so shedding activates. Shed = 10,000 - 9,600 = 400 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-056",
      "type": "multiple-choice",
      "question": "Service mesh rate limit: 1,000 req/min per service. 10 instances of client service. Effective limit to backend?",
      "options": [
        "~1,000 req/min total",
        "~10,000 req/min total",
        "~100 req/min per instance",
        "Depends on rate limit scope"
      ],
      "correct": 3,
      "explanation": "Rate limit could be per-source-instance, per-source-service, or global. Without knowing the scope, we can't determine the effective limit.",
      "detailedExplanation": "Rate limit could be per-source-instance, per-source-service, or global. Without knowing the scope, we can't determine the effective limit. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-057",
      "type": "multiple-choice",
      "question": "Amdahl's Law: 90% of code is parallelizable. Max speedup with infinite processors?",
      "options": ["~9×", "~10×", "~90×", "Infinite"],
      "correct": 1,
      "explanation": "Speedup = 1 / (1 - P) where P is parallel fraction. 1 / (1 - 0.9) = 1 / 0.1 = 10× maximum.",
      "detailedExplanation": "Speedup = 1 / (1 - P) where P is parallel fraction. 1 / (1 - 0.9) = 1 / 0.1 = 10× maximum. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-058",
      "type": "multiple-choice",
      "question": "Request tracing shows: API (10ms) → Auth (20ms) → DB (30ms) → Cache write (5ms). Critical path?",
      "options": ["~10ms", "~30ms", "~65ms", "Depends on parallelism"],
      "correct": 3,
      "explanation": "If calls are sequential, total = 65ms. If some are parallel (e.g., cache write async), critical path is shorter. Need to know the execution graph.",
      "detailedExplanation": "If calls are sequential, total = 65ms. If some are parallel (e.g., cache write async), critical path is shorter. Need to know the execution graph. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-059",
      "type": "multiple-choice",
      "question": "Bulkhead pattern: 10 threads for Service A, 10 for Service B. A is slow (100ms), B is fast (10ms). A's impact on B?",
      "options": [
        "B is unaffected",
        "B slows down 10×",
        "B loses half its throughput",
        "Both degrade equally"
      ],
      "correct": 0,
      "explanation": "Bulkhead isolates resources. A's threads are separate from B's. A being slow doesn't consume B's threads, so B is unaffected.",
      "detailedExplanation": "Bulkhead isolates resources. A's threads are separate from B's. A being slow doesn't consume B's threads, so B is unaffected. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-060",
      "type": "multiple-choice",
      "question": "Your CDN has 50 PoPs. Request routing adds 5ms. Without CDN, origin latency is 200ms. CDN latency improvement?",
      "options": [
        "~5ms worse (routing overhead)",
        "~195ms better (local PoP)",
        "Depends on cache hit rate",
        "Depends on user location"
      ],
      "correct": 2,
      "explanation": "If cache hits, latency ≈ 5ms (routing only). If cache misses, latency = 5ms + 200ms = 205ms (worse than direct). Improvement depends on hit rate.",
      "detailedExplanation": "If cache hits, latency ≈ 5ms (routing only). If cache misses, latency = 5ms + 200ms = 205ms (worse than direct). Improvement depends on hit rate. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "qps-061",
      "type": "multiple-choice",
      "question": "gRPC keepalive ping every 30 seconds. 100,000 connections. Keepalive QPS?",
      "options": ["~33 QPS", "~333 QPS", "~3,333 QPS", "~33,333 QPS"],
      "correct": 2,
      "explanation": "100,000 connections ÷ 30 seconds = 3,333 pings/second.",
      "detailedExplanation": "100,000 connections ÷ 30 seconds = 3,333 pings/second. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-062",
      "type": "multiple-choice",
      "question": "Database connection overhead is 50ms. Query is 5ms. Connection pooling speedup for single query?",
      "options": [
        "~1.1× faster",
        "~10× faster",
        "~11× faster",
        "No speedup for single query"
      ],
      "correct": 2,
      "explanation": "Without pool: 50ms + 5ms = 55ms. With pool: 5ms (connection already established). Speedup = 55/5 = 11×.",
      "detailedExplanation": "Without pool: 50ms + 5ms = 55ms. With pool: 5ms (connection already established). Speedup = 55/5 = 11×. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-063",
      "type": "multiple-choice",
      "question": "Your service has 3 dependencies, each with 99% availability (independent failures). Service availability?",
      "options": ["~97%", "~99%", "~99.9%", "~99.97%"],
      "correct": 0,
      "explanation": "All must be up: 0.99 × 0.99 × 0.99 = 0.97 = 97%.",
      "detailedExplanation": "All must be up: 0.99 × 0.99 × 0.99 = 0.97 = 97%. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-064",
      "type": "multiple-choice",
      "question": "Request queuing time is 50ms. Processing time is 50ms. Server utilization?",
      "options": ["~25%", "~50%", "~75%", "~100%"],
      "correct": 1,
      "explanation": "Response time = queue + processing. If queue = processing, system is at 50% utilization (queuing theory: at 100% utilization, queue time → infinity).",
      "detailedExplanation": "Response time = queue + processing. If queue = processing, system is at 50% utilization (queuing theory: at 100% utilization, queue time → infinity). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-065",
      "type": "multiple-choice",
      "question": "You increase server count from 10 to 15. Traffic is unchanged. Per-server load change?",
      "options": [
        "~33% decrease",
        "~50% decrease",
        "~33% increase",
        "~50% increase"
      ],
      "correct": 0,
      "explanation": "Load per server: 1/10 → 1/15. Change = (1/15 - 1/10) / (1/10) = (0.067 - 0.1) / 0.1 = -33%.",
      "detailedExplanation": "Load per server: 1/10 → 1/15. Change = (1/15 - 1/10) / (1/10) = (0.067 - 0.1) / 0.1 = -33%. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-066",
      "type": "multiple-choice",
      "question": "Canary deployment: 5% traffic to canary. Canary latency is 2× baseline. Overall latency impact?",
      "options": ["~2× slower", "~5% slower", "~10% slower", "Negligible"],
      "correct": 1,
      "explanation": "5% of requests take 2× time. Average = 95% × 1 + 5% × 2 = 0.95 + 0.10 = 1.05 = 5% slower overall.",
      "detailedExplanation": "5% of requests take 2× time. Average = 95% × 1 + 5% × 2 = 0.95 + 0.10 = 1.05 = 5% slower overall. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-067",
      "type": "multiple-choice",
      "question": "TCP connection takes 3 round trips to establish (SYN, SYN-ACK, ACK + request). RTT is 50ms. Connection overhead?",
      "options": ["~50ms", "~100ms", "~150ms", "~200ms"],
      "correct": 2,
      "explanation": "3 round trips × 50ms RTT = 150ms connection overhead.",
      "detailedExplanation": "3 round trips × 50ms RTT = 150ms connection overhead. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-068",
      "type": "multiple-choice",
      "question": "Your API returns paginated results, 100 items per page. Client needs all 10,000 items. Sequential requests at 50ms each. Total time?",
      "options": ["~0.5 seconds", "~5 seconds", "~50 seconds", "~500 seconds"],
      "correct": 1,
      "explanation": "10,000 ÷ 100 = 100 pages. 100 × 50ms = 5,000ms = 5 seconds.",
      "detailedExplanation": "10,000 ÷ 100 = 100 pages. 100 × 50ms = 5,000ms = 5 seconds. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-069",
      "type": "multiple-choice",
      "question": "Semaphore limits concurrent requests to downstream service to 50. Each request takes 100ms. Max throughput to downstream?",
      "options": [
        "~50 QPS",
        "~500 QPS",
        "~5,000 QPS",
        "Unlimited (semaphore doesn't limit throughput)"
      ],
      "correct": 1,
      "explanation": "50 concurrent × 10 requests/second per slot (1/0.1s) = 500 QPS max.",
      "detailedExplanation": "50 concurrent × 10 requests/second per slot (1/0.1s) = 500 QPS max. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-070",
      "type": "multiple-choice",
      "question": "Your read replica lags primary by 100ms. Read-after-write consistency required. Options?",
      "options": [
        "Read from primary",
        "Wait 100ms then read replica",
        "Use session affinity to primary",
        "All are valid approaches"
      ],
      "correct": 3,
      "explanation": "All are valid: read from primary (guaranteed fresh), wait for lag (eventually consistent), or session affinity (user's writes always visible to them).",
      "detailedExplanation": "All are valid: read from primary (guaranteed fresh), wait for lag (eventually consistent), or session affinity (user's writes always visible to them). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-071",
      "type": "multiple-choice",
      "question": "Global load balancer routes by latency. US users: 50ms to US region, 200ms to EU. EU users: 200ms to US, 50ms to EU. Cross-region traffic?",
      "options": [
        "0% (optimal routing)",
        "~50%",
        "~100%",
        "Depends on load balancing algorithm"
      ],
      "correct": 0,
      "explanation": "Latency-based routing sends US users to US (50ms < 200ms) and EU users to EU (50ms < 200ms). No cross-region traffic needed.",
      "detailedExplanation": "Latency-based routing sends US users to US (50ms < 200ms) and EU users to EU (50ms < 200ms). No cross-region traffic needed. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-072",
      "type": "multiple-choice",
      "question": "Write-ahead log (WAL) flush every 10ms. Crash occurs. Maximum data loss?",
      "options": [
        "0 (WAL protects all data)",
        "~10ms of writes",
        "~20ms of writes",
        "Depends on flush timing"
      ],
      "correct": 1,
      "explanation": "WAL flushes every 10ms. Crash loses unflushed writes since last flush = up to 10ms of writes.",
      "detailedExplanation": "WAL flushes every 10ms. Crash loses unflushed writes since last flush = up to 10ms of writes. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-073",
      "type": "multiple-choice",
      "question": "Your API has 100ms SLO. Database adds 40ms, network adds 20ms. Processing budget?",
      "options": ["~40ms", "~60ms", "~100ms", "~160ms"],
      "correct": 0,
      "explanation": "100ms SLO - 40ms DB - 20ms network = 40ms for processing.",
      "detailedExplanation": "100ms SLO - 40ms DB - 20ms network = 40ms for processing. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "qps-074",
      "type": "multiple-choice",
      "question": "Fan-in pattern: 10 upstream services each send 100 req/sec to your service. Your inbound QPS?",
      "options": [
        "~100 QPS",
        "~1,000 QPS",
        "~10,000 QPS",
        "Depends on request timing"
      ],
      "correct": 1,
      "explanation": "Fan-in: 10 services × 100 req/sec = 1,000 QPS total inbound.",
      "detailedExplanation": "Fan-in: 10 services × 100 req/sec = 1,000 QPS total inbound. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-075",
      "type": "multiple-choice",
      "question": "Fan-out pattern: your service sends to 10 downstream services per request. Inbound is 100 QPS. Outbound QPS?",
      "options": [
        "~100 QPS",
        "~1,000 QPS",
        "~10,000 QPS",
        "Depends on parallelism"
      ],
      "correct": 1,
      "explanation": "Fan-out: 100 inbound × 10 downstream calls = 1,000 QPS outbound.",
      "detailedExplanation": "Fan-out: 100 inbound × 10 downstream calls = 1,000 QPS outbound. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-076",
      "type": "multiple-choice",
      "question": "Server GC pauses 100ms every 10 seconds. Impact on P99 latency?",
      "options": [
        "P99 increases by ~1ms",
        "P99 increases by ~10ms",
        "P99 ≥ 100ms",
        "Depends on request duration"
      ],
      "correct": 2,
      "explanation": "GC pause affects 100ms/10s = 1% of time. If P99 includes GC pause, P99 ≥ 100ms (the pause duration).",
      "detailedExplanation": "GC pause affects 100ms/10s = 1% of time. If P99 includes GC pause, P99 ≥ 100ms (the pause duration). Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-077",
      "type": "multiple-choice",
      "question": "Async processing: request enqueues job, returns immediately (5ms). Job takes 5 seconds. User-perceived latency?",
      "options": [
        "~5ms",
        "~5 seconds",
        "~5.005 seconds",
        "Depends on user's next action"
      ],
      "correct": 0,
      "explanation": "User sees 5ms response (job is async). The 5-second processing happens in background.",
      "detailedExplanation": "User sees 5ms response (job is async). The 5-second processing happens in background. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-078",
      "type": "multiple-choice",
      "question": "CQRS: reads go to read model, writes go to write model. Write model publishes events. Event delay is 500ms. Read-after-write behavior?",
      "options": [
        "Always consistent",
        "~500ms eventual consistency",
        "Inconsistent forever",
        "Depends on where read is routed"
      ],
      "correct": 1,
      "explanation": "Write publishes event, read model updates in ~500ms. Read-after-write may see stale data for up to 500ms.",
      "detailedExplanation": "Write publishes event, read model updates in ~500ms. Read-after-write may see stale data for up to 500ms. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-079",
      "type": "multiple-choice",
      "question": "Your service handles 10,000 QPS. You deploy a change that increases latency from 50ms to 100ms. Thread pool is 500. What happens?",
      "options": [
        "Latency doubles, throughput unchanged",
        "Latency doubles, throughput halves",
        "Service becomes overloaded",
        "Depends on queue size"
      ],
      "correct": 2,
      "explanation": "Old capacity: 500 threads × 20 req/s = 10,000 QPS. New capacity: 500 × 10 = 5,000 QPS. Traffic (10,000) > capacity (5,000) = overload, queuing, and degradation.",
      "detailedExplanation": "Old capacity: 500 threads × 20 req/s = 10,000 QPS. New capacity: 500 × 10 = 5,000 QPS. Traffic (10,000) > capacity (5,000) = overload, queuing, and degradation. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-080",
      "type": "multiple-choice",
      "question": "Circuit breaker: closed (normal), open (failing), half-open (testing). Open duration is 30 seconds. Recovery time after errors stop?",
      "options": [
        "Immediate",
        "~30 seconds minimum",
        "~60 seconds",
        "Depends on half-open success rate"
      ],
      "correct": 1,
      "explanation": "Circuit stays open for 30 seconds before entering half-open state. Minimum 30 seconds before any recovery begins.",
      "detailedExplanation": "Circuit stays open for 30 seconds before entering half-open state. Minimum 30 seconds before any recovery begins. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-081",
      "type": "multiple-choice",
      "question": "Request batching: 10 requests batched into 1 backend call. Individual request latency was 20ms. Batched request latency?",
      "options": [
        "~2ms per request",
        "~20ms per request",
        "~200ms per request",
        "Depends on batch processing time"
      ],
      "correct": 3,
      "explanation": "Batch latency depends on: batch wait time + backend processing for batch + response distribution. Could be faster (amortized) or slower (wait for batch to fill).",
      "detailedExplanation": "Batch latency depends on: batch wait time + backend processing for batch + response distribution. Could be faster (amortized) or slower (wait for batch to fill). Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-082",
      "type": "multiple-choice",
      "question": "You add distributed tracing. Trace context is 100 bytes per request. At 100,000 QPS, trace overhead bandwidth?",
      "options": ["~1 MB/s", "~10 MB/s", "~100 MB/s", "~1 GB/s"],
      "correct": 1,
      "explanation": "100 bytes × 100,000 req/s = 10 MB/s bandwidth for trace context.",
      "detailedExplanation": "100 bytes × 100,000 req/s = 10 MB/s bandwidth for trace context. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "qps-083",
      "type": "multiple-choice",
      "question": "Service discovery refreshes every 30 seconds. New server deployed. Max time until all clients discover it?",
      "options": [
        "~30 seconds",
        "~60 seconds",
        "~90 seconds",
        "Depends on health check timing"
      ],
      "correct": 0,
      "explanation": "Clients refresh every 30 seconds. New server appears on next refresh = max 30 seconds.",
      "detailedExplanation": "Clients refresh every 30 seconds. New server appears on next refresh = max 30 seconds. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-084",
      "type": "multiple-choice",
      "question": "Your database supports 10,000 QPS. You add a caching layer with 80% hit rate. New effective capacity?",
      "options": ["~12,500 QPS", "~50,000 QPS", "~80,000 QPS", "~100,000 QPS"],
      "correct": 1,
      "explanation": "20% of requests hit DB. If DB max is 10,000, that's 20% of total. Total capacity = 10,000 ÷ 0.2 = 50,000 QPS.",
      "detailedExplanation": "20% of requests hit DB. If DB max is 10,000, that's 20% of total. Total capacity = 10,000 ÷ 0.2 = 50,000 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-085",
      "type": "multiple-choice",
      "question": "Synchronous microservice call: 50ms. Converting to async (message queue): enqueue 5ms, dequeue+process 50ms. Latency comparison for caller?",
      "options": [
        "Same latency",
        "~5ms latency (async)",
        "~55ms latency",
        "~105ms latency"
      ],
      "correct": 1,
      "explanation": "Caller only waits for enqueue (5ms), not processing. Caller latency drops from 50ms to 5ms.",
      "detailedExplanation": "Caller only waits for enqueue (5ms), not processing. Caller latency drops from 50ms to 5ms. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-086",
      "type": "multiple-choice",
      "question": "Request coalescing window is 5ms. Identical requests arrive at 0ms, 3ms, 7ms. How many backend calls?",
      "options": ["1 call", "2 calls", "3 calls", "Depends on implementation"],
      "correct": 1,
      "explanation": "0ms and 3ms are within 5ms window → 1 call. 7ms starts new window → 1 call. Total: 2 backend calls.",
      "detailedExplanation": "0ms and 3ms are within 5ms window → 1 call. 7ms starts new window → 1 call. Total: 2 backend calls. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-087",
      "type": "multiple-choice",
      "question": "Your service P50=10ms, P99=100ms. Which has more impact on user experience at scale?",
      "options": [
        "P50 (affects most users)",
        "P99 (affects power users)",
        "Equal impact",
        "Depends on business context"
      ],
      "correct": 3,
      "explanation": "Impact depends on context. P50 affects 50% of requests. P99 affects 1% but may be critical paths, power users, or cascade into retries. Business context matters.",
      "detailedExplanation": "Impact depends on context. P50 affects 50% of requests. P99 affects 1% but may be critical paths, power users, or cascade into retries. Business context matters. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-088",
      "type": "multiple-choice",
      "question": "Database transaction takes 50ms. Lock contention under high load. At 1,000 TPS, concurrent transactions holding locks?",
      "options": ["~5", "~50", "~500", "~5,000"],
      "correct": 1,
      "explanation": "Little's Law: concurrent = rate × duration. 1,000 TPS × 0.05s = 50 concurrent transactions.",
      "detailedExplanation": "Little's Law: concurrent = rate × duration. 1,000 TPS × 0.05s = 50 concurrent transactions. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-089",
      "type": "multiple-choice",
      "question": "Horizontal scaling: 1 server handles 1,000 QPS at 70% CPU. To handle 10,000 QPS with 50% headroom (capacity = 2× load), servers needed?",
      "options": ["10 servers", "14 servers", "20 servers", "28 servers"],
      "correct": 2,
      "explanation": "50% headroom means capacity = 2× needed load. Need 20,000 QPS capacity for 10,000 QPS load. At 1,000 QPS per server: 20,000 ÷ 1,000 = 20 servers.",
      "detailedExplanation": "50% headroom means capacity = 2× needed load. Need 20,000 QPS capacity for 10,000 QPS load. At 1,000 QPS per server: 20,000 ÷ 1,000 = 20 servers. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-090",
      "type": "multiple-choice",
      "question": "You add request validation (2ms). Previously: API (10ms) → DB (40ms). New total latency?",
      "options": ["~42ms", "~50ms", "~52ms", "~54ms"],
      "correct": 2,
      "explanation": "Validation (2ms) + API (10ms) + DB (40ms) = 52ms.",
      "detailedExplanation": "Validation (2ms) + API (10ms) + DB (40ms) = 52ms. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-091",
      "type": "multiple-choice",
      "question": "Idempotency check adds 5ms per request (cache lookup). At 10,000 QPS, additional latency budget consumed?",
      "options": [
        "5ms per request",
        "50 seconds per second",
        "~50,000ms per second",
        "5ms per request, 0.5% of typical budget"
      ],
      "correct": 0,
      "explanation": "Each request adds 5ms latency. At 10,000 QPS, that's 50,000ms of total latency added per second, but each individual request only sees 5ms increase.",
      "detailedExplanation": "Each request adds 5ms latency. At 10,000 QPS, that's 50,000ms of total latency added per second, but each individual request only sees 5ms increase. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-092",
      "type": "multiple-choice",
      "question": "Service mesh sidecar crashes. Main application container is healthy. What happens to traffic?",
      "options": [
        "Traffic flows normally",
        "Traffic fails (no proxy)",
        "Traffic bypasses sidecar",
        "Depends on mesh configuration"
      ],
      "correct": 3,
      "explanation": "Behavior depends on configuration. Some meshes fail closed (no traffic), others fail open (bypass proxy), some have automatic recovery. Check your mesh settings.",
      "detailedExplanation": "Behavior depends on configuration. Some meshes fail closed (no traffic), others fail open (bypass proxy), some have automatic recovery. Check your mesh settings. A good sanity check compares your estimate to one or two known anchor numbers. If the ratio is wildly off, call out which assumption is likely wrong and recompute quickly.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-093",
      "type": "multiple-choice",
      "question": "Request retry policy: 3 retries, jitter up to 100ms. Two clients retry simultaneously. Collision probability?",
      "options": ["~100%", "~50%", "~10%", "Very low (jitter spreads retries)"],
      "correct": 3,
      "explanation": "Jitter randomizes retry timing across 100ms window. Probability of exact collision is very low (retries spread across time).",
      "detailedExplanation": "Jitter randomizes retry timing across 100ms window. Probability of exact collision is very low (retries spread across time). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-094",
      "type": "multiple-choice",
      "question": "Load balancer health check: 3 consecutive failures to mark unhealthy, 5 second interval. Minimum detection time?",
      "options": ["~5 seconds", "~10 seconds", "~15 seconds", "~20 seconds"],
      "correct": 1,
      "explanation": "Need 3 consecutive failures. First check fails at t=0, second at t=5, third at t=10. Marked unhealthy at 10 seconds.",
      "detailedExplanation": "Need 3 consecutive failures. First check fails at t=0, second at t=5, third at t=10. Marked unhealthy at 10 seconds. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-095",
      "type": "multiple-choice",
      "question": "Your API serves both mobile (slow networks, high latency tolerance) and web (fast networks, low latency expectation). Same SLO for both?",
      "options": [
        "Yes, simplifies monitoring",
        "No, different SLOs per client type",
        "Yes, design for worst case",
        "No, but same backend limits apply"
      ],
      "correct": 1,
      "explanation": "Different clients have different expectations. Mobile users tolerate higher latency than web users. SLOs should reflect user expectations per platform.",
      "detailedExplanation": "Different clients have different expectations. Mobile users tolerate higher latency than web users. SLOs should reflect user expectations per platform. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-096",
      "type": "multiple-choice",
      "question": "Pre-warming: load 1 million cache entries at startup. Each entry takes 1ms to load. Startup time impact?",
      "options": ["~1 second", "~17 minutes", "~1.7 hours", "~17 hours"],
      "correct": 1,
      "explanation": "1M entries × 1ms = 1,000,000ms = 1,000 seconds = 16.7 minutes.",
      "detailedExplanation": "1M entries × 1ms = 1,000,000ms = 1,000 seconds = 16.7 minutes. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-097",
      "type": "multiple-choice",
      "question": "A/B test: 50% traffic to variant A (100ms latency), 50% to variant B (50ms latency). Median user latency?",
      "options": ["~50ms", "~75ms", "~100ms", "Either 50ms or 100ms (bimodal)"],
      "correct": 3,
      "explanation": "50% see 50ms, 50% see 100ms. Median is the middle value. With bimodal distribution, median depends on exact split—could be either 50ms or 100ms, distribution is bimodal not normal.",
      "detailedExplanation": "50% see 50ms, 50% see 100ms. Median is the middle value. With bimodal distribution, median depends on exact split—could be either 50ms or 100ms, distribution is bimodal not normal. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-098",
      "type": "multiple-choice",
      "question": "Daily traffic pattern: 10× spike at noon vs 3am. You provision for peak. Night-time utilization?",
      "options": ["~10%", "~30%", "~50%", "~90%"],
      "correct": 0,
      "explanation": "Peak is 10× trough. If provisioned for peak, trough utilization = 1/10 = 10%.",
      "detailedExplanation": "Peak is 10× trough. If provisioned for peak, trough utilization = 1/10 = 10%. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-099",
      "type": "multiple-choice",
      "question": "Database max connections: 100. Connection pool per app server: 10. Max app servers?",
      "options": [
        "10 servers",
        "100 servers",
        "1,000 servers",
        "Depends on connection pooler"
      ],
      "correct": 0,
      "explanation": "100 total connections ÷ 10 per server = 10 app servers max before exhausting DB connections.",
      "detailedExplanation": "100 total connections ÷ 10 per server = 10 app servers max before exhausting DB connections. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-100",
      "type": "multiple-choice",
      "question": "Your SLO is 99.9% requests under 200ms. Current P99.9 is 190ms. Error budget remaining?",
      "options": ["~5%", "~10%", "~50%", "~95%"],
      "correct": 0,
      "explanation": "P99.9 of 190ms vs SLO of 200ms. Headroom = (200-190)/200 = 5% of latency budget remaining before SLO breach.",
      "detailedExplanation": "P99.9 of 190ms vs SLO of 200ms. Headroom = (200-190)/200 = 5% of latency budget remaining before SLO breach. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n001",
      "type": "numeric-input",
      "question": "100 million DAU, 20 requests per user per day. Average QPS?",
      "answer": 23148,
      "tolerance": 0.1,
      "explanation": "100M × 20 = 2B requests/day ÷ 86,400 = 23,148 QPS.",
      "detailedExplanation": "100M × 20 = 2B requests/day ÷ 86,400 = 23,148 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n002",
      "type": "numeric-input",
      "question": "Peak traffic is 5× average. Average QPS is 1,000. Peak QPS?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "1,000 × 5 = 5,000 QPS at peak.",
      "detailedExplanation": "1,000 × 5 = 5,000 QPS at peak. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n003",
      "type": "numeric-input",
      "question": "A server handles 2,000 QPS. How many servers for 50,000 QPS with 50% headroom?",
      "answer": 38,
      "tolerance": 0.15,
      "explanation": "50,000 × 1.5 (headroom) = 75,000 capacity needed. 75,000 ÷ 2,000 = 37.5, round up to 38.",
      "detailedExplanation": "50,000 × 1.5 (headroom) = 75,000 capacity needed. 75,000 ÷ 2,000 = 37.5, round up to 38. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n004",
      "type": "numeric-input",
      "question": "Request takes 50ms. Single-threaded max QPS?",
      "answer": 20,
      "tolerance": 0.1,
      "explanation": "1000ms ÷ 50ms = 20 requests per second.",
      "detailedExplanation": "1000ms ÷ 50ms = 20 requests per second. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n005",
      "type": "numeric-input",
      "question": "10 threads, 100ms per request. Max QPS for this service?",
      "answer": 100,
      "tolerance": 0.1,
      "explanation": "Each thread handles 10 QPS. 10 threads × 10 = 100 QPS.",
      "detailedExplanation": "Each thread handles 10 QPS. 10 threads × 10 = 100 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n006",
      "type": "numeric-input",
      "question": "5,000 QPS for 8 hours during business day, 500 QPS for 16 off-hours. Daily requests in millions?",
      "answer": 173,
      "unit": "million",
      "tolerance": 0.1,
      "explanation": "5000 × 8 × 3600 + 500 × 16 × 3600 = 144M + 28.8M = 172.8M.",
      "detailedExplanation": "5000 × 8 × 3600 + 500 × 16 × 3600 = 144M + 28.8M = 172.8M. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n007",
      "type": "numeric-input",
      "question": "Rate limit: 100 requests per minute per user. Max requests per second per user?",
      "answer": 1.67,
      "tolerance": 0.15,
      "explanation": "100 ÷ 60 = 1.67 requests per second.",
      "detailedExplanation": "100 ÷ 60 = 1.67 requests per second. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "qps-n008",
      "type": "numeric-input",
      "question": "Database handles 5,000 reads/sec and 500 writes/sec. Read:write ratio?",
      "answer": 10,
      "tolerance": 0.1,
      "explanation": "5,000 ÷ 500 = 10:1 read:write ratio.",
      "detailedExplanation": "5,000 ÷ 500 = 10:1 read:write ratio. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n009",
      "type": "numeric-input",
      "question": "Cache hit rate is 95%. 10,000 QPS total. Database QPS (cache misses)?",
      "answer": 500,
      "tolerance": 0.1,
      "explanation": "5% miss rate. 10,000 × 0.05 = 500 QPS to database.",
      "detailedExplanation": "5% miss rate. 10,000 × 0.05 = 500 QPS to database. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n010",
      "type": "numeric-input",
      "question": "1 million concurrent WebSocket connections, each sends 1 message per 10 seconds. Messages per second?",
      "answer": 100000,
      "tolerance": 0.1,
      "explanation": "1M ÷ 10 = 100,000 messages per second.",
      "detailedExplanation": "1M ÷ 10 = 100,000 messages per second. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n011",
      "type": "numeric-input",
      "question": "API gateway handles 100,000 QPS. 60% goes to service A. Service A's QPS?",
      "answer": 60000,
      "tolerance": 0.1,
      "explanation": "100,000 × 0.60 = 60,000 QPS.",
      "detailedExplanation": "100,000 × 0.60 = 60,000 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n012",
      "type": "numeric-input",
      "question": "Search index handles 1,000 QPS per shard. Need 15,000 QPS capacity. Minimum shards?",
      "answer": 15,
      "tolerance": 0.1,
      "explanation": "15,000 ÷ 1,000 = 15 shards.",
      "detailedExplanation": "15,000 ÷ 1,000 = 15 shards. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n013",
      "type": "numeric-input",
      "question": "Load balancer distributes to 5 servers. Total 25,000 QPS. QPS per server?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "25,000 ÷ 5 = 5,000 QPS per server.",
      "detailedExplanation": "25,000 ÷ 5 = 5,000 QPS per server. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n014",
      "type": "numeric-input",
      "question": "Retry rate is 5%. Original QPS is 10,000. Total QPS including retries?",
      "answer": 10500,
      "tolerance": 0.1,
      "explanation": "10,000 + (10,000 × 0.05) = 10,500 QPS.",
      "detailedExplanation": "10,000 + (10,000 × 0.05) = 10,500 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n015",
      "type": "numeric-input",
      "question": "A/B test: 10% traffic to variant B. Total 50,000 QPS. Variant B QPS?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "50,000 × 0.10 = 5,000 QPS.",
      "detailedExplanation": "50,000 × 0.10 = 5,000 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n016",
      "type": "numeric-input",
      "question": "Connection pool has 100 connections, each handles 50 QPS. Max throughput?",
      "answer": 5000,
      "tolerance": 0.1,
      "explanation": "100 × 50 = 5,000 QPS max.",
      "detailedExplanation": "100 × 50 = 5,000 QPS max. Bandwidth sizing should start with clean unit conversions (bits vs bytes and per-second rates), then include peak multipliers and protocol overhead. That keeps transfer and egress estimates realistic under production traffic.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n017",
      "type": "numeric-input",
      "question": "Kafka partition handles 10,000 messages/sec. Need 80,000 msg/sec. Minimum partitions?",
      "answer": 8,
      "tolerance": 0.1,
      "explanation": "80,000 ÷ 10,000 = 8 partitions.",
      "detailedExplanation": "80,000 ÷ 10,000 = 8 partitions. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-n018",
      "type": "numeric-input",
      "question": "Redis handles 100,000 ops/sec. Your app needs 25 ops per request at 2,000 QPS. Redis utilization %?",
      "answer": 50,
      "unit": "%",
      "tolerance": 0.1,
      "explanation": "2,000 × 25 = 50,000 ops/sec. 50,000 ÷ 100,000 = 50%.",
      "detailedExplanation": "2,000 × 25 = 50,000 ops/sec. 50,000 ÷ 100,000 = 50%. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-o001",
      "type": "ordering",
      "question": "Rank by typical QPS capacity (lowest to highest).",
      "items": [
        "Single PostgreSQL",
        "Redis instance",
        "Single web server",
        "Kafka broker"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "PostgreSQL (~5K) < Web server (~10K) < Redis (~100K) < Kafka (~1M msg/sec).",
      "detailedExplanation": "PostgreSQL (~5K) < Web server (~10K) < Redis (~100K) < Kafka (~1M msg/sec). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-o002",
      "type": "ordering",
      "question": "Rank these user bases by expected QPS (lowest to highest), assuming 10 req/user/day.",
      "items": ["100K DAU", "10M DAU", "1M DAU", "100M DAU"],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "100K (12 QPS) < 1M (116 QPS) < 10M (1,157 QPS) < 100M (11,574 QPS).",
      "detailedExplanation": "100K (12 QPS) < 1M (116 QPS) < 10M (1,157 QPS) < 100M (11,574 QPS). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-o003",
      "type": "ordering",
      "question": "Rank by operations per request (typically lowest to highest).",
      "items": [
        "Static file serve",
        "User login",
        "News feed render",
        "Search query"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Static (1 op) < Login (2-3 ops) < Search (5-10 ops) < Feed (10-100 ops).",
      "detailedExplanation": "Static (1 op) < Login (2-3 ops) < Search (5-10 ops) < Feed (10-100 ops). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-o004",
      "type": "ordering",
      "question": "Rank by QPS multiplier effect (lowest to highest).",
      "items": [
        "1 retry on failure",
        "Fan-out to 10 services",
        "95% cache hit rate",
        "3 database queries per request"
      ],
      "correctOrder": [2, 0, 3, 1],
      "explanation": "Cache reduces load. 1 retry adds ~5%. 3 queries = 3×. Fan-out = 10×.",
      "detailedExplanation": "Cache reduces load. 1 retry adds ~5%. 3 queries = 3×. Fan-out = 10×. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-o005",
      "type": "ordering",
      "question": "Rank these scaling strategies by QPS increase potential (lowest to highest).",
      "items": [
        "Add more RAM",
        "Add read replicas",
        "Add caching layer",
        "Horizontal sharding"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "RAM (marginal) < Replicas (2-5×) < Caching (10-100×) < Sharding (unlimited).",
      "detailedExplanation": "RAM (marginal) < Replicas (2-5×) < Caching (10-100×) < Sharding (unlimited). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-o006",
      "type": "ordering",
      "question": "Rank by latency impact on max QPS (lowest to highest latency = highest QPS).",
      "items": [
        "50ms response",
        "200ms response",
        "10ms response",
        "500ms response"
      ],
      "correctOrder": [3, 1, 0, 2],
      "explanation": "Lower latency = higher QPS. 500ms (2 QPS) < 200ms (5) < 50ms (20) < 10ms (100).",
      "detailedExplanation": "Lower latency = higher QPS. 500ms (2 QPS) < 200ms (5) < 50ms (20) < 10ms (100). Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-o007",
      "type": "ordering",
      "question": "Rank traffic patterns by peak:average ratio (lowest to highest).",
      "items": [
        "Enterprise SaaS",
        "Social media",
        "E-commerce flash sale",
        "Banking"
      ],
      "correctOrder": [0, 3, 1, 2],
      "explanation": "Enterprise (~1.5×) < Banking (~2×) < Social (~3-5×) < Flash sale (~10-100×).",
      "detailedExplanation": "Enterprise (~1.5×) < Banking (~2×) < Social (~3-5×) < Flash sale (~10-100×). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-o008",
      "type": "ordering",
      "question": "Rank by requests generated per action (lowest to highest).",
      "items": [
        "Page view",
        "User signup",
        "Checkout flow",
        "Real-time dashboard"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Page view (1-3) < Signup (5-10) < Checkout (10-20) < Dashboard (continuous polling).",
      "detailedExplanation": "Page view (1-3) < Signup (5-10) < Checkout (10-20) < Dashboard (continuous polling). Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-m001",
      "type": "multi-select",
      "question": "Which equal ~1,000 QPS?",
      "options": [
        "86.4 million requests/day",
        "2.6 billion requests/month",
        "3.6 million requests/hour",
        "60,000 requests/minute"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All convert to ~1,000 QPS: 86.4M/86.4K, 2.6B/2.6M, 3.6M/3.6K, 60K/60.",
      "detailedExplanation": "All convert to ~1,000 QPS: 86.4M/86.4K, 2.6B/2.6M, 3.6M/3.6K, 60K/60. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-m002",
      "type": "multi-select",
      "question": "Which increase effective QPS capacity?",
      "options": [
        "Adding caching",
        "Reducing response payload",
        "Adding more threads",
        "Reducing database queries per request"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Caching reduces backend load, more threads handle more concurrent requests, fewer DB queries reduce latency. Payload size affects bandwidth, not QPS directly.",
      "detailedExplanation": "Caching reduces backend load, more threads handle more concurrent requests, fewer DB queries reduce latency. Payload size affects bandwidth, not QPS directly. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "qps-m003",
      "type": "multi-select",
      "question": "At 10,000 QPS with 100ms latency, which are true? (Little's Law)",
      "options": [
        "1,000 concurrent requests",
        "100 concurrent requests",
        "10 requests in flight per ms",
        "Need 1,000 threads for sync processing"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "L = λW = 10,000 × 0.1 = 1,000 concurrent. 1000/100ms = 10/ms. Sync needs 1 thread per concurrent request.",
      "detailedExplanation": "L = λW = 10,000 × 0.1 = 1,000 concurrent. 1000/100ms = 10/ms. Sync needs 1 thread per concurrent request. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-m004",
      "type": "multi-select",
      "question": "Which scenarios result in QPS amplification?",
      "options": [
        "Client retries",
        "Fan-out queries",
        "Response caching",
        "Batch processing"
      ],
      "correctIndices": [0, 1],
      "explanation": "Retries and fan-out multiply requests. Caching and batching reduce QPS.",
      "detailedExplanation": "Retries and fan-out multiply requests. Caching and batching reduce QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-m005",
      "type": "multi-select",
      "question": "A service handles 5,000 QPS. Which are valid scaling options for 20,000 QPS?",
      "options": [
        "4 instances behind load balancer",
        "Increase thread pool 4×",
        "Add caching with 75% hit rate",
        "4 database shards"
      ],
      "correctIndices": [0, 2],
      "explanation": "4 instances = 20K QPS ✓. 4× threads helps but rarely 4× QPS. 75% cache = 4× effective capacity ✓. DB shards help DB, not service QPS directly.",
      "detailedExplanation": "4 instances = 20K QPS ✓. 4× threads helps but rarely 4× QPS. 75% cache = 4× effective capacity ✓. DB shards help DB, not service QPS directly. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-m006",
      "type": "multi-select",
      "question": "Which affect peak-to-average QPS ratio?",
      "options": [
        "Geographic distribution of users",
        "Time-based usage patterns",
        "Viral content events",
        "Database replication lag"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Geography (time zones), time patterns (business hours), and viral events affect peak:avg. Replication lag doesn't affect traffic patterns.",
      "detailedExplanation": "Geography (time zones), time patterns (business hours), and viral events affect peak:avg. Replication lag doesn't affect traffic patterns. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "qps-m007",
      "type": "multi-select",
      "question": "Server handles 1,000 QPS at 50% CPU. Which are likely true?",
      "options": [
        "Can probably handle 1,500 QPS",
        "Theoretical max around 2,000 QPS",
        "Should add capacity before 80% CPU",
        "CPU is the bottleneck"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "50% CPU suggests ~2× headroom. Best practice: scale before 80%. Can't determine if CPU is THE bottleneck without more info.",
      "detailedExplanation": "50% CPU suggests ~2× headroom. Best practice: scale before 80%. Can't determine if CPU is THE bottleneck without more info. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-m008",
      "type": "multi-select",
      "question": "Rate limiting at 100 req/min/user. With 10,000 concurrent users, which are true?",
      "options": [
        "Max 1 million req/min total",
        "Max ~16,667 QPS system-wide",
        "Each user gets ~1.67 QPS",
        "Bursty users may hit limits even at lower average"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "10K × 100 = 1M/min max. 1M/60 = 16,667 QPS. 100/60 = 1.67/user. Bursts can exceed sustained average.",
      "detailedExplanation": "10K × 100 = 1M/min max. 1M/60 = 16,667 QPS. 100/60 = 1.67/user. Bursts can exceed sustained average. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        }
      ]
    },
    {
      "id": "qps-t001",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your app has 50 million DAU, each making 30 requests/day. What's your average QPS?",
          "options": [
            "~1,700 QPS",
            "~17,000 QPS",
            "~170,000 QPS",
            "~1.7 million QPS"
          ],
          "correct": 1,
          "explanation": "50M × 30 = 1.5B/day ÷ 86,400 = 17,361 QPS.",
          "detailedExplanation": "50M × 30 = 1.5B/day ÷ 86,400 = 17,361 QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit."
        },
        {
          "question": "If peak is 3× average, and each server handles 5,000 QPS, how many servers?",
          "options": [
            "~4 servers",
            "~11 servers",
            "~21 servers",
            "~52 servers"
          ],
          "correct": 1,
          "explanation": "Peak = 52,000 QPS. 52,000 ÷ 5,000 = 10.4, round up to 11.",
          "detailedExplanation": "Peak = 52,000 QPS. 52,000 ÷ 5,000 = 10.4, round up to 11. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit."
        }
      ],
      "explanation": "Capacity planning from DAU to server count.",
      "detailedExplanation": "Capacity planning from DAU to server count. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-t002",
      "type": "two-stage",
      "stages": [
        {
          "question": "Each request makes 3 database queries. At 10,000 app QPS, database QPS?",
          "options": [
            "~3,333 QPS",
            "~10,000 QPS",
            "~30,000 QPS",
            "~300,000 QPS"
          ],
          "correct": 2,
          "explanation": "10,000 × 3 = 30,000 database QPS.",
          "detailedExplanation": "10,000 × 3 = 30,000 database QPS. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit."
        },
        {
          "question": "Database handles 10,000 QPS per replica. How many read replicas needed?",
          "options": ["1 replica", "3 replicas", "5 replicas", "10 replicas"],
          "correct": 1,
          "explanation": "30,000 ÷ 10,000 = 3 replicas minimum.",
          "detailedExplanation": "30,000 ÷ 10,000 = 3 replicas minimum. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit."
        }
      ],
      "explanation": "Query amplification affects database scaling.",
      "detailedExplanation": "Query amplification affects database scaling. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "qps-t003",
      "type": "two-stage",
      "stages": [
        {
          "question": "Request latency is 200ms. What's the max QPS per thread?",
          "options": ["~2 QPS", "~5 QPS", "~20 QPS", "~50 QPS"],
          "correct": 1,
          "explanation": "1000ms ÷ 200ms = 5 QPS per thread.",
          "detailedExplanation": "1000ms ÷ 200ms = 5 QPS per thread. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit."
        },
        {
          "question": "You need 500 QPS. How many threads in your thread pool?",
          "options": ["25 threads", "50 threads", "100 threads", "500 threads"],
          "correct": 2,
          "explanation": "500 QPS ÷ 5 QPS/thread = 100 threads.",
          "detailedExplanation": "500 QPS ÷ 5 QPS/thread = 100 threads. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit."
        }
      ],
      "explanation": "Thread pool sizing from latency and throughput requirements.",
      "detailedExplanation": "Thread pool sizing from latency and throughput requirements. Reliability targets are easier to reason about when converted to concrete counts and minutes. That conversion makes tradeoffs around redundancy, retries, and operational response times explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "qps-t004",
      "type": "two-stage",
      "stages": [
        {
          "question": "Cache hit rate is 90%. Backend handles 1,000 QPS max. What total QPS can you serve?",
          "options": [
            "~1,000 QPS",
            "~9,000 QPS",
            "~10,000 QPS",
            "~100,000 QPS"
          ],
          "correct": 2,
          "explanation": "10% misses hit backend. 1,000 = 10% of X. X = 10,000 QPS total.",
          "detailedExplanation": "10% misses hit backend. 1,000 = 10% of X. X = 10,000 QPS total. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit."
        },
        {
          "question": "To serve 50,000 QPS total, what cache hit rate do you need?",
          "options": ["95%", "96%", "98%", "99%"],
          "correct": 2,
          "explanation": "Backend max 1,000 QPS = 2% of 50,000. Need 98% hit rate.",
          "detailedExplanation": "Backend max 1,000 QPS = 2% of 50,000. Need 98% hit rate. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit."
        }
      ],
      "explanation": "Cache hit rate determines effective capacity.",
      "detailedExplanation": "Cache hit rate determines effective capacity. Load estimates should separate average QPS from peak QPS and then map both to per-instance throughput limits. This makes scaling triggers and headroom decisions explicit.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    }
  ]
}
