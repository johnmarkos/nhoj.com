{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 7,
  "chapterTitle": "Notification System Core Architecture",
  "chapterDescription": "Decompose notification event intake, preference evaluation, dedupe, scheduling, and channel dispatch.",
  "problems": [
    {
      "id": "cd-nc-001",
      "type": "multiple-choice",
      "question": "Case Alpha: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"notification event ingest API\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-nc-002",
      "type": "multiple-choice",
      "question": "Case Beta: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"user preference evaluation service\". Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-003",
      "type": "multiple-choice",
      "question": "Case Gamma: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"template rendering service\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-004",
      "type": "multiple-choice",
      "question": "Case Delta: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Separate transactional and marketing queues with strict priority controls."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"dedupe/idempotency processor\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"send-time scheduler\". Eliminate options that ignore delivery semantics or backpressure behavior. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-006",
      "type": "multiple-choice",
      "question": "Case Zeta: channel routing engine. Dominant risk is timezone scheduling errors missing user windows. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"channel routing engine\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-007",
      "type": "multiple-choice",
      "question": "Case Eta: email provider adapter. Dominant risk is idempotency window too short for delayed retries. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Track provider quotas and apply adaptive throttling before hard rate limits.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"email provider adapter\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-008",
      "type": "multiple-choice",
      "question": "Case Theta: push provider adapter. Dominant risk is priority messages delayed behind bulk marketing traffic. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Persist audit records independently from channel send acknowledgements."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"push provider adapter\". Eliminate options that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-009",
      "type": "multiple-choice",
      "question": "Case Iota: SMS dispatch adapter. Dominant risk is provider rate-limit breaches causing drops. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Design scheduler to survive restarts with durable job state and safe replays.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"sMS dispatch adapter\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-010",
      "type": "multiple-choice",
      "question": "Case Kappa: notification audit log service. Dominant risk is audit trail gaps during partial failures. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Route by policy matrix so fallback channel behavior is explicit and testable.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"notification audit log service\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-011",
      "type": "multiple-choice",
      "question": "Case Lambda: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"notification event ingest API\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-nc-012",
      "type": "multiple-choice",
      "question": "Case Mu: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"user preference evaluation service\". Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-013",
      "type": "multiple-choice",
      "question": "Case Nu: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"template rendering service\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-014",
      "type": "multiple-choice",
      "question": "Case Xi: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Separate transactional and marketing queues with strict priority controls.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"dedupe/idempotency processor\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-015",
      "type": "multiple-choice",
      "question": "Case Omicron: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"send-time scheduler\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-016",
      "type": "multiple-choice",
      "question": "Case Pi: channel routing engine. Dominant risk is timezone scheduling errors missing user windows. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use per-user timezone scheduling with validated daylight-saving-safe rules."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"channel routing engine\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-017",
      "type": "multiple-choice",
      "question": "Case Rho: email provider adapter. Dominant risk is idempotency window too short for delayed retries. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Track provider quotas and apply adaptive throttling before hard rate limits.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"email provider adapter\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-018",
      "type": "multiple-choice",
      "question": "Case Sigma: push provider adapter. Dominant risk is priority messages delayed behind bulk marketing traffic. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Persist audit records independently from channel send acknowledgements.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"push provider adapter\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-019",
      "type": "multiple-choice",
      "question": "Case Tau: SMS dispatch adapter. Dominant risk is provider rate-limit breaches causing drops. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Design scheduler to survive restarts with durable job state and safe replays.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"sMS dispatch adapter\", then pressure-test the result against the options. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: notification audit log service. Dominant risk is audit trail gaps during partial failures. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Route by policy matrix so fallback channel behavior is explicit and testable."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"notification audit log service\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-021",
      "type": "multiple-choice",
      "question": "Case Phi: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"notification event ingest API\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-nc-022",
      "type": "multiple-choice",
      "question": "Case Chi: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"user preference evaluation service\". Reject options that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-023",
      "type": "multiple-choice",
      "question": "Case Psi: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"template rendering service\" in view, the correct answer separates faster. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-024",
      "type": "multiple-choice",
      "question": "Case Omega: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Separate transactional and marketing queues with strict priority controls."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"dedupe/idempotency processor\", then pressure-test the result against the options. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-025",
      "type": "multiple-choice",
      "question": "Case Atlas: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"send-time scheduler\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-026",
      "type": "multiple-choice",
      "question": "Case Nova: channel routing engine. Dominant risk is timezone scheduling errors missing user windows. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"channel routing engine\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-027",
      "type": "multiple-choice",
      "question": "Case Orion: email provider adapter. Dominant risk is idempotency window too short for delayed retries. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Track provider quotas and apply adaptive throttling before hard rate limits.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"email provider adapter\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-028",
      "type": "multiple-choice",
      "question": "Case Vega: push provider adapter. Dominant risk is priority messages delayed behind bulk marketing traffic. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Persist audit records independently from channel send acknowledgements."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"push provider adapter\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-029",
      "type": "multiple-choice",
      "question": "Case Helios: SMS dispatch adapter. Dominant risk is provider rate-limit breaches causing drops. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Design scheduler to survive restarts with durable job state and safe replays.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"sMS dispatch adapter\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-030",
      "type": "multiple-choice",
      "question": "Case Aurora: notification audit log service. Dominant risk is audit trail gaps during partial failures. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Route by policy matrix so fallback channel behavior is explicit and testable.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"notification audit log service\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"notification event ingest API\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-nc-032",
      "type": "multiple-choice",
      "question": "Case Pulse: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"user preference evaluation service\" as your starting point, then verify tradeoffs carefully. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-033",
      "type": "multiple-choice",
      "question": "Case Forge: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"template rendering service\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-034",
      "type": "multiple-choice",
      "question": "Case Harbor: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Separate transactional and marketing queues with strict priority controls.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"dedupe/idempotency processor\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-035",
      "type": "multiple-choice",
      "question": "Case Vector: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"send-time scheduler\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for notification event ingest API: signal points to template render\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for notification event ingest API:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Core Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for user preference evaluation service: signal points to global\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for user preference evaluation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Track provider quotas and apply adaptive throttling before hard rate limits.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"notification System Core Architecture\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for template rendering service: signal points to timezone scheduling errors missing user windows. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for template rendering service: signal points to timezone scheduling\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for template rendering service: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Persist audit records independently from channel send acknowledgements."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"notification System Core Architecture\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for dedupe/idempotency processor: signal points to idempotency window\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for dedupe/idempotency processor:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Design scheduler to survive restarts with durable job state and safe replays.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"notification System Core Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for send-time scheduler: signal points to priority messages delayed\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for send-time scheduler: signal points\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Route by policy matrix so fallback channel behavior is explicit and testable.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"notification System Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for channel routing engine: signal points to provider rate-limit breaches causing drops. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around channel routing engine mismatches provider rate-limit breaches causing drops, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for channel routing engine: signal points to provider rate-limit\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for channel routing engine: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Enforce idempotency keys per event-channel and dedupe before dispatch.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for email provider adapter: signal points to audit trail gaps during partial failures. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around email provider adapter mismatches audit trail gaps during partial failures, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for email provider adapter: signal points to audit trail gaps during\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for email provider adapter: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Evaluate preferences against versioned snapshots with deterministic fallback policy."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"notification System Core Architecture\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push provider adapter: signal points to duplicate sends from retry races. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around push provider adapter mismatches duplicate sends from retry races, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for push provider adapter: signal points to duplicate sends from retry\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for push provider adapter: signal\", which next step is strongest under current constraints?",
          "options": [
            "Isolate channel adapters with bulkheads and per-channel retry budgets.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for SMS dispatch adapter: signal points to preference stale reads causing policy violations. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around SMS dispatch adapter mismatches preference stale reads causing policy violations, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for SMS dispatch adapter: signal points to preference stale reads\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for SMS dispatch adapter: signal points\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Separate transactional and marketing queues with strict priority controls.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification audit log service: signal points to channel adapter outage cascading to all sends. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around notification audit log service mismatches channel adapter outage cascading to all sends, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for notification audit log service: signal points to channel adapter\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for notification audit log service:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Make template failures non-blocking for critical notifications via fallback templates.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"notification System Core Architecture\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for notification event ingest API: signal points to template render\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for notification event ingest API:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Use per-user timezone scheduling with validated daylight-saving-safe rules."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for user preference evaluation service: signal points to global\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for user preference evaluation service:\" scenario, what should change first before wider rollout?",
          "options": [
            "Track provider quotas and apply adaptive throttling before hard rate limits.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"notification System Core Architecture\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for template rendering service: signal points to timezone scheduling errors missing user windows. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for template rendering service: signal points to timezone scheduling\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for template rendering service: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Persist audit records independently from channel send acknowledgements.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for dedupe/idempotency processor: signal points to idempotency window\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident review for dedupe/idempotency processor:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Design scheduler to survive restarts with durable job state and safe replays.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The decision turns on \"notification System Core Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for send-time scheduler: signal points to priority messages delayed\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for send-time scheduler: signal points\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Route by policy matrix so fallback channel behavior is explicit and testable."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"notification System Core Architecture\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for channel routing engine: signal points to provider rate-limit breaches causing drops. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around channel routing engine mismatches provider rate-limit breaches causing drops, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for channel routing engine: signal points to provider rate-limit\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for channel routing engine: signal\", what should change first before wider rollout?",
          "options": [
            "Enforce idempotency keys per event-channel and dedupe before dispatch.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"notification System Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for email provider adapter: signal points to audit trail gaps during partial failures. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around email provider adapter mismatches audit trail gaps during partial failures, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for email provider adapter: signal points to audit trail gaps during\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for email provider adapter: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Evaluate preferences against versioned snapshots with deterministic fallback policy.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"notification System Core Architecture\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push provider adapter: signal points to duplicate sends from retry races. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around push provider adapter mismatches duplicate sends from retry races, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for push provider adapter: signal points to duplicate sends from retry\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for push provider adapter: signal\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Isolate channel adapters with bulkheads and per-channel retry budgets.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for SMS dispatch adapter: signal points to preference stale reads causing policy violations. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around SMS dispatch adapter mismatches preference stale reads causing policy violations, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for SMS dispatch adapter: signal points to preference stale reads\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for SMS dispatch adapter: signal points\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Separate transactional and marketing queues with strict priority controls."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification audit log service: signal points to channel adapter outage cascading to all sends. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around notification audit log service mismatches channel adapter outage cascading to all sends, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for notification audit log service: signal points to channel adapter\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for notification audit log service:\", what first move gives the best reliability impact?",
          "options": [
            "Make template failures non-blocking for critical notifications via fallback templates.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Start from \"notification System Core Architecture\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for notification event ingest API: signal points to template render\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for notification event ingest API:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for user preference evaluation service: signal points to global\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for user preference evaluation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Track provider quotas and apply adaptive throttling before hard rate limits.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"notification System Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for template rendering service: signal points to timezone scheduling errors missing user windows. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for template rendering service: signal points to timezone scheduling\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for template rendering service: signal\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Persist audit records independently from channel send acknowledgements."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"notification System Core Architecture\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for dedupe/idempotency processor: signal points to idempotency window\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for dedupe/idempotency processor:\", which next step is strongest under current constraints?",
          "options": [
            "Design scheduler to survive restarts with durable job state and safe replays.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"notification System Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for send-time scheduler: signal points to priority messages delayed\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for send-time scheduler: signal points\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Route by policy matrix so fallback channel behavior is explicit and testable.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"notification System Core Architecture\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-061",
      "type": "multi-select",
      "question": "Pick all statements that fit: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The key clue in this question is \"signals best identify decomposition boundary mistakes? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-062",
      "type": "multi-select",
      "question": "Pick all statements that fit: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Read this as a scenario about \"controls improve safety on critical write paths? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-063",
      "type": "multi-select",
      "question": "Pick all statements that fit: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The decision turns on \"practices reduce hot-key or hot-partition impact? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-nc-064",
      "type": "multi-select",
      "question": "Pick all statements that fit: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "This prompt is really about \"improves reliability when mixing sync and async paths? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-nc-065",
      "type": "multi-select",
      "question": "Pick all statements that fit: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Use \"choices usually lower operational risk at scale? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-066",
      "type": "multi-select",
      "question": "Pick all statements that fit: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The core signal here is \"be explicit in API/service contracts for this design? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-067",
      "type": "multi-select",
      "question": "Pick all statements that fit: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "If you keep \"anti-patterns often cause incident recurrence? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-068",
      "type": "multi-select",
      "question": "Pick all statements that fit: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Start from \"increases confidence before broad traffic rollout? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-069",
      "type": "multi-select",
      "question": "Pick all statements that fit: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The key clue in this question is \"controls protect high-priority traffic during spikes? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-070",
      "type": "multi-select",
      "question": "Pick all statements that fit: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The decision turns on \"telemetry dimensions are most actionable for design triage? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-071",
      "type": "multi-select",
      "question": "Pick all statements that fit: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Read this as a scenario about \"governance actions improve cross-team reliability ownership? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-072",
      "type": "multi-select",
      "question": "Pick all statements that fit: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The key clue in this question is \"helps prevent retry amplification cascades? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-nc-073",
      "type": "multi-select",
      "question": "Pick all statements that fit: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Start from \"fallback strategies are strong when dependencies degrade? (Select all that apply)\", then pressure-test the result against the options. Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-074",
      "type": "multi-select",
      "question": "Pick all statements that fit: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "If you keep \"reduces data-quality regressions in eventual pipelines? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-075",
      "type": "multi-select",
      "question": "Pick all statements that fit: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The core signal here is \"runbook components improve incident execution quality? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-076",
      "type": "multi-select",
      "question": "Pick all statements that fit: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Use \"architecture choices improve blast-radius containment? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-077",
      "type": "multi-select",
      "question": "Pick all statements that fit: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "This prompt is really about \"evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Determine failures/day.",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "0.0018*5,400,000=9,720.",
      "detailedExplanation": "The decision turns on \"critical path handles 5,400,000 requests/day and 0\". Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 5,400 and 000 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Determine net drain rate.",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "2,530-2,200=330.",
      "detailedExplanation": "Read this as a scenario about \"queue ingest is 2,200 events/min and drain is 2,530 events/min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Determine effective attempts/sec.",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "75,000*1.28=96,000.",
      "detailedExplanation": "The core signal here is \"retries add 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Determine total failover seconds/day.",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "16*24=384.",
      "detailedExplanation": "If you keep \"failover takes 16s and occurs 24 times/day\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 16s and 24 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Determine percent over target.",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(845-650)/650=30%.",
      "detailedExplanation": "This prompt is really about \"target p99 is 650ms\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 650ms and 845ms should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-083",
      "type": "numeric-input",
      "question": "Given the context, if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "0.34*130,000=44,200.",
      "detailedExplanation": "Use \"if 34% of 130,000 req/min are high-priority, how many high-priority req/min\" as your starting point, then verify tradeoffs carefully. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 34 and 130,000 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Determine percent reduction.",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1.0-0.22)/1.0=78%.",
      "detailedExplanation": "Read this as a scenario about \"error rate drops from 1\". Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 1.0 and 0.22 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Determine minimum acknowledgements.",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Majority of 9 is 5.",
      "detailedExplanation": "The decision turns on \"9-node quorum cluster requires majority writes\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 9 and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Determine minutes to clear.",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "56,000/350=160.",
      "detailedExplanation": "Start from \"backlog is 56,000 tasks with net drain 350 tasks/min\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Determine percent remaining available.",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "15/18=83.33%.",
      "detailedExplanation": "The key clue in this question is \"fleet has 18 zones and 3 are unavailable\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 18 and 3 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Determine percent reduction.",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(52-34)/52=34.62%.",
      "detailedExplanation": "The core signal here is \"mTTR improved from 52 min to 34 min\". Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 52 min and 34 min appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-089",
      "type": "numeric-input",
      "question": "Given the context, if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "0.11*2,800,000=308,000.",
      "detailedExplanation": "If you keep \"if 11% of 2,800,000 daily ops need manual checks, checks/day\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 11 and 2,800 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-090",
      "type": "ordering",
      "question": "In this notification system core architecture context, order a classic-design decomposition workflow.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start with outcomes/invariants, then decompose, control, and validate.",
      "detailedExplanation": "The key clue in this question is \"order a classic-design decomposition workflow\". Build the rank from biggest differences first, then refine with adjacent checks. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-091",
      "type": "ordering",
      "question": "Considering notification system core architecture, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Risk rises as boundaries and ownership degrade.",
      "detailedExplanation": "Start from \"order by increasing design risk\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-092",
      "type": "ordering",
      "question": "For notification system core architecture, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope, contain, fix, then harden.",
      "detailedExplanation": "The decision turns on \"order safe incident mitigation steps\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-nc-093",
      "type": "ordering",
      "question": "Within notification system core architecture, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Maturity improves with safeguards and observability.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing retry-control maturity\". Order by relative scale and bottleneck effect, then validate neighboring items. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "cd-nc-094",
      "type": "ordering",
      "question": "Order fallback sophistication. Use a notification system core architecture perspective.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sophistication rises with explicit policy and automation.",
      "detailedExplanation": "Use \"order fallback sophistication\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-095",
      "type": "ordering",
      "question": "Order failover validation rigor. Focus on notification system core architecture tradeoffs.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rigor increases with validation and reversible control.",
      "detailedExplanation": "This prompt is really about \"order failover validation rigor\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-096",
      "type": "ordering",
      "question": "Rank by blast radius (smallest first).",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Blast radius expands from local failures to regional control issues.",
      "detailedExplanation": "If you keep \"order by increasing blast radius\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-097",
      "type": "ordering",
      "question": "Order data-path durability confidence. (notification system core architecture lens)",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Durability confidence grows with replication and verification.",
      "detailedExplanation": "The core signal here is \"order data-path durability confidence\". Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-098",
      "type": "ordering",
      "question": "In this notification system core architecture context, order by increasing operational discipline.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Discipline improves with structure and accountability.",
      "detailedExplanation": "The key clue in this question is \"order by increasing operational discipline\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-099",
      "type": "ordering",
      "question": "Considering notification system core architecture, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rollout should progress from limited exposure to institutionalization.",
      "detailedExplanation": "Start from \"order rollout safety for major design changes\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-nc-100",
      "type": "ordering",
      "question": "From a notification system core architecture viewpoint, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Evidence strength rises with sustained production and drill results.",
      "detailedExplanation": "If you keep \"order evidence strength for fix success\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    }
  ]
}
