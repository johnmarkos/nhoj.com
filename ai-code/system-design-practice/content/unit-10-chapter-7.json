{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 7,
  "chapterTitle": "Notification System Core Architecture",
  "chapterDescription": "Decompose notification event intake, preference evaluation, dedupe, scheduling, and channel dispatch.",
  "problems": [
    {
      "id": "cd-nc-001",
      "type": "multiple-choice",
      "question": "Case Alpha: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For notification event ingest API, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Enforce idempotency keys per event-channel and dedupe before dispatch\" outperforms the alternatives because it targets duplicate sends from retry races and preserves safe recovery behavior. It is also the most compatible with The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-002",
      "type": "multiple-choice",
      "question": "Case Beta: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat user preference evaluation service as a reliability-control decision, not an averages-only optimization. \"Evaluate preferences against versioned snapshots with deterministic fallback policy\" is correct since it mitigates preference stale reads causing policy violations while keeping containment local. The decision remains valid given: Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-003",
      "type": "multiple-choice",
      "question": "Case Gamma: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Template rendering service should be solved at the failure boundary named in Notification System Core Architecture. \"Isolate channel adapters with bulkheads and per-channel retry budgets\" is strongest because it directly addresses channel adapter outage cascading to all sends and improves repeatability under stress. This aligns with the extra condition (Leadership asked for a fix that reduces recurrence, not just MTTR).",
      "detailedExplanation": "Generalize from template rendering service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-004",
      "type": "multiple-choice",
      "question": "Case Delta: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Separate transactional and marketing queues with strict priority controls."
      ],
      "correct": 3,
      "explanation": "In Notification System Core Architecture, dedupe/idempotency processor fails mainly through template render failures blocking high-priority alerts. The best choice is \"Separate transactional and marketing queues with strict priority controls\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A previous rollback fixed averages but not tail impact.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For send-time scheduler, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Make template failures non-blocking for critical notifications via fallback templates\" outperforms the alternatives because it targets global campaign spike overwhelming shared queue and preserves safe recovery behavior. It is also the most compatible with User trust risk is highest on this path.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that ignore delivery semantics or backpressure behavior. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-006",
      "type": "multiple-choice",
      "question": "Case Zeta: channel routing engine. Dominant risk is timezone scheduling errors missing user windows. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat channel routing engine as a reliability-control decision, not an averages-only optimization. \"Use per-user timezone scheduling with validated daylight-saving-safe rules\" is correct since it mitigates timezone scheduling errors missing user windows while keeping containment local. The decision remains valid given: A shared dependency has uncertain health right now.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-007",
      "type": "multiple-choice",
      "question": "Case Eta: email provider adapter. Dominant risk is idempotency window too short for delayed retries. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Track provider quotas and apply adaptive throttling before hard rate limits.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Email provider adapter should be solved at the failure boundary named in Notification System Core Architecture. \"Track provider quotas and apply adaptive throttling before hard rate limits\" is strongest because it directly addresses idempotency window too short for delayed retries and improves repeatability under stress. This aligns with the extra condition (The change must preserve cost discipline during peak).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-008",
      "type": "multiple-choice",
      "question": "Case Theta: push provider adapter. Dominant risk is priority messages delayed behind bulk marketing traffic. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Persist audit records independently from channel send acknowledgements."
      ],
      "correct": 3,
      "explanation": "In Notification System Core Architecture, push provider adapter fails mainly through priority messages delayed behind bulk marketing traffic. The best choice is \"Persist audit records independently from channel send acknowledgements\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry shows risk concentrated in one partition class.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-009",
      "type": "multiple-choice",
      "question": "Case Iota: SMS dispatch adapter. Dominant risk is provider rate-limit breaches causing drops. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Design scheduler to survive restarts with durable job state and safe replays.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For SMS dispatch adapter, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Design scheduler to survive restarts with durable job state and safe replays\" outperforms the alternatives because it targets provider rate-limit breaches causing drops and preserves safe recovery behavior. It is also the most compatible with The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-010",
      "type": "multiple-choice",
      "question": "Case Kappa: notification audit log service. Dominant risk is audit trail gaps during partial failures. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Route by policy matrix so fallback channel behavior is explicit and testable.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Notification audit log service should be solved at the failure boundary named in Notification System Core Architecture. \"Route by policy matrix so fallback channel behavior is explicit and testable\" is strongest because it directly addresses audit trail gaps during partial failures and improves repeatability under stress. This aligns with the extra condition (Current runbooks are missing explicit ownership for this boundary).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-011",
      "type": "multiple-choice",
      "question": "Case Lambda: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Notification System Core Architecture, notification event ingest API fails mainly through duplicate sends from retry races. The best choice is \"Enforce idempotency keys per event-channel and dedupe before dispatch\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A cross-region path recently changed behavior after migration.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-012",
      "type": "multiple-choice",
      "question": "Case Mu: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy."
      ],
      "correct": 3,
      "explanation": "For user preference evaluation service, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Evaluate preferences against versioned snapshots with deterministic fallback policy\" outperforms the alternatives because it targets preference stale reads causing policy violations and preserves safe recovery behavior. It is also the most compatible with Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-013",
      "type": "multiple-choice",
      "question": "Case Nu: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat template rendering service as a reliability-control decision, not an averages-only optimization. \"Isolate channel adapters with bulkheads and per-channel retry budgets\" is correct since it mitigates channel adapter outage cascading to all sends while keeping containment local. The decision remains valid given: Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-014",
      "type": "multiple-choice",
      "question": "Case Xi: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Separate transactional and marketing queues with strict priority controls.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Dedupe/idempotency processor should be solved at the failure boundary named in Notification System Core Architecture. \"Separate transactional and marketing queues with strict priority controls\" is strongest because it directly addresses template render failures blocking high-priority alerts and improves repeatability under stress. This aligns with the extra condition (A partial failure is masking itself as success in metrics).",
      "detailedExplanation": "Generalize from dedupe/idempotency processor to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-015",
      "type": "multiple-choice",
      "question": "Case Omicron: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Notification System Core Architecture, send-time scheduler fails mainly through global campaign spike overwhelming shared queue. The best choice is \"Make template failures non-blocking for critical notifications via fallback templates\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This fix must hold under celebrity or campaign spike conditions.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-016",
      "type": "multiple-choice",
      "question": "Case Pi: channel routing engine. Dominant risk is timezone scheduling errors missing user windows. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Use per-user timezone scheduling with validated daylight-saving-safe rules."
      ],
      "correct": 3,
      "explanation": "For channel routing engine, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Use per-user timezone scheduling with validated daylight-saving-safe rules\" outperforms the alternatives because it targets timezone scheduling errors missing user windows and preserves safe recovery behavior. It is also the most compatible with SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-017",
      "type": "multiple-choice",
      "question": "Case Rho: email provider adapter. Dominant risk is idempotency window too short for delayed retries. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Track provider quotas and apply adaptive throttling before hard rate limits.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat email provider adapter as a reliability-control decision, not an averages-only optimization. \"Track provider quotas and apply adaptive throttling before hard rate limits\" is correct since it mitigates idempotency window too short for delayed retries while keeping containment local. The decision remains valid given: On-call requested a reversible operational first step.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-018",
      "type": "multiple-choice",
      "question": "Case Sigma: push provider adapter. Dominant risk is priority messages delayed behind bulk marketing traffic. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Persist audit records independently from channel send acknowledgements.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Push provider adapter should be solved at the failure boundary named in Notification System Core Architecture. \"Persist audit records independently from channel send acknowledgements\" is strongest because it directly addresses priority messages delayed behind bulk marketing traffic and improves repeatability under stress. This aligns with the extra condition (The system mixes strict and eventual paths with unclear contracts).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer the approach that preserves correctness guarantees for the stated consistency boundary. Consistency decisions should be explicit about which conflicts are acceptable and why. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-019",
      "type": "multiple-choice",
      "question": "Case Tau: SMS dispatch adapter. Dominant risk is provider rate-limit breaches causing drops. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Design scheduler to survive restarts with durable job state and safe replays.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Notification System Core Architecture, SMS dispatch adapter fails mainly through provider rate-limit breaches causing drops. The best choice is \"Design scheduler to survive restarts with durable job state and safe replays\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A hot-key pattern is likely from real traffic skew.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: notification audit log service. Dominant risk is audit trail gaps during partial failures. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Route by policy matrix so fallback channel behavior is explicit and testable."
      ],
      "correct": 3,
      "explanation": "Treat notification audit log service as a reliability-control decision, not an averages-only optimization. \"Route by policy matrix so fallback channel behavior is explicit and testable\" is correct since it mitigates audit trail gaps during partial failures while keeping containment local. The decision remains valid given: The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-021",
      "type": "multiple-choice",
      "question": "Case Phi: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Notification event ingest API should be solved at the failure boundary named in Notification System Core Architecture. \"Enforce idempotency keys per event-channel and dedupe before dispatch\" is strongest because it directly addresses duplicate sends from retry races and improves repeatability under stress. This aligns with the extra condition (Compliance requires explicit behavior for edge-case failures).",
      "detailedExplanation": "Generalize from notification event ingest API to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-022",
      "type": "multiple-choice",
      "question": "Case Chi: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Notification System Core Architecture, user preference evaluation service fails mainly through preference stale reads causing policy violations. The best choice is \"Evaluate preferences against versioned snapshots with deterministic fallback policy\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: This boundary has failed during the last two game days.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-023",
      "type": "multiple-choice",
      "question": "Case Psi: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For template rendering service, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Isolate channel adapters with bulkheads and per-channel retry budgets\" outperforms the alternatives because it targets channel adapter outage cascading to all sends and preserves safe recovery behavior. It is also the most compatible with A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-024",
      "type": "multiple-choice",
      "question": "Case Omega: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Separate transactional and marketing queues with strict priority controls."
      ],
      "correct": 3,
      "explanation": "Treat dedupe/idempotency processor as a reliability-control decision, not an averages-only optimization. \"Separate transactional and marketing queues with strict priority controls\" is correct since it mitigates template render failures blocking high-priority alerts while keeping containment local. The decision remains valid given: Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-025",
      "type": "multiple-choice",
      "question": "Case Atlas: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Send-time scheduler should be solved at the failure boundary named in Notification System Core Architecture. \"Make template failures non-blocking for critical notifications via fallback templates\" is strongest because it directly addresses global campaign spike overwhelming shared queue and improves repeatability under stress. This aligns with the extra condition (The fix should avoid broad architectural rewrites this quarter).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-026",
      "type": "multiple-choice",
      "question": "Case Nova: channel routing engine. Dominant risk is timezone scheduling errors missing user windows. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Notification System Core Architecture, channel routing engine fails mainly through timezone scheduling errors missing user windows. The best choice is \"Use per-user timezone scheduling with validated daylight-saving-safe rules\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Current metrics hide per-tenant variance that matters.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-027",
      "type": "multiple-choice",
      "question": "Case Orion: email provider adapter. Dominant risk is idempotency window too short for delayed retries. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Track provider quotas and apply adaptive throttling before hard rate limits.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For email provider adapter, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Track provider quotas and apply adaptive throttling before hard rate limits\" outperforms the alternatives because it targets idempotency window too short for delayed retries and preserves safe recovery behavior. It is also the most compatible with The fallback path is under-tested in production-like load.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-028",
      "type": "multiple-choice",
      "question": "Case Vega: push provider adapter. Dominant risk is priority messages delayed behind bulk marketing traffic. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Persist audit records independently from channel send acknowledgements."
      ],
      "correct": 3,
      "explanation": "Treat push provider adapter as a reliability-control decision, not an averages-only optimization. \"Persist audit records independently from channel send acknowledgements\" is correct since it mitigates priority messages delayed behind bulk marketing traffic while keeping containment local. The decision remains valid given: A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-029",
      "type": "multiple-choice",
      "question": "Case Helios: SMS dispatch adapter. Dominant risk is provider rate-limit breaches causing drops. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Design scheduler to survive restarts with durable job state and safe replays.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "SMS dispatch adapter should be solved at the failure boundary named in Notification System Core Architecture. \"Design scheduler to survive restarts with durable job state and safe replays\" is strongest because it directly addresses provider rate-limit breaches causing drops and improves repeatability under stress. This aligns with the extra condition (The system must preserve critical events over bulk traffic).",
      "detailedExplanation": "Generalize from sMS dispatch adapter to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-030",
      "type": "multiple-choice",
      "question": "Case Aurora: notification audit log service. Dominant risk is audit trail gaps during partial failures. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Route by policy matrix so fallback channel behavior is explicit and testable.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For notification audit log service, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Route by policy matrix so fallback channel behavior is explicit and testable\" outperforms the alternatives because it targets audit trail gaps during partial failures and preserves safe recovery behavior. It is also the most compatible with Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: notification event ingest API. Dominant risk is duplicate sends from retry races. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Enforce idempotency keys per event-channel and dedupe before dispatch.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat notification event ingest API as a reliability-control decision, not an averages-only optimization. \"Enforce idempotency keys per event-channel and dedupe before dispatch\" is correct since it mitigates duplicate sends from retry races while keeping containment local. The decision remains valid given: The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-032",
      "type": "multiple-choice",
      "question": "Case Pulse: user preference evaluation service. Dominant risk is preference stale reads causing policy violations. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Evaluate preferences against versioned snapshots with deterministic fallback policy."
      ],
      "correct": 3,
      "explanation": "User preference evaluation service should be solved at the failure boundary named in Notification System Core Architecture. \"Evaluate preferences against versioned snapshots with deterministic fallback policy\" is strongest because it directly addresses preference stale reads causing policy violations and improves repeatability under stress. This aligns with the extra condition (Operational complexity is rising faster than team onboarding).",
      "detailedExplanation": "Generalize from user preference evaluation service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-033",
      "type": "multiple-choice",
      "question": "Case Forge: template rendering service. Dominant risk is channel adapter outage cascading to all sends. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Isolate channel adapters with bulkheads and per-channel retry budgets.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In Notification System Core Architecture, template rendering service fails mainly through channel adapter outage cascading to all sends. The best choice is \"Isolate channel adapters with bulkheads and per-channel retry budgets\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Stakeholders need clear trade-off rationale in the postmortem.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-034",
      "type": "multiple-choice",
      "question": "Case Harbor: dedupe/idempotency processor. Dominant risk is template render failures blocking high-priority alerts. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Separate transactional and marketing queues with strict priority controls.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For dedupe/idempotency processor, prefer the option that prevents reoccurrence in Notification System Core Architecture. \"Separate transactional and marketing queues with strict priority controls\" outperforms the alternatives because it targets template render failures blocking high-priority alerts and preserves safe recovery behavior. It is also the most compatible with A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-035",
      "type": "multiple-choice",
      "question": "Case Vector: send-time scheduler. Dominant risk is global campaign spike overwhelming shared queue. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Make template failures non-blocking for critical notifications via fallback templates.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat send-time scheduler as a reliability-control decision, not an averages-only optimization. \"Make template failures non-blocking for critical notifications via fallback templates\" is correct since it mitigates global campaign spike overwhelming shared queue while keeping containment local. The decision remains valid given: Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures\" best matches Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts by targeting template render failures blocking high-priority alerts and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for notification event ingest API:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Notification System Core Architecture: for Given the diagnosis in \"incident review for notification event ingest API:\", which next step is strongest under current constraints, \"Use per-user timezone scheduling with validated daylight-saving-safe rules\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue is a two-step reliability decision. At stage 1, \"The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures\" wins because it balances immediate containment with long-term prevention around global campaign spike overwhelming shared queue.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for user preference evaluation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Track provider quotas and apply adaptive throttling before hard rate limits.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Track provider quotas and apply adaptive throttling before hard rate limits\" best matches With diagnosis confirmed in \"incident review for user preference evaluation service:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for template rendering service: signal points to timezone scheduling errors missing user windows. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Notification System Core Architecture, the best answer is \"The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures\". It is the option most directly aligned to timezone scheduling errors missing user windows while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for template rendering service: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Persist audit records independently from channel send acknowledgements."
          ],
          "correct": 3,
          "explanation": "Using the diagnosis from \"incident review for template rendering service: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Persist audit records independently from channel send acknowledgements\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Notification System Core Architecture: for Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries, \"The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures\" is correct because it addresses idempotency window too short for delayed retries and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for dedupe/idempotency processor:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Design scheduler to survive restarts with durable job state and safe replays.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Notification System Core Architecture, the best answer is \"Design scheduler to survive restarts with durable job state and safe replays\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic is a two-step reliability decision. At stage 1, \"The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures\" wins because it balances immediate containment with long-term prevention around priority messages delayed behind bulk marketing traffic.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for send-time scheduler: signal points\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Route by policy matrix so fallback channel behavior is explicit and testable.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Route by policy matrix so fallback channel behavior is explicit and testable\" best matches For \"incident review for send-time scheduler: signal points\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for channel routing engine: signal points to provider rate-limit breaches causing drops. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around channel routing engine mismatches provider rate-limit breaches causing drops, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Notification System Core Architecture, the best answer is \"The current decomposition around channel routing engine mismatches provider rate-limit breaches causing drops, creating repeated failures\". It is the option most directly aligned to provider rate-limit breaches causing drops while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident review for channel routing engine: signal points to provider rate-limit to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for channel routing engine: signal\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Enforce idempotency keys per event-channel and dedupe before dispatch.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "With root cause identified for \"incident review for channel routing engine: signal\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Enforce idempotency keys per event-channel and dedupe before dispatch\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for email provider adapter: signal points to audit trail gaps during partial failures. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around email provider adapter mismatches audit trail gaps during partial failures, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Notification System Core Architecture: for Incident review for email provider adapter: signal points to audit trail gaps during partial failures, \"The current decomposition around email provider adapter mismatches audit trail gaps during partial failures, creating repeated failures\" is correct because it addresses audit trail gaps during partial failures and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for email provider adapter: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Evaluate preferences against versioned snapshots with deterministic fallback policy."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Notification System Core Architecture, the best answer is \"Evaluate preferences against versioned snapshots with deterministic fallback policy\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push provider adapter: signal points to duplicate sends from retry races. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around push provider adapter mismatches duplicate sends from retry races, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around push provider adapter mismatches duplicate sends from retry races, creating repeated failures\" best matches Incident review for push provider adapter: signal points to duplicate sends from retry races by targeting duplicate sends from retry races and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for push provider adapter: signal\", which next step is strongest under current constraints?",
          "options": [
            "Isolate channel adapters with bulkheads and per-channel retry budgets.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Notification System Core Architecture: for Given the diagnosis in \"incident review for push provider adapter: signal\", which next step is strongest under current constraints, \"Isolate channel adapters with bulkheads and per-channel retry budgets\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for SMS dispatch adapter: signal points to preference stale reads causing policy violations. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around SMS dispatch adapter mismatches preference stale reads causing policy violations, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for SMS dispatch adapter: signal points to preference stale reads causing policy violations is a two-step reliability decision. At stage 1, \"The current decomposition around SMS dispatch adapter mismatches preference stale reads causing policy violations, creating repeated failures\" wins because it balances immediate containment with long-term prevention around preference stale reads causing policy violations.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for SMS dispatch adapter: signal points\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Separate transactional and marketing queues with strict priority controls.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Separate transactional and marketing queues with strict priority controls\" best matches Now that \"incident review for SMS dispatch adapter: signal points\" is diagnosed, which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification audit log service: signal points to channel adapter outage cascading to all sends. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around notification audit log service mismatches channel adapter outage cascading to all sends, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Notification System Core Architecture, the best answer is \"The current decomposition around notification audit log service mismatches channel adapter outage cascading to all sends, creating repeated failures\". It is the option most directly aligned to channel adapter outage cascading to all sends while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for notification audit log service:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Make template failures non-blocking for critical notifications via fallback templates.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident review for notification audit log service:\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Make template failures non-blocking for critical notifications via fallback templates\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Notification System Core Architecture: for Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts, \"The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures\" is correct because it addresses template render failures blocking high-priority alerts and improves controllability.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for notification event ingest API:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Use per-user timezone scheduling with validated daylight-saving-safe rules."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Notification System Core Architecture, the best answer is \"Use per-user timezone scheduling with validated daylight-saving-safe rules\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures\" best matches Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue by targeting global campaign spike overwhelming shared queue and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for user preference evaluation service:\" scenario, what should change first before wider rollout?",
          "options": [
            "Track provider quotas and apply adaptive throttling before hard rate limits.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Notification System Core Architecture: for In the \"incident review for user preference evaluation service:\" scenario, what should change first before wider rollout, \"Track provider quotas and apply adaptive throttling before hard rate limits\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from notification System Core Architecture to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for template rendering service: signal points to timezone scheduling errors missing user windows. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for template rendering service: signal points to timezone scheduling errors missing user windows is a two-step reliability decision. At stage 1, \"The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures\" wins because it balances immediate containment with long-term prevention around timezone scheduling errors missing user windows.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for template rendering service: signal\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Persist audit records independently from channel send acknowledgements.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Persist audit records independently from channel send acknowledgements\" best matches For \"incident review for template rendering service: signal\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Notification System Core Architecture, the best answer is \"The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures\". It is the option most directly aligned to idempotency window too short for delayed retries while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident review for dedupe/idempotency processor: signal points to idempotency window to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident review for dedupe/idempotency processor:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Design scheduler to survive restarts with durable job state and safe replays.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "With root cause identified for \"incident review for dedupe/idempotency processor:\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Design scheduler to survive restarts with durable job state and safe replays\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures\" best matches Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic by targeting priority messages delayed behind bulk marketing traffic and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for send-time scheduler: signal points\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Route by policy matrix so fallback channel behavior is explicit and testable."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Notification System Core Architecture: for In the \"incident review for send-time scheduler: signal points\" scenario, what is the highest-leverage change to make now, \"Route by policy matrix so fallback channel behavior is explicit and testable\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from notification System Core Architecture to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for channel routing engine: signal points to provider rate-limit breaches causing drops. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around channel routing engine mismatches provider rate-limit breaches causing drops, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for channel routing engine: signal points to provider rate-limit breaches causing drops is a two-step reliability decision. At stage 1, \"The current decomposition around channel routing engine mismatches provider rate-limit breaches causing drops, creating repeated failures\" wins because it balances immediate containment with long-term prevention around provider rate-limit breaches causing drops.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for channel routing engine: signal\", what should change first before wider rollout?",
          "options": [
            "Enforce idempotency keys per event-channel and dedupe before dispatch.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Enforce idempotency keys per event-channel and dedupe before dispatch\" best matches After diagnosing \"incident review for channel routing engine: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for email provider adapter: signal points to audit trail gaps during partial failures. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around email provider adapter mismatches audit trail gaps during partial failures, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Notification System Core Architecture, the best answer is \"The current decomposition around email provider adapter mismatches audit trail gaps during partial failures, creating repeated failures\". It is the option most directly aligned to audit trail gaps during partial failures while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for email provider adapter: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Evaluate preferences against versioned snapshots with deterministic fallback policy.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Using the diagnosis from \"incident review for email provider adapter: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Evaluate preferences against versioned snapshots with deterministic fallback policy\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for push provider adapter: signal points to duplicate sends from retry races. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around push provider adapter mismatches duplicate sends from retry races, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Notification System Core Architecture: for Incident review for push provider adapter: signal points to duplicate sends from retry races, \"The current decomposition around push provider adapter mismatches duplicate sends from retry races, creating repeated failures\" is correct because it addresses duplicate sends from retry races and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for push provider adapter: signal\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Isolate channel adapters with bulkheads and per-channel retry budgets.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Notification System Core Architecture, the best answer is \"Isolate channel adapters with bulkheads and per-channel retry budgets\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for SMS dispatch adapter: signal points to preference stale reads causing policy violations. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around SMS dispatch adapter mismatches preference stale reads causing policy violations, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around SMS dispatch adapter mismatches preference stale reads causing policy violations, creating repeated failures\" best matches Incident review for SMS dispatch adapter: signal points to preference stale reads causing policy violations by targeting preference stale reads causing policy violations and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for SMS dispatch adapter: signal points\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Separate transactional and marketing queues with strict priority controls."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Notification System Core Architecture: for Given the diagnosis in \"incident review for SMS dispatch adapter: signal points\", which immediate adjustment best addresses the risk, \"Separate transactional and marketing queues with strict priority controls\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification audit log service: signal points to channel adapter outage cascading to all sends. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around notification audit log service mismatches channel adapter outage cascading to all sends, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for notification audit log service: signal points to channel adapter outage cascading to all sends is a two-step reliability decision. At stage 1, \"The current decomposition around notification audit log service mismatches channel adapter outage cascading to all sends, creating repeated failures\" wins because it balances immediate containment with long-term prevention around channel adapter outage cascading to all sends.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for notification audit log service:\", what first move gives the best reliability impact?",
          "options": [
            "Make template failures non-blocking for critical notifications via fallback templates.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Make template failures non-blocking for critical notifications via fallback templates\" best matches With diagnosis confirmed in \"incident review for notification audit log service:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for notification event ingest API: signal points to template render failures blocking high-priority alerts. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Notification System Core Architecture, the best answer is \"The current decomposition around notification event ingest API mismatches template render failures blocking high-priority alerts, creating repeated failures\". It is the option most directly aligned to template render failures blocking high-priority alerts while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident review for notification event ingest API: signal points to template render to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for notification event ingest API:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Use per-user timezone scheduling with validated daylight-saving-safe rules.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "With root cause identified for \"incident review for notification event ingest API:\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Use per-user timezone scheduling with validated daylight-saving-safe rules\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Notification System Core Architecture: for Incident review for user preference evaluation service: signal points to global campaign spike overwhelming shared queue, \"The current decomposition around user preference evaluation service mismatches global campaign spike overwhelming shared queue, creating repeated failures\" is correct because it addresses global campaign spike overwhelming shared queue and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for user preference evaluation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Track provider quotas and apply adaptive throttling before hard rate limits.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Notification System Core Architecture, the best answer is \"Track provider quotas and apply adaptive throttling before hard rate limits\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for template rendering service: signal points to timezone scheduling errors missing user windows. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around template rendering service mismatches timezone scheduling errors missing user windows, creating repeated failures\" best matches Incident review for template rendering service: signal points to timezone scheduling errors missing user windows by targeting timezone scheduling errors missing user windows and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for template rendering service: signal\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Persist audit records independently from channel send acknowledgements."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Notification System Core Architecture: for In the \"incident review for template rendering service: signal\" scenario, which next step is strongest under current constraints, \"Persist audit records independently from channel send acknowledgements\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from notification System Core Architecture to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for dedupe/idempotency processor: signal points to idempotency window too short for delayed retries is a two-step reliability decision. At stage 1, \"The current decomposition around dedupe/idempotency processor mismatches idempotency window too short for delayed retries, creating repeated failures\" wins because it balances immediate containment with long-term prevention around idempotency window too short for delayed retries.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for dedupe/idempotency processor:\", which next step is strongest under current constraints?",
          "options": [
            "Design scheduler to survive restarts with durable job state and safe replays.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Design scheduler to survive restarts with durable job state and safe replays\" best matches After diagnosing \"incident review for dedupe/idempotency processor:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Notification System Core Architecture: for Incident review for send-time scheduler: signal points to priority messages delayed behind bulk marketing traffic, \"The current decomposition around send-time scheduler mismatches priority messages delayed behind bulk marketing traffic, creating repeated failures\" is correct because it addresses priority messages delayed behind bulk marketing traffic and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for send-time scheduler: signal points\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Route by policy matrix so fallback channel behavior is explicit and testable.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Notification System Core Architecture, the best answer is \"Route by policy matrix so fallback channel behavior is explicit and testable\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-061",
      "type": "multi-select",
      "question": "Pick all statements that fit: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Pick all statements that fit: which signals best identify decomposition boundary mistakes is intentionally multi-dimensional in Notification System Core Architecture. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-062",
      "type": "multi-select",
      "question": "Pick all statements that fit: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Notification System Core Architecture, Pick all statements that fit: which controls improve safety on critical write paths needs layered controls, not one silver bullet. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-063",
      "type": "multi-select",
      "question": "Pick all statements that fit: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Pick all statements that fit: which practices reduce hot-key or hot-partition impact, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-064",
      "type": "multi-select",
      "question": "Pick all statements that fit: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Notification System Core Architecture: The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-065",
      "type": "multi-select",
      "question": "Pick all statements that fit: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Pick all statements that fit: which choices usually lower operational risk at scale is intentionally multi-dimensional in Notification System Core Architecture. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from choices usually lower operational risk at scale? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-066",
      "type": "multi-select",
      "question": "Pick all statements that fit: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Notification System Core Architecture, Pick all statements that fit: what should be explicit in API/service contracts for this design needs layered controls, not one silver bullet. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-067",
      "type": "multi-select",
      "question": "Pick all statements that fit: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Pick all statements that fit: which anti-patterns often cause incident recurrence, the highest-signal answer is a bundle of controls. The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-068",
      "type": "multi-select",
      "question": "Pick all statements that fit: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Notification System Core Architecture: The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-069",
      "type": "multi-select",
      "question": "Pick all statements that fit: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Pick all statements that fit: which controls protect high-priority traffic during spikes is intentionally multi-dimensional in Notification System Core Architecture. The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-070",
      "type": "multi-select",
      "question": "Pick all statements that fit: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Pick all statements that fit: which telemetry dimensions are most actionable for design triage, the highest-signal answer is a bundle of controls. The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-071",
      "type": "multi-select",
      "question": "Pick all statements that fit: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Notification System Core Architecture: The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-072",
      "type": "multi-select",
      "question": "Pick all statements that fit: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Pick all statements that fit: what helps prevent retry amplification cascades is intentionally multi-dimensional in Notification System Core Architecture. The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-073",
      "type": "multi-select",
      "question": "Pick all statements that fit: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Notification System Core Architecture, Pick all statements that fit: which fallback strategies are strong when dependencies degrade needs layered controls, not one silver bullet. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-074",
      "type": "multi-select",
      "question": "Pick all statements that fit: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Pick all statements that fit: what reduces data-quality regressions in eventual pipelines, the highest-signal answer is a bundle of controls. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-075",
      "type": "multi-select",
      "question": "Pick all statements that fit: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Notification System Core Architecture: The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-076",
      "type": "multi-select",
      "question": "Pick all statements that fit: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Pick all statements that fit: which architecture choices improve blast-radius containment is intentionally multi-dimensional in Notification System Core Architecture. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from architecture choices improve blast-radius containment? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-077",
      "type": "multi-select",
      "question": "Pick all statements that fit: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Notification System Core Architecture, Pick all statements that fit: what evidence demonstrates a fix worked beyond short-term recovery needs layered controls, not one silver bullet. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Determine failures/day.",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "Use first-pass reliability arithmetic for A critical path handles 5,400,000 requests/day and 0: 9720 requests. Answers within +/-3% show correct directional reasoning for Notification System Core Architecture.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Normalize units before computing so conversion mistakes do not propagate. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 5,400 and 000 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Determine net drain rate.",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "The operational math for Queue ingest is 2,200 events/min and drain is 2,530 events/min gives 330 events/min. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Determine effective attempts/sec.",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "For Retries add 0, the computed target in Notification System Core Architecture is 96000 attempts/sec. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Determine total failover seconds/day.",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Failover takes 16s and occurs 24 times/day: 384 seconds. Answers within +/-0% show correct directional reasoning for Notification System Core Architecture.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 16s and 24 in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Determine percent over target.",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for Target p99 is 650ms; observed p99 is 845ms gives 30 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 650ms and 845ms should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-083",
      "type": "numeric-input",
      "question": "Given the context, if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "Notification System Core Architecture expects quick quantitative triage: Given the context, if 34% of 130,000 req/min are high-priority, how many high-priority req/min evaluates to 44200 requests/min. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "Generalize from if 34% of 130,000 req/min are high-priority, how many high-priority req/min to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 34 and 130,000 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Determine percent reduction.",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Error rate drops from 1, the computed target in Notification System Core Architecture is 78 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 1.0 and 0.22 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Determine minimum acknowledgements.",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for A 9-node quorum cluster requires majority writes: 5 acks. Answers within +/-0% show correct directional reasoning for Notification System Core Architecture.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Consistency decisions should be explicit about which conflicts are acceptable and why. Numbers such as 9 and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Determine minutes to clear.",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "The operational math for Backlog is 56,000 tasks with net drain 350 tasks/min gives 160 minutes. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Determine percent remaining available.",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Notification System Core Architecture expects quick quantitative triage: A fleet has 18 zones and 3 are unavailable evaluates to 83.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 18 and 3 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Determine percent reduction.",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For MTTR improved from 52 min to 34 min, the computed target in Notification System Core Architecture is 34.62 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Normalize units before computing so conversion mistakes do not propagate. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 52 min and 34 min appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-089",
      "type": "numeric-input",
      "question": "Given the context, if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for Given the context, if 11% of 2,800,000 daily ops need manual checks, checks/day: 308000 operations. Answers within +/-2% show correct directional reasoning for Notification System Core Architecture.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 11 and 2,800 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-090",
      "type": "ordering",
      "question": "In this notification system core architecture context, order a classic-design decomposition workflow.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Identify critical user journey and invariants must happen before Validate with load/failure drills and refine. That ordering matches incident-safe flow in Notification System Core Architecture.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Build the rank from biggest differences first, then refine with adjacent checks. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-091",
      "type": "ordering",
      "question": "Considering notification system core architecture, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Notification System Core Architecture should start with Explicit boundaries with contracts and end with Implicit coupling with no ownership. Considering notification system core architecture, order by increasing design risk rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-092",
      "type": "ordering",
      "question": "For notification system core architecture, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For notification system core architecture, order safe incident mitigation steps, the correct ordering runs from Scope blast radius and affected flows to Run recurrence checks and hardening actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-093",
      "type": "ordering",
      "question": "Within notification system core architecture, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Notification System Core Architecture emphasizes safe recovery order. Beginning at Fixed immediate retries and finishing at Jittered retries with retry budgets and telemetry keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-094",
      "type": "ordering",
      "question": "Order fallback sophistication. Use a notification system core architecture perspective.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Implicit fallback behavior must happen before Policy-driven automated fallback with tests. That ordering matches incident-safe flow in Notification System Core Architecture.",
      "detailedExplanation": "Generalize from order fallback sophistication to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-095",
      "type": "ordering",
      "question": "Order failover validation rigor. Focus on notification system core architecture tradeoffs.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Notification System Core Architecture should start with Host health check only and end with Staged shift plus failback rehearsal and rollback gates. Order failover validation rigor rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-096",
      "type": "ordering",
      "question": "Rank by blast radius (smallest first).",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Rank by blast radius (smallest first), the correct ordering runs from Single process failure to Cross-region control-plane failure. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-097",
      "type": "ordering",
      "question": "Order data-path durability confidence. (notification system core architecture lens)",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Notification System Core Architecture emphasizes safe recovery order. Beginning at In-memory only acknowledgment and finishing at Replicated durable write plus replay/integrity verification keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-098",
      "type": "ordering",
      "question": "In this notification system core architecture context, order by increasing operational discipline.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc incident response must happen before Role-based response plus action closure tracking. That ordering matches incident-safe flow in Notification System Core Architecture.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-099",
      "type": "ordering",
      "question": "Considering notification system core architecture, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Notification System Core Architecture should start with Canary small cohort and end with Finalize runbook and ownership updates. Considering notification system core architecture, order rollout safety for major design changes rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-nc-100",
      "type": "ordering",
      "question": "From a notification system core architecture viewpoint, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a notification system core architecture viewpoint, order evidence strength for fix success, the correct ordering runs from Single successful test run to Sustained recovery plus failure-drill pass. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["classic-designs", "notification-system-core-architecture"],
      "difficulty": "staff-level"
    }
  ]
}
