{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 4,
  "chapterTitle": "URL Shortener Scale, Analytics & Operations",
  "chapterDescription": "Decompose high-scale redirect operations, analytics pipelines, retention design, and reliability controls.",
  "problems": [
    {
      "id": "cd-ua-001",
      "type": "multiple-choice",
      "question": "Case Alpha: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"click analytics ingest stream\". Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-002",
      "type": "multiple-choice",
      "question": "Case Beta: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"redirect edge fleet\". Prefer the choice that remains viable across the planning horizon, not just today. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: forecasting volume without resource thresholds.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-003",
      "type": "multiple-choice",
      "question": "Case Gamma: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"top-link hot shard mitigation path\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-ua-004",
      "type": "multiple-choice",
      "question": "Case Delta: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"analytics ETL job\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "cd-ua-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"fraud/bot filtering stage\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-006",
      "type": "multiple-choice",
      "question": "Case Zeta: campaign tracking API. Dominant risk is clock skew breaking hourly aggregation windows. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Align aggregation windows with watermark logic to handle skewed late events safely.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"campaign tracking API\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-007",
      "type": "multiple-choice",
      "question": "Case Eta: historical reporting warehouse. Dominant risk is reporting queries contending with ingestion writes. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Isolate reporting compute from ingest path to avoid mutual saturation.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"historical reporting warehouse\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-008",
      "type": "multiple-choice",
      "question": "Case Theta: regional edge failover routing. Dominant risk is edge failover causing duplicated click events. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Model failover replay explicitly so duplicate clicks are measurable and bounded."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"regional edge failover routing\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-009",
      "type": "multiple-choice",
      "question": "Case Iota: retention/archival worker. Dominant risk is missing idempotency in analytics consumer path. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Build SLO dashboards that expose redirect health and analytics freshness separately.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"retention/archival worker\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "cd-ua-010",
      "type": "multiple-choice",
      "question": "Case Kappa: ops dashboard aggregation service. Dominant risk is SLO blind spots between redirect and analytics layers. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Test campaign surge scenarios with load + replay drills before peak events.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"ops dashboard aggregation service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-011",
      "type": "multiple-choice",
      "question": "Case Lambda: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"click analytics ingest stream\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-012",
      "type": "multiple-choice",
      "question": "Case Mu: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"redirect edge fleet\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-013",
      "type": "multiple-choice",
      "question": "Case Nu: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"top-link hot shard mitigation path\", then pressure-test the result against the options. Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-ua-014",
      "type": "multiple-choice",
      "question": "Case Xi: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"analytics ETL job\" in view, the correct answer separates faster. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "cd-ua-015",
      "type": "multiple-choice",
      "question": "Case Omicron: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"fraud/bot filtering stage\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-016",
      "type": "multiple-choice",
      "question": "Case Pi: campaign tracking API. Dominant risk is clock skew breaking hourly aggregation windows. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Align aggregation windows with watermark logic to handle skewed late events safely."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"campaign tracking API\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-017",
      "type": "multiple-choice",
      "question": "Case Rho: historical reporting warehouse. Dominant risk is reporting queries contending with ingestion writes. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Isolate reporting compute from ingest path to avoid mutual saturation.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"historical reporting warehouse\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-018",
      "type": "multiple-choice",
      "question": "Case Sigma: regional edge failover routing. Dominant risk is edge failover causing duplicated click events. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Model failover replay explicitly so duplicate clicks are measurable and bounded.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"regional edge failover routing\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-019",
      "type": "multiple-choice",
      "question": "Case Tau: retention/archival worker. Dominant risk is missing idempotency in analytics consumer path. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Build SLO dashboards that expose redirect health and analytics freshness separately.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"retention/archival worker\". Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "cd-ua-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: ops dashboard aggregation service. Dominant risk is SLO blind spots between redirect and analytics layers. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Test campaign surge scenarios with load + replay drills before peak events."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"ops dashboard aggregation service\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-021",
      "type": "multiple-choice",
      "question": "Case Phi: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"click analytics ingest stream\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-022",
      "type": "multiple-choice",
      "question": "Case Chi: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"redirect edge fleet\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-023",
      "type": "multiple-choice",
      "question": "Case Psi: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"top-link hot shard mitigation path\". Reject options that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-ua-024",
      "type": "multiple-choice",
      "question": "Case Omega: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"analytics ETL job\". Eliminate options that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "cd-ua-025",
      "type": "multiple-choice",
      "question": "Case Atlas: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"fraud/bot filtering stage\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-026",
      "type": "multiple-choice",
      "question": "Case Nova: campaign tracking API. Dominant risk is clock skew breaking hourly aggregation windows. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Align aggregation windows with watermark logic to handle skewed late events safely.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"campaign tracking API\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-027",
      "type": "multiple-choice",
      "question": "Case Orion: historical reporting warehouse. Dominant risk is reporting queries contending with ingestion writes. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Isolate reporting compute from ingest path to avoid mutual saturation.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"historical reporting warehouse\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-028",
      "type": "multiple-choice",
      "question": "Case Vega: regional edge failover routing. Dominant risk is edge failover causing duplicated click events. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Model failover replay explicitly so duplicate clicks are measurable and bounded."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Read this as a scenario about \"regional edge failover routing\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-029",
      "type": "multiple-choice",
      "question": "Case Helios: retention/archival worker. Dominant risk is missing idempotency in analytics consumer path. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Build SLO dashboards that expose redirect health and analytics freshness separately.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The decision turns on \"retention/archival worker\". Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "cd-ua-030",
      "type": "multiple-choice",
      "question": "Case Aurora: ops dashboard aggregation service. Dominant risk is SLO blind spots between redirect and analytics layers. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Test campaign surge scenarios with load + replay drills before peak events.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Use \"ops dashboard aggregation service\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-ua-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "This prompt is really about \"click analytics ingest stream\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-032",
      "type": "multiple-choice",
      "question": "Case Pulse: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics."
      ],
      "correct": 3,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "If you keep \"redirect edge fleet\" in view, the correct answer separates faster. Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-033",
      "type": "multiple-choice",
      "question": "Case Forge: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The core signal here is \"top-link hot shard mitigation path\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-034",
      "type": "multiple-choice",
      "question": "Case Harbor: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "The key clue in this question is \"analytics ETL job\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "cd-ua-035",
      "type": "multiple-choice",
      "question": "Case Vector: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "The strongest choice applies the right control at the right boundary to reduce recurrence and preserve system goals.",
      "detailedExplanation": "Start from \"fraud/bot filtering stage\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for click analytics ingest stream: signal points to storage costs\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for click analytics ingest stream:\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Align aggregation windows with watermark logic to handle skewed late events safely.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"uRL Shortener Scale, Analytics & Operations\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect edge fleet: signal points to event loss during regional stream failover. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for redirect edge fleet: signal points to event loss during regional\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for redirect edge fleet: signal points\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Isolate reporting compute from ingest path to avoid mutual saturation.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"uRL Shortener Scale, Analytics & Operations\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for top-link hot shard mitigation path: signal points to clock skew\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for top-link hot shard mitigation path:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Model failover replay explicitly so duplicate clicks are measurable and bounded."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Use \"uRL Shortener Scale, Analytics & Operations\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for analytics ETL job: signal points to reporting queries contending\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for analytics ETL job: signal points to\", which immediate adjustment best addresses the risk?",
          "options": [
            "Build SLO dashboards that expose redirect health and analytics freshness separately.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"uRL Shortener Scale, Analytics & Operations\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for fraud/bot filtering stage: signal points to edge failover causing\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for fraud/bot filtering stage: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Test campaign surge scenarios with load + replay drills before peak events.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"uRL Shortener Scale, Analytics & Operations\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for campaign tracking API: signal points to missing idempotency in analytics consumer path. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around campaign tracking API mismatches missing idempotency in analytics consumer path, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for campaign tracking API: signal points to missing idempotency in\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for campaign tracking API: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"uRL Shortener Scale, Analytics & Operations\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for historical reporting warehouse: signal points to SLO blind spots between redirect and analytics layers. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around historical reporting warehouse mismatches SLO blind spots between redirect and analytics layers, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for historical reporting warehouse: signal points to SLO blind spots\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for historical reporting warehouse:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Apply bot filtering and confidence scoring before committing events to authoritative metrics."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"uRL Shortener Scale, Analytics & Operations\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional edge failover routing: signal points to analytics lag hiding real-time campaign outcomes. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around regional edge failover routing mismatches analytics lag hiding real-time campaign outcomes, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for regional edge failover routing: signal points to analytics lag\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for regional edge failover routing:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"uRL Shortener Scale, Analytics & Operations\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for retention/archival worker: signal points to bot traffic inflating click metrics. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around retention/archival worker mismatches bot traffic inflating click metrics, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for retention/archival worker: signal points to bot traffic inflating\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for retention/archival worker: signal\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Define retention tiers with compaction/downsampling for long-term analytics economics.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"uRL Shortener Scale, Analytics & Operations\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ops dashboard aggregation service: signal points to hot partition overload from viral redirect. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around ops dashboard aggregation service mismatches hot partition overload from viral redirect, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for ops dashboard aggregation service: signal points to hot partition\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for ops dashboard aggregation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"uRL Shortener Scale, Analytics & Operations\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for click analytics ingest stream: signal points to storage costs\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for click analytics ingest stream:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Align aggregation windows with watermark logic to handle skewed late events safely."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"uRL Shortener Scale, Analytics & Operations\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect edge fleet: signal points to event loss during regional stream failover. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for redirect edge fleet: signal points to event loss during regional\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for redirect edge fleet: signal points\", what should change first before wider rollout?",
          "options": [
            "Isolate reporting compute from ingest path to avoid mutual saturation.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The decision turns on \"uRL Shortener Scale, Analytics & Operations\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for top-link hot shard mitigation path: signal points to clock skew\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for top-link hot shard mitigation path:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Model failover replay explicitly so duplicate clicks are measurable and bounded.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"uRL Shortener Scale, Analytics & Operations\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for analytics ETL job: signal points to reporting queries contending\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for analytics ETL job: signal points to\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Build SLO dashboards that expose redirect health and analytics freshness separately.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Use \"uRL Shortener Scale, Analytics & Operations\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for fraud/bot filtering stage: signal points to edge failover causing\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for fraud/bot filtering stage: signal\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Test campaign surge scenarios with load + replay drills before peak events."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"uRL Shortener Scale, Analytics & Operations\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for campaign tracking API: signal points to missing idempotency in analytics consumer path. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around campaign tracking API mismatches missing idempotency in analytics consumer path, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for campaign tracking API: signal points to missing idempotency in\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for campaign tracking API: signal\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"uRL Shortener Scale, Analytics & Operations\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for historical reporting warehouse: signal points to SLO blind spots between redirect and analytics layers. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around historical reporting warehouse mismatches SLO blind spots between redirect and analytics layers, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The decision turns on \"incident review for historical reporting warehouse: signal points to SLO blind spots\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for historical reporting warehouse:\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Apply bot filtering and confidence scoring before committing events to authoritative metrics.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"uRL Shortener Scale, Analytics & Operations\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional edge failover routing: signal points to analytics lag hiding real-time campaign outcomes. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around regional edge failover routing mismatches analytics lag hiding real-time campaign outcomes, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Start from \"incident review for regional edge failover routing: signal points to analytics lag\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for regional edge failover routing:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"uRL Shortener Scale, Analytics & Operations\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for retention/archival worker: signal points to bot traffic inflating click metrics. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around retention/archival worker mismatches bot traffic inflating click metrics, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Use \"incident review for retention/archival worker: signal points to bot traffic inflating\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for retention/archival worker: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Define retention tiers with compaction/downsampling for long-term analytics economics."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"uRL Shortener Scale, Analytics & Operations\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ops dashboard aggregation service: signal points to hot partition overload from viral redirect. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around ops dashboard aggregation service mismatches hot partition overload from viral redirect, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for ops dashboard aggregation service: signal points to hot partition\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for ops dashboard aggregation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"uRL Shortener Scale, Analytics & Operations\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "If you keep \"incident review for click analytics ingest stream: signal points to storage costs\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for click analytics ingest stream:\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Align aggregation windows with watermark logic to handle skewed late events safely.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"uRL Shortener Scale, Analytics & Operations\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect edge fleet: signal points to event loss during regional stream failover. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "This prompt is really about \"incident review for redirect edge fleet: signal points to event loss during regional\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for redirect edge fleet: signal points\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Isolate reporting compute from ingest path to avoid mutual saturation.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"uRL Shortener Scale, Analytics & Operations\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The key clue in this question is \"incident review for top-link hot shard mitigation path: signal points to clock skew\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for top-link hot shard mitigation path:\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Model failover replay explicitly so duplicate clicks are measurable and bounded."
          ],
          "correct": 3,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"uRL Shortener Scale, Analytics & Operations\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for analytics ETL job: signal points to reporting queries contending\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for analytics ETL job: signal points to\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Build SLO dashboards that expose redirect health and analytics freshness separately.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"uRL Shortener Scale, Analytics & Operations\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Diagnosis should identify the control/boundary mismatch driving repeated failure under realistic load.",
          "detailedExplanation": "The core signal here is \"incident review for fraud/bot filtering stage: signal points to edge failover causing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for fraud/bot filtering stage: signal\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Test campaign surge scenarios with load + replay drills before peak events.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Choose the smallest high-leverage change that directly closes the observed reliability and design gap.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"uRL Shortener Scale, Analytics & Operations\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-061",
      "type": "multi-select",
      "question": "Mark all correct choices here: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "If you keep \"signals best identify decomposition boundary mistakes? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-062",
      "type": "multi-select",
      "question": "Mark all correct choices here: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "This prompt is really about \"controls improve safety on critical write paths? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-063",
      "type": "multi-select",
      "question": "Mark all correct choices here: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Use \"practices reduce hot-key or hot-partition impact? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-ua-064",
      "type": "multi-select",
      "question": "Mark all correct choices here: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Read this as a scenario about \"improves reliability when mixing sync and async paths? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-ua-065",
      "type": "multi-select",
      "question": "Mark all correct choices here: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The decision turns on \"choices usually lower operational risk at scale? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-066",
      "type": "multi-select",
      "question": "Mark all correct choices here: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Start from \"be explicit in API/service contracts for this design? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-067",
      "type": "multi-select",
      "question": "Mark all correct choices here: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The key clue in this question is \"anti-patterns often cause incident recurrence? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-068",
      "type": "multi-select",
      "question": "Mark all correct choices here: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The core signal here is \"increases confidence before broad traffic rollout? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-069",
      "type": "multi-select",
      "question": "Mark all correct choices here: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "If you keep \"controls protect high-priority traffic during spikes? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-070",
      "type": "multi-select",
      "question": "Mark all correct choices here: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The key clue in this question is \"telemetry dimensions are most actionable for design triage? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-071",
      "type": "multi-select",
      "question": "Mark all correct choices here: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Start from \"governance actions improve cross-team reliability ownership? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-072",
      "type": "multi-select",
      "question": "Mark all correct choices here: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The decision turns on \"helps prevent retry amplification cascades? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-ua-073",
      "type": "multi-select",
      "question": "Mark all correct choices here: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Read this as a scenario about \"fallback strategies are strong when dependencies degrade? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-074",
      "type": "multi-select",
      "question": "Mark all correct choices here: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "Use \"reduces data-quality regressions in eventual pipelines? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-075",
      "type": "multi-select",
      "question": "Mark all correct choices here: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "This prompt is really about \"runbook components improve incident execution quality? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-076",
      "type": "multi-select",
      "question": "Mark all correct choices here: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "If you keep \"architecture choices improve blast-radius containment? (Select all that apply)\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-077",
      "type": "multi-select",
      "question": "Mark all correct choices here: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Correct choices are concrete controls that improve design clarity, reliability outcomes, and operational safety.",
      "detailedExplanation": "The core signal here is \"evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Estimate this: failures/day.",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "0.0018*5,400,000=9,720.",
      "detailedExplanation": "The key clue in this question is \"critical path handles 5,400,000 requests/day and 0\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 5,400 and 000 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Estimate this: net drain rate.",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "2,530-2,200=330.",
      "detailedExplanation": "Start from \"queue ingest is 2,200 events/min and drain is 2,530 events/min\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Estimate this: effective attempts/sec.",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "75,000*1.28=96,000.",
      "detailedExplanation": "Start from \"retries add 0\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Estimate this: total failover seconds/day.",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "16*24=384.",
      "detailedExplanation": "The key clue in this question is \"failover takes 16s and occurs 24 times/day\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 16s and 24 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Estimate this: percent over target.",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(845-650)/650=30%.",
      "detailedExplanation": "Read this as a scenario about \"target p99 is 650ms\". Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 650ms and 845ms in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-083",
      "type": "numeric-input",
      "question": "What is the best answer here: if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "0.34*130,000=44,200.",
      "detailedExplanation": "The decision turns on \"if 34% of 130,000 req/min are high-priority, how many high-priority req/min\". Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 34 and 130,000 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Estimate this: percent reduction.",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1.0-0.22)/1.0=78%.",
      "detailedExplanation": "This prompt is really about \"error rate drops from 1\". Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 1.0 and 0.22 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Estimate this: minimum acknowledgements.",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Majority of 9 is 5.",
      "detailedExplanation": "Use \"9-node quorum cluster requires majority writes\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. Strong answers connect quorum/coordination settings to concrete correctness goals. Keep quantities like 9 and 5 in aligned units before selecting an answer. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Estimate this: minutes to clear.",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "56,000/350=160.",
      "detailedExplanation": "The core signal here is \"backlog is 56,000 tasks with net drain 350 tasks/min\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 56,000 and 350 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Estimate this: percent remaining available.",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "15/18=83.33%.",
      "detailedExplanation": "If you keep \"fleet has 18 zones and 3 are unavailable\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 18 and 3 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Estimate this: percent reduction.",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(52-34)/52=34.62%.",
      "detailedExplanation": "Start from \"mTTR improved from 52 min to 34 min\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 52 min and 34 min in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-089",
      "type": "numeric-input",
      "question": "What is the best answer here: if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "0.11*2,800,000=308,000.",
      "detailedExplanation": "The key clue in this question is \"if 11% of 2,800,000 daily ops need manual checks, checks/day\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 11 and 2,800 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-090",
      "type": "ordering",
      "question": "Order a classic-design decomposition workflow. Focus on url shortener scale, analytics & operations tradeoffs.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start with outcomes/invariants, then decompose, control, and validate.",
      "detailedExplanation": "The decision turns on \"order a classic-design decomposition workflow\". Place obvious extremes first, then sort the middle by pairwise comparison. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-091",
      "type": "ordering",
      "question": "For url shortener scale, analytics & operations, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Risk rises as boundaries and ownership degrade.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing design risk\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-092",
      "type": "ordering",
      "question": "Within url shortener scale, analytics & operations, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope, contain, fix, then harden.",
      "detailedExplanation": "The key clue in this question is \"order safe incident mitigation steps\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-ua-093",
      "type": "ordering",
      "question": "In this url shortener scale, analytics & operations context, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Maturity improves with safeguards and observability.",
      "detailedExplanation": "Start from \"order by increasing retry-control maturity\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "cd-ua-094",
      "type": "ordering",
      "question": "Considering url shortener scale, analytics & operations, order fallback sophistication.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sophistication rises with explicit policy and automation.",
      "detailedExplanation": "If you keep \"order fallback sophistication\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-095",
      "type": "ordering",
      "question": "From a url shortener scale, analytics & operations viewpoint, order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rigor increases with validation and reversible control.",
      "detailedExplanation": "The core signal here is \"order failover validation rigor\". Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-096",
      "type": "ordering",
      "question": "Arrange from least to greatest blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Blast radius expands from local failures to regional control issues.",
      "detailedExplanation": "Use \"order by increasing blast radius\" as your starting point, then verify tradeoffs carefully. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-097",
      "type": "ordering",
      "question": "Order data-path durability confidence. Use a url shortener scale, analytics & operations perspective.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Durability confidence grows with replication and verification.",
      "detailedExplanation": "This prompt is really about \"order data-path durability confidence\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-098",
      "type": "ordering",
      "question": "Order by increasing operational discipline. Focus on url shortener scale, analytics & operations tradeoffs.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Discipline improves with structure and accountability.",
      "detailedExplanation": "The decision turns on \"order by increasing operational discipline\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-099",
      "type": "ordering",
      "question": "For url shortener scale, analytics & operations, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rollout should progress from limited exposure to institutionalization.",
      "detailedExplanation": "Read this as a scenario about \"order rollout safety for major design changes\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-ua-100",
      "type": "ordering",
      "question": "Within url shortener scale, analytics & operations, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Evidence strength rises with sustained production and drill results.",
      "detailedExplanation": "The key clue in this question is \"order evidence strength for fix success\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    }
  ]
}
