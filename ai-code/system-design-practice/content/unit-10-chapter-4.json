{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 4,
  "chapterTitle": "URL Shortener Scale, Analytics & Operations",
  "chapterDescription": "Decompose high-scale redirect operations, analytics pipelines, retention design, and reliability controls.",
  "problems": [
    {
      "id": "cd-ua-001",
      "type": "multiple-choice",
      "question": "Case Alpha: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In URL Shortener Scale, Analytics & Operations, click analytics ingest stream fails mainly through analytics lag hiding real-time campaign outcomes. The best choice is \"Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-002",
      "type": "multiple-choice",
      "question": "Case Beta: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Redirect edge fleet should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Apply bot filtering and confidence scoring before committing events to authoritative metrics\" is strongest because it directly addresses bot traffic inflating click metrics and improves repeatability under stress. This aligns with the extra condition (Recent traffic growth exposed this bottleneck repeatedly).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that remains viable across the planning horizon, not just today. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: forecasting volume without resource thresholds.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-003",
      "type": "multiple-choice",
      "question": "Case Gamma: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat top-link hot shard mitigation path as a reliability-control decision, not an averages-only optimization. \"Use key-splitting and adaptive partitioning for viral-link hotspot containment\" is correct since it mitigates hot partition overload from viral redirect while keeping containment local. The decision remains valid given: Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-004",
      "type": "multiple-choice",
      "question": "Case Delta: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics."
      ],
      "correct": 3,
      "explanation": "For analytics ETL job, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Define retention tiers with compaction/downsampling for long-term analytics economics\" outperforms the alternatives because it targets storage costs rising from unbounded event retention and preserves safe recovery behavior. It is also the most compatible with A previous rollback fixed averages but not tail impact.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In URL Shortener Scale, Analytics & Operations, fraud/bot filtering stage fails mainly through event loss during regional stream failover. The best choice is \"Use exactly-once-like semantics via idempotent event keys and dedupe windows\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: User trust risk is highest on this path.",
      "detailedExplanation": "Generalize from fraud/bot filtering stage to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-006",
      "type": "multiple-choice",
      "question": "Case Zeta: campaign tracking API. Dominant risk is clock skew breaking hourly aggregation windows. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Align aggregation windows with watermark logic to handle skewed late events safely.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Campaign tracking API should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Align aggregation windows with watermark logic to handle skewed late events safely\" is strongest because it directly addresses clock skew breaking hourly aggregation windows and improves repeatability under stress. This aligns with the extra condition (A shared dependency has uncertain health right now).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-007",
      "type": "multiple-choice",
      "question": "Case Eta: historical reporting warehouse. Dominant risk is reporting queries contending with ingestion writes. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Isolate reporting compute from ingest path to avoid mutual saturation.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat historical reporting warehouse as a reliability-control decision, not an averages-only optimization. \"Isolate reporting compute from ingest path to avoid mutual saturation\" is correct since it mitigates reporting queries contending with ingestion writes while keeping containment local. The decision remains valid given: The change must preserve cost discipline during peak.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-008",
      "type": "multiple-choice",
      "question": "Case Theta: regional edge failover routing. Dominant risk is edge failover causing duplicated click events. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Model failover replay explicitly so duplicate clicks are measurable and bounded."
      ],
      "correct": 3,
      "explanation": "For regional edge failover routing, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Model failover replay explicitly so duplicate clicks are measurable and bounded\" outperforms the alternatives because it targets edge failover causing duplicated click events and preserves safe recovery behavior. It is also the most compatible with Telemetry shows risk concentrated in one partition class.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-009",
      "type": "multiple-choice",
      "question": "Case Iota: retention/archival worker. Dominant risk is missing idempotency in analytics consumer path. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Build SLO dashboards that expose redirect health and analytics freshness separately.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In URL Shortener Scale, Analytics & Operations, retention/archival worker fails mainly through missing idempotency in analytics consumer path. The best choice is \"Build SLO dashboards that expose redirect health and analytics freshness separately\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-010",
      "type": "multiple-choice",
      "question": "Case Kappa: ops dashboard aggregation service. Dominant risk is SLO blind spots between redirect and analytics layers. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Test campaign surge scenarios with load + replay drills before peak events.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat ops dashboard aggregation service as a reliability-control decision, not an averages-only optimization. \"Test campaign surge scenarios with load + replay drills before peak events\" is correct since it mitigates SLO blind spots between redirect and analytics layers while keeping containment local. The decision remains valid given: Current runbooks are missing explicit ownership for this boundary.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-011",
      "type": "multiple-choice",
      "question": "Case Lambda: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For click analytics ingest stream, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract\" outperforms the alternatives because it targets analytics lag hiding real-time campaign outcomes and preserves safe recovery behavior. It is also the most compatible with A cross-region path recently changed behavior after migration.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-012",
      "type": "multiple-choice",
      "question": "Case Mu: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics."
      ],
      "correct": 3,
      "explanation": "In URL Shortener Scale, Analytics & Operations, redirect edge fleet fails mainly through bot traffic inflating click metrics. The best choice is \"Apply bot filtering and confidence scoring before committing events to authoritative metrics\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-013",
      "type": "multiple-choice",
      "question": "Case Nu: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Top-link hot shard mitigation path should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Use key-splitting and adaptive partitioning for viral-link hotspot containment\" is strongest because it directly addresses hot partition overload from viral redirect and improves repeatability under stress. This aligns with the extra condition (Capacity headroom exists but only in specific pools).",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Reject approaches that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-014",
      "type": "multiple-choice",
      "question": "Case Xi: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat analytics ETL job as a reliability-control decision, not an averages-only optimization. \"Define retention tiers with compaction/downsampling for long-term analytics economics\" is correct since it mitigates storage costs rising from unbounded event retention while keeping containment local. The decision remains valid given: A partial failure is masking itself as success in metrics.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-015",
      "type": "multiple-choice",
      "question": "Case Omicron: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For fraud/bot filtering stage, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Use exactly-once-like semantics via idempotent event keys and dedupe windows\" outperforms the alternatives because it targets event loss during regional stream failover and preserves safe recovery behavior. It is also the most compatible with This fix must hold under celebrity or campaign spike conditions.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-016",
      "type": "multiple-choice",
      "question": "Case Pi: campaign tracking API. Dominant risk is clock skew breaking hourly aggregation windows. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Align aggregation windows with watermark logic to handle skewed late events safely."
      ],
      "correct": 3,
      "explanation": "In URL Shortener Scale, Analytics & Operations, campaign tracking API fails mainly through clock skew breaking hourly aggregation windows. The best choice is \"Align aggregation windows with watermark logic to handle skewed late events safely\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "Generalize from campaign tracking API to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-017",
      "type": "multiple-choice",
      "question": "Case Rho: historical reporting warehouse. Dominant risk is reporting queries contending with ingestion writes. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Isolate reporting compute from ingest path to avoid mutual saturation.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Historical reporting warehouse should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Isolate reporting compute from ingest path to avoid mutual saturation\" is strongest because it directly addresses reporting queries contending with ingestion writes and improves repeatability under stress. This aligns with the extra condition (On-call requested a reversible operational first step).",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. In real systems, reject plans that assume linear scaling across shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-018",
      "type": "multiple-choice",
      "question": "Case Sigma: regional edge failover routing. Dominant risk is edge failover causing duplicated click events. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Model failover replay explicitly so duplicate clicks are measurable and bounded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat regional edge failover routing as a reliability-control decision, not an averages-only optimization. \"Model failover replay explicitly so duplicate clicks are measurable and bounded\" is correct since it mitigates edge failover causing duplicated click events while keeping containment local. The decision remains valid given: The system mixes strict and eventual paths with unclear contracts.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-019",
      "type": "multiple-choice",
      "question": "Case Tau: retention/archival worker. Dominant risk is missing idempotency in analytics consumer path. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Build SLO dashboards that expose redirect health and analytics freshness separately.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For retention/archival worker, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Build SLO dashboards that expose redirect health and analytics freshness separately\" outperforms the alternatives because it targets missing idempotency in analytics consumer path and preserves safe recovery behavior. It is also the most compatible with A hot-key pattern is likely from real traffic skew.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: ops dashboard aggregation service. Dominant risk is SLO blind spots between redirect and analytics layers. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Test campaign surge scenarios with load + replay drills before peak events."
      ],
      "correct": 3,
      "explanation": "Ops dashboard aggregation service should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Test campaign surge scenarios with load + replay drills before peak events\" is strongest because it directly addresses SLO blind spots between redirect and analytics layers and improves repeatability under stress. This aligns with the extra condition (The path must remain mobile-latency friendly under stress).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-021",
      "type": "multiple-choice",
      "question": "Case Phi: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat click analytics ingest stream as a reliability-control decision, not an averages-only optimization. \"Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract\" is correct since it mitigates analytics lag hiding real-time campaign outcomes while keeping containment local. The decision remains valid given: Compliance requires explicit behavior for edge-case failures.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-022",
      "type": "multiple-choice",
      "question": "Case Chi: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For redirect edge fleet, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Apply bot filtering and confidence scoring before committing events to authoritative metrics\" outperforms the alternatives because it targets bot traffic inflating click metrics and preserves safe recovery behavior. It is also the most compatible with This boundary has failed during the last two game days.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-023",
      "type": "multiple-choice",
      "question": "Case Psi: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In URL Shortener Scale, Analytics & Operations, top-link hot shard mitigation path fails mainly through hot partition overload from viral redirect. The best choice is \"Use key-splitting and adaptive partitioning for viral-link hotspot containment\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that improve speed but weaken freshness or invalidation correctness. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-024",
      "type": "multiple-choice",
      "question": "Case Omega: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics."
      ],
      "correct": 3,
      "explanation": "Analytics ETL job should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Define retention tiers with compaction/downsampling for long-term analytics economics\" is strongest because it directly addresses storage costs rising from unbounded event retention and improves repeatability under stress. This aligns with the extra condition (Dependency retries currently exceed safe server limits).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that ignore delivery semantics or backpressure behavior. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-025",
      "type": "multiple-choice",
      "question": "Case Atlas: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat fraud/bot filtering stage as a reliability-control decision, not an averages-only optimization. \"Use exactly-once-like semantics via idempotent event keys and dedupe windows\" is correct since it mitigates event loss during regional stream failover while keeping containment local. The decision remains valid given: The fix should avoid broad architectural rewrites this quarter.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-026",
      "type": "multiple-choice",
      "question": "Case Nova: campaign tracking API. Dominant risk is clock skew breaking hourly aggregation windows. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Align aggregation windows with watermark logic to handle skewed late events safely.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For campaign tracking API, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Align aggregation windows with watermark logic to handle skewed late events safely\" outperforms the alternatives because it targets clock skew breaking hourly aggregation windows and preserves safe recovery behavior. It is also the most compatible with Current metrics hide per-tenant variance that matters.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-027",
      "type": "multiple-choice",
      "question": "Case Orion: historical reporting warehouse. Dominant risk is reporting queries contending with ingestion writes. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Isolate reporting compute from ingest path to avoid mutual saturation.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In URL Shortener Scale, Analytics & Operations, historical reporting warehouse fails mainly through reporting queries contending with ingestion writes. The best choice is \"Isolate reporting compute from ingest path to avoid mutual saturation\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The fallback path is under-tested in production-like load.",
      "detailedExplanation": "Generalize from historical reporting warehouse to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-028",
      "type": "multiple-choice",
      "question": "Case Vega: regional edge failover routing. Dominant risk is edge failover causing duplicated click events. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Model failover replay explicitly so duplicate clicks are measurable and bounded."
      ],
      "correct": 3,
      "explanation": "Regional edge failover routing should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Model failover replay explicitly so duplicate clicks are measurable and bounded\" is strongest because it directly addresses edge failover causing duplicated click events and improves repeatability under stress. This aligns with the extra condition (A control-plane issue is bleeding into data-plane reliability).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-029",
      "type": "multiple-choice",
      "question": "Case Helios: retention/archival worker. Dominant risk is missing idempotency in analytics consumer path. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Build SLO dashboards that expose redirect health and analytics freshness separately.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat retention/archival worker as a reliability-control decision, not an averages-only optimization. \"Build SLO dashboards that expose redirect health and analytics freshness separately\" is correct since it mitigates missing idempotency in analytics consumer path while keeping containment local. The decision remains valid given: The system must preserve critical events over bulk traffic.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject designs that improve throughput while weakening reliability guarantees. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-030",
      "type": "multiple-choice",
      "question": "Case Aurora: ops dashboard aggregation service. Dominant risk is SLO blind spots between redirect and analytics layers. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Test campaign surge scenarios with load + replay drills before peak events.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In URL Shortener Scale, Analytics & Operations, ops dashboard aggregation service fails mainly through SLO blind spots between redirect and analytics layers. The best choice is \"Test campaign surge scenarios with load + replay drills before peak events\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "Generalize from ops dashboard aggregation service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: click analytics ingest stream. Dominant risk is analytics lag hiding real-time campaign outcomes. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Click analytics ingest stream should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract\" is strongest because it directly addresses analytics lag hiding real-time campaign outcomes and improves repeatability under stress. This aligns with the extra condition (The edge layer is healthy but origin saturation is growing).",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-032",
      "type": "multiple-choice",
      "question": "Case Pulse: redirect edge fleet. Dominant risk is bot traffic inflating click metrics. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Apply bot filtering and confidence scoring before committing events to authoritative metrics."
      ],
      "correct": 3,
      "explanation": "Treat redirect edge fleet as a reliability-control decision, not an averages-only optimization. \"Apply bot filtering and confidence scoring before committing events to authoritative metrics\" is correct since it mitigates bot traffic inflating click metrics while keeping containment local. The decision remains valid given: Operational complexity is rising faster than team onboarding.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-033",
      "type": "multiple-choice",
      "question": "Case Forge: top-link hot shard mitigation path. Dominant risk is hot partition overload from viral redirect. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For top-link hot shard mitigation path, prefer the option that prevents reoccurrence in URL Shortener Scale, Analytics & Operations. \"Use key-splitting and adaptive partitioning for viral-link hotspot containment\" outperforms the alternatives because it targets hot partition overload from viral redirect and preserves safe recovery behavior. It is also the most compatible with Stakeholders need clear trade-off rationale in the postmortem.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-034",
      "type": "multiple-choice",
      "question": "Case Harbor: analytics ETL job. Dominant risk is storage costs rising from unbounded event retention. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Define retention tiers with compaction/downsampling for long-term analytics economics.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In URL Shortener Scale, Analytics & Operations, analytics ETL job fails mainly through storage costs rising from unbounded event retention. The best choice is \"Define retention tiers with compaction/downsampling for long-term analytics economics\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prefer the approach that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-035",
      "type": "multiple-choice",
      "question": "Case Vector: fraud/bot filtering stage. Dominant risk is event loss during regional stream failover. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Fraud/bot filtering stage should be solved at the failure boundary named in URL Shortener Scale, Analytics & Operations. \"Use exactly-once-like semantics via idempotent event keys and dedupe windows\" is strongest because it directly addresses event loss during regional stream failover and improves repeatability under stress. This aligns with the extra condition (Recovery sequencing matters as much as immediate containment).",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in URL Shortener Scale, Analytics & Operations: for Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention, \"The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures\" is correct because it addresses storage costs rising from unbounded event retention and improves controllability.",
          "detailedExplanation": "Generalize from incident review for click analytics ingest stream: signal points to storage costs to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for click analytics ingest stream:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Align aggregation windows with watermark logic to handle skewed late events safely.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in URL Shortener Scale, Analytics & Operations, the best answer is \"Align aggregation windows with watermark logic to handle skewed late events safely\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect edge fleet: signal points to event loss during regional stream failover. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures\" best matches Incident review for redirect edge fleet: signal points to event loss during regional stream failover by targeting event loss during regional stream failover and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for redirect edge fleet: signal points\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Isolate reporting compute from ingest path to avoid mutual saturation.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in URL Shortener Scale, Analytics & Operations: for With diagnosis confirmed in \"incident review for redirect edge fleet: signal points\", which immediate adjustment best addresses the risk, \"Isolate reporting compute from ingest path to avoid mutual saturation\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows is a two-step reliability decision. At stage 1, \"The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures\" wins because it balances immediate containment with long-term prevention around clock skew breaking hourly aggregation windows.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for top-link hot shard mitigation path:\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Model failover replay explicitly so duplicate clicks are measurable and bounded."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Model failover replay explicitly so duplicate clicks are measurable and bounded\" best matches With root cause identified for \"incident review for top-link hot shard mitigation path:\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize from uRL Shortener Scale, Analytics & Operations to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in URL Shortener Scale, Analytics & Operations, the best answer is \"The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures\". It is the option most directly aligned to reporting queries contending with ingestion writes while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for analytics ETL job: signal points to\", which immediate adjustment best addresses the risk?",
          "options": [
            "Build SLO dashboards that expose redirect health and analytics freshness separately.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For \"incident review for analytics ETL job: signal points to\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Build SLO dashboards that expose redirect health and analytics freshness separately\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures\" best matches Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events by targeting edge failover causing duplicated click events and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for fraud/bot filtering stage: signal\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Test campaign surge scenarios with load + replay drills before peak events.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in URL Shortener Scale, Analytics & Operations: for \"incident review for fraud/bot filtering stage: signal\", what first move gives the best reliability impact, \"Test campaign surge scenarios with load + replay drills before peak events\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for campaign tracking API: signal points to missing idempotency in analytics consumer path. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around campaign tracking API mismatches missing idempotency in analytics consumer path, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Incident review for campaign tracking API: signal points to missing idempotency in analytics consumer path is a two-step reliability decision. At stage 1, \"The current decomposition around campaign tracking API mismatches missing idempotency in analytics consumer path, creating repeated failures\" wins because it balances immediate containment with long-term prevention around missing idempotency in analytics consumer path.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for campaign tracking API: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract\" best matches With root cause identified for \"incident review for campaign tracking API: signal\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from uRL Shortener Scale, Analytics & Operations to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for historical reporting warehouse: signal points to SLO blind spots between redirect and analytics layers. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around historical reporting warehouse mismatches SLO blind spots between redirect and analytics layers, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in URL Shortener Scale, Analytics & Operations, the best answer is \"The current decomposition around historical reporting warehouse mismatches SLO blind spots between redirect and analytics layers, creating repeated failures\". It is the option most directly aligned to SLO blind spots between redirect and analytics layers while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for historical reporting warehouse:\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Apply bot filtering and confidence scoring before committing events to authoritative metrics."
          ],
          "correct": 3,
          "explanation": "After diagnosing \"incident review for historical reporting warehouse:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Apply bot filtering and confidence scoring before committing events to authoritative metrics\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional edge failover routing: signal points to analytics lag hiding real-time campaign outcomes. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around regional edge failover routing mismatches analytics lag hiding real-time campaign outcomes, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in URL Shortener Scale, Analytics & Operations: for Incident review for regional edge failover routing: signal points to analytics lag hiding real-time campaign outcomes, \"The current decomposition around regional edge failover routing mismatches analytics lag hiding real-time campaign outcomes, creating repeated failures\" is correct because it addresses analytics lag hiding real-time campaign outcomes and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for regional edge failover routing:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in URL Shortener Scale, Analytics & Operations, the best answer is \"Use key-splitting and adaptive partitioning for viral-link hotspot containment\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for retention/archival worker: signal points to bot traffic inflating click metrics. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around retention/archival worker mismatches bot traffic inflating click metrics, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around retention/archival worker mismatches bot traffic inflating click metrics, creating repeated failures\" best matches Incident review for retention/archival worker: signal points to bot traffic inflating click metrics by targeting bot traffic inflating click metrics and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for retention/archival worker: signal\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Define retention tiers with compaction/downsampling for long-term analytics economics.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in URL Shortener Scale, Analytics & Operations: for Now that \"incident review for retention/archival worker: signal\" is diagnosed, what is the highest-leverage change to make now, \"Define retention tiers with compaction/downsampling for long-term analytics economics\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ops dashboard aggregation service: signal points to hot partition overload from viral redirect. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around ops dashboard aggregation service mismatches hot partition overload from viral redirect, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Incident review for ops dashboard aggregation service: signal points to hot partition overload from viral redirect is a two-step reliability decision. At stage 1, \"The current decomposition around ops dashboard aggregation service mismatches hot partition overload from viral redirect, creating repeated failures\" wins because it balances immediate containment with long-term prevention around hot partition overload from viral redirect.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for ops dashboard aggregation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use exactly-once-like semantics via idempotent event keys and dedupe windows\" best matches Using the diagnosis from \"incident review for ops dashboard aggregation service:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in URL Shortener Scale, Analytics & Operations, the best answer is \"The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures\". It is the option most directly aligned to storage costs rising from unbounded event retention while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for click analytics ingest stream:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Align aggregation windows with watermark logic to handle skewed late events safely."
          ],
          "correct": 3,
          "explanation": "With diagnosis confirmed in \"incident review for click analytics ingest stream:\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Align aggregation windows with watermark logic to handle skewed late events safely\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect edge fleet: signal points to event loss during regional stream failover. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in URL Shortener Scale, Analytics & Operations: for Incident review for redirect edge fleet: signal points to event loss during regional stream failover, \"The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures\" is correct because it addresses event loss during regional stream failover and improves controllability.",
          "detailedExplanation": "Generalize from incident review for redirect edge fleet: signal points to event loss during regional to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for redirect edge fleet: signal points\", what should change first before wider rollout?",
          "options": [
            "Isolate reporting compute from ingest path to avoid mutual saturation.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in URL Shortener Scale, Analytics & Operations, the best answer is \"Isolate reporting compute from ingest path to avoid mutual saturation\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures\" best matches Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows by targeting clock skew breaking hourly aggregation windows and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for top-link hot shard mitigation path:\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Model failover replay explicitly so duplicate clicks are measurable and bounded.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in URL Shortener Scale, Analytics & Operations: for \"incident review for top-link hot shard mitigation path:\", which next change should be prioritized first, \"Model failover replay explicitly so duplicate clicks are measurable and bounded\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes is a two-step reliability decision. At stage 1, \"The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures\" wins because it balances immediate containment with long-term prevention around reporting queries contending with ingestion writes.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for analytics ETL job: signal points to\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Build SLO dashboards that expose redirect health and analytics freshness separately.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Build SLO dashboards that expose redirect health and analytics freshness separately\" best matches After diagnosing \"incident review for analytics ETL job: signal points to\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize from uRL Shortener Scale, Analytics & Operations to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in URL Shortener Scale, Analytics & Operations: for Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events, \"The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures\" is correct because it addresses edge failover causing duplicated click events and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for fraud/bot filtering stage: signal\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Test campaign surge scenarios with load + replay drills before peak events."
          ],
          "correct": 3,
          "explanation": "For stage 2 in URL Shortener Scale, Analytics & Operations, the best answer is \"Test campaign surge scenarios with load + replay drills before peak events\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for campaign tracking API: signal points to missing idempotency in analytics consumer path. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around campaign tracking API mismatches missing idempotency in analytics consumer path, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around campaign tracking API mismatches missing idempotency in analytics consumer path, creating repeated failures\" best matches Incident review for campaign tracking API: signal points to missing idempotency in analytics consumer path by targeting missing idempotency in analytics consumer path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for campaign tracking API: signal\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in URL Shortener Scale, Analytics & Operations: for In the \"incident review for campaign tracking API: signal\" scenario, which next step is strongest under current constraints, \"Separate redirect SLO path from analytics processing and make analytics eventually consistent by contract\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for historical reporting warehouse: signal points to SLO blind spots between redirect and analytics layers. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around historical reporting warehouse mismatches SLO blind spots between redirect and analytics layers, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for historical reporting warehouse: signal points to SLO blind spots between redirect and analytics layers is a two-step reliability decision. At stage 1, \"The current decomposition around historical reporting warehouse mismatches SLO blind spots between redirect and analytics layers, creating repeated failures\" wins because it balances immediate containment with long-term prevention around SLO blind spots between redirect and analytics layers.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for historical reporting warehouse:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Apply bot filtering and confidence scoring before committing events to authoritative metrics.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Apply bot filtering and confidence scoring before committing events to authoritative metrics\" best matches After diagnosing \"incident review for historical reporting warehouse:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from uRL Shortener Scale, Analytics & Operations to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for regional edge failover routing: signal points to analytics lag hiding real-time campaign outcomes. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around regional edge failover routing mismatches analytics lag hiding real-time campaign outcomes, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in URL Shortener Scale, Analytics & Operations, the best answer is \"The current decomposition around regional edge failover routing mismatches analytics lag hiding real-time campaign outcomes, creating repeated failures\". It is the option most directly aligned to analytics lag hiding real-time campaign outcomes while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for regional edge failover routing:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Use key-splitting and adaptive partitioning for viral-link hotspot containment.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "With root cause identified for \"incident review for regional edge failover routing:\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Use key-splitting and adaptive partitioning for viral-link hotspot containment\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for retention/archival worker: signal points to bot traffic inflating click metrics. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around retention/archival worker mismatches bot traffic inflating click metrics, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in URL Shortener Scale, Analytics & Operations: for Incident review for retention/archival worker: signal points to bot traffic inflating click metrics, \"The current decomposition around retention/archival worker mismatches bot traffic inflating click metrics, creating repeated failures\" is correct because it addresses bot traffic inflating click metrics and improves controllability.",
          "detailedExplanation": "Generalize from incident review for retention/archival worker: signal points to bot traffic inflating to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for retention/archival worker: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Define retention tiers with compaction/downsampling for long-term analytics economics."
          ],
          "correct": 3,
          "explanation": "For stage 2 in URL Shortener Scale, Analytics & Operations, the best answer is \"Define retention tiers with compaction/downsampling for long-term analytics economics\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ops dashboard aggregation service: signal points to hot partition overload from viral redirect. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around ops dashboard aggregation service mismatches hot partition overload from viral redirect, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around ops dashboard aggregation service mismatches hot partition overload from viral redirect, creating repeated failures\" best matches Incident review for ops dashboard aggregation service: signal points to hot partition overload from viral redirect by targeting hot partition overload from viral redirect and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for ops dashboard aggregation service:\", what is the highest-leverage change to make now?",
          "options": [
            "Use exactly-once-like semantics via idempotent event keys and dedupe windows.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in URL Shortener Scale, Analytics & Operations: for Given the diagnosis in \"incident review for ops dashboard aggregation service:\", what is the highest-leverage change to make now, \"Use exactly-once-like semantics via idempotent event keys and dedupe windows\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for click analytics ingest stream: signal points to storage costs rising from unbounded event retention is a two-step reliability decision. At stage 1, \"The current decomposition around click analytics ingest stream mismatches storage costs rising from unbounded event retention, creating repeated failures\" wins because it balances immediate containment with long-term prevention around storage costs rising from unbounded event retention.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for click analytics ingest stream:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Align aggregation windows with watermark logic to handle skewed late events safely.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Align aggregation windows with watermark logic to handle skewed late events safely\" best matches With diagnosis confirmed in \"incident review for click analytics ingest stream:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect edge fleet: signal points to event loss during regional stream failover. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in URL Shortener Scale, Analytics & Operations, the best answer is \"The current decomposition around redirect edge fleet mismatches event loss during regional stream failover, creating repeated failures\". It is the option most directly aligned to event loss during regional stream failover while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for redirect edge fleet: signal points\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Isolate reporting compute from ingest path to avoid mutual saturation.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident review for redirect edge fleet: signal points\", what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Isolate reporting compute from ingest path to avoid mutual saturation\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in URL Shortener Scale, Analytics & Operations: for Incident review for top-link hot shard mitigation path: signal points to clock skew breaking hourly aggregation windows, \"The current decomposition around top-link hot shard mitigation path mismatches clock skew breaking hourly aggregation windows, creating repeated failures\" is correct because it addresses clock skew breaking hourly aggregation windows and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for top-link hot shard mitigation path:\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Model failover replay explicitly so duplicate clicks are measurable and bounded."
          ],
          "correct": 3,
          "explanation": "For stage 2 in URL Shortener Scale, Analytics & Operations, the best answer is \"Model failover replay explicitly so duplicate clicks are measurable and bounded\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around analytics ETL job mismatches reporting queries contending with ingestion writes, creating repeated failures\" best matches Incident review for analytics ETL job: signal points to reporting queries contending with ingestion writes by targeting reporting queries contending with ingestion writes and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for analytics ETL job: signal points to\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Build SLO dashboards that expose redirect health and analytics freshness separately.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in URL Shortener Scale, Analytics & Operations: for In the \"incident review for analytics ETL job: signal points to\" scenario, what first move gives the best reliability impact, \"Build SLO dashboards that expose redirect health and analytics freshness separately\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for fraud/bot filtering stage: signal points to edge failover causing duplicated click events. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "For stage 1 in URL Shortener Scale, Analytics & Operations, the best answer is \"The current decomposition around fraud/bot filtering stage mismatches edge failover causing duplicated click events, creating repeated failures\". It is the option most directly aligned to edge failover causing duplicated click events while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for fraud/bot filtering stage: signal\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Test campaign surge scenarios with load + replay drills before peak events.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In the \"incident review for fraud/bot filtering stage: signal\" scenario, what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Test campaign surge scenarios with load + replay drills before peak events\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-061",
      "type": "multi-select",
      "question": "Mark all correct choices here: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for URL Shortener Scale, Analytics & Operations: The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-062",
      "type": "multi-select",
      "question": "Mark all correct choices here: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Mark all correct choices here: which controls improve safety on critical write paths, the highest-signal answer is a bundle of controls. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-063",
      "type": "multi-select",
      "question": "Mark all correct choices here: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In URL Shortener Scale, Analytics & Operations, Mark all correct choices here: which practices reduce hot-key or hot-partition impact needs layered controls, not one silver bullet. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize from practices reduce hot-key or hot-partition impact? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-064",
      "type": "multi-select",
      "question": "Mark all correct choices here: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Mark all correct choices here: what improves reliability when mixing sync and async paths is intentionally multi-dimensional in URL Shortener Scale, Analytics & Operations. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-065",
      "type": "multi-select",
      "question": "Mark all correct choices here: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for URL Shortener Scale, Analytics & Operations: The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-066",
      "type": "multi-select",
      "question": "Mark all correct choices here: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Mark all correct choices here: what should be explicit in API/service contracts for this design, the highest-signal answer is a bundle of controls. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-067",
      "type": "multi-select",
      "question": "Mark all correct choices here: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "In URL Shortener Scale, Analytics & Operations, Mark all correct choices here: which anti-patterns often cause incident recurrence needs layered controls, not one silver bullet. The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-068",
      "type": "multi-select",
      "question": "Mark all correct choices here: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Mark all correct choices here: what increases confidence before broad traffic rollout is intentionally multi-dimensional in URL Shortener Scale, Analytics & Operations. The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-069",
      "type": "multi-select",
      "question": "Mark all correct choices here: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for URL Shortener Scale, Analytics & Operations: The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-070",
      "type": "multi-select",
      "question": "Mark all correct choices here: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In URL Shortener Scale, Analytics & Operations, Mark all correct choices here: which telemetry dimensions are most actionable for design triage needs layered controls, not one silver bullet. The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-071",
      "type": "multi-select",
      "question": "Mark all correct choices here: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Mark all correct choices here: which governance actions improve cross-team reliability ownership is intentionally multi-dimensional in URL Shortener Scale, Analytics & Operations. The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-072",
      "type": "multi-select",
      "question": "Mark all correct choices here: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for URL Shortener Scale, Analytics & Operations: The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-073",
      "type": "multi-select",
      "question": "Mark all correct choices here: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Mark all correct choices here: which fallback strategies are strong when dependencies degrade, the highest-signal answer is a bundle of controls. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-074",
      "type": "multi-select",
      "question": "Mark all correct choices here: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In URL Shortener Scale, Analytics & Operations, Mark all correct choices here: what reduces data-quality regressions in eventual pipelines needs layered controls, not one silver bullet. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize from reduces data-quality regressions in eventual pipelines? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-075",
      "type": "multi-select",
      "question": "Mark all correct choices here: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Mark all correct choices here: which runbook components improve incident execution quality is intentionally multi-dimensional in URL Shortener Scale, Analytics & Operations. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-076",
      "type": "multi-select",
      "question": "Mark all correct choices here: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for URL Shortener Scale, Analytics & Operations: The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-077",
      "type": "multi-select",
      "question": "Mark all correct choices here: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Mark all correct choices here: what evidence demonstrates a fix worked beyond short-term recovery, the highest-signal answer is a bundle of controls. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-078",
      "type": "numeric-input",
      "question": "A critical path handles 5,400,000 requests/day and 0.18% fail SLO. Estimate this: failures/day.",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "For A critical path handles 5,400,000 requests/day and 0, the computed target in URL Shortener Scale, Analytics & Operations is 9720 requests. Responses within +/-3% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 5,400 and 000 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-079",
      "type": "numeric-input",
      "question": "Queue ingest is 2,200 events/min and drain is 2,530 events/min. Estimate this: net drain rate.",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "URL Shortener Scale, Analytics & Operations expects quick quantitative triage: Queue ingest is 2,200 events/min and drain is 2,530 events/min evaluates to 330 events/min. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A good message-system answer defines guarantees clearly for both producer and consumer paths. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-080",
      "type": "numeric-input",
      "question": "Retries add 0.28 extra attempts at 75,000 req/sec. Estimate this: effective attempts/sec.",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for Retries add 0: 96000 attempts/sec. Answers within +/-2% show correct directional reasoning for URL Shortener Scale, Analytics & Operations.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-081",
      "type": "numeric-input",
      "question": "Failover takes 16s and occurs 24 times/day. Estimate this: total failover seconds/day.",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "For Failover takes 16s and occurs 24 times/day, the computed target in URL Shortener Scale, Analytics & Operations is 384 seconds. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 16s and 24 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-082",
      "type": "numeric-input",
      "question": "Target p99 is 650ms; observed p99 is 845ms. Estimate this: percent over target.",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "URL Shortener Scale, Analytics & Operations expects quick quantitative triage: Target p99 is 650ms; observed p99 is 845ms evaluates to 30 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 650ms and 845ms in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-083",
      "type": "numeric-input",
      "question": "What is the best answer here: if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "The operational math for What is the best answer here: if 34% of 130,000 req/min are high-priority, how many high-priority req/min gives 44200 requests/min. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 34 and 130,000 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.0% to 0.22%. Estimate this: percent reduction.",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Error rate drops from 1: 78 %. Answers within +/-30% show correct directional reasoning for URL Shortener Scale, Analytics & Operations.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 1.0 and 0.22 in aligned units before deciding on an implementation approach. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-085",
      "type": "numeric-input",
      "question": "A 9-node quorum cluster requires majority writes. Estimate this: minimum acknowledgements.",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "For A 9-node quorum cluster requires majority writes, the computed target in URL Shortener Scale, Analytics & Operations is 5 acks. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize from 9-node quorum cluster requires majority writes to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Strong answers connect quorum/coordination settings to concrete correctness goals. Keep quantities like 9 and 5 in aligned units before deciding on an implementation approach. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-086",
      "type": "numeric-input",
      "question": "Backlog is 56,000 tasks with net drain 350 tasks/min. Estimate this: minutes to clear.",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "URL Shortener Scale, Analytics & Operations expects quick quantitative triage: Backlog is 56,000 tasks with net drain 350 tasks/min evaluates to 160 minutes. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 56,000 and 350 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-087",
      "type": "numeric-input",
      "question": "A fleet has 18 zones and 3 are unavailable. Estimate this: percent remaining available.",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for A fleet has 18 zones and 3 are unavailable gives 83.33 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 18 and 3 in aligned units before deciding on an implementation approach. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-088",
      "type": "numeric-input",
      "question": "MTTR improved from 52 min to 34 min. Estimate this: percent reduction.",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for MTTR improved from 52 min to 34 min: 34.62 %. Answers within +/-30% show correct directional reasoning for URL Shortener Scale, Analytics & Operations.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep every transformation in one unit system and check order of magnitude at the end. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 52 min and 34 min in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-089",
      "type": "numeric-input",
      "question": "What is the best answer here: if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "For What is the best answer here: if 11% of 2,800,000 daily ops need manual checks, checks/day, the computed target in URL Shortener Scale, Analytics & Operations is 308000 operations. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 11 and 2,800 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-090",
      "type": "ordering",
      "question": "Order a classic-design decomposition workflow. Focus on url shortener scale, analytics & operations tradeoffs.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "URL Shortener Scale, Analytics & Operations emphasizes safe recovery order. Beginning at Identify critical user journey and invariants and finishing at Validate with load/failure drills and refine keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Place obvious extremes first, then sort the middle by pairwise comparison. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-091",
      "type": "ordering",
      "question": "For url shortener scale, analytics & operations, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For url shortener scale, analytics & operations, order by increasing design risk, the correct ordering runs from Explicit boundaries with contracts to Implicit coupling with no ownership. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-092",
      "type": "ordering",
      "question": "Within url shortener scale, analytics & operations, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in URL Shortener Scale, Analytics & Operations should start with Scope blast radius and affected flows and end with Run recurrence checks and hardening actions. Within url shortener scale, analytics & operations, order safe incident mitigation steps rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-093",
      "type": "ordering",
      "question": "In this url shortener scale, analytics & operations context, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Fixed immediate retries must happen before Jittered retries with retry budgets and telemetry. That ordering matches incident-safe flow in URL Shortener Scale, Analytics & Operations.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Place obvious extremes first, then sort the middle by pairwise comparison. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-094",
      "type": "ordering",
      "question": "Considering url shortener scale, analytics & operations, order fallback sophistication.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "URL Shortener Scale, Analytics & Operations emphasizes safe recovery order. Beginning at Implicit fallback behavior and finishing at Policy-driven automated fallback with tests keeps blast radius controlled while restoring service.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-095",
      "type": "ordering",
      "question": "From a url shortener scale, analytics & operations viewpoint, order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a url shortener scale, analytics & operations viewpoint, order failover validation rigor, the correct ordering runs from Host health check only to Staged shift plus failback rehearsal and rollback gates. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Place obvious extremes first, then sort the middle by pairwise comparison. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-096",
      "type": "ordering",
      "question": "Arrange from least to greatest blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in URL Shortener Scale, Analytics & Operations should start with Single process failure and end with Cross-region control-plane failure. Arrange from least to greatest blast radius rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Generalize from order by increasing blast radius to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-097",
      "type": "ordering",
      "question": "Order data-path durability confidence. Use a url shortener scale, analytics & operations perspective.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: In-memory only acknowledgment must happen before Replicated durable write plus replay/integrity verification. That ordering matches incident-safe flow in URL Shortener Scale, Analytics & Operations.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-098",
      "type": "ordering",
      "question": "Order by increasing operational discipline. Focus on url shortener scale, analytics & operations tradeoffs.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "URL Shortener Scale, Analytics & Operations emphasizes safe recovery order. Beginning at Ad hoc incident response and finishing at Role-based response plus action closure tracking keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-099",
      "type": "ordering",
      "question": "For url shortener scale, analytics & operations, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For url shortener scale, analytics & operations, order rollout safety for major design changes, the correct ordering runs from Canary small cohort to Finalize runbook and ownership updates. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-ua-100",
      "type": "ordering",
      "question": "Within url shortener scale, analytics & operations, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in URL Shortener Scale, Analytics & Operations should start with Single successful test run and end with Sustained recovery plus failure-drill pass. Within url shortener scale, analytics & operations, order evidence strength for fix success rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "url-shortener-scale-analytics-and-operations"
      ],
      "difficulty": "staff-level"
    }
  ]
}
