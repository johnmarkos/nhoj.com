{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 4,
  "chapterTitle": "Redundancy, Replication & Failover Strategy",
  "chapterDescription": "Build availability with N+1 capacity, multi-AZ/region redundancy, and safe failover/failback procedures.",
  "problems": [
    {
      "id": "rel-rf-001",
      "type": "multiple-choice",
      "question": "Case Alpha: payments write cluster. Primary reliability risk is insufficient N+1 headroom in one AZ. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Design active/passive boundaries per workload and validate capacity with N+1 drills.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Redundancy, Replication & Failover Strategy, payments write cluster fails mainly through insufficient N+1 headroom in one AZ. The best choice is \"Design active/passive boundaries per workload and validate capacity with N+1 drills\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A rollback window is still available for the next 15 minutes.",
      "detailedExplanation": "The key clue in this question is \"payments write cluster\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 1 and 15 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-002",
      "type": "multiple-choice",
      "question": "Case Beta: session state datastore. Primary reliability risk is split-brain during regional failover. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Gate failover on freshness and quorum-health checks, not only host availability.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Session state datastore should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Gate failover on freshness and quorum-health checks, not only host availability\" is strongest because it directly addresses split-brain during regional failover and improves repeatability under stress. This aligns with the extra condition (Leadership asked for an action that lowers recurrence, not just symptoms).",
      "detailedExplanation": "Read this as a scenario about \"session state datastore\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-003",
      "type": "multiple-choice",
      "question": "Case Gamma: search serving tier. Primary reliability risk is stale secondary promoted without quorum checks. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use controlled failback with write-fencing and reconciliation before traffic return.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat search serving tier as a reliability-control decision, not an averages-only optimization. \"Use controlled failback with write-fencing and reconciliation before traffic return\" is correct since it mitigates stale secondary promoted without quorum checks while keeping containment local. The decision remains valid given: Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "The decision turns on \"search serving tier\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-rf-004",
      "type": "multiple-choice",
      "question": "Case Delta: control-plane metadata store. Primary reliability risk is replication lag violating RPO target. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Replicate control-plane state alongside data-plane state to avoid hidden SPOFs."
      ],
      "correct": 3,
      "explanation": "For control-plane metadata store, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Replicate control-plane state alongside data-plane state to avoid hidden SPOFs\" outperforms the alternatives because it targets replication lag violating RPO target and preserves safe recovery behavior. It is also the most compatible with Recent game-day results showed hidden cross-zone coupling.",
      "detailedExplanation": "This prompt is really about \"control-plane metadata store\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: notification router. Primary reliability risk is manual failover runbook missing rollback gates. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Continuously verify standby parity through synthetic readiness probes.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Redundancy, Replication & Failover Strategy, notification router fails mainly through manual failover runbook missing rollback gates. The best choice is \"Continuously verify standby parity through synthetic readiness probes\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Customer impact is concentrated on invariant-critical transactions.",
      "detailedExplanation": "Use \"notification router\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-006",
      "type": "multiple-choice",
      "question": "Case Zeta: feature config service. Primary reliability risk is failback causing replayed stale writes. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Define hot/warm/cold tiers by RTO objective and automate transitions.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Feature config service should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Define hot/warm/cold tiers by RTO objective and automate transitions\" is strongest because it directly addresses failback causing replayed stale writes and improves repeatability under stress. This aligns with the extra condition (The previous mitigation improved averages but not tail behavior).",
      "detailedExplanation": "The core signal here is \"feature config service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-007",
      "type": "multiple-choice",
      "question": "Case Eta: media pipeline scheduler. Primary reliability risk is warm standby drift from primary config. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Document failover runbooks with explicit abort criteria and ownership.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat media pipeline scheduler as a reliability-control decision, not an averages-only optimization. \"Document failover runbooks with explicit abort criteria and ownership\" is correct since it mitigates warm standby drift from primary config while keeping containment local. The decision remains valid given: Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "If you keep \"media pipeline scheduler\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-008",
      "type": "multiple-choice",
      "question": "Case Theta: inventory ledger. Primary reliability risk is cross-region dependency not replicated. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Prefer regional isolation over global coupling when blast radius is unclear."
      ],
      "correct": 3,
      "explanation": "For inventory ledger, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Prefer regional isolation over global coupling when blast radius is unclear\" outperforms the alternatives because it targets cross-region dependency not replicated and preserves safe recovery behavior. It is also the most compatible with Operations wants a reversible step before broader architecture changes.",
      "detailedExplanation": "Start from \"inventory ledger\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-009",
      "type": "multiple-choice",
      "question": "Case Iota: fraud scoring service. Primary reliability risk is capacity cliff after zonal evacuation. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Reserve surge capacity for evacuation scenarios before declaring resilience.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Redundancy, Replication & Failover Strategy, fraud scoring service fails mainly through capacity cliff after zonal evacuation. The best choice is \"Reserve surge capacity for evacuation scenarios before declaring resilience\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: SLO burn rate accelerated after a config rollout this morning.",
      "detailedExplanation": "The key clue in this question is \"fraud scoring service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "rel-rf-010",
      "type": "multiple-choice",
      "question": "Case Kappa: tenant routing gateway. Primary reliability risk is asymmetric routing during partial failover. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Make replication lag SLOs first-class inputs to routing and promotion.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat tenant routing gateway as a reliability-control decision, not an averages-only optimization. \"Make replication lag SLOs first-class inputs to routing and promotion\" is correct since it mitigates asymmetric routing during partial failover while keeping containment local. The decision remains valid given: A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "The decision turns on \"tenant routing gateway\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-011",
      "type": "multiple-choice",
      "question": "Case Lambda: payments write cluster. Primary reliability risk is insufficient N+1 headroom in one AZ. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Design active/passive boundaries per workload and validate capacity with N+1 drills.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For payments write cluster, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Design active/passive boundaries per workload and validate capacity with N+1 drills\" outperforms the alternatives because it targets insufficient N+1 headroom in one AZ and preserves safe recovery behavior. It is also the most compatible with The incident review highlighted missing boundary ownership.",
      "detailedExplanation": "Read this as a scenario about \"payments write cluster\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 1 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-012",
      "type": "multiple-choice",
      "question": "Case Mu: session state datastore. Primary reliability risk is split-brain during regional failover. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Gate failover on freshness and quorum-health checks, not only host availability."
      ],
      "correct": 3,
      "explanation": "In Redundancy, Replication & Failover Strategy, session state datastore fails mainly through split-brain during regional failover. The best choice is \"Gate failover on freshness and quorum-health checks, not only host availability\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "detailedExplanation": "The key clue in this question is \"session state datastore\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-013",
      "type": "multiple-choice",
      "question": "Case Nu: search serving tier. Primary reliability risk is stale secondary promoted without quorum checks. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Use controlled failback with write-fencing and reconciliation before traffic return.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Search serving tier should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Use controlled failback with write-fencing and reconciliation before traffic return\" is strongest because it directly addresses stale secondary promoted without quorum checks and improves repeatability under stress. This aligns with the extra condition (A canary can be deployed immediately if the strategy is clear).",
      "detailedExplanation": "Start from \"search serving tier\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-rf-014",
      "type": "multiple-choice",
      "question": "Case Xi: control-plane metadata store. Primary reliability risk is replication lag violating RPO target. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Replicate control-plane state alongside data-plane state to avoid hidden SPOFs.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat control-plane metadata store as a reliability-control decision, not an averages-only optimization. \"Replicate control-plane state alongside data-plane state to avoid hidden SPOFs\" is correct since it mitigates replication lag violating RPO target while keeping containment local. The decision remains valid given: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "If you keep \"control-plane metadata store\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "rel-rf-015",
      "type": "multiple-choice",
      "question": "Case Omicron: notification router. Primary reliability risk is manual failover runbook missing rollback gates. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Continuously verify standby parity through synthetic readiness probes.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For notification router, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Continuously verify standby parity through synthetic readiness probes\" outperforms the alternatives because it targets manual failover runbook missing rollback gates and preserves safe recovery behavior. It is also the most compatible with Client retries are already elevated and could amplify mistakes.",
      "detailedExplanation": "The core signal here is \"notification router\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-016",
      "type": "multiple-choice",
      "question": "Case Pi: feature config service. Primary reliability risk is failback causing replayed stale writes. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Define hot/warm/cold tiers by RTO objective and automate transitions."
      ],
      "correct": 3,
      "explanation": "In Redundancy, Replication & Failover Strategy, feature config service fails mainly through failback causing replayed stale writes. The best choice is \"Define hot/warm/cold tiers by RTO objective and automate transitions\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The team must preserve core write correctness under mitigation.",
      "detailedExplanation": "Use \"feature config service\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-017",
      "type": "multiple-choice",
      "question": "Case Rho: media pipeline scheduler. Primary reliability risk is warm standby drift from primary config. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Document failover runbooks with explicit abort criteria and ownership.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Media pipeline scheduler should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Document failover runbooks with explicit abort criteria and ownership\" is strongest because it directly addresses warm standby drift from primary config and improves repeatability under stress. This aligns with the extra condition (Recent staffing changes require simpler operational controls).",
      "detailedExplanation": "This prompt is really about \"media pipeline scheduler\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-018",
      "type": "multiple-choice",
      "question": "Case Sigma: inventory ledger. Primary reliability risk is cross-region dependency not replicated. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Prefer regional isolation over global coupling when blast radius is unclear.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat inventory ledger as a reliability-control decision, not an averages-only optimization. \"Prefer regional isolation over global coupling when blast radius is unclear\" is correct since it mitigates cross-region dependency not replicated while keeping containment local. The decision remains valid given: Cross-region latency variance increased during the event.",
      "detailedExplanation": "The decision turns on \"inventory ledger\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-rf-019",
      "type": "multiple-choice",
      "question": "Case Tau: fraud scoring service. Primary reliability risk is capacity cliff after zonal evacuation. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Reserve surge capacity for evacuation scenarios before declaring resilience.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For fraud scoring service, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Reserve surge capacity for evacuation scenarios before declaring resilience\" outperforms the alternatives because it targets capacity cliff after zonal evacuation and preserves safe recovery behavior. It is also the most compatible with This path mixes latency-sensitive and correctness-sensitive requests.",
      "detailedExplanation": "Read this as a scenario about \"fraud scoring service\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "rel-rf-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: tenant routing gateway. Primary reliability risk is asymmetric routing during partial failover. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Make replication lag SLOs first-class inputs to routing and promotion."
      ],
      "correct": 3,
      "explanation": "Tenant routing gateway should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Make replication lag SLOs first-class inputs to routing and promotion\" is strongest because it directly addresses asymmetric routing during partial failover and improves repeatability under stress. This aligns with the extra condition (The service has one hidden shared component with no backup path).",
      "detailedExplanation": "Read this as a scenario about \"tenant routing gateway\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "rel-rf-021",
      "type": "multiple-choice",
      "question": "Case Phi: payments write cluster. Primary reliability risk is insufficient N+1 headroom in one AZ. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Design active/passive boundaries per workload and validate capacity with N+1 drills.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat payments write cluster as a reliability-control decision, not an averages-only optimization. \"Design active/passive boundaries per workload and validate capacity with N+1 drills\" is correct since it mitigates insufficient N+1 headroom in one AZ while keeping containment local. The decision remains valid given: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "The decision turns on \"payments write cluster\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 1 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-022",
      "type": "multiple-choice",
      "question": "Case Chi: session state datastore. Primary reliability risk is split-brain during regional failover. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Gate failover on freshness and quorum-health checks, not only host availability.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For session state datastore, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Gate failover on freshness and quorum-health checks, not only host availability\" outperforms the alternatives because it targets split-brain during regional failover and preserves safe recovery behavior. It is also the most compatible with Change approval favors narrowly scoped policies over global flips.",
      "detailedExplanation": "Start from \"session state datastore\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-023",
      "type": "multiple-choice",
      "question": "Case Psi: search serving tier. Primary reliability risk is stale secondary promoted without quorum checks. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use controlled failback with write-fencing and reconciliation before traffic return.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Redundancy, Replication & Failover Strategy, search serving tier fails mainly through stale secondary promoted without quorum checks. The best choice is \"Use controlled failback with write-fencing and reconciliation before traffic return\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "The key clue in this question is \"search serving tier\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-rf-024",
      "type": "multiple-choice",
      "question": "Case Omega: control-plane metadata store. Primary reliability risk is replication lag violating RPO target. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Replicate control-plane state alongside data-plane state to avoid hidden SPOFs."
      ],
      "correct": 3,
      "explanation": "Control-plane metadata store should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Replicate control-plane state alongside data-plane state to avoid hidden SPOFs\" is strongest because it directly addresses replication lag violating RPO target and improves repeatability under stress. This aligns with the extra condition (On-call needs mitigation that is observable by explicit metrics).",
      "detailedExplanation": "The core signal here is \"control-plane metadata store\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-025",
      "type": "multiple-choice",
      "question": "Case Atlas: notification router. Primary reliability risk is manual failover runbook missing rollback gates. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Continuously verify standby parity through synthetic readiness probes.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat notification router as a reliability-control decision, not an averages-only optimization. \"Continuously verify standby parity through synthetic readiness probes\" is correct since it mitigates manual failover runbook missing rollback gates while keeping containment local. The decision remains valid given: A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "If you keep \"notification router\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-026",
      "type": "multiple-choice",
      "question": "Case Nova: feature config service. Primary reliability risk is failback causing replayed stale writes. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Define hot/warm/cold tiers by RTO objective and automate transitions.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For feature config service, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Define hot/warm/cold tiers by RTO objective and automate transitions\" outperforms the alternatives because it targets failback causing replayed stale writes and preserves safe recovery behavior. It is also the most compatible with Business impact is highest in the top 5% of critical flows.",
      "detailedExplanation": "This prompt is really about \"feature config service\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 5 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-027",
      "type": "multiple-choice",
      "question": "Case Orion: media pipeline scheduler. Primary reliability risk is warm standby drift from primary config. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Document failover runbooks with explicit abort criteria and ownership.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Redundancy, Replication & Failover Strategy, media pipeline scheduler fails mainly through warm standby drift from primary config. The best choice is \"Document failover runbooks with explicit abort criteria and ownership\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Regional failover is possible but expensive if used prematurely.",
      "detailedExplanation": "Use \"media pipeline scheduler\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-028",
      "type": "multiple-choice",
      "question": "Case Vega: inventory ledger. Primary reliability risk is cross-region dependency not replicated. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Prefer regional isolation over global coupling when blast radius is unclear."
      ],
      "correct": 3,
      "explanation": "Inventory ledger should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Prefer regional isolation over global coupling when blast radius is unclear\" is strongest because it directly addresses cross-region dependency not replicated and improves repeatability under stress. This aligns with the extra condition (A hot tenant currently consumes disproportionate worker capacity).",
      "detailedExplanation": "Read this as a scenario about \"inventory ledger\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "rel-rf-029",
      "type": "multiple-choice",
      "question": "Case Helios: fraud scoring service. Primary reliability risk is capacity cliff after zonal evacuation. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Reserve surge capacity for evacuation scenarios before declaring resilience.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat fraud scoring service as a reliability-control decision, not an averages-only optimization. \"Reserve surge capacity for evacuation scenarios before declaring resilience\" is correct since it mitigates capacity cliff after zonal evacuation while keeping containment local. The decision remains valid given: The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "The decision turns on \"fraud scoring service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "rel-rf-030",
      "type": "multiple-choice",
      "question": "Case Aurora: tenant routing gateway. Primary reliability risk is asymmetric routing during partial failover. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Make replication lag SLOs first-class inputs to routing and promotion.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Redundancy, Replication & Failover Strategy, tenant routing gateway fails mainly through asymmetric routing during partial failover. The best choice is \"Make replication lag SLOs first-class inputs to routing and promotion\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Queue age is rising even though average CPU appears normal.",
      "detailedExplanation": "Use \"tenant routing gateway\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-rf-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: payments write cluster. Primary reliability risk is insufficient N+1 headroom in one AZ. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Design active/passive boundaries per workload and validate capacity with N+1 drills.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Payments write cluster should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Design active/passive boundaries per workload and validate capacity with N+1 drills\" is strongest because it directly addresses insufficient N+1 headroom in one AZ and improves repeatability under stress. This aligns with the extra condition (A control-plane API is healthy but data-plane errors are increasing).",
      "detailedExplanation": "This prompt is really about \"payments write cluster\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 1 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "rel-rf-032",
      "type": "multiple-choice",
      "question": "Case Pulse: session state datastore. Primary reliability risk is split-brain during regional failover. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Gate failover on freshness and quorum-health checks, not only host availability."
      ],
      "correct": 3,
      "explanation": "Treat session state datastore as a reliability-control decision, not an averages-only optimization. \"Gate failover on freshness and quorum-health checks, not only host availability\" is correct since it mitigates split-brain during regional failover while keeping containment local. The decision remains valid given: Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "If you keep \"session state datastore\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-033",
      "type": "multiple-choice",
      "question": "Case Forge: search serving tier. Primary reliability risk is stale secondary promoted without quorum checks. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Use controlled failback with write-fencing and reconciliation before traffic return.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For search serving tier, prefer the option that prevents reoccurrence in Redundancy, Replication & Failover Strategy. \"Use controlled failback with write-fencing and reconciliation before traffic return\" outperforms the alternatives because it targets stale secondary promoted without quorum checks and preserves safe recovery behavior. It is also the most compatible with Legal/compliance constraints require explicit behavior in degraded mode.",
      "detailedExplanation": "The core signal here is \"search serving tier\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "rel-rf-034",
      "type": "multiple-choice",
      "question": "Case Harbor: control-plane metadata store. Primary reliability risk is replication lag violating RPO target. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Replicate control-plane state alongside data-plane state to avoid hidden SPOFs.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Redundancy, Replication & Failover Strategy, control-plane metadata store fails mainly through replication lag violating RPO target. The best choice is \"Replicate control-plane state alongside data-plane state to avoid hidden SPOFs\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "The key clue in this question is \"control-plane metadata store\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-035",
      "type": "multiple-choice",
      "question": "Case Vector: notification router. Primary reliability risk is manual failover runbook missing rollback gates. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Continuously verify standby parity through synthetic readiness probes.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Notification router should be solved at the failure boundary named in Redundancy, Replication & Failover Strategy. \"Continuously verify standby parity through synthetic readiness probes\" is strongest because it directly addresses manual failover runbook missing rollback gates and improves repeatability under stress. This aligns with the extra condition (User trust impact is tied to visible inconsistency, not only downtime).",
      "detailedExplanation": "Start from \"notification router\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for payments write cluster: signal points to replication lag violating RPO target. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for payments write cluster is mismatched to replication lag violating RPO target, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Redundancy, Replication & Failover Strategy: for payments write cluster, \"The design for payments write cluster is mismatched to replication lag violating RPO target, creating repeat reliability incidents\" is correct because it addresses replication lag violating RPO target and improves controllability.",
          "detailedExplanation": "Use \"incident diagnosis for payments write cluster: signal points to replication lag\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for payments write cluster: signal\" scenario, which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Define hot/warm/cold tiers by RTO objective and automate transitions.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Define hot/warm/cold tiers by RTO objective and automate transitions\" best matches In the \"incident diagnosis for payments write cluster: signal\" scenario, which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The decision turns on \"redundancy, Replication & Failover Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session state datastore: signal points to manual failover runbook missing rollback gates. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for session state datastore is mismatched to manual failover runbook missing rollback gates, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Redundancy, Replication & Failover Strategy, the best answer is \"The design for session state datastore is mismatched to manual failover runbook missing rollback gates, creating repeat reliability incidents\". It is the option most directly aligned to manual failover runbook missing rollback gates while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for session state datastore: signal points to manual failover\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident diagnosis for session state datastore: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Document failover runbooks with explicit abort criteria and ownership.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Redundancy, Replication & Failover Strategy: for After diagnosing \"incident diagnosis for session state datastore: signal\", which next change should be prioritized first, \"Document failover runbooks with explicit abort criteria and ownership\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"redundancy, Replication & Failover Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search serving tier: signal points to failback causing replayed stale writes. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for search serving tier is mismatched to failback causing replayed stale writes, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Search serving tier is a two-step reliability decision. At stage 1, \"The design for search serving tier is mismatched to failback causing replayed stale writes, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around failback causing replayed stale writes.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for search serving tier: signal points to failback causing replayed\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for search serving tier: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Prefer regional isolation over global coupling when blast radius is unclear."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Redundancy, Replication & Failover Strategy, the best answer is \"Prefer regional isolation over global coupling when blast radius is unclear\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Use \"redundancy, Replication & Failover Strategy\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for control-plane metadata store: signal points to warm standby drift from primary config. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for control-plane metadata store is mismatched to warm standby drift from primary config, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for control-plane metadata store is mismatched to warm standby drift from primary config, creating repeat reliability incidents\" best matches control-plane metadata store by targeting warm standby drift from primary config and lowering repeat risk.",
          "detailedExplanation": "Start from \"incident diagnosis for control-plane metadata store: signal points to warm standby\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident diagnosis for control-plane metadata store:\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Reserve surge capacity for evacuation scenarios before declaring resilience.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Now that \"incident diagnosis for control-plane metadata store:\" is diagnosed, what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Reserve surge capacity for evacuation scenarios before declaring resilience\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "This prompt is really about \"redundancy, Replication & Failover Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification router: signal points to cross-region dependency not replicated. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for notification router is mismatched to cross-region dependency not replicated, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Redundancy, Replication & Failover Strategy, the best answer is \"The design for notification router is mismatched to cross-region dependency not replicated, creating repeat reliability incidents\". It is the option most directly aligned to cross-region dependency not replicated while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"incident diagnosis for notification router: signal points to cross-region dependency\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for notification router: signal\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Make replication lag SLOs first-class inputs to routing and promotion.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Redundancy, Replication & Failover Strategy: for Now that \"incident diagnosis for notification router: signal\" is diagnosed, which next change should be prioritized first, \"Make replication lag SLOs first-class inputs to routing and promotion\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"redundancy, Replication & Failover Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature config service: signal points to capacity cliff after zonal evacuation. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for feature config service is mismatched to capacity cliff after zonal evacuation, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Feature config service is a two-step reliability decision. At stage 1, \"The design for feature config service is mismatched to capacity cliff after zonal evacuation, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around capacity cliff after zonal evacuation.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for feature config service: signal points to capacity cliff after\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for feature config service: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Design active/passive boundaries per workload and validate capacity with N+1 drills.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Redundancy, Replication & Failover Strategy, the best answer is \"Design active/passive boundaries per workload and validate capacity with N+1 drills\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"redundancy, Replication & Failover Strategy\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for media pipeline scheduler: signal points to asymmetric routing during partial failover. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for media pipeline scheduler is mismatched to asymmetric routing during partial failover, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for media pipeline scheduler is mismatched to asymmetric routing during partial failover, creating repeat reliability incidents\" best matches media pipeline scheduler by targeting asymmetric routing during partial failover and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for media pipeline scheduler: signal points to asymmetric routing\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for media pipeline scheduler: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Gate failover on freshness and quorum-health checks, not only host availability."
          ],
          "correct": 3,
          "explanation": "With diagnosis confirmed in \"incident diagnosis for media pipeline scheduler: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Gate failover on freshness and quorum-health checks, not only host availability\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"redundancy, Replication & Failover Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for inventory ledger: signal points to insufficient N+1 headroom in one AZ. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for inventory ledger is mismatched to insufficient N+1 headroom in one AZ, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Redundancy, Replication & Failover Strategy: for inventory ledger, \"The design for inventory ledger is mismatched to insufficient N+1 headroom in one AZ, creating repeat reliability incidents\" is correct because it addresses insufficient N+1 headroom in one AZ and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for inventory ledger: signal points to insufficient N+1 headroom in\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 1 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for inventory ledger: signal points\", which next change should be prioritized first?",
          "options": [
            "Use controlled failback with write-fencing and reconciliation before traffic return.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use controlled failback with write-fencing and reconciliation before traffic return\" best matches Given the diagnosis in \"incident diagnosis for inventory ledger: signal points\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"redundancy, Replication & Failover Strategy\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud scoring service: signal points to split-brain during regional failover. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for fraud scoring service is mismatched to split-brain during regional failover, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Redundancy, Replication & Failover Strategy, the best answer is \"The design for fraud scoring service is mismatched to split-brain during regional failover, creating repeat reliability incidents\". It is the option most directly aligned to split-brain during regional failover while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for fraud scoring service: signal points to split-brain during\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident diagnosis for fraud scoring service: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Replicate control-plane state alongside data-plane state to avoid hidden SPOFs.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Redundancy, Replication & Failover Strategy: for For \"incident diagnosis for fraud scoring service: signal\", what should change first before wider rollout, \"Replicate control-plane state alongside data-plane state to avoid hidden SPOFs\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Start from \"redundancy, Replication & Failover Strategy\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for tenant routing gateway: signal points to stale secondary promoted without quorum checks. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for tenant routing gateway is mismatched to stale secondary promoted without quorum checks, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Tenant routing gateway is a two-step reliability decision. At stage 1, \"The design for tenant routing gateway is mismatched to stale secondary promoted without quorum checks, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around stale secondary promoted without quorum checks.",
          "detailedExplanation": "If you keep \"incident diagnosis for tenant routing gateway: signal points to stale secondary\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for tenant routing gateway: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Continuously verify standby parity through synthetic readiness probes.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Redundancy, Replication & Failover Strategy, the best answer is \"Continuously verify standby parity through synthetic readiness probes\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"redundancy, Replication & Failover Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for payments write cluster: signal points to replication lag violating RPO target. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for payments write cluster is mismatched to replication lag violating RPO target, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for payments write cluster is mismatched to replication lag violating RPO target, creating repeat reliability incidents\" best matches payments write cluster by targeting replication lag violating RPO target and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for payments write cluster: signal points to replication lag\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for payments write cluster: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Define hot/warm/cold tiers by RTO objective and automate transitions."
          ],
          "correct": 3,
          "explanation": "After diagnosing \"incident diagnosis for payments write cluster: signal\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Define hot/warm/cold tiers by RTO objective and automate transitions\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"redundancy, Replication & Failover Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session state datastore: signal points to manual failover runbook missing rollback gates. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for session state datastore is mismatched to manual failover runbook missing rollback gates, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Redundancy, Replication & Failover Strategy: for session state datastore, \"The design for session state datastore is mismatched to manual failover runbook missing rollback gates, creating repeat reliability incidents\" is correct because it addresses manual failover runbook missing rollback gates and improves controllability.",
          "detailedExplanation": "Use \"incident diagnosis for session state datastore: signal points to manual failover\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for session state datastore: signal\" scenario, which next change should be prioritized first?",
          "options": [
            "Document failover runbooks with explicit abort criteria and ownership.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Document failover runbooks with explicit abort criteria and ownership\" best matches In the \"incident diagnosis for session state datastore: signal\" scenario, which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"redundancy, Replication & Failover Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search serving tier: signal points to failback causing replayed stale writes. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for search serving tier is mismatched to failback causing replayed stale writes, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Redundancy, Replication & Failover Strategy, the best answer is \"The design for search serving tier is mismatched to failback causing replayed stale writes, creating repeat reliability incidents\". It is the option most directly aligned to failback causing replayed stale writes while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"incident diagnosis for search serving tier: signal points to failback causing replayed\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for search serving tier: signal\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Prefer regional isolation over global coupling when blast radius is unclear.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Redundancy, Replication & Failover Strategy: for Now that \"incident diagnosis for search serving tier: signal\" is diagnosed, what is the highest-leverage change to make now, \"Prefer regional isolation over global coupling when blast radius is unclear\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"redundancy, Replication & Failover Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for control-plane metadata store: signal points to warm standby drift from primary config. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for control-plane metadata store is mismatched to warm standby drift from primary config, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Control-plane metadata store is a two-step reliability decision. At stage 1, \"The design for control-plane metadata store is mismatched to warm standby drift from primary config, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around warm standby drift from primary config.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for control-plane metadata store: signal points to warm standby\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for control-plane metadata store:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Reserve surge capacity for evacuation scenarios before declaring resilience.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Redundancy, Replication & Failover Strategy, the best answer is \"Reserve surge capacity for evacuation scenarios before declaring resilience\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"redundancy, Replication & Failover Strategy\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification router: signal points to cross-region dependency not replicated. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for notification router is mismatched to cross-region dependency not replicated, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Redundancy, Replication & Failover Strategy: for notification router, \"The design for notification router is mismatched to cross-region dependency not replicated, creating repeat reliability incidents\" is correct because it addresses cross-region dependency not replicated and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for notification router: signal points to cross-region dependency\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for notification router: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Make replication lag SLOs first-class inputs to routing and promotion."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Make replication lag SLOs first-class inputs to routing and promotion\" best matches Given the diagnosis in \"incident diagnosis for notification router: signal\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"redundancy, Replication & Failover Strategy\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature config service: signal points to capacity cliff after zonal evacuation. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for feature config service is mismatched to capacity cliff after zonal evacuation, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Redundancy, Replication & Failover Strategy, the best answer is \"The design for feature config service is mismatched to capacity cliff after zonal evacuation, creating repeat reliability incidents\". It is the option most directly aligned to capacity cliff after zonal evacuation while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for feature config service: signal points to capacity cliff after\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for feature config service: signal\", what first move gives the best reliability impact?",
          "options": [
            "Design active/passive boundaries per workload and validate capacity with N+1 drills.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Redundancy, Replication & Failover Strategy: for With diagnosis confirmed in \"incident diagnosis for feature config service: signal\", what first move gives the best reliability impact, \"Design active/passive boundaries per workload and validate capacity with N+1 drills\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The core signal here is \"redundancy, Replication & Failover Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for media pipeline scheduler: signal points to asymmetric routing during partial failover. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for media pipeline scheduler is mismatched to asymmetric routing during partial failover, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Media pipeline scheduler is a two-step reliability decision. At stage 1, \"The design for media pipeline scheduler is mismatched to asymmetric routing during partial failover, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around asymmetric routing during partial failover.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for media pipeline scheduler: signal points to asymmetric routing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for media pipeline scheduler: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Gate failover on freshness and quorum-health checks, not only host availability.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Redundancy, Replication & Failover Strategy, the best answer is \"Gate failover on freshness and quorum-health checks, not only host availability\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"redundancy, Replication & Failover Strategy\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for inventory ledger: signal points to insufficient N+1 headroom in one AZ. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for inventory ledger is mismatched to insufficient N+1 headroom in one AZ, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for inventory ledger is mismatched to insufficient N+1 headroom in one AZ, creating repeat reliability incidents\" best matches inventory ledger by targeting insufficient N+1 headroom in one AZ and lowering repeat risk.",
          "detailedExplanation": "Start from \"incident diagnosis for inventory ledger: signal points to insufficient N+1 headroom in\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 1 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for inventory ledger: signal points\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Use controlled failback with write-fencing and reconciliation before traffic return.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Now that \"incident diagnosis for inventory ledger: signal points\" is diagnosed, which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Use controlled failback with write-fencing and reconciliation before traffic return\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "This prompt is really about \"redundancy, Replication & Failover Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for fraud scoring service: signal points to split-brain during regional failover. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for fraud scoring service is mismatched to split-brain during regional failover, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Redundancy, Replication & Failover Strategy: for fraud scoring service, \"The design for fraud scoring service is mismatched to split-brain during regional failover, creating repeat reliability incidents\" is correct because it addresses split-brain during regional failover and improves controllability.",
          "detailedExplanation": "Use \"incident diagnosis for fraud scoring service: signal points to split-brain during\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for fraud scoring service: signal\" scenario, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Replicate control-plane state alongside data-plane state to avoid hidden SPOFs."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Replicate control-plane state alongside data-plane state to avoid hidden SPOFs\" best matches In the \"incident diagnosis for fraud scoring service: signal\" scenario, which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The decision turns on \"redundancy, Replication & Failover Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for tenant routing gateway: signal points to stale secondary promoted without quorum checks. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for tenant routing gateway is mismatched to stale secondary promoted without quorum checks, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Redundancy, Replication & Failover Strategy, the best answer is \"The design for tenant routing gateway is mismatched to stale secondary promoted without quorum checks, creating repeat reliability incidents\". It is the option most directly aligned to stale secondary promoted without quorum checks while preserving safe follow-on actions.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for tenant routing gateway: signal points to stale secondary\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident diagnosis for tenant routing gateway: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Continuously verify standby parity through synthetic readiness probes.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Redundancy, Replication & Failover Strategy: for After diagnosing \"incident diagnosis for tenant routing gateway: signal\", which immediate adjustment best addresses the risk, \"Continuously verify standby parity through synthetic readiness probes\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"redundancy, Replication & Failover Strategy\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for payments write cluster: signal points to replication lag violating RPO target. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for payments write cluster is mismatched to replication lag violating RPO target, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Payments write cluster is a two-step reliability decision. At stage 1, \"The design for payments write cluster is mismatched to replication lag violating RPO target, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around replication lag violating RPO target.",
          "detailedExplanation": "If you keep \"incident diagnosis for payments write cluster: signal points to replication lag\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for payments write cluster: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Define hot/warm/cold tiers by RTO objective and automate transitions.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Redundancy, Replication & Failover Strategy, the best answer is \"Define hot/warm/cold tiers by RTO objective and automate transitions\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"redundancy, Replication & Failover Strategy\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for session state datastore: signal points to manual failover runbook missing rollback gates. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for session state datastore is mismatched to manual failover runbook missing rollback gates, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for session state datastore is mismatched to manual failover runbook missing rollback gates, creating repeat reliability incidents\" best matches session state datastore by targeting manual failover runbook missing rollback gates and lowering repeat risk.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for session state datastore: signal points to manual failover\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for session state datastore: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Document failover runbooks with explicit abort criteria and ownership.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "With root cause identified for \"incident diagnosis for session state datastore: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Document failover runbooks with explicit abort criteria and ownership\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"redundancy, Replication & Failover Strategy\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search serving tier: signal points to failback causing replayed stale writes. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for search serving tier is mismatched to failback causing replayed stale writes, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Redundancy, Replication & Failover Strategy: for search serving tier, \"The design for search serving tier is mismatched to failback causing replayed stale writes, creating repeat reliability incidents\" is correct because it addresses failback causing replayed stale writes and improves controllability.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for search serving tier: signal points to failback causing replayed\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for search serving tier: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Prefer regional isolation over global coupling when blast radius is unclear."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Prefer regional isolation over global coupling when blast radius is unclear\" best matches For \"incident diagnosis for search serving tier: signal\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"redundancy, Replication & Failover Strategy\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for control-plane metadata store: signal points to warm standby drift from primary config. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for control-plane metadata store is mismatched to warm standby drift from primary config, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Redundancy, Replication & Failover Strategy, the best answer is \"The design for control-plane metadata store is mismatched to warm standby drift from primary config, creating repeat reliability incidents\". It is the option most directly aligned to warm standby drift from primary config while preserving safe follow-on actions.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for control-plane metadata store: signal points to warm standby\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for control-plane metadata store:\", what is the highest-leverage change to make now?",
          "options": [
            "Reserve surge capacity for evacuation scenarios before declaring resilience.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Redundancy, Replication & Failover Strategy: for Given the diagnosis in \"incident diagnosis for control-plane metadata store:\", what is the highest-leverage change to make now, \"Reserve surge capacity for evacuation scenarios before declaring resilience\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"redundancy, Replication & Failover Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for notification router: signal points to cross-region dependency not replicated. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for notification router is mismatched to cross-region dependency not replicated, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for notification router is mismatched to cross-region dependency not replicated, creating repeat reliability incidents\" best matches notification router by targeting cross-region dependency not replicated and lowering repeat risk.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for notification router: signal points to cross-region dependency\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for notification router: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Make replication lag SLOs first-class inputs to routing and promotion.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Given the diagnosis in \"incident diagnosis for notification router: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Make replication lag SLOs first-class inputs to routing and promotion\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"redundancy, Replication & Failover Strategy\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-061",
      "type": "multi-select",
      "question": "Mark all correct choices here: which indicators most directly reveal cross-domain blast radius.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Redundancy, Replication & Failover Strategy: The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "If you keep \"indicators most directly reveal cross-domain blast radius? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-062",
      "type": "multi-select",
      "question": "Mark all correct choices here: which controls reduce hidden single points of failure.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Mark all correct choices here: which controls reduce hidden single points of failure, the highest-signal answer is a bundle of controls. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "This prompt is really about \"controls reduce hidden single points of failure? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-063",
      "type": "multi-select",
      "question": "Mark all correct choices here: during partial failures, which practices improve diagnosis quality.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "In Redundancy, Replication & Failover Strategy, Mark all correct choices here: during partial failures, which practices improve diagnosis quality needs layered controls, not one silver bullet. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Use \"during partial failures, which practices improve diagnosis quality? (Select all that\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-064",
      "type": "multi-select",
      "question": "Mark all correct choices here: what belongs in a useful dependency failure taxonomy.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Mark all correct choices here: what belongs in a useful dependency failure taxonomy is intentionally multi-dimensional in Redundancy, Replication & Failover Strategy. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Read this as a scenario about \"belongs in a useful dependency failure taxonomy? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-065",
      "type": "multi-select",
      "question": "Mark all correct choices here: which patterns limit correlated failures across zones.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Redundancy, Replication & Failover Strategy: The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The decision turns on \"patterns limit correlated failures across zones? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-066",
      "type": "multi-select",
      "question": "Mark all correct choices here: which runbook elements increase incident execution reliability.",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For Mark all correct choices here: which runbook elements increase incident execution reliability, the highest-signal answer is a bundle of controls. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Start from \"runbook elements increase incident execution reliability? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-067",
      "type": "multi-select",
      "question": "Mark all correct choices here: which signals should trigger graceful isolation first.",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "In Redundancy, Replication & Failover Strategy, Mark all correct choices here: which signals should trigger graceful isolation first needs layered controls, not one silver bullet. The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The key clue in this question is \"signals should trigger graceful isolation first? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-068",
      "type": "multi-select",
      "question": "Mark all correct choices here: which architectural choices help contain tenant-induced overload.",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Mark all correct choices here: which architectural choices help contain tenant-induced overload is intentionally multi-dimensional in Redundancy, Replication & Failover Strategy. The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The core signal here is \"architectural choices help contain tenant-induced overload? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-069",
      "type": "multi-select",
      "question": "Mark all correct choices here: for reliability policies, which items should be explicit per endpoint.",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Redundancy, Replication & Failover Strategy: The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "If you keep \"for reliability policies, which items should be explicit per endpoint? (Select all that\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "rel-rf-070",
      "type": "multi-select",
      "question": "Mark all correct choices here: which anti-patterns commonly enlarge outage blast radius.",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Redundancy, Replication & Failover Strategy, Mark all correct choices here: which anti-patterns commonly enlarge outage blast radius needs layered controls, not one silver bullet. The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The key clue in this question is \"anti-patterns commonly enlarge outage blast radius? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-071",
      "type": "multi-select",
      "question": "Mark all correct choices here: what improves confidence in failover assumptions.",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Mark all correct choices here: what improves confidence in failover assumptions is intentionally multi-dimensional in Redundancy, Replication & Failover Strategy. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Start from \"improves confidence in failover assumptions? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-072",
      "type": "multi-select",
      "question": "Mark all correct choices here: which data is essential when classifying partial vs fail-stop incidents.",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Redundancy, Replication & Failover Strategy: The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The decision turns on \"data is essential when classifying partial vs fail-stop incidents? (Select all that\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-073",
      "type": "multi-select",
      "question": "Mark all correct choices here: which controls improve safety when control-plane health is uncertain.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Mark all correct choices here: which controls improve safety when control-plane health is uncertain, the highest-signal answer is a bundle of controls. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Read this as a scenario about \"controls improve safety when control-plane health is uncertain? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-074",
      "type": "multi-select",
      "question": "Mark all correct choices here: for critical writes, which guardrails reduce corruption risk under faults.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Redundancy, Replication & Failover Strategy, Mark all correct choices here: for critical writes, which guardrails reduce corruption risk under faults needs layered controls, not one silver bullet. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Use \"for critical writes, which guardrails reduce corruption risk under faults? (Select all\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-075",
      "type": "multi-select",
      "question": "Mark all correct choices here: which recurring reviews keep reliability boundaries accurate over time.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "Mark all correct choices here: which recurring reviews keep reliability boundaries accurate over time is intentionally multi-dimensional in Redundancy, Replication & Failover Strategy. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "This prompt is really about \"recurring reviews keep reliability boundaries accurate over time? (Select all that\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-076",
      "type": "multi-select",
      "question": "Mark all correct choices here: which decisions help teams align on reliability trade-offs during incidents.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Redundancy, Replication & Failover Strategy: The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "If you keep \"decisions help teams align on reliability trade-offs during incidents? (Select all that\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-077",
      "type": "multi-select",
      "question": "Mark all correct choices here: what evidence best shows a mitigation reduced recurrence risk.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Mark all correct choices here: what evidence best shows a mitigation reduced recurrence risk, the highest-signal answer is a bundle of controls. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The core signal here is \"evidence best shows a mitigation reduced recurrence risk? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-078",
      "type": "numeric-input",
      "question": "A service processes 4,200,000 requests/day and 0.22% violate reliability SLO. Estimate this: how many violations/day.",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "For A service processes 4,200,000 requests/day and 0, the computed target in Redundancy, Replication & Failover Strategy is 9240 requests. Responses within +/-3% indicate sound sizing judgment.",
      "detailedExplanation": "The key clue in this question is \"service processes 4,200,000 requests/day and 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 4,200 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-079",
      "type": "numeric-input",
      "question": "Incident queue receives 1,800 items/min and drains 2,050 items/min. Estimate this: net drain rate.",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "Redundancy, Replication & Failover Strategy expects quick quantitative triage: Incident queue receives 1,800 items/min and drains 2,050 items/min evaluates to 250 items/min. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "Start from \"incident queue receives 1,800 items/min and drains 2,050 items/min\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 1,800 and 2,050 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "rel-rf-080",
      "type": "numeric-input",
      "question": "Retry policy adds 0.35 extra attempts per request at 60,000 req/sec. Estimate this: effective attempts/sec.",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Use first-pass reliability arithmetic for Retry policy adds 0: 81000 attempts/sec. Answers within +/-2% show correct directional reasoning for Redundancy, Replication & Failover Strategy.",
      "detailedExplanation": "Start from \"retry policy adds 0\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 0.35 and 60,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-081",
      "type": "numeric-input",
      "question": "Failover takes 18 seconds and happens 21 times/day. Estimate this: total failover seconds/day.",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "For Failover takes 18 seconds and happens 21 times/day, the computed target in Redundancy, Replication & Failover Strategy is 378 seconds. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "The key clue in this question is \"failover takes 18 seconds and happens 21 times/day\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 18 seconds and 21 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-082",
      "type": "numeric-input",
      "question": "Target p99 latency is 700ms; observed p99 is 980ms. Estimate this: percent over target.",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Redundancy, Replication & Failover Strategy expects quick quantitative triage: Target p99 latency is 700ms; observed p99 is 980ms evaluates to 40 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "Read this as a scenario about \"target p99 latency is 700ms\". Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 700ms and 980ms appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-083",
      "type": "numeric-input",
      "question": "What is the best answer here: if 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "The operational math for What is the best answer here: if 31% of 120,000 requests/min are critical-path, how many critical requests/min gives 37200 requests/min. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "The decision turns on \"if 31% of 120,000 requests/min are critical-path, how many critical requests/min\". Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 31 and 120,000 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.2% to 0.3%. Estimate this: percent reduction.",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Error rate drops from 1: 75 %. Answers within +/-30% show correct directional reasoning for Redundancy, Replication & Failover Strategy.",
      "detailedExplanation": "This prompt is really about \"error rate drops from 1\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 1.2 and 0.3 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-085",
      "type": "numeric-input",
      "question": "A 7-node quorum system requires majority writes. Estimate this: minimum acknowledgements required.",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "For A 7-node quorum system requires majority writes, the computed target in Redundancy, Replication & Failover Strategy is 4 acks. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "Use \"7-node quorum system requires majority writes\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 7 and 4 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-086",
      "type": "numeric-input",
      "question": "Backlog is 48,000 tasks and net drain is 320 tasks/min. Estimate this: minutes to clear backlog.",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Redundancy, Replication & Failover Strategy expects quick quantitative triage: Backlog is 48,000 tasks and net drain is 320 tasks/min evaluates to 150 minutes. Any answer within +/-0% is acceptable.",
      "detailedExplanation": "The core signal here is \"backlog is 48,000 tasks and net drain is 320 tasks/min\". Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 48,000 and 320 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-087",
      "type": "numeric-input",
      "question": "A system with 14 zones has 2 unavailable. Estimate this: what percent remain available.",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for A system with 14 zones has 2 unavailable gives 85.71 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "If you keep \"system with 14 zones has 2 unavailable\" in view, the correct answer separates faster. Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 14 and 2 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-088",
      "type": "numeric-input",
      "question": "MTTR improved from 45 min to 30 min. Estimate this: percent reduction.",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for MTTR improved from 45 min to 30 min: 33.33 %. Answers within +/-30% show correct directional reasoning for Redundancy, Replication & Failover Strategy.",
      "detailedExplanation": "Start from \"mTTR improved from 45 min to 30 min\", then pressure-test the result against the options. Normalize units before computing so conversion mistakes do not propagate. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 45 min and 30 min appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-089",
      "type": "numeric-input",
      "question": "What is the best answer here: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "For What is the best answer here: if 9% of 2,500,000 daily operations need manual recovery checks, checks/day, the computed target in Redundancy, Replication & Failover Strategy is 225000 operations. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "The key clue in this question is \"if 9% of 2,500,000 daily operations need manual recovery checks, checks/day\". Normalize units before computing so conversion mistakes do not propagate. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 9 and 2,500 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle. Focus on redundancy, replication & failover strategy tradeoffs.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Redundancy, Replication & Failover Strategy emphasizes safe recovery order. Beginning at Detect and scope affected fault domains and finishing at Validate recovery and harden recurrence defenses keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The decision turns on \"order a reliability response lifecycle\". Order by relative scale and bottleneck effect, then validate neighboring items. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-091",
      "type": "ordering",
      "question": "For redundancy, replication & failover strategy, order from lowest to highest reliability risk.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For redundancy, replication & failover strategy, order from lowest to highest reliability risk, the correct ordering runs from Isolated dependency with fallback and budget to Implicit dependency with no failure policy. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Read this as a scenario about \"order from lowest to highest reliability risk\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-092",
      "type": "ordering",
      "question": "Within redundancy, replication & failover strategy, order failover safety steps.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Redundancy, Replication & Failover Strategy should start with Verify candidate health and freshness and end with Run failback readiness checks before restoration. Within redundancy, replication & failover strategy, order failover safety steps rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The key clue in this question is \"order failover safety steps\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-093",
      "type": "ordering",
      "question": "In this redundancy, replication & failover strategy context, order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: No admission limits must happen before Priority-aware shedding plus per-domain concurrency bounds. That ordering matches incident-safe flow in Redundancy, Replication & Failover Strategy.",
      "detailedExplanation": "Start from \"order by increasing overload-protection strength\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-094",
      "type": "ordering",
      "question": "Considering redundancy, replication & failover strategy, order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Redundancy, Replication & Failover Strategy emphasizes safe recovery order. Beginning at Select recovery point by RPO target and finishing at Promote and re-enable writes with monitoring keeps blast radius controlled while restoring service.",
      "detailedExplanation": "If you keep \"order data recovery execution\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-095",
      "type": "ordering",
      "question": "From a redundancy, replication & failover strategy viewpoint, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a redundancy, replication & failover strategy viewpoint, order reliability operations loop, the correct ordering runs from Define SLIs tied to user impact to Review incidents and close corrective actions. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The core signal here is \"order reliability operations loop\". Order by relative scale and bottleneck effect, then validate neighboring items. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-096",
      "type": "ordering",
      "question": "Arrange from least to greatest blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Redundancy, Replication & Failover Strategy should start with Single process failure and end with Cross-region control-plane failure. Arrange from least to greatest blast radius rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Use \"order by increasing blast radius\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-097",
      "type": "ordering",
      "question": "Order retry-policy maturity. Use a redundancy, replication & failover strategy perspective.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Fixed immediate retries must happen before Jittered backoff with retry budgets and telemetry. That ordering matches incident-safe flow in Redundancy, Replication & Failover Strategy.",
      "detailedExplanation": "This prompt is really about \"order retry-policy maturity\". Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-098",
      "type": "ordering",
      "question": "Order degradation sophistication. Focus on redundancy, replication & failover strategy tradeoffs.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Redundancy, Replication & Failover Strategy emphasizes safe recovery order. Beginning at Undocumented ad hoc fallback and finishing at Automated policy-driven degradation with user semantics keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The decision turns on \"order degradation sophistication\". Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "rel-rf-099",
      "type": "ordering",
      "question": "For redundancy, replication & failover strategy, order incident command rigor.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For redundancy, replication & failover strategy, order incident command rigor, the correct ordering runs from Ad hoc responders with no roles to Role-defined operations plus decision log and action tracking. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "Read this as a scenario about \"order incident command rigor\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "rel-rf-100",
      "type": "ordering",
      "question": "Within redundancy, replication & failover strategy, order reliability validation confidence.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Redundancy, Replication & Failover Strategy should start with Single success in staging and end with Sustained recovery plus recurrence drill pass. Within redundancy, replication & failover strategy, order reliability validation confidence rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The key clue in this question is \"order reliability validation confidence\". Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    }
  ]
}
