{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 7,
  "chapterTitle": "Compute Selection & Platform Trade-offs",
  "chapterDescription": "Choosing between VMs, containers, serverless, and managed runtimes using workload shape, reliability goals, and operations/cost trade-offs.",
  "problems": [
    {
      "id": "sc-pt-001",
      "type": "multiple-choice",
      "question": "A team runs a latency-sensitive API gateway. Their main concern is cold-start latency penalties. Which compute-platform decision is strongest? Recent incident data shows rollback speed is now a top requirement.",
      "options": [
        "For this workload, prefer containers on orchestrator with autoscaling and pod-level isolation controls.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Given the observed bottleneck and guardrails, compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a latency-sensitive API gateway, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The decision turns on \"team runs a latency-sensitive API gateway\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-002",
      "type": "multiple-choice",
      "question": "A team runs a spiky image thumbnail endpoint. Their main concern is bin-packing efficiency vs noisy neighbors. Which compute-platform decision is strongest? Latency and cost both regressed after the last platform change.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer serverless for burst-heavy short-lived handlers with strict timeout fit."
      ],
      "correct": 3,
      "explanation": "From an incident-first perspective, compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a spiky image thumbnail endpoint, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"team runs a spiky image thumbnail endpoint\", then pressure-test the result against the options. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-003",
      "type": "multiple-choice",
      "question": "A team runs a steady background ETL workers. Their main concern is ops overhead and patching burden. Which compute-platform decision is strongest? On-call load rose due to weak operational ergonomics.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer VMs when host-level control/compliance and stable workloads dominate.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Under the stated reliability and cost constraints, compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a steady background ETL workers, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The key clue in this question is \"team runs a steady background ETL workers\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-004",
      "type": "multiple-choice",
      "question": "A team runs a event-driven notification fanout. Their main concern is deployment velocity and rollback control. Which compute-platform decision is strongest? Tail latency sensitivity increased after product growth.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, use mixed model: serverless ingress + containerized core services.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Prioritizing blast-radius reduction first, compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a event-driven notification fanout, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The core signal here is \"team runs a event-driven notification fanout\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-005",
      "type": "multiple-choice",
      "question": "A team runs a GPU-bound inference service. Their main concern is portability vs managed-service lock-in. Which compute-platform decision is strongest? Compliance review added stronger tenancy-isolation constraints.",
      "options": [
        "For this workload, choose managed runtime for speed, but enforce portability seams at boundaries.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "With latency and correctness objectives explicit, compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a GPU-bound inference service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "If you keep \"team runs a GPU-bound inference service\" in view, the correct answer separates faster. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-006",
      "type": "multiple-choice",
      "question": "A team runs a long-running media transcoding jobs. Their main concern is cost predictability under burst load. Which compute-platform decision is strongest? Traffic burstiness is higher than prior capacity assumptions.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, use dedicated node pools for noisy-neighbor-sensitive workloads."
      ],
      "correct": 3,
      "explanation": "Looking at rollback safety and operational load, compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a long-running media transcoding jobs, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "This prompt is really about \"team runs a long-running media transcoding jobs\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-007",
      "type": "multiple-choice",
      "question": "A team runs a stateful websocket chat service. Their main concern is runtime limits for long-lived tasks. Which compute-platform decision is strongest? Dependency startup overhead dominates low-traffic periods.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, shift long-running jobs off strict serverless limits into container workers.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a stateful websocket chat service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "Use \"team runs a stateful websocket chat service\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-008",
      "type": "multiple-choice",
      "question": "A team runs a bursty webhook processor. Their main concern is debugging/observability depth requirements. Which compute-platform decision is strongest? Regional expansion exposed portability gaps in runtime choices.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, opt for containers with canary/blue-green rollout automation.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a bursty webhook processor, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"team runs a bursty webhook processor\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-009",
      "type": "multiple-choice",
      "question": "A team runs a compliance batch scanner. Their main concern is per-tenant isolation requirements. Which compute-platform decision is strongest? Noisy-neighbor contention appears in peak-hour traces.",
      "options": [
        "For this workload, segment tenants with per-workload isolation tiers.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a compliance batch scanner, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The decision turns on \"team runs a compliance batch scanner\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-010",
      "type": "multiple-choice",
      "question": "A team runs a high-QPS key-value read API. Their main concern is compliance boundary and host control. Which compute-platform decision is strongest? Team capacity for infrastructure operations is currently limited.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, adopt managed platform where undifferentiated ops cost exceeds lock-in risk."
      ],
      "correct": 3,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a high-QPS key-value read API, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"team runs a high-QPS key-value read API\" as your starting point, then verify tradeoffs carefully. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-011",
      "type": "multiple-choice",
      "question": "A team runs a multi-tenant cron execution service. Their main concern is cold-start latency penalties. Which compute-platform decision is strongest? Feature rollout cadence is blocked by environment drift.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer containers on orchestrator with autoscaling and pod-level isolation controls.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a multi-tenant cron execution service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "This prompt is really about \"team runs a multi-tenant cron execution service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-012",
      "type": "multiple-choice",
      "question": "A team runs a search query frontend. Their main concern is bin-packing efficiency vs noisy neighbors. Which compute-platform decision is strongest? Workload duration moved beyond prior runtime limits.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer serverless for burst-heavy short-lived handlers with strict timeout fit.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a search query frontend, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "If you keep \"team runs a search query frontend\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-013",
      "type": "multiple-choice",
      "question": "A team runs a document rendering service. Their main concern is ops overhead and patching burden. Which compute-platform decision is strongest? Observability needs now require deeper runtime controls.",
      "options": [
        "For this workload, prefer VMs when host-level control/compliance and stable workloads dominate.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a document rendering service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The core signal here is \"team runs a document rendering service\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-014",
      "type": "multiple-choice",
      "question": "A team runs a fraud scoring microservice. Their main concern is deployment velocity and rollback control. Which compute-platform decision is strongest? Cost per request became unpredictable under seasonal spikes.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, use mixed model: serverless ingress + containerized core services."
      ],
      "correct": 3,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a fraud scoring microservice, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The key clue in this question is \"team runs a fraud scoring microservice\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-015",
      "type": "multiple-choice",
      "question": "A team runs a scheduled billing pipeline. Their main concern is portability vs managed-service lock-in. Which compute-platform decision is strongest? Platform lock-in risk was raised in architecture review.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, choose managed runtime for speed, but enforce portability seams at boundaries.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a scheduled billing pipeline, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "Start from \"team runs a scheduled billing pipeline\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-016",
      "type": "multiple-choice",
      "question": "A team runs a control-plane reconciliation loop. Their main concern is cost predictability under burst load. Which compute-platform decision is strongest? Failure drills revealed weak fallback behavior on cutover.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, use dedicated node pools for noisy-neighbor-sensitive workloads.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a control-plane reconciliation loop, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"team runs a control-plane reconciliation loop\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-017",
      "type": "multiple-choice",
      "question": "A team runs a ML feature extraction workers. Their main concern is runtime limits for long-lived tasks. Which compute-platform decision is strongest? Service startup profile makes cold paths user-visible.",
      "options": [
        "For this workload, shift long-running jobs off strict serverless limits into container workers.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a ML feature extraction workers, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "Read this as a scenario about \"team runs a ML feature extraction workers\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-018",
      "type": "multiple-choice",
      "question": "A team runs a tenant-isolated report generation. Their main concern is debugging/observability depth requirements. Which compute-platform decision is strongest? The same workload class now needs stricter performance guarantees.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, opt for containers with canary/blue-green rollout automation."
      ],
      "correct": 3,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a tenant-isolated report generation, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Use \"team runs a tenant-isolated report generation\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-019",
      "type": "multiple-choice",
      "question": "A team runs a internal admin API. Their main concern is per-tenant isolation requirements. Which compute-platform decision is strongest? Migration plan requires safer phased rollout behavior.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, segment tenants with per-workload isolation tiers.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a internal admin API, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "This prompt is really about \"team runs a internal admin API\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-020",
      "type": "multiple-choice",
      "question": "A team runs a global edge request validator. Their main concern is compliance boundary and host control. Which compute-platform decision is strongest? Cross-team ownership boundaries now affect platform fit.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, adopt managed platform where undifferentiated ops cost exceeds lock-in risk.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a global edge request validator, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"team runs a global edge request validator\", then pressure-test the result against the options. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-021",
      "type": "multiple-choice",
      "question": "A team runs a latency-sensitive API gateway. Their main concern is cold-start latency penalties. Which compute-platform decision is strongest? Infrastructure patch burden is competing with product work.",
      "options": [
        "For this workload, prefer containers on orchestrator with autoscaling and pod-level isolation controls.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a latency-sensitive API gateway, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The key clue in this question is \"team runs a latency-sensitive API gateway\". Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-022",
      "type": "multiple-choice",
      "question": "A team runs a spiky image thumbnail endpoint. Their main concern is bin-packing efficiency vs noisy neighbors. Which compute-platform decision is strongest? Model size growth stressed existing deployment assumptions.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer serverless for burst-heavy short-lived handlers with strict timeout fit."
      ],
      "correct": 3,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a spiky image thumbnail endpoint, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Read this as a scenario about \"team runs a spiky image thumbnail endpoint\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-023",
      "type": "multiple-choice",
      "question": "A team runs a steady background ETL workers. Their main concern is ops overhead and patching burden. Which compute-platform decision is strongest? Connection-heavy paths require stricter graceful-shutdown behavior.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer VMs when host-level control/compliance and stable workloads dominate.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a steady background ETL workers, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The decision turns on \"team runs a steady background ETL workers\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-024",
      "type": "multiple-choice",
      "question": "A team runs a event-driven notification fanout. Their main concern is deployment velocity and rollback control. Which compute-platform decision is strongest? Batch windows are tightening against current runtime ceilings.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, use mixed model: serverless ingress + containerized core services.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a event-driven notification fanout, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "This prompt is really about \"team runs a event-driven notification fanout\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-025",
      "type": "multiple-choice",
      "question": "A team runs a GPU-bound inference service. Their main concern is portability vs managed-service lock-in. Which compute-platform decision is strongest? Canary data showed heterogeneous workload fit across teams.",
      "options": [
        "For this workload, choose managed runtime for speed, but enforce portability seams at boundaries.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a GPU-bound inference service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "Use \"team runs a GPU-bound inference service\" as your starting point, then verify tradeoffs carefully. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-026",
      "type": "multiple-choice",
      "question": "A team runs a long-running media transcoding jobs. Their main concern is cost predictability under burst load. Which compute-platform decision is strongest? Managed-service boundaries changed after vendor update.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, use dedicated node pools for noisy-neighbor-sensitive workloads."
      ],
      "correct": 3,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a long-running media transcoding jobs, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The core signal here is \"team runs a long-running media transcoding jobs\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-027",
      "type": "multiple-choice",
      "question": "A team runs a stateful websocket chat service. Their main concern is runtime limits for long-lived tasks. Which compute-platform decision is strongest? Container density gains came with intermittent latency spikes.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, shift long-running jobs off strict serverless limits into container workers.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a stateful websocket chat service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "If you keep \"team runs a stateful websocket chat service\" in view, the correct answer separates faster. Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "RFC 6455: The WebSocket Protocol",
          "url": "https://www.rfc-editor.org/rfc/rfc6455"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-028",
      "type": "multiple-choice",
      "question": "A team runs a bursty webhook processor. Their main concern is debugging/observability depth requirements. Which compute-platform decision is strongest? Unexpected timeout rates increased under concurrent bursts.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, opt for containers with canary/blue-green rollout automation.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a bursty webhook processor, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "Start from \"team runs a bursty webhook processor\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-029",
      "type": "multiple-choice",
      "question": "A team runs a compliance batch scanner. Their main concern is per-tenant isolation requirements. Which compute-platform decision is strongest? Long-tail jobs now dominate compute spend variance.",
      "options": [
        "For this workload, segment tenants with per-workload isolation tiers.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a compliance batch scanner, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The key clue in this question is \"team runs a compliance batch scanner\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-030",
      "type": "multiple-choice",
      "question": "A team runs a high-QPS key-value read API. Their main concern is compliance boundary and host control. Which compute-platform decision is strongest? Governance requires clearer change control and auditability.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, adopt managed platform where undifferentiated ops cost exceeds lock-in risk."
      ],
      "correct": 3,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a high-QPS key-value read API, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The decision turns on \"team runs a high-QPS key-value read API\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-031",
      "type": "multiple-choice",
      "question": "A team runs a multi-tenant cron execution service. Their main concern is cold-start latency penalties. Which compute-platform decision is strongest? SLOs are now stricter for customer-facing request paths.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer containers on orchestrator with autoscaling and pod-level isolation controls.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a multi-tenant cron execution service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "Read this as a scenario about \"team runs a multi-tenant cron execution service\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-032",
      "type": "multiple-choice",
      "question": "A team runs a search query frontend. Their main concern is bin-packing efficiency vs noisy neighbors. Which compute-platform decision is strongest? Platform standardization is clashing with workload diversity.",
      "options": [
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, prefer serverless for burst-heavy short-lived handlers with strict timeout fit.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity."
      ],
      "correct": 1,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a search query frontend, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "The key clue in this question is \"team runs a search query frontend\". Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-033",
      "type": "multiple-choice",
      "question": "A team runs a document rendering service. Their main concern is ops overhead and patching burden. Which compute-platform decision is strongest? Release velocity targets now require safer progressive delivery.",
      "options": [
        "For this workload, prefer VMs when host-level control/compliance and stable workloads dominate.",
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior."
      ],
      "correct": 0,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a document rendering service, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "Start from \"team runs a document rendering service\", then pressure-test the result against the options. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-034",
      "type": "multiple-choice",
      "question": "A team runs a fraud scoring microservice. Their main concern is deployment velocity and rollback control. Which compute-platform decision is strongest? Cost optimization must avoid sacrificing reliability posture.",
      "options": [
        "Standardize every workload on one compute type regardless constraints.",
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, use mixed model: serverless ingress + containerized core services."
      ],
      "correct": 3,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a fraud scoring microservice, this is the strongest fit in Compute Selection & Platform Trade-offs. Keep mitigation tied to the stated constraints.",
      "detailedExplanation": "If you keep \"team runs a fraud scoring microservice\" in view, the correct answer separates faster. Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-035",
      "type": "multiple-choice",
      "question": "A team runs a scheduled billing pipeline. Their main concern is portability vs managed-service lock-in. Which compute-platform decision is strongest? Legacy runtime constraints are blocking architecture simplification.",
      "options": [
        "Maximize portability only and ignore operational velocity.",
        "Treat cost as fixed and avoid measuring runtime behavior.",
        "For this workload, choose managed runtime for speed, but enforce portability seams at boundaries.",
        "Standardize every workload on one compute type regardless constraints."
      ],
      "correct": 2,
      "explanation": "Compute selection should match workload shape, operational constraints, and economic behavior rather than forcing a single platform. For A team runs a scheduled billing pipeline, this is the strongest fit in Compute Selection & Platform Trade-offs.",
      "detailedExplanation": "The core signal here is \"team runs a scheduled billing pipeline\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a long-running media transcoding jobs, incident reviews show repeated issues tied to deployment velocity and rollback control. What is the most likely diagnosis?",
          "options": [
            "Current platform choice mismatches long-running media transcoding jobs constraints around deployment velocity and rollback control.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability."
          ],
          "correct": 0,
          "explanation": "long-running media transcoding jobs is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The decision turns on \"for a long-running media transcoding jobs, incident reviews show repeated issues tied\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for long-running media transcoding jobs, what is the strongest next platform move while preserving rollback speed?",
          "options": [
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: choose managed runtime for speed, but enforce portability seams at boundaries, then validate latency/cost/operability before broad rollout."
          ],
          "correct": 3,
          "explanation": "Adopt the targeted fit correction for long-running media transcoding jobs: Implement this next: choose managed runtime for speed, but enforce portability seams at boundaries, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Start from \"after confirming diagnosis for long-running media transcoding jobs, what is the\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: propagating an early bad assumption through all steps."
        }
      ],
      "detailedExplanation": "Use \"compute Selection & Platform Trade-offs\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a stateful websocket chat service, incident reviews show repeated issues tied to portability vs managed-service lock-in. What is the most likely diagnosis?",
          "options": [
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches stateful websocket chat service constraints around portability vs managed-service lock-in."
          ],
          "correct": 3,
          "explanation": "stateful websocket chat service is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Start from \"for a stateful websocket chat service, incident reviews show repeated issues tied to\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for stateful websocket chat service, what is the strongest next platform move under tighter latency SLOs?",
          "options": [
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: use dedicated node pools for noisy-neighbor-sensitive workloads, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged."
          ],
          "correct": 2,
          "explanation": "Adopt the targeted fit correction for stateful websocket chat service: Implement this next: use dedicated node pools for noisy-neighbor-sensitive workloads, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis for stateful websocket chat service, what is the strongest\". Do not reset assumptions between stages; carry forward prior constraints directly. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: propagating an early bad assumption through all steps."
        }
      ],
      "detailedExplanation": "This prompt is really about \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a bursty webhook processor, incident reviews show repeated issues tied to cost predictability under burst load. What is the most likely diagnosis?",
          "options": [
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches bursty webhook processor constraints around cost predictability under burst load.",
            "Cloud region selection is always the root cause of platform fit issues."
          ],
          "correct": 2,
          "explanation": "bursty webhook processor is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Use \"for a bursty webhook processor, incident reviews show repeated issues tied to cost\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis for bursty webhook processor, what is the strongest next platform move with constrained ops bandwidth?",
          "options": [
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: shift long-running jobs off strict serverless limits into container workers, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding."
          ],
          "correct": 1,
          "explanation": "Adopt the targeted fit correction for bursty webhook processor: Implement this next: shift long-running jobs off strict serverless limits into container workers, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis for bursty webhook processor, what is the strongest next\". Solve this as chained reasoning where stage two must respect stage one assumptions. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Common pitfall: missing protocol/compression overhead in capacity math."
        }
      ],
      "detailedExplanation": "The decision turns on \"compute Selection & Platform Trade-offs\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a compliance batch scanner, incident reviews show repeated issues tied to runtime limits for long-lived tasks. What is the most likely diagnosis?",
          "options": [
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches compliance batch scanner constraints around runtime limits for long-lived tasks.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless."
          ],
          "correct": 1,
          "explanation": "compliance batch scanner is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Read this as a scenario about \"for a compliance batch scanner, incident reviews show repeated issues tied to runtime\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for compliance batch scanner, what is the strongest next platform move without worsening lock-in exposure?",
          "options": [
            "Implement this next: opt for containers with canary/blue-green rollout automation, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing."
          ],
          "correct": 0,
          "explanation": "Adopt the targeted fit correction for compliance batch scanner: Implement this next: opt for containers with canary/blue-green rollout automation, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis for compliance batch scanner, what is the strongest next\". Do not reset assumptions between stages; carry forward prior constraints directly. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a high-QPS key-value read API, incident reviews show repeated issues tied to debugging/observability depth requirements. What is the most likely diagnosis?",
          "options": [
            "Current platform choice mismatches high-QPS key-value read API constraints around debugging/observability depth requirements.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability."
          ],
          "correct": 0,
          "explanation": "high-QPS key-value read API is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The core signal here is \"for a high-QPS key-value read API, incident reviews show repeated issues tied to\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for high-QPS key-value read API, what is the strongest next platform move while controlling noisy-neighbor risk?",
          "options": [
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: segment tenants with per-workload isolation tiers, then validate latency/cost/operability before broad rollout."
          ],
          "correct": 3,
          "explanation": "Adopt the targeted fit correction for high-QPS key-value read API: Implement this next: segment tenants with per-workload isolation tiers, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Use \"after confirming diagnosis for high-QPS key-value read API, what is the strongest next\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "The core signal here is \"compute Selection & Platform Trade-offs\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a multi-tenant cron execution service, incident reviews show repeated issues tied to per-tenant isolation requirements. What is the most likely diagnosis?",
          "options": [
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches multi-tenant cron execution service constraints around per-tenant isolation requirements."
          ],
          "correct": 3,
          "explanation": "multi-tenant cron execution service is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The key clue in this question is \"for a multi-tenant cron execution service, incident reviews show repeated issues tied\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for multi-tenant cron execution service, what is the strongest next platform move with predictable burst-cost behavior?",
          "options": [
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: adopt managed platform where undifferentiated ops cost exceeds lock-in risk, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged."
          ],
          "correct": 2,
          "explanation": "Adopt the targeted fit correction for multi-tenant cron execution service: Implement this next: adopt managed platform where undifferentiated ops cost exceeds lock-in risk, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis for multi-tenant cron execution service, what is the\". Solve this as chained reasoning where stage two must respect stage one assumptions. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: skipping anchor checks against known scale."
        }
      ],
      "detailedExplanation": "If you keep \"compute Selection & Platform Trade-offs\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a search query frontend, incident reviews show repeated issues tied to compliance boundary and host control. What is the most likely diagnosis?",
          "options": [
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches search query frontend constraints around compliance boundary and host control.",
            "Cloud region selection is always the root cause of platform fit issues."
          ],
          "correct": 2,
          "explanation": "search query frontend is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Start from \"for a search query frontend, incident reviews show repeated issues tied to compliance\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for search query frontend, what is the strongest next platform move under stricter compliance boundaries?",
          "options": [
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: prefer containers on orchestrator with autoscaling and pod-level isolation controls, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding."
          ],
          "correct": 1,
          "explanation": "Adopt the targeted fit correction for search query frontend: Implement this next: prefer containers on orchestrator with autoscaling and pod-level isolation controls, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis for search query frontend, what is the strongest next\". Do not reset assumptions between stages; carry forward prior constraints directly. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: accepting implausible outputs because arithmetic is clean."
        }
      ],
      "detailedExplanation": "This prompt is really about \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a document rendering service, incident reviews show repeated issues tied to cold-start latency penalties. What is the most likely diagnosis?",
          "options": [
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches document rendering service constraints around cold-start latency penalties.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless."
          ],
          "correct": 1,
          "explanation": "document rendering service is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The decision turns on \"for a document rendering service, incident reviews show repeated issues tied to\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for document rendering service, what is the strongest next platform move while keeping deployment velocity high?",
          "options": [
            "Implement this next: prefer serverless for burst-heavy short-lived handlers with strict timeout fit, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing."
          ],
          "correct": 0,
          "explanation": "Adopt the targeted fit correction for document rendering service: Implement this next: prefer serverless for burst-heavy short-lived handlers with strict timeout fit, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Start from \"after confirming diagnosis for document rendering service, what is the strongest next\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"compute Selection & Platform Trade-offs\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a fraud scoring microservice, incident reviews show repeated issues tied to bin-packing efficiency vs noisy neighbors. What is the most likely diagnosis?",
          "options": [
            "Current platform choice mismatches fraud scoring microservice constraints around bin-packing efficiency vs noisy neighbors.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability."
          ],
          "correct": 0,
          "explanation": "fraud scoring microservice is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Read this as a scenario about \"for a fraud scoring microservice, incident reviews show repeated issues tied to\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for fraud scoring microservice, what is the strongest next platform move before broad migration rollout?",
          "options": [
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: prefer VMs when host-level control/compliance and stable workloads dominate, then validate latency/cost/operability before broad rollout."
          ],
          "correct": 3,
          "explanation": "Adopt the targeted fit correction for fraud scoring microservice: Implement this next: prefer VMs when host-level control/compliance and stable workloads dominate, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis for fraud scoring microservice, what is the strongest next\". Do not reset assumptions between stages; carry forward prior constraints directly. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: skipping anchor checks against known scale."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"compute Selection & Platform Trade-offs\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a scheduled billing pipeline, incident reviews show repeated issues tied to ops overhead and patching burden. What is the most likely diagnosis?",
          "options": [
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches scheduled billing pipeline constraints around ops overhead and patching burden."
          ],
          "correct": 3,
          "explanation": "scheduled billing pipeline is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Use \"for a scheduled billing pipeline, incident reviews show repeated issues tied to ops\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis for scheduled billing pipeline, what is the strongest next platform move under long-running workload pressure?",
          "options": [
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: use mixed model: serverless ingress + containerized core services, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged."
          ],
          "correct": 2,
          "explanation": "Adopt the targeted fit correction for scheduled billing pipeline: Implement this next: use mixed model: serverless ingress + containerized core services, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis for scheduled billing pipeline, what is the strongest next\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Bandwidth planning should account for protocol overhead and burst behavior, not raw payload only. Common pitfall: bits-vs-bytes conversion mistakes."
        }
      ],
      "detailedExplanation": "The decision turns on \"compute Selection & Platform Trade-offs\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a control-plane reconciliation loop, incident reviews show repeated issues tied to deployment velocity and rollback control. What is the most likely diagnosis?",
          "options": [
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches control-plane reconciliation loop constraints around deployment velocity and rollback control.",
            "Cloud region selection is always the root cause of platform fit issues."
          ],
          "correct": 2,
          "explanation": "control-plane reconciliation loop is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "This prompt is really about \"for a control-plane reconciliation loop, incident reviews show repeated issues tied to\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for control-plane reconciliation loop, what is the strongest next platform move with production observability requirements?",
          "options": [
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: choose managed runtime for speed, but enforce portability seams at boundaries, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding."
          ],
          "correct": 1,
          "explanation": "Adopt the targeted fit correction for control-plane reconciliation loop: Implement this next: choose managed runtime for speed, but enforce portability seams at boundaries, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "If you keep \"after confirming diagnosis for control-plane reconciliation loop, what is the strongest\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: accepting implausible outputs because arithmetic is clean."
        }
      ],
      "detailedExplanation": "Start from \"compute Selection & Platform Trade-offs\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a ML feature extraction workers, incident reviews show repeated issues tied to portability vs managed-service lock-in. What is the most likely diagnosis?",
          "options": [
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches ML feature extraction workers constraints around portability vs managed-service lock-in.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless."
          ],
          "correct": 1,
          "explanation": "ML feature extraction workers is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "If you keep \"for a ML feature extraction workers, incident reviews show repeated issues tied to\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for ML feature extraction workers, what is the strongest next platform move while minimizing platform sprawl?",
          "options": [
            "Implement this next: use dedicated node pools for noisy-neighbor-sensitive workloads, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing."
          ],
          "correct": 0,
          "explanation": "Adopt the targeted fit correction for ML feature extraction workers: Implement this next: use dedicated node pools for noisy-neighbor-sensitive workloads, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis for ML feature extraction workers, what is the strongest\". Solve this as chained reasoning where stage two must respect stage one assumptions. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: propagating an early bad assumption through all steps."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"compute Selection & Platform Trade-offs\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a tenant-isolated report generation, incident reviews show repeated issues tied to cost predictability under burst load. What is the most likely diagnosis?",
          "options": [
            "Current platform choice mismatches tenant-isolated report generation constraints around cost predictability under burst load.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability."
          ],
          "correct": 0,
          "explanation": "tenant-isolated report generation is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The core signal here is \"for a tenant-isolated report generation, incident reviews show repeated issues tied to\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis for tenant-isolated report generation, what is the strongest next platform move with explicit fallback expectations?",
          "options": [
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: shift long-running jobs off strict serverless limits into container workers, then validate latency/cost/operability before broad rollout."
          ],
          "correct": 3,
          "explanation": "Adopt the targeted fit correction for tenant-isolated report generation: Implement this next: shift long-running jobs off strict serverless limits into container workers, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Use \"after confirming diagnosis for tenant-isolated report generation, what is the strongest\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean."
        }
      ],
      "detailedExplanation": "The core signal here is \"compute Selection & Platform Trade-offs\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a internal admin API, incident reviews show repeated issues tied to runtime limits for long-lived tasks. What is the most likely diagnosis?",
          "options": [
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches internal admin API constraints around runtime limits for long-lived tasks."
          ],
          "correct": 3,
          "explanation": "internal admin API is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The key clue in this question is \"for a internal admin API, incident reviews show repeated issues tied to runtime limits\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for internal admin API, what is the strongest next platform move during peak seasonal load?",
          "options": [
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: opt for containers with canary/blue-green rollout automation, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged."
          ],
          "correct": 2,
          "explanation": "Adopt the targeted fit correction for internal admin API: Implement this next: opt for containers with canary/blue-green rollout automation, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis for internal admin API, what is the strongest next platform\". Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation."
        }
      ],
      "detailedExplanation": "If you keep \"compute Selection & Platform Trade-offs\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a global edge request validator, incident reviews show repeated issues tied to debugging/observability depth requirements. What is the most likely diagnosis?",
          "options": [
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches global edge request validator constraints around debugging/observability depth requirements.",
            "Cloud region selection is always the root cause of platform fit issues."
          ],
          "correct": 2,
          "explanation": "global edge request validator is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "If you keep \"for a global edge request validator, incident reviews show repeated issues tied to\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for global edge request validator, what is the strongest next platform move under tenant-isolation constraints?",
          "options": [
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: segment tenants with per-workload isolation tiers, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding."
          ],
          "correct": 1,
          "explanation": "Adopt the targeted fit correction for global edge request validator: Implement this next: segment tenants with per-workload isolation tiers, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis for global edge request validator, what is the strongest\". Solve this as chained reasoning where stage two must respect stage one assumptions. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: skipping anchor checks against known scale."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"compute Selection & Platform Trade-offs\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a latency-sensitive API gateway, incident reviews show repeated issues tied to per-tenant isolation requirements. What is the most likely diagnosis?",
          "options": [
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches latency-sensitive API gateway constraints around per-tenant isolation requirements.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless."
          ],
          "correct": 1,
          "explanation": "latency-sensitive API gateway is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "This prompt is really about \"for a latency-sensitive API gateway, incident reviews show repeated issues tied to\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for latency-sensitive API gateway, what is the strongest next platform move while reducing incident toil?",
          "options": [
            "Implement this next: adopt managed platform where undifferentiated ops cost exceeds lock-in risk, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing."
          ],
          "correct": 0,
          "explanation": "Adopt the targeted fit correction for latency-sensitive API gateway: Implement this next: adopt managed platform where undifferentiated ops cost exceeds lock-in risk, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "If you keep \"after confirming diagnosis for latency-sensitive API gateway, what is the strongest\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Start from \"compute Selection & Platform Trade-offs\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a spiky image thumbnail endpoint, incident reviews show repeated issues tied to compliance boundary and host control. What is the most likely diagnosis?",
          "options": [
            "Current platform choice mismatches spiky image thumbnail endpoint constraints around compliance boundary and host control.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability."
          ],
          "correct": 0,
          "explanation": "spiky image thumbnail endpoint is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Use \"for a spiky image thumbnail endpoint, incident reviews show repeated issues tied to\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for spiky image thumbnail endpoint, what is the strongest next platform move for mixed short/long job profiles?",
          "options": [
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: prefer containers on orchestrator with autoscaling and pod-level isolation controls, then validate latency/cost/operability before broad rollout."
          ],
          "correct": 3,
          "explanation": "Adopt the targeted fit correction for spiky image thumbnail endpoint: Implement this next: prefer containers on orchestrator with autoscaling and pod-level isolation controls, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The core signal here is \"after confirming diagnosis for spiky image thumbnail endpoint, what is the strongest\". Solve this as chained reasoning where stage two must respect stage one assumptions. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution."
        }
      ],
      "detailedExplanation": "The decision turns on \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a steady background ETL workers, incident reviews show repeated issues tied to cold-start latency penalties. What is the most likely diagnosis?",
          "options": [
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches steady background ETL workers constraints around cold-start latency penalties."
          ],
          "correct": 3,
          "explanation": "steady background ETL workers is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Read this as a scenario about \"for a steady background ETL workers, incident reviews show repeated issues tied to\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming diagnosis for steady background ETL workers, what is the strongest next platform move with clear ownership boundaries?",
          "options": [
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: prefer serverless for burst-heavy short-lived handlers with strict timeout fit, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged."
          ],
          "correct": 2,
          "explanation": "Adopt the targeted fit correction for steady background ETL workers: Implement this next: prefer serverless for burst-heavy short-lived handlers with strict timeout fit, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The key clue in this question is \"after confirming diagnosis for steady background ETL workers, what is the strongest\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a event-driven notification fanout, incident reviews show repeated issues tied to bin-packing efficiency vs noisy neighbors. What is the most likely diagnosis?",
          "options": [
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches event-driven notification fanout constraints around bin-packing efficiency vs noisy neighbors.",
            "Cloud region selection is always the root cause of platform fit issues."
          ],
          "correct": 2,
          "explanation": "event-driven notification fanout is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The decision turns on \"for a event-driven notification fanout, incident reviews show repeated issues tied to\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for event-driven notification fanout, what is the strongest next platform move under evolving workload shape?",
          "options": [
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: prefer VMs when host-level control/compliance and stable workloads dominate, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding."
          ],
          "correct": 1,
          "explanation": "Adopt the targeted fit correction for event-driven notification fanout: Implement this next: prefer VMs when host-level control/compliance and stable workloads dominate, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Start from \"after confirming diagnosis for event-driven notification fanout, what is the strongest\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "Use \"compute Selection & Platform Trade-offs\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a GPU-bound inference service, incident reviews show repeated issues tied to ops overhead and patching burden. What is the most likely diagnosis?",
          "options": [
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches GPU-bound inference service constraints around ops overhead and patching burden.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless."
          ],
          "correct": 1,
          "explanation": "GPU-bound inference service is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Start from \"for a GPU-bound inference service, incident reviews show repeated issues tied to ops\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for GPU-bound inference service, what is the strongest next platform move while avoiding cold-path regressions?",
          "options": [
            "Implement this next: use mixed model: serverless ingress + containerized core services, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing."
          ],
          "correct": 0,
          "explanation": "Adopt the targeted fit correction for GPU-bound inference service: Implement this next: use mixed model: serverless ingress + containerized core services, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis for GPU-bound inference service, what is the strongest next\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Treat network capacity as a steady-state constraint, then test against peak windows. Common pitfall: missing protocol/compression overhead in capacity math."
        }
      ],
      "detailedExplanation": "This prompt is really about \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a long-running media transcoding jobs, incident reviews show repeated issues tied to deployment velocity and rollback control. What is the most likely diagnosis?",
          "options": [
            "Current platform choice mismatches long-running media transcoding jobs constraints around deployment velocity and rollback control.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability."
          ],
          "correct": 0,
          "explanation": "long-running media transcoding jobs is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The key clue in this question is \"for a long-running media transcoding jobs, incident reviews show repeated issues tied\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for long-running media transcoding jobs, what is the strongest next platform move with measured cost guardrails?",
          "options": [
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: choose managed runtime for speed, but enforce portability seams at boundaries, then validate latency/cost/operability before broad rollout."
          ],
          "correct": 3,
          "explanation": "Adopt the targeted fit correction for long-running media transcoding jobs: Implement this next: choose managed runtime for speed, but enforce portability seams at boundaries, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Read this as a scenario about \"after confirming diagnosis for long-running media transcoding jobs, what is the\". Solve this as chained reasoning where stage two must respect stage one assumptions. Treat plausibility validation as mandatory, even when the arithmetic is internally consistent. Common pitfall: skipping anchor checks against known scale."
        }
      ],
      "detailedExplanation": "If you keep \"compute Selection & Platform Trade-offs\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a stateful websocket chat service, incident reviews show repeated issues tied to portability vs managed-service lock-in. What is the most likely diagnosis?",
          "options": [
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches stateful websocket chat service constraints around portability vs managed-service lock-in."
          ],
          "correct": 3,
          "explanation": "stateful websocket chat service is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "The core signal here is \"for a stateful websocket chat service, incident reviews show repeated issues tied to\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis for stateful websocket chat service, what is the strongest next platform move during phased adoption?",
          "options": [
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: use dedicated node pools for noisy-neighbor-sensitive workloads, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged."
          ],
          "correct": 2,
          "explanation": "Adopt the targeted fit correction for stateful websocket chat service: Implement this next: use dedicated node pools for noisy-neighbor-sensitive workloads, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "Use \"after confirming diagnosis for stateful websocket chat service, what is the strongest\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: propagating an early bad assumption through all steps."
        }
      ],
      "detailedExplanation": "The core signal here is \"compute Selection & Platform Trade-offs\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a bursty webhook processor, incident reviews show repeated issues tied to cost predictability under burst load. What is the most likely diagnosis?",
          "options": [
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches bursty webhook processor constraints around cost predictability under burst load.",
            "Cloud region selection is always the root cause of platform fit issues."
          ],
          "correct": 2,
          "explanation": "bursty webhook processor is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "If you keep \"for a bursty webhook processor, incident reviews show repeated issues tied to cost\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis for bursty webhook processor, what is the strongest next platform move without sacrificing reliability?",
          "options": [
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: shift long-running jobs off strict serverless limits into container workers, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding."
          ],
          "correct": 1,
          "explanation": "Adopt the targeted fit correction for bursty webhook processor: Implement this next: shift long-running jobs off strict serverless limits into container workers, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "This prompt is really about \"after confirming diagnosis for bursty webhook processor, what is the strongest next\". Solve this as chained reasoning where stage two must respect stage one assumptions. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a compliance batch scanner, incident reviews show repeated issues tied to runtime limits for long-lived tasks. What is the most likely diagnosis?",
          "options": [
            "Compute model never affects rollout speed or reliability.",
            "Current platform choice mismatches compliance batch scanner constraints around runtime limits for long-lived tasks.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless."
          ],
          "correct": 1,
          "explanation": "compliance batch scanner is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "This prompt is really about \"for a compliance batch scanner, incident reviews show repeated issues tied to runtime\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming diagnosis for compliance batch scanner, what is the strongest next platform move under migration time pressure?",
          "options": [
            "Implement this next: opt for containers with canary/blue-green rollout automation, then validate latency/cost/operability before broad rollout.",
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing."
          ],
          "correct": 0,
          "explanation": "Adopt the targeted fit correction for compliance batch scanner: Implement this next: opt for containers with canary/blue-green rollout automation, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "If you keep \"after confirming diagnosis for compliance batch scanner, what is the strongest next\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: skipping anchor checks against known scale."
        }
      ],
      "detailedExplanation": "Start from \"compute Selection & Platform Trade-offs\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "For a high-QPS key-value read API, incident reviews show repeated issues tied to debugging/observability depth requirements. What is the most likely diagnosis?",
          "options": [
            "Current platform choice mismatches high-QPS key-value read API constraints around debugging/observability depth requirements.",
            "Cloud region selection is always the root cause of platform fit issues.",
            "Noisy neighbors only happen on VMs, never containers/serverless.",
            "Compute model never affects rollout speed or reliability."
          ],
          "correct": 0,
          "explanation": "high-QPS key-value read API is showing a platform-fit mismatch where runtime model and operational constraints are misaligned.",
          "detailedExplanation": "Start from \"for a high-QPS key-value read API, incident reviews show repeated issues tied to\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming diagnosis for high-QPS key-value read API, what is the strongest next platform move with strict change-control requirements?",
          "options": [
            "Add retries everywhere and keep current platform unchanged.",
            "Disable observability to reduce overhead before deciding.",
            "Choose the cheapest hourly price without performance testing.",
            "Implement this next: segment tenants with per-workload isolation tiers, then validate latency/cost/operability before broad rollout."
          ],
          "correct": 3,
          "explanation": "Adopt the targeted fit correction for high-QPS key-value read API: Implement this next: segment tenants with per-workload isolation tiers, then validate latency/cost/operability before broad rollout.",
          "detailedExplanation": "The decision turns on \"after confirming diagnosis for high-QPS key-value read API, what is the strongest next\". Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"compute Selection & Platform Trade-offs\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-061",
      "type": "multi-select",
      "question": "When is serverless usually a strong fit? (Select all that apply)",
      "options": [
        "Spiky event-driven workloads",
        "Short-lived handlers with clear timeout bounds",
        "Always-on heavy jobs with long execution windows",
        "Teams optimizing for low ops overhead"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Serverless excels for bursty short tasks and reduced ops burden.",
      "detailedExplanation": "Use \"serverless usually a strong fit? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-062",
      "type": "multi-select",
      "question": "Common container-platform advantages include which? (Select all that apply)",
      "options": [
        "Fine-grained rollout control",
        "Portable packaging across environments",
        "Zero noisy-neighbor risk by default",
        "Flexible runtime/resource tuning"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Containers offer strong deployment and runtime control, but isolation still needs policy.",
      "detailedExplanation": "The core signal here is \"common container-platform advantages include which? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-063",
      "type": "multi-select",
      "question": "Reasons to choose VMs for a workload include which? (Select all that apply)",
      "options": [
        "Host-level control and custom kernel/runtime needs",
        "Predictable long-running workload shape",
        "Guaranteed fastest deployment velocity always",
        "Strict compliance boundaries requiring stronger tenancy control"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "VMs are useful where host control, compliance, and stable long-running profiles dominate.",
      "detailedExplanation": "If you keep \"reasons to choose VMs for a workload include which? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-064",
      "type": "multi-select",
      "question": "Which practices reduce serverless cold-start impact? (Select all that apply)",
      "options": [
        "Provisioned concurrency/warm pools",
        "Smaller deployment artifacts",
        "Unbounded startup dependencies",
        "Split latency-critical path from heavy init tasks"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Cold starts improve with warm capacity, lightweight init, and path separation.",
      "detailedExplanation": "Start from \"practices reduce serverless cold-start impact? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-065",
      "type": "multi-select",
      "question": "What helps control noisy-neighbor effects in containerized environments? (Select all that apply)",
      "options": [
        "Resource requests/limits and QoS classes",
        "Dedicated node pools for sensitive workloads",
        "Ignoring throttling and steal-time metrics",
        "Pod anti-affinity and placement rules"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Isolation requires scheduling/resource controls plus observability.",
      "detailedExplanation": "The key clue in this question is \"helps control noisy-neighbor effects in containerized environments? (Select all that\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-066",
      "type": "multi-select",
      "question": "Portability-vs-lock-in tradeoff handling should include which? (Select all that apply)",
      "options": [
        "Define abstraction seams around managed dependencies",
        "Document exit cost and migration paths",
        "Avoid all managed services categorically",
        "Use capability-driven adoption with explicit constraints"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Pragmatic lock-in management is about explicit seams and migration planning.",
      "detailedExplanation": "Read this as a scenario about \"portability-vs-lock-in tradeoff handling should include which? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-067",
      "type": "multi-select",
      "question": "Which signals suggest a workload is a poor fit for strict serverless execution limits? (Select all that apply)",
      "options": [
        "Frequent timeout near platform max duration",
        "Large in-memory working set and long warmup",
        "Short burst processing under 100ms",
        "Need for persistent long-lived connections"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Long-lived/stateful/heavy-init workloads often exceed serverless sweet spot.",
      "detailedExplanation": "The decision turns on \"signals suggest a workload is a poor fit for strict serverless execution limits?\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-068",
      "type": "multi-select",
      "question": "Strong compute-platform migration guardrails include which? (Select all that apply)",
      "options": [
        "Canary rollout with rollback thresholds",
        "Baseline vs post-change cost and latency comparison",
        "Big-bang migration with no fallback",
        "Operational readiness checklist for on-call/support"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Migration safety depends on staged rollout and explicit operational readiness.",
      "detailedExplanation": "This prompt is really about \"strong compute-platform migration guardrails include which? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-069",
      "type": "multi-select",
      "question": "Which factors matter most when picking managed runtime vs self-managed orchestration? (Select all that apply)",
      "options": [
        "Team ops bandwidth",
        "Required control depth over runtime/networking",
        "Popularity on social media only",
        "SLA/responsibility boundaries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Decision should reflect control needs, ops capacity, and responsibility split.",
      "detailedExplanation": "Use \"factors matter most when picking managed runtime vs self-managed orchestration? (Select\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-070",
      "type": "multi-select",
      "question": "Hybrid compute strategies are useful when which conditions hold? (Select all that apply)",
      "options": [
        "Workloads have distinct latency/duration profiles",
        "One team can own platform complexity appropriately",
        "Uniform workload pattern across all services",
        "Cost and reliability targets differ by path"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Hybrid models work when workload classes have materially different constraints.",
      "detailedExplanation": "If you keep \"hybrid compute strategies are useful when which conditions hold? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-071",
      "type": "multi-select",
      "question": "For deployment velocity, which compute characteristics help? (Select all that apply)",
      "options": [
        "Immutable artifact pipelines",
        "Progressive delivery controls",
        "Manual snowflake host edits",
        "Fast rollback primitives"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Velocity improves with automation, progressive rollout, and quick rollback paths.",
      "detailedExplanation": "The core signal here is \"for deployment velocity, which compute characteristics help? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-072",
      "type": "multi-select",
      "question": "Which anti-patterns often increase compute TCO unexpectedly? (Select all that apply)",
      "options": [
        "Choosing platform by list price only",
        "Ignoring observability and incident toil costs",
        "Measuring end-to-end cost per request/job",
        "Overprovisioning for unmanaged cold-start risk"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Total cost includes toil, overprovisioning, and reliability effects, not just hourly price.",
      "detailedExplanation": "Use \"anti-patterns often increase compute TCO unexpectedly? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-073",
      "type": "multi-select",
      "question": "When evaluating platform fit for compliance-heavy systems, which are relevant? (Select all that apply)",
      "options": [
        "Tenant isolation model",
        "Auditability and change controls",
        "Whether stack is trendy this quarter",
        "Boundary control over data residency and hosts"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Compliance depends on isolation, auditability, and boundary control.",
      "detailedExplanation": "This prompt is really about \"evaluating platform fit for compliance-heavy systems, which are relevant? (Select all\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-074",
      "type": "multi-select",
      "question": "Which indicators suggest container bin packing is too aggressive? (Select all that apply)",
      "options": [
        "Frequent CPU throttling on critical pods",
        "Latency spikes correlated with co-location",
        "Stable p99 and low contention variance",
        "Memory pressure evictions under peak"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Contention and evictions indicate overpacked nodes for sensitive workloads.",
      "detailedExplanation": "The decision turns on \"indicators suggest container bin packing is too aggressive? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-075",
      "type": "multi-select",
      "question": "For platform portability planning, useful actions include which? (Select all that apply)",
      "options": [
        "Minimize proprietary assumptions in core domain code",
        "Track managed API usage inventory",
        "Treat migration feasibility as unknowable",
        "Prototype critical fallback path periodically"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Portability improves when dependencies are explicit and fallback paths are tested.",
      "detailedExplanation": "Read this as a scenario about \"for platform portability planning, useful actions include which? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-076",
      "type": "multi-select",
      "question": "Which are valid reasons to keep some workloads on VMs while others move to containers/serverless? (Select all that apply)",
      "options": [
        "Different runtime/control requirements by workload class",
        "Phased migration risk management",
        "Need to maximize accidental platform sprawl",
        "Legacy constraints with clear retirement path"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Mixed estates can be rational when bounded by clear constraints and migration plans.",
      "detailedExplanation": "The key clue in this question is \"valid reasons to keep some workloads on VMs while others move to containers/serverless?\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-077",
      "type": "multi-select",
      "question": "For stateful long-lived connection services, which compute concerns are critical? (Select all that apply)",
      "options": [
        "Connection draining and graceful shutdown behavior",
        "Session/state externalization strategy",
        "Assuming instant migration with zero disruption",
        "Capacity planning for reconnect storms"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Stateful connection services need controlled drain/reconnect and state handling.",
      "detailedExplanation": "Start from \"for stateful long-lived connection services, which compute concerns are critical?\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-078",
      "type": "numeric-input",
      "question": "A serverless function runs 8,000,000 times/day at 180ms average. Total compute seconds/day?",
      "answer": 1440000,
      "unit": "seconds",
      "tolerance": 0.01,
      "explanation": "8,000,000 * 0.18 = 1,440,000 seconds.",
      "detailedExplanation": "If you keep \"serverless function runs 8,000,000 times/day at 180ms average\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 8,000 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-079",
      "type": "numeric-input",
      "question": "A container worker handles 250 jobs/min. You need 10,000 jobs/min with 20% headroom. Minimum workers?",
      "answer": 50,
      "unit": "workers",
      "tolerance": 0,
      "explanation": "Required capacity = 10,000 / 0.8 = 12,500 jobs/min. 12,500 / 250 = 50 workers.",
      "detailedExplanation": "The core signal here is \"container worker handles 250 jobs/min\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 250 and 10,000 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-080",
      "type": "numeric-input",
      "question": "A VM pool serves 48,000 rps at 65% target utilization. Forecast is +30% traffic. What rps capacity is needed at same target?",
      "answer": 62400,
      "unit": "rps",
      "tolerance": 0.02,
      "explanation": "48,000 * 1.3 = 62,400 rps required.",
      "detailedExplanation": "Read this as a scenario about \"vM pool serves 48,000 rps at 65% target utilization\". Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 48,000 rps and 65 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-081",
      "type": "numeric-input",
      "question": "Cold start adds 350ms to 4% of requests on a serverless endpoint. Average added latency across all requests?",
      "answer": 14,
      "unit": "ms",
      "tolerance": 0.2,
      "explanation": "0.04 * 350ms = 14ms average added latency.",
      "detailedExplanation": "The decision turns on \"cold start adds 350ms to 4% of requests on a serverless endpoint\". Keep every transformation in one unit system and check order of magnitude at the end. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Keep quantities like 350ms and 4 in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-082",
      "type": "numeric-input",
      "question": "Container node has 32 vCPU. Policy reserves 25% for system overhead. Usable vCPU per node?",
      "answer": 24,
      "unit": "vCPU",
      "tolerance": 0,
      "explanation": "32 * 0.75 = 24 usable vCPU.",
      "detailedExplanation": "Start from \"container node has 32 vCPU\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 32 and 25 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-083",
      "type": "numeric-input",
      "question": "A managed runtime costs $0.000002 per request. At 140 million requests/day, daily request cost?",
      "answer": 280,
      "unit": "USD",
      "tolerance": 0.02,
      "explanation": "140,000,000 * 0.000002 = $280/day.",
      "detailedExplanation": "The key clue in this question is \"managed runtime costs $0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 0.000002 and 140 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-084",
      "type": "numeric-input",
      "question": "A VM instance costs $0.84/hour and handles 3,600 rps at target. Cost per 1,000 rps-hour?",
      "answer": 0.2333,
      "unit": "USD",
      "tolerance": 0.03,
      "explanation": "$0.84 / 3.6 = $0.2333 per 1,000 rps-hour.",
      "detailedExplanation": "The core signal here is \"vM instance costs $0\". Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 0.84 and 3,600 rps appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-085",
      "type": "numeric-input",
      "question": "Autoscaled container fleet has 36 pods, each 2 vCPU request. Cluster allocatable CPU is 96 vCPU. Requested utilization percent?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.2,
      "explanation": "(36*2)/96 = 72/96 = 75% requested.",
      "detailedExplanation": "If you keep \"autoscaled container fleet has 36 pods, each 2 vCPU request\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 36 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-086",
      "type": "numeric-input",
      "question": "A long-running job takes 25 minutes. Serverless max duration is 15 minutes. Minimum chained invocations needed?",
      "answer": 2,
      "unit": "invocations",
      "tolerance": 0,
      "explanation": "25/15 = 1.67, so at least 2 chained invocations.",
      "detailedExplanation": "This prompt is really about \"long-running job takes 25 minutes\". Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 25 minutes and 15 minutes in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-087",
      "type": "numeric-input",
      "question": "Platform migration reduced deployment rollback time from 18 minutes to 6 minutes. Percent reduction?",
      "answer": 66.67,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(18-6)/18 = 66.67% reduction.",
      "detailedExplanation": "Use \"platform migration reduced deployment rollback time from 18 minutes to 6 minutes\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 18 minutes and 6 minutes in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-088",
      "type": "numeric-input",
      "question": "A service needs 12,000 persistent connections. One pod safely handles 750 connections. Minimum pods?",
      "answer": 16,
      "unit": "pods",
      "tolerance": 0,
      "explanation": "12,000 / 750 = 16 pods.",
      "detailedExplanation": "Read this as a scenario about \"service needs 12,000 persistent connections\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 12,000 and 750 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-089",
      "type": "numeric-input",
      "question": "A serverless app keeps 30 provisioned instances warm at $0.012/hour each. Daily warm cost?",
      "answer": 8.64,
      "unit": "USD",
      "tolerance": 0.02,
      "explanation": "30 * 0.012 * 24 = $8.64/day.",
      "detailedExplanation": "The decision turns on \"serverless app keeps 30 provisioned instances warm at $0\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 30 and 0.012 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-090",
      "type": "ordering",
      "question": "Order a pragmatic compute-platform selection flow.",
      "items": [
        "Characterize workload/runtime constraints",
        "Map candidate platforms to constraints",
        "Run cost/performance/operability experiments",
        "Adopt with rollout guardrails and review loop"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Select platform from measured fit, not assumptions.",
      "detailedExplanation": "Use \"order a pragmatic compute-platform selection flow\" as your starting point, then verify tradeoffs carefully. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-091",
      "type": "ordering",
      "question": "Order by increasing host-control depth.",
      "items": [
        "Fully managed serverless",
        "Managed container runtime",
        "Self-managed container orchestration",
        "VM-based host-managed deployment"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Control generally increases from managed abstractions to host-managed compute.",
      "detailedExplanation": "This prompt is really about \"order by increasing host-control depth\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-092",
      "type": "ordering",
      "question": "Order migration rollout from safest to riskiest.",
      "items": [
        "Canary migration",
        "Incremental service-by-service migration",
        "Large batch migration",
        "Big-bang cutover"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Risk grows with blast radius and reduced rollback options.",
      "detailedExplanation": "If you keep \"order migration rollout from safest to riskiest\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-093",
      "type": "ordering",
      "question": "Order by likely cold-start sensitivity (least to most).",
      "items": [
        "Always-on VM fleet",
        "Always-on container service",
        "Autoscaled container scale-to-zero path",
        "On-demand serverless with infrequent traffic"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Intermittent on-demand paths typically experience highest cold-start impact.",
      "detailedExplanation": "The core signal here is \"order by likely cold-start sensitivity (least to most)\". Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-094",
      "type": "ordering",
      "question": "Order by increasing lock-in risk (typical case).",
      "items": [
        "Portable containers with open standards",
        "Managed containers with provider-specific addons",
        "Provider-specific managed runtime",
        "Deeply coupled proprietary workflow stack"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Coupling depth and proprietary dependencies drive lock-in risk.",
      "detailedExplanation": "The key clue in this question is \"order by increasing lock-in risk (typical case)\". Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-095",
      "type": "ordering",
      "question": "Order by increasing deployment velocity potential.",
      "items": [
        "Manual VM patch workflow",
        "Scripted VM image rollout",
        "Container CI/CD with progressive delivery",
        "Managed platform with built-in progressive deploy controls"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Automation and built-in rollout primitives usually improve velocity.",
      "detailedExplanation": "Start from \"order by increasing deployment velocity potential\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-096",
      "type": "ordering",
      "question": "Order by strongest isolation for noisy-neighbor mitigation (default posture).",
      "items": [
        "Shared serverless concurrency pool",
        "Shared container nodes",
        "Dedicated container nodes",
        "Dedicated VM hosts"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Isolation typically improves with dedicated resource boundaries.",
      "detailedExplanation": "The decision turns on \"order by strongest isolation for noisy-neighbor mitigation (default posture)\". Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-097",
      "type": "ordering",
      "question": "Order by increasing suitability for very long-running tasks.",
      "items": [
        "Strict-duration serverless handlers",
        "Managed short-job runtime",
        "Container workers",
        "VM-based long-lived workers"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Long-running support grows as runtime limits relax and control increases.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing suitability for very long-running tasks\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-098",
      "type": "ordering",
      "question": "Order by increasing ops burden on the application team.",
      "items": [
        "Fully managed serverless",
        "Managed containers",
        "Self-managed containers",
        "Self-managed VM fleet"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Operational burden increases with infrastructure control responsibility.",
      "detailedExplanation": "Use \"order by increasing ops burden on the application team\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-099",
      "type": "ordering",
      "question": "Order by strongest evidence quality for platform-fit decision.",
      "items": [
        "Anecdotal team preference",
        "Benchmark on developer laptop",
        "Staging benchmark with synthetic load",
        "Production-like canary with SLO and cost telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Decision confidence grows with realistic telemetry and production-like validation.",
      "detailedExplanation": "This prompt is really about \"order by strongest evidence quality for platform-fit decision\". Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    },
    {
      "id": "sc-pt-100",
      "type": "ordering",
      "question": "Order fallback planning maturity from weakest to strongest.",
      "items": [
        "No fallback documented",
        "Fallback idea in wiki only",
        "Fallback runbook with owners",
        "Fallback runbook tested in drills"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Operational resilience requires owned and tested fallback paths.",
      "detailedExplanation": "The decision turns on \"order fallback planning maturity from weakest to strongest\". Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Kubernetes Horizontal Pod Autoscaling",
          "url": "https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"
        }
      ],
      "tags": ["scaling-compute", "compute-selection-and-platform-trade-offs"],
      "difficulty": "senior"
    }
  ]
}
