{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 3,
  "chapterTitle": "Circuit Breakers, Load Shedding & Admission Control",
  "chapterDescription": "Protect systems under stress by bounding concurrency and failing safely before saturation cascades.",
  "problems": [
    {
      "id": "rel-ca-001",
      "type": "multiple-choice",
      "question": "Case Alpha: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-002",
      "type": "multiple-choice",
      "question": "Case Beta: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-003",
      "type": "multiple-choice",
      "question": "Case Gamma: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-004",
      "type": "multiple-choice",
      "question": "Case Delta: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Apply brownout toggles to disable expensive non-critical work during stress."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Separate queues by workload class to prevent priority inversion.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-006",
      "type": "multiple-choice",
      "question": "Case Zeta: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-007",
      "type": "multiple-choice",
      "question": "Case Eta: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-008",
      "type": "multiple-choice",
      "question": "Case Theta: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-009",
      "type": "multiple-choice",
      "question": "Case Iota: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-010",
      "type": "multiple-choice",
      "question": "Case Kappa: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-011",
      "type": "multiple-choice",
      "question": "Case Lambda: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-012",
      "type": "multiple-choice",
      "question": "Case Mu: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-013",
      "type": "multiple-choice",
      "question": "Case Nu: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-014",
      "type": "multiple-choice",
      "question": "Case Xi: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply brownout toggles to disable expensive non-critical work during stress.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-015",
      "type": "multiple-choice",
      "question": "Case Omicron: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate queues by workload class to prevent priority inversion.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-016",
      "type": "multiple-choice",
      "question": "Case Pi: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-017",
      "type": "multiple-choice",
      "question": "Case Rho: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-018",
      "type": "multiple-choice",
      "question": "Case Sigma: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-019",
      "type": "multiple-choice",
      "question": "Case Tau: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-021",
      "type": "multiple-choice",
      "question": "Case Phi: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-022",
      "type": "multiple-choice",
      "question": "Case Chi: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-023",
      "type": "multiple-choice",
      "question": "Case Psi: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-024",
      "type": "multiple-choice",
      "question": "Case Omega: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Apply brownout toggles to disable expensive non-critical work during stress."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-025",
      "type": "multiple-choice",
      "question": "Case Atlas: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Separate queues by workload class to prevent priority inversion.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-026",
      "type": "multiple-choice",
      "question": "Case Nova: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-027",
      "type": "multiple-choice",
      "question": "Case Orion: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-028",
      "type": "multiple-choice",
      "question": "Case Vega: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-029",
      "type": "multiple-choice",
      "question": "Case Helios: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-030",
      "type": "multiple-choice",
      "question": "Case Aurora: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-032",
      "type": "multiple-choice",
      "question": "Case Pulse: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits."
      ],
      "correct": 3,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-033",
      "type": "multiple-choice",
      "question": "Case Forge: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode.",
      "options": [
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-034",
      "type": "multiple-choice",
      "question": "Case Harbor: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply brownout toggles to disable expensive non-critical work during stress.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-035",
      "type": "multiple-choice",
      "question": "Case Vector: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate queues by workload class to prevent priority inversion.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "The strongest response addresses the dominant failure mode directly while preserving reliability boundaries and controlled blast radius."
    },
    {
      "id": "rel-ca-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for document conversion service: signal points to degraded dependency still accepted at full rate. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Use per-priority admission control and shed low-value traffic before core flows.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for billing reconciliation job: signal points to overload spillover across tenants. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Configure circuit breakers with stable windows and explicit half-open probe limits."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for image processing pipeline: signal points to queue growth to unbounded latency. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Bound concurrency at each dependency boundary and reject above safe capacity.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat history service: signal points to thread pool exhaustion under burst. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Apply brownout toggles to disable expensive non-critical work during stress.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for analytics ingestion endpoint: signal points to brownout on non-critical features. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Separate queues by workload class to prevent priority inversion.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for document conversion service: signal points to degraded dependency still accepted at full rate. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Use per-priority admission control and shed low-value traffic before core flows.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for billing reconciliation job: signal points to overload spillover across tenants. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Configure circuit breakers with stable windows and explicit half-open probe limits.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for image processing pipeline: signal points to queue growth to unbounded latency. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Bound concurrency at each dependency boundary and reject above safe capacity.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat history service: signal points to thread pool exhaustion under burst. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Apply brownout toggles to disable expensive non-critical work during stress."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for analytics ingestion endpoint: signal points to brownout on non-critical features. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Separate queues by workload class to prevent priority inversion.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
          ],
          "correct": 3,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "The first step is identifying the control mismatch between required reliability behavior and actual system behavior."
        },
        {
          "question": "After diagnosis, what is the strongest next change?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Pick the smallest high-leverage change that closes the identified reliability gap and reduces recurrence."
        }
      ]
    },
    {
      "id": "rel-ca-061",
      "type": "multi-select",
      "question": "Which indicators most directly reveal cross-domain blast radius? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-062",
      "type": "multi-select",
      "question": "Which controls reduce hidden single points of failure? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-063",
      "type": "multi-select",
      "question": "During partial failures, which practices improve diagnosis quality? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-064",
      "type": "multi-select",
      "question": "What belongs in a useful dependency failure taxonomy? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-065",
      "type": "multi-select",
      "question": "Which patterns limit correlated failures across zones? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-066",
      "type": "multi-select",
      "question": "Which runbook elements increase incident execution reliability? (Select all that apply)",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-067",
      "type": "multi-select",
      "question": "Which signals should trigger graceful isolation first? (Select all that apply)",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-068",
      "type": "multi-select",
      "question": "Which architectural choices help contain tenant-induced overload? (Select all that apply)",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-069",
      "type": "multi-select",
      "question": "For reliability policies, which items should be explicit per endpoint? (Select all that apply)",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-070",
      "type": "multi-select",
      "question": "Which anti-patterns commonly enlarge outage blast radius? (Select all that apply)",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-071",
      "type": "multi-select",
      "question": "What improves confidence in failover assumptions? (Select all that apply)",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-072",
      "type": "multi-select",
      "question": "Which data is essential when classifying partial vs fail-stop incidents? (Select all that apply)",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-073",
      "type": "multi-select",
      "question": "Which controls improve safety when control-plane health is uncertain? (Select all that apply)",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-074",
      "type": "multi-select",
      "question": "For critical writes, which guardrails reduce corruption risk under faults? (Select all that apply)",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-075",
      "type": "multi-select",
      "question": "Which recurring reviews keep reliability boundaries accurate over time? (Select all that apply)",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-076",
      "type": "multi-select",
      "question": "Which decisions help teams align on reliability trade-offs during incidents? (Select all that apply)",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-077",
      "type": "multi-select",
      "question": "What evidence best shows a mitigation reduced recurrence risk? (Select all that apply)",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "The strongest selections are concrete controls that improve containment, clarity, and controlled recovery."
    },
    {
      "id": "rel-ca-078",
      "type": "numeric-input",
      "question": "A service processes 4,200,000 requests/day and 0.22% violate reliability SLO. How many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "0.0022 * 4,200,000 = 9,240."
    },
    {
      "id": "rel-ca-079",
      "type": "numeric-input",
      "question": "Incident queue receives 1,800 items/min and drains 2,050 items/min. Net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "2,050 - 1,800 = 250."
    },
    {
      "id": "rel-ca-080",
      "type": "numeric-input",
      "question": "Retry policy adds 0.35 extra attempts per request at 60,000 req/sec. Effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "60,000 * 1.35 = 81,000."
    },
    {
      "id": "rel-ca-081",
      "type": "numeric-input",
      "question": "Failover takes 18 seconds and happens 21 times/day. Total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "18 * 21 = 378."
    },
    {
      "id": "rel-ca-082",
      "type": "numeric-input",
      "question": "Target p99 latency is 700ms; observed p99 is 980ms. Percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(980 - 700) / 700 = 40%."
    },
    {
      "id": "rel-ca-083",
      "type": "numeric-input",
      "question": "If 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "0.31 * 120,000 = 37,200."
    },
    {
      "id": "rel-ca-084",
      "type": "numeric-input",
      "question": "Error rate drops from 1.2% to 0.3%. Percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(1.2 - 0.3) / 1.2 = 75%."
    },
    {
      "id": "rel-ca-085",
      "type": "numeric-input",
      "question": "A 7-node quorum system requires majority writes. Minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "Majority of 7 is 4."
    },
    {
      "id": "rel-ca-086",
      "type": "numeric-input",
      "question": "Backlog is 48,000 tasks and net drain is 320 tasks/min. Minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "48,000 / 320 = 150."
    },
    {
      "id": "rel-ca-087",
      "type": "numeric-input",
      "question": "A system with 14 zones has 2 unavailable. What percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "12 / 14 = 85.71%."
    },
    {
      "id": "rel-ca-088",
      "type": "numeric-input",
      "question": "MTTR improved from 45 min to 30 min. Percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "(45 - 30) / 45 = 33.33%."
    },
    {
      "id": "rel-ca-089",
      "type": "numeric-input",
      "question": "If 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "0.09 * 2,500,000 = 225,000."
    },
    {
      "id": "rel-ca-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope, contain, fix, then harden."
    },
    {
      "id": "rel-ca-091",
      "type": "ordering",
      "question": "Order from lowest to highest reliability risk.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Risk grows as boundaries and controls are removed."
    },
    {
      "id": "rel-ca-092",
      "type": "ordering",
      "question": "Order failover safety steps.",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Safety comes from validation, fencing, gradual shift, and planned restoration."
    },
    {
      "id": "rel-ca-093",
      "type": "ordering",
      "question": "Order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Protection strengthens with class-aware and domain-aware controls."
    },
    {
      "id": "rel-ca-094",
      "type": "ordering",
      "question": "Order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Reliable recovery is staged and verified before write promotion."
    },
    {
      "id": "rel-ca-095",
      "type": "ordering",
      "question": "Order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The operations loop ties objective targets to execution and learning."
    },
    {
      "id": "rel-ca-096",
      "type": "ordering",
      "question": "Order by increasing blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Blast radius expands from local process to regional control failure."
    },
    {
      "id": "rel-ca-097",
      "type": "ordering",
      "question": "Order retry-policy maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Maturity grows with safeguards and measurable control."
    },
    {
      "id": "rel-ca-098",
      "type": "ordering",
      "question": "Order degradation sophistication.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sophistication increases with explicit, automated, user-visible policy."
    },
    {
      "id": "rel-ca-099",
      "type": "ordering",
      "question": "Order incident command rigor.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Rigor improves with role clarity, timeline, and accountability."
    },
    {
      "id": "rel-ca-100",
      "type": "ordering",
      "question": "Order reliability validation confidence.",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Confidence rises with sustained production behavior and recurrence testing."
    }
  ]
}
