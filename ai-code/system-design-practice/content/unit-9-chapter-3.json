{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 3,
  "chapterTitle": "Circuit Breakers, Load Shedding & Admission Control",
  "chapterDescription": "Protect systems under stress by bounding concurrency and failing safely before saturation cascades.",
  "problems": [
    {
      "id": "rel-ca-001",
      "type": "multiple-choice",
      "question": "Case Alpha: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat checkout aggregation tier as a reliability-control decision, not an averages-only optimization. \"Use per-priority admission control and shed low-value traffic before core flows\" is correct since it mitigates queue growth to unbounded latency while keeping containment local. The decision remains valid given: A rollback window is still available for the next 15 minutes.",
      "detailedExplanation": "Read this as a scenario about \"checkout aggregation tier\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 15 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-002",
      "type": "multiple-choice",
      "question": "Case Beta: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For recommendation API, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" outperforms the alternatives because it targets thread pool exhaustion under burst and preserves safe recovery behavior. It is also the most compatible with Leadership asked for an action that lowers recurrence, not just symptoms.",
      "detailedExplanation": "The key clue in this question is \"recommendation API\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-003",
      "type": "multiple-choice",
      "question": "Case Gamma: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, profile update worker pool fails mainly through brownout on non-critical features. The best choice is \"Bound concurrency at each dependency boundary and reject above safe capacity\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "Start from \"profile update worker pool\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-004",
      "type": "multiple-choice",
      "question": "Case Delta: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Apply brownout toggles to disable expensive non-critical work during stress."
      ],
      "correct": 3,
      "explanation": "Search fanout coordinator should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Apply brownout toggles to disable expensive non-critical work during stress\" is strongest because it directly addresses circuit thrashing due to poor thresholds and improves repeatability under stress. This aligns with the extra condition (Recent game-day results showed hidden cross-zone coupling).",
      "detailedExplanation": "If you keep \"search fanout coordinator\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Separate queues by workload class to prevent priority inversion.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat feature-flag control API as a reliability-control decision, not an averages-only optimization. \"Separate queues by workload class to prevent priority inversion\" is correct since it mitigates priority inversion under shared queue while keeping containment local. The decision remains valid given: Customer impact is concentrated on invariant-critical transactions.",
      "detailedExplanation": "The core signal here is \"feature-flag control API\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-006",
      "type": "multiple-choice",
      "question": "Case Zeta: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "For document conversion service, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" outperforms the alternatives because it targets head-of-line blocking in dependency pool and preserves safe recovery behavior. It is also the most compatible with The previous mitigation improved averages but not tail behavior.",
      "detailedExplanation": "Use \"document conversion service\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-007",
      "type": "multiple-choice",
      "question": "Case Eta: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, billing reconciliation job fails mainly through slow drain after overload event. The best choice is \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "This prompt is really about \"billing reconciliation job\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-008",
      "type": "multiple-choice",
      "question": "Case Theta: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
      ],
      "correct": 3,
      "explanation": "Image processing pipeline should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" is strongest because it directly addresses token bucket mis-sized for burst profile and improves repeatability under stress. This aligns with the extra condition (Operations wants a reversible step before broader architecture changes).",
      "detailedExplanation": "The decision turns on \"image processing pipeline\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-009",
      "type": "multiple-choice",
      "question": "Case Iota: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Treat chat history service as a reliability-control decision, not an averages-only optimization. \"Precompute fallback responses for degraded mode to reduce dependency calls\" is correct since it mitigates degraded dependency still accepted at full rate while keeping containment local. The decision remains valid given: SLO burn rate accelerated after a config rollout this morning.",
      "detailedExplanation": "Read this as a scenario about \"chat history service\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-010",
      "type": "multiple-choice",
      "question": "Case Kappa: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, analytics ingestion endpoint fails mainly through overload spillover across tenants. The best choice is \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "Start from \"analytics ingestion endpoint\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-011",
      "type": "multiple-choice",
      "question": "Case Lambda: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Checkout aggregation tier should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Use per-priority admission control and shed low-value traffic before core flows\" is strongest because it directly addresses queue growth to unbounded latency and improves repeatability under stress. This aligns with the extra condition (The incident review highlighted missing boundary ownership).",
      "detailedExplanation": "The key clue in this question is \"checkout aggregation tier\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-012",
      "type": "multiple-choice",
      "question": "Case Mu: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits."
      ],
      "correct": 3,
      "explanation": "Treat recommendation API as a reliability-control decision, not an averages-only optimization. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" is correct since it mitigates thread pool exhaustion under burst while keeping containment local. The decision remains valid given: Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "detailedExplanation": "Read this as a scenario about \"recommendation API\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-013",
      "type": "multiple-choice",
      "question": "Case Nu: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For profile update worker pool, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Bound concurrency at each dependency boundary and reject above safe capacity\" outperforms the alternatives because it targets brownout on non-critical features and preserves safe recovery behavior. It is also the most compatible with A canary can be deployed immediately if the strategy is clear.",
      "detailedExplanation": "The decision turns on \"profile update worker pool\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-014",
      "type": "multiple-choice",
      "question": "Case Xi: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply brownout toggles to disable expensive non-critical work during stress.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, search fanout coordinator fails mainly through circuit thrashing due to poor thresholds. The best choice is \"Apply brownout toggles to disable expensive non-critical work during stress\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "This prompt is really about \"search fanout coordinator\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-015",
      "type": "multiple-choice",
      "question": "Case Omicron: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate queues by workload class to prevent priority inversion.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Feature-flag control API should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Separate queues by workload class to prevent priority inversion\" is strongest because it directly addresses priority inversion under shared queue and improves repeatability under stress. This aligns with the extra condition (Client retries are already elevated and could amplify mistakes).",
      "detailedExplanation": "Use \"feature-flag control API\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-016",
      "type": "multiple-choice",
      "question": "Case Pi: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates."
      ],
      "correct": 3,
      "explanation": "Treat document conversion service as a reliability-control decision, not an averages-only optimization. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" is correct since it mitigates head-of-line blocking in dependency pool while keeping containment local. The decision remains valid given: The team must preserve core write correctness under mitigation.",
      "detailedExplanation": "The core signal here is \"document conversion service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-017",
      "type": "multiple-choice",
      "question": "Case Rho: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "For billing reconciliation job, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" outperforms the alternatives because it targets slow drain after overload event and preserves safe recovery behavior. It is also the most compatible with Recent staffing changes require simpler operational controls.",
      "detailedExplanation": "If you keep \"billing reconciliation job\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-018",
      "type": "multiple-choice",
      "question": "Case Sigma: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, image processing pipeline fails mainly through token bucket mis-sized for burst profile. The best choice is \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Cross-region latency variance increased during the event.",
      "detailedExplanation": "Start from \"image processing pipeline\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-019",
      "type": "multiple-choice",
      "question": "Case Tau: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Chat history service should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Precompute fallback responses for degraded mode to reduce dependency calls\" is strongest because it directly addresses degraded dependency still accepted at full rate and improves repeatability under stress. This aligns with the extra condition (This path mixes latency-sensitive and correctness-sensitive requests).",
      "detailedExplanation": "The key clue in this question is \"chat history service\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior."
      ],
      "correct": 3,
      "explanation": "For analytics ingestion endpoint, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" outperforms the alternatives because it targets overload spillover across tenants and preserves safe recovery behavior. It is also the most compatible with The service has one hidden shared component with no backup path.",
      "detailedExplanation": "Use \"analytics ingestion endpoint\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-021",
      "type": "multiple-choice",
      "question": "Case Phi: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, checkout aggregation tier fails mainly through queue growth to unbounded latency. The best choice is \"Use per-priority admission control and shed low-value traffic before core flows\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "This prompt is really about \"checkout aggregation tier\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-022",
      "type": "multiple-choice",
      "question": "Case Chi: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Recommendation API should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" is strongest because it directly addresses thread pool exhaustion under burst and improves repeatability under stress. This aligns with the extra condition (Change approval favors narrowly scoped policies over global flips).",
      "detailedExplanation": "If you keep \"recommendation API\" in view, the correct answer separates faster. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-023",
      "type": "multiple-choice",
      "question": "Case Psi: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat profile update worker pool as a reliability-control decision, not an averages-only optimization. \"Bound concurrency at each dependency boundary and reject above safe capacity\" is correct since it mitigates brownout on non-critical features while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "The core signal here is \"profile update worker pool\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-024",
      "type": "multiple-choice",
      "question": "Case Omega: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Apply brownout toggles to disable expensive non-critical work during stress."
      ],
      "correct": 3,
      "explanation": "For search fanout coordinator, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Apply brownout toggles to disable expensive non-critical work during stress\" outperforms the alternatives because it targets circuit thrashing due to poor thresholds and preserves safe recovery behavior. It is also the most compatible with On-call needs mitigation that is observable by explicit metrics.",
      "detailedExplanation": "The key clue in this question is \"search fanout coordinator\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-025",
      "type": "multiple-choice",
      "question": "Case Atlas: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Separate queues by workload class to prevent priority inversion.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, feature-flag control API fails mainly through priority inversion under shared queue. The best choice is \"Separate queues by workload class to prevent priority inversion\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "Start from \"feature-flag control API\", then pressure-test the result against the options. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-026",
      "type": "multiple-choice",
      "question": "Case Nova: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Document conversion service should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" is strongest because it directly addresses head-of-line blocking in dependency pool and improves repeatability under stress. This aligns with the extra condition (Business impact is highest in the top 5% of critical flows).",
      "detailedExplanation": "The decision turns on \"document conversion service\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 5 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-027",
      "type": "multiple-choice",
      "question": "Case Orion: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "Treat billing reconciliation job as a reliability-control decision, not an averages-only optimization. \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" is correct since it mitigates slow drain after overload event while keeping containment local. The decision remains valid given: Regional failover is possible but expensive if used prematurely.",
      "detailedExplanation": "Read this as a scenario about \"billing reconciliation job\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-028",
      "type": "multiple-choice",
      "question": "Case Vega: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
      ],
      "correct": 3,
      "explanation": "For image processing pipeline, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" outperforms the alternatives because it targets token bucket mis-sized for burst profile and preserves safe recovery behavior. It is also the most compatible with A hot tenant currently consumes disproportionate worker capacity.",
      "detailedExplanation": "Use \"image processing pipeline\" as your starting point, then verify tradeoffs carefully. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-029",
      "type": "multiple-choice",
      "question": "Case Helios: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, chat history service fails mainly through degraded dependency still accepted at full rate. The best choice is \"Precompute fallback responses for degraded mode to reduce dependency calls\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "This prompt is really about \"chat history service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-030",
      "type": "multiple-choice",
      "question": "Case Aurora: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat analytics ingestion endpoint as a reliability-control decision, not an averages-only optimization. \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" is correct since it mitigates overload spillover across tenants while keeping containment local. The decision remains valid given: Queue age is rising even though average CPU appears normal.",
      "detailedExplanation": "Read this as a scenario about \"analytics ingestion endpoint\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For checkout aggregation tier, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Use per-priority admission control and shed low-value traffic before core flows\" outperforms the alternatives because it targets queue growth to unbounded latency and preserves safe recovery behavior. It is also the most compatible with A control-plane API is healthy but data-plane errors are increasing.",
      "detailedExplanation": "The decision turns on \"checkout aggregation tier\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-032",
      "type": "multiple-choice",
      "question": "Case Pulse: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits."
      ],
      "correct": 3,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, recommendation API fails mainly through thread pool exhaustion under burst. The best choice is \"Configure circuit breakers with stable windows and explicit half-open probe limits\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "Start from \"recommendation API\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-033",
      "type": "multiple-choice",
      "question": "Case Forge: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 0,
      "explanation": "Profile update worker pool should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Bound concurrency at each dependency boundary and reject above safe capacity\" is strongest because it directly addresses brownout on non-critical features and improves repeatability under stress. This aligns with the extra condition (Legal/compliance constraints require explicit behavior in degraded mode).",
      "detailedExplanation": "The key clue in this question is \"profile update worker pool\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-034",
      "type": "multiple-choice",
      "question": "Case Harbor: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply brownout toggles to disable expensive non-critical work during stress.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 1,
      "explanation": "Treat search fanout coordinator as a reliability-control decision, not an averages-only optimization. \"Apply brownout toggles to disable expensive non-critical work during stress\" is correct since it mitigates circuit thrashing due to poor thresholds while keeping containment local. The decision remains valid given: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "The core signal here is \"search fanout coordinator\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-035",
      "type": "multiple-choice",
      "question": "Case Vector: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Ignore the domain boundary and optimize only global average latency.",
        "Apply one uniform policy everywhere regardless workload criticality.",
        "Separate queues by workload class to prevent priority inversion.",
        "Rely on manual intervention as the primary control during recurring incidents."
      ],
      "correct": 2,
      "explanation": "For feature-flag control API, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Separate queues by workload class to prevent priority inversion\" outperforms the alternatives because it targets priority inversion under shared queue and preserves safe recovery behavior. It is also the most compatible with User trust impact is tied to visible inconsistency, not only downtime.",
      "detailedExplanation": "If you keep \"feature-flag control API\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Checkout aggregation tier is a two-step reliability decision. At stage 1, \"The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around circuit thrashing due to poor thresholds.",
          "detailedExplanation": "Start from \"incident diagnosis for checkout aggregation tier: signal points to circuit thrashing\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for checkout aggregation tier:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"circuit Breakers, Load Shedding & Admission Control\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents\" best matches recommendation API by targeting priority inversion under shared queue and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for recommendation API: signal points to priority inversion under\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for recommendation API: signal\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident diagnosis for recommendation API: signal\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"circuit Breakers, Load Shedding & Admission Control\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for profile update worker pool, \"The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents\" is correct because it addresses head-of-line blocking in dependency pool and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for profile update worker pool: signal points to head-of-line\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for profile update worker pool:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" best matches After diagnosing \"incident diagnosis for profile update worker pool:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"circuit Breakers, Load Shedding & Admission Control\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents\". It is the option most directly aligned to slow drain after overload event while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"incident diagnosis for search fanout coordinator: signal points to slow drain after\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for search fanout coordinator:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for In the \"incident diagnosis for search fanout coordinator:\" scenario, which next step is strongest under current constraints, \"Precompute fallback responses for degraded mode to reduce dependency calls\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The decision turns on \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents\" best matches feature-flag control API by targeting token bucket mis-sized for burst profile and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for feature-flag control API: signal points to token bucket\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for feature-flag control API: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Given the diagnosis in \"incident diagnosis for feature-flag control API: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"circuit Breakers, Load Shedding & Admission Control\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for document conversion service: signal points to degraded dependency still accepted at full rate. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for document conversion service, \"The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents\" is correct because it addresses degraded dependency still accepted at full rate and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for document conversion service: signal points to degraded\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for document conversion service:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Use per-priority admission control and shed low-value traffic before core flows.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use per-priority admission control and shed low-value traffic before core flows\" best matches With diagnosis confirmed in \"incident diagnosis for document conversion service:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The core signal here is \"circuit Breakers, Load Shedding & Admission Control\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for billing reconciliation job: signal points to overload spillover across tenants. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents\". It is the option most directly aligned to overload spillover across tenants while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for billing reconciliation job: signal points to overload spillover\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for billing reconciliation job:\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Configure circuit breakers with stable windows and explicit half-open probe limits."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for Using the diagnosis from \"incident diagnosis for billing reconciliation job:\", what first move gives the best reliability impact, \"Configure circuit breakers with stable windows and explicit half-open probe limits\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"circuit Breakers, Load Shedding & Admission Control\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for image processing pipeline: signal points to queue growth to unbounded latency. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Image processing pipeline is a two-step reliability decision. At stage 1, \"The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around queue growth to unbounded latency.",
          "detailedExplanation": "Start from \"incident diagnosis for image processing pipeline: signal points to queue growth to\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for image processing pipeline:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Bound concurrency at each dependency boundary and reject above safe capacity.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Bound concurrency at each dependency boundary and reject above safe capacity\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "This prompt is really about \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat history service: signal points to thread pool exhaustion under burst. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents\" best matches chat history service by targeting thread pool exhaustion under burst and lowering repeat risk.",
          "detailedExplanation": "Use \"incident diagnosis for chat history service: signal points to thread pool exhaustion\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for chat history service: signal\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Apply brownout toggles to disable expensive non-critical work during stress.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In the \"incident diagnosis for chat history service: signal\" scenario, what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Apply brownout toggles to disable expensive non-critical work during stress\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The decision turns on \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for analytics ingestion endpoint: signal points to brownout on non-critical features. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for analytics ingestion endpoint, \"The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents\" is correct because it addresses brownout on non-critical features and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for analytics ingestion endpoint: signal points to brownout on\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for analytics ingestion endpoint:\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Separate queues by workload class to prevent priority inversion.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Separate queues by workload class to prevent priority inversion\" best matches After diagnosing \"incident diagnosis for analytics ingestion endpoint:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"circuit Breakers, Load Shedding & Admission Control\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents\". It is the option most directly aligned to circuit thrashing due to poor thresholds while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"incident diagnosis for checkout aggregation tier: signal points to circuit thrashing\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for checkout aggregation tier:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for With root cause identified for \"incident diagnosis for checkout aggregation tier:\", which next change should be prioritized first, \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"circuit Breakers, Load Shedding & Admission Control\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Recommendation API is a two-step reliability decision. At stage 1, \"The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around priority inversion under shared queue.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for recommendation API: signal points to priority inversion under\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for recommendation API: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Start from \"circuit Breakers, Load Shedding & Admission Control\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents\" best matches profile update worker pool by targeting head-of-line blocking in dependency pool and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for profile update worker pool: signal points to head-of-line\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for profile update worker pool:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Given the diagnosis in \"incident diagnosis for profile update worker pool:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"circuit Breakers, Load Shedding & Admission Control\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for search fanout coordinator, \"The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents\" is correct because it addresses slow drain after overload event and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for search fanout coordinator: signal points to slow drain after\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for search fanout coordinator:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Precompute fallback responses for degraded mode to reduce dependency calls\" best matches With diagnosis confirmed in \"incident diagnosis for search fanout coordinator:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"circuit Breakers, Load Shedding & Admission Control\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Feature-flag control API is a two-step reliability decision. At stage 1, \"The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around token bucket mis-sized for burst profile.",
          "detailedExplanation": "Start from \"incident diagnosis for feature-flag control API: signal points to token bucket\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for feature-flag control API: signal\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "This prompt is really about \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for document conversion service: signal points to degraded dependency still accepted at full rate. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents\" best matches document conversion service by targeting degraded dependency still accepted at full rate and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for document conversion service: signal points to degraded\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for document conversion service:\", what is the highest-leverage change to make now?",
          "options": [
            "Use per-priority admission control and shed low-value traffic before core flows.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Using the diagnosis from \"incident diagnosis for document conversion service:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Use per-priority admission control and shed low-value traffic before core flows\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Use \"circuit Breakers, Load Shedding & Admission Control\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for billing reconciliation job: signal points to overload spillover across tenants. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for billing reconciliation job, \"The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents\" is correct because it addresses overload spillover across tenants and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident diagnosis for billing reconciliation job: signal points to overload spillover\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for billing reconciliation job:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Configure circuit breakers with stable windows and explicit half-open probe limits.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" best matches With diagnosis confirmed in \"incident diagnosis for billing reconciliation job:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The core signal here is \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for image processing pipeline: signal points to queue growth to unbounded latency. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents\". It is the option most directly aligned to queue growth to unbounded latency while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"incident diagnosis for image processing pipeline: signal points to queue growth to\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for image processing pipeline:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Bound concurrency at each dependency boundary and reject above safe capacity.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for Given the diagnosis in \"incident diagnosis for image processing pipeline:\", what should change first before wider rollout, \"Bound concurrency at each dependency boundary and reject above safe capacity\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "If you keep \"circuit Breakers, Load Shedding & Admission Control\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat history service: signal points to thread pool exhaustion under burst. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Chat history service is a two-step reliability decision. At stage 1, \"The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around thread pool exhaustion under burst.",
          "detailedExplanation": "This prompt is really about \"incident diagnosis for chat history service: signal points to thread pool exhaustion\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident diagnosis for chat history service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Apply brownout toggles to disable expensive non-critical work during stress."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Apply brownout toggles to disable expensive non-critical work during stress\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Start from \"circuit Breakers, Load Shedding & Admission Control\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for analytics ingestion endpoint: signal points to brownout on non-critical features. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents\" best matches analytics ingestion endpoint by targeting brownout on non-critical features and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident diagnosis for analytics ingestion endpoint: signal points to brownout on\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for analytics ingestion endpoint:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Separate queues by workload class to prevent priority inversion.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "With root cause identified for \"incident diagnosis for analytics ingestion endpoint:\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Separate queues by workload class to prevent priority inversion\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for checkout aggregation tier, \"The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents\" is correct because it addresses circuit thrashing due to poor thresholds and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident diagnosis for checkout aggregation tier: signal points to circuit thrashing\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for checkout aggregation tier:\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" best matches After diagnosing \"incident diagnosis for checkout aggregation tier:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents\". It is the option most directly aligned to priority inversion under shared queue while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"incident diagnosis for recommendation API: signal points to priority inversion under\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for recommendation API: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for In the \"incident diagnosis for recommendation API: signal\" scenario, what should change first before wider rollout, \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The decision turns on \"circuit Breakers, Load Shedding & Admission Control\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 2,
          "explanation": "Profile update worker pool is a two-step reliability decision. At stage 1, \"The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around head-of-line blocking in dependency pool.",
          "detailedExplanation": "Start from \"incident diagnosis for profile update worker pool: signal points to head-of-line\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for profile update worker pool:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"circuit Breakers, Load Shedding & Admission Control\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents\" best matches search fanout coordinator by targeting slow drain after overload event and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident diagnosis for search fanout coordinator: signal points to slow drain after\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for search fanout coordinator:\", which next step is strongest under current constraints?",
          "options": [
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 0,
          "explanation": "Using the diagnosis from \"incident diagnosis for search fanout coordinator:\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Precompute fallback responses for degraded mode to reduce dependency calls\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"circuit Breakers, Load Shedding & Admission Control\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "No diagnosis is needed because short-term retries will resolve the issue naturally.",
            "The event is random variance and does not indicate a reliability control gap.",
            "This is purely a monitoring issue with no architecture or policy implications."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents\". It is the option most directly aligned to token bucket mis-sized for burst profile while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"incident diagnosis for feature-flag control API: signal points to token bucket\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for feature-flag control API: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily so the system can process backlog faster.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
            "Delay architecture changes and continue current runbook without policy updates.",
            "Expand traffic immediately to prove confidence in current behavior."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for With root cause identified for \"incident diagnosis for feature-flag control API: signal\", which next step is strongest under current constraints, \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"circuit Breakers, Load Shedding & Admission Control\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-061",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which indicators most directly reveal cross-domain blast radius.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which indicators most directly reveal cross-domain blast radius needs layered controls, not one silver bullet. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"indicators most directly reveal cross-domain blast radius? (Select all that apply)\", then pressure-test the result against the options. Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-062",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which controls reduce hidden single points of failure.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Choose every valid option for this prompt: which controls reduce hidden single points of failure is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The decision turns on \"controls reduce hidden single points of failure? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-063",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: during partial failures, which practices improve diagnosis quality.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Read this as a scenario about \"during partial failures, which practices improve diagnosis quality? (Select all that\". Treat every option as a separate true/false test under the same constraints. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-064",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what belongs in a useful dependency failure taxonomy.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Choose every valid option for this prompt: what belongs in a useful dependency failure taxonomy, the highest-signal answer is a bundle of controls. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Use \"belongs in a useful dependency failure taxonomy? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-065",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which patterns limit correlated failures across zones.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which patterns limit correlated failures across zones needs layered controls, not one silver bullet. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "This prompt is really about \"patterns limit correlated failures across zones? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-066",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which runbook elements increase incident execution reliability.",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Choose every valid option for this prompt: which runbook elements increase incident execution reliability is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "If you keep \"runbook elements increase incident execution reliability? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-067",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which signals should trigger graceful isolation first.",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The core signal here is \"signals should trigger graceful isolation first? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-068",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which architectural choices help contain tenant-induced overload.",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Choose every valid option for this prompt: which architectural choices help contain tenant-induced overload, the highest-signal answer is a bundle of controls. The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The key clue in this question is \"architectural choices help contain tenant-induced overload? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-069",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: for reliability policies, which items should be explicit per endpoint.",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: for reliability policies, which items should be explicit per endpoint needs layered controls, not one silver bullet. The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"for reliability policies, which items should be explicit per endpoint? (Select all that\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-070",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which anti-patterns commonly enlarge outage blast radius. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The core signal here is \"anti-patterns commonly enlarge outage blast radius? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-071",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what improves confidence in failover assumptions. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Choose every valid option for this prompt: what improves confidence in failover assumptions, the highest-signal answer is a bundle of controls. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "If you keep \"improves confidence in failover assumptions? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-072",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which data is essential when classifying partial vs fail-stop incidents.",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which data is essential when classifying partial vs fail-stop incidents needs layered controls, not one silver bullet. The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "This prompt is really about \"data is essential when classifying partial vs fail-stop incidents? (Select all that\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-073",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which controls improve safety when control-plane health is uncertain.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Choose every valid option for this prompt: which controls improve safety when control-plane health is uncertain is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Use \"controls improve safety when control-plane health is uncertain? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-074",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: for critical writes, which guardrails reduce corruption risk under faults.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Read this as a scenario about \"for critical writes, which guardrails reduce corruption risk under faults? (Select all\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-075",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which recurring reviews keep reliability boundaries accurate over time.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Choose every valid option for this prompt: which recurring reviews keep reliability boundaries accurate over time, the highest-signal answer is a bundle of controls. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The decision turns on \"recurring reviews keep reliability boundaries accurate over time? (Select all that\". Validate each option independently; do not select statements that are only partially true. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-076",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which decisions help teams align on reliability trade-offs during incidents.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which decisions help teams align on reliability trade-offs during incidents needs layered controls, not one silver bullet. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"decisions help teams align on reliability trade-offs during incidents? (Select all that\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-077",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what evidence best shows a mitigation reduced recurrence risk.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Choose every valid option for this prompt: what evidence best shows a mitigation reduced recurrence risk is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The key clue in this question is \"evidence best shows a mitigation reduced recurrence risk? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-078",
      "type": "numeric-input",
      "question": "Based on a service processes 4,200,000 requests/day and 0.22% violate reliability SLO, how many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for Based on a service processes 4,200,000 requests/day and 0 gives 9240 requests. In interview pacing, hitting this value within +/-3% is the pass condition.",
      "detailedExplanation": "The core signal here is \"service processes 4,200,000 requests/day and 0\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 4,200 and 000 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-079",
      "type": "numeric-input",
      "question": "Based on incident queue receives 1,800 items/min and drains 2,050 items/min, net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Based on incident queue receives 1,800 items/min and drains 2,050 items/min, net drain rate: 250 items/min. Answers within +/-0% show correct directional reasoning for Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "If you keep \"incident queue receives 1,800 items/min and drains 2,050 items/min\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 1,800 and 2,050 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-080",
      "type": "numeric-input",
      "question": "Based on retry policy adds 0.35 extra attempts per request at 60,000 req/sec, effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Circuit Breakers, Load Shedding & Admission Control expects quick quantitative triage: Based on retry policy adds 0 evaluates to 81000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "The decision turns on \"retry policy adds 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 0.35 and 60,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-081",
      "type": "numeric-input",
      "question": "Based on failover takes 18 seconds and happens 21 times/day, total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for Based on failover takes 18 seconds and happens 21 times/day, total failover seconds/day gives 378 seconds. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Read this as a scenario about \"failover takes 18 seconds and happens 21 times/day\". Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 18 seconds and 21 in aligned units before selecting an answer. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-082",
      "type": "numeric-input",
      "question": "Based on target p99 latency is 700ms; observed p99 is 980ms, percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Based on target p99 latency is 700ms; observed p99 is 980ms, percent over target: 40 %. Answers within +/-30% show correct directional reasoning for Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "The key clue in this question is \"target p99 latency is 700ms\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 700ms and 980ms appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-083",
      "type": "numeric-input",
      "question": "For this case, if 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For For this case, if 31% of 120,000 requests/min are critical-path, how many critical requests/min, the computed target in Circuit Breakers, Load Shedding & Admission Control is 37200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Start from \"if 31% of 120,000 requests/min are critical-path, how many critical requests/min\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 31 and 120,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-084",
      "type": "numeric-input",
      "question": "Based on error rate drops from 1.2% to 0.3%, percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Circuit Breakers, Load Shedding & Admission Control expects quick quantitative triage: Based on error rate drops from 1 evaluates to 75 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "If you keep \"error rate drops from 1\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 1.2 and 0.3 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-085",
      "type": "numeric-input",
      "question": "Based on a 7-node quorum system requires majority writes, minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for Based on a 7-node quorum system requires majority writes, minimum acknowledgements required gives 4 acks. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "The core signal here is \"7-node quorum system requires majority writes\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 7 and 4 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-086",
      "type": "numeric-input",
      "question": "Based on backlog is 48,000 tasks and net drain is 320 tasks/min, minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Based on backlog is 48,000 tasks and net drain is 320 tasks/min, minutes to clear backlog: 150 minutes. Answers within +/-0% show correct directional reasoning for Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "Use \"backlog is 48,000 tasks and net drain is 320 tasks/min\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 48,000 and 320 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-087",
      "type": "numeric-input",
      "question": "Based on a system with 14 zones has 2 unavailable, what percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Based on a system with 14 zones has 2 unavailable, what percent remain available, the computed target in Circuit Breakers, Load Shedding & Admission Control is 85.71 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "This prompt is really about \"system with 14 zones has 2 unavailable\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 14 and 2 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-088",
      "type": "numeric-input",
      "question": "Based on mTTR improved from 45 min to 30 min, percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Circuit Breakers, Load Shedding & Admission Control expects quick quantitative triage: Based on mTTR improved from 45 min to 30 min, percent reduction evaluates to 33.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "The decision turns on \"mTTR improved from 45 min to 30 min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 45 min and 30 min should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-089",
      "type": "numeric-input",
      "question": "For this case, if 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for For this case, if 9% of 2,500,000 daily operations need manual recovery checks, checks/day gives 225000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "Read this as a scenario about \"if 9% of 2,500,000 daily operations need manual recovery checks, checks/day\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 9 and 2,500 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle. Focus on circuit breakers, load shedding & admission control tradeoffs.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Circuit Breakers, Load Shedding & Admission Control should start with Detect and scope affected fault domains and end with Validate recovery and harden recurrence defenses. Order a reliability response lifecycle rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Start from \"order a reliability response lifecycle\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-091",
      "type": "ordering",
      "question": "Order from lowest to highest reliability risk. Use a circuit breakers, load shedding & admission control perspective.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Isolated dependency with fallback and budget must happen before Implicit dependency with no failure policy. That ordering matches incident-safe flow in Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "The key clue in this question is \"order from lowest to highest reliability risk\". Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-092",
      "type": "ordering",
      "question": "Order failover safety steps. (circuit breakers, load shedding & admission control lens)",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Circuit Breakers, Load Shedding & Admission Control emphasizes safe recovery order. Beginning at Verify candidate health and freshness and finishing at Run failback readiness checks before restoration keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Read this as a scenario about \"order failover safety steps\". Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-093",
      "type": "ordering",
      "question": "From a circuit breakers, load shedding & admission control viewpoint, order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a circuit breakers, load shedding & admission control viewpoint, order by increasing overload-protection strength, the correct ordering runs from No admission limits to Priority-aware shedding plus per-domain concurrency bounds. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The decision turns on \"order by increasing overload-protection strength\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-094",
      "type": "ordering",
      "question": "Considering circuit breakers, load shedding & admission control, order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Circuit Breakers, Load Shedding & Admission Control should start with Select recovery point by RPO target and end with Promote and re-enable writes with monitoring. Considering circuit breakers, load shedding & admission control, order data recovery execution rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "This prompt is really about \"order data recovery execution\". Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-095",
      "type": "ordering",
      "question": "In this circuit breakers, load shedding & admission control context, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Define SLIs tied to user impact must happen before Review incidents and close corrective actions. That ordering matches incident-safe flow in Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "Use \"order reliability operations loop\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-096",
      "type": "ordering",
      "question": "Sort these in ascending blast-radius impact. (Circuit Breakers, Load Shedding & Admission Control context)",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Circuit Breakers, Load Shedding & Admission Control emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The core signal here is \"order by increasing blast radius\". Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-097",
      "type": "ordering",
      "question": "For circuit breakers, load shedding & admission control, order retry-policy maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For circuit breakers, load shedding & admission control, order retry-policy maturity, the correct ordering runs from Fixed immediate retries to Jittered backoff with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "If you keep \"order retry-policy maturity\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-098",
      "type": "ordering",
      "question": "Order degradation sophistication. Focus on circuit breakers, load shedding & admission control tradeoffs.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Circuit Breakers, Load Shedding & Admission Control should start with Undocumented ad hoc fallback and end with Automated policy-driven degradation with user semantics. Order degradation sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Start from \"order degradation sophistication\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-099",
      "type": "ordering",
      "question": "Order incident command rigor. Use a circuit breakers, load shedding & admission control perspective.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc responders with no roles must happen before Role-defined operations plus decision log and action tracking. That ordering matches incident-safe flow in Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "The key clue in this question is \"order incident command rigor\". Order by relative scale and bottleneck effect, then validate neighboring items. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-100",
      "type": "ordering",
      "question": "Order reliability validation confidence. (circuit breakers, load shedding & admission control lens)",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Circuit Breakers, Load Shedding & Admission Control emphasizes safe recovery order. Beginning at Single success in staging and finishing at Sustained recovery plus recurrence drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Read this as a scenario about \"order reliability validation confidence\". Build the rank from biggest differences first, then refine with adjacent checks. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    }
  ]
}
