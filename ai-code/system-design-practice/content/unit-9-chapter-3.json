{
  "unit": 9,
  "unitTitle": "Reliability",
  "chapter": 3,
  "chapterTitle": "Circuit Breakers, Load Shedding & Admission Control",
  "chapterDescription": "Protect systems under stress by bounding concurrency and failing safely before saturation cascades.",
  "problems": [
    {
      "id": "rel-ca-001",
      "type": "multiple-choice",
      "question": "Case Alpha: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? A rollback window is still available for the next 15 minutes.",
      "options": [
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat checkout aggregation tier as a reliability-control decision, not an averages-only optimization. \"Use per-priority admission control and shed low-value traffic before core flows\" is correct since it mitigates queue growth to unbounded latency while keeping containment local. The decision remains valid given: A rollback window is still available for the next 15 minutes.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 15 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-002",
      "type": "multiple-choice",
      "question": "Case Beta: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Leadership asked for an action that lowers recurrence, not just symptoms.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "For recommendation API, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" outperforms the alternatives because it targets thread pool exhaustion under burst and preserves safe recovery behavior. It is also the most compatible with Leadership asked for an action that lowers recurrence, not just symptoms.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-003",
      "type": "multiple-choice",
      "question": "Case Gamma: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? Two downstream teams depend on this path during peak traffic.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, profile update worker pool fails mainly through brownout on non-critical features. The best choice is \"Bound concurrency at each dependency boundary and reject above safe capacity\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Two downstream teams depend on this path during peak traffic.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-004",
      "type": "multiple-choice",
      "question": "Case Delta: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Recent game-day results showed hidden cross-zone coupling.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Apply brownout toggles to disable expensive non-critical work during stress."
      ],
      "correct": 3,
      "explanation": "Search fanout coordinator should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Apply brownout toggles to disable expensive non-critical work during stress\" is strongest because it directly addresses circuit thrashing due to poor thresholds and improves repeatability under stress. This aligns with the extra condition (Recent game-day results showed hidden cross-zone coupling).",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? Customer impact is concentrated on invariant-critical transactions.",
      "options": [
        "Separate queues by workload class to prevent priority inversion.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat feature-flag control API as a reliability-control decision, not an averages-only optimization. \"Separate queues by workload class to prevent priority inversion\" is correct since it mitigates priority inversion under shared queue while keeping containment local. The decision remains valid given: Customer impact is concentrated on invariant-critical transactions.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prioritize the approach that best protects reliability objectives under stated failure conditions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-006",
      "type": "multiple-choice",
      "question": "Case Zeta: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? The previous mitigation improved averages but not tail behavior.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "For document conversion service, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" outperforms the alternatives because it targets head-of-line blocking in dependency pool and preserves safe recovery behavior. It is also the most compatible with The previous mitigation improved averages but not tail behavior.",
      "detailedExplanation": "Generalize from document conversion service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-007",
      "type": "multiple-choice",
      "question": "Case Eta: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Telemetry indicates one fault domain is driving most failures.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, billing reconciliation job fails mainly through slow drain after overload event. The best choice is \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Telemetry indicates one fault domain is driving most failures.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-008",
      "type": "multiple-choice",
      "question": "Case Theta: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? Operations wants a reversible step before broader architecture changes.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
      ],
      "correct": 3,
      "explanation": "Image processing pipeline should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" is strongest because it directly addresses token bucket mis-sized for burst profile and improves repeatability under stress. This aligns with the extra condition (Operations wants a reversible step before broader architecture changes).",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-009",
      "type": "multiple-choice",
      "question": "Case Iota: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? SLO burn rate accelerated after a config rollout this morning.",
      "options": [
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Treat chat history service as a reliability-control decision, not an averages-only optimization. \"Precompute fallback responses for degraded mode to reduce dependency calls\" is correct since it mitigates degraded dependency still accepted at full rate while keeping containment local. The decision remains valid given: SLO burn rate accelerated after a config rollout this morning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-010",
      "type": "multiple-choice",
      "question": "Case Kappa: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? A shared dependency has uncertain health signals right now.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, analytics ingestion endpoint fails mainly through overload spillover across tenants. The best choice is \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A shared dependency has uncertain health signals right now.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-011",
      "type": "multiple-choice",
      "question": "Case Lambda: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? The incident review highlighted missing boundary ownership.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Checkout aggregation tier should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Use per-priority admission control and shed low-value traffic before core flows\" is strongest because it directly addresses queue growth to unbounded latency and improves repeatability under stress. This aligns with the extra condition (The incident review highlighted missing boundary ownership).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-012",
      "type": "multiple-choice",
      "question": "Case Mu: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits."
      ],
      "correct": 3,
      "explanation": "Treat recommendation API as a reliability-control decision, not an averages-only optimization. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" is correct since it mitigates thread pool exhaustion under burst while keeping containment local. The decision remains valid given: Current runbooks assume fail-stop behavior, but reality is partial failure.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-013",
      "type": "multiple-choice",
      "question": "Case Nu: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? A canary can be deployed immediately if the strategy is clear.",
      "options": [
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "For profile update worker pool, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Bound concurrency at each dependency boundary and reject above safe capacity\" outperforms the alternatives because it targets brownout on non-critical features and preserves safe recovery behavior. It is also the most compatible with A canary can be deployed immediately if the strategy is clear.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-014",
      "type": "multiple-choice",
      "question": "Case Xi: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Capacity remains available only in one neighboring zone.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Apply brownout toggles to disable expensive non-critical work during stress.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, search fanout coordinator fails mainly through circuit thrashing due to poor thresholds. The best choice is \"Apply brownout toggles to disable expensive non-critical work during stress\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Capacity remains available only in one neighboring zone.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-015",
      "type": "multiple-choice",
      "question": "Case Omicron: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? Client retries are already elevated and could amplify mistakes.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Separate queues by workload class to prevent priority inversion.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Feature-flag control API should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Separate queues by workload class to prevent priority inversion\" is strongest because it directly addresses priority inversion under shared queue and improves repeatability under stress. This aligns with the extra condition (Client retries are already elevated and could amplify mistakes).",
      "detailedExplanation": "Generalize from feature-flag control API to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-016",
      "type": "multiple-choice",
      "question": "Case Pi: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? The team must preserve core write correctness under mitigation.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates."
      ],
      "correct": 3,
      "explanation": "Treat document conversion service as a reliability-control decision, not an averages-only optimization. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" is correct since it mitigates head-of-line blocking in dependency pool while keeping containment local. The decision remains valid given: The team must preserve core write correctness under mitigation.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-017",
      "type": "multiple-choice",
      "question": "Case Rho: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Recent staffing changes require simpler operational controls.",
      "options": [
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "For billing reconciliation job, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" outperforms the alternatives because it targets slow drain after overload event and preserves safe recovery behavior. It is also the most compatible with Recent staffing changes require simpler operational controls.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-018",
      "type": "multiple-choice",
      "question": "Case Sigma: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? Cross-region latency variance increased during the event.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, image processing pipeline fails mainly through token bucket mis-sized for burst profile. The best choice is \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Cross-region latency variance increased during the event.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-019",
      "type": "multiple-choice",
      "question": "Case Tau: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? This path mixes latency-sensitive and correctness-sensitive requests.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Chat history service should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Precompute fallback responses for degraded mode to reduce dependency calls\" is strongest because it directly addresses degraded dependency still accepted at full rate and improves repeatability under stress. This aligns with the extra condition (This path mixes latency-sensitive and correctness-sensitive requests).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Reject approaches that sound good in general but do not reduce concrete reliability risk. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? The service has one hidden shared component with no backup path.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior."
      ],
      "correct": 3,
      "explanation": "For analytics ingestion endpoint, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" outperforms the alternatives because it targets overload spillover across tenants and preserves safe recovery behavior. It is also the most compatible with The service has one hidden shared component with no backup path.",
      "detailedExplanation": "Generalize from analytics ingestion endpoint to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-021",
      "type": "multiple-choice",
      "question": "Case Phi: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? The product team accepts degraded reads but not incorrect writes.",
      "options": [
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, checkout aggregation tier fails mainly through queue growth to unbounded latency. The best choice is \"Use per-priority admission control and shed low-value traffic before core flows\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts degraded reads but not incorrect writes.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-022",
      "type": "multiple-choice",
      "question": "Case Chi: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Change approval favors narrowly scoped policies over global flips.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Recommendation API should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" is strongest because it directly addresses thread pool exhaustion under burst and improves repeatability under stress. This aligns with the extra condition (Change approval favors narrowly scoped policies over global flips).",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-023",
      "type": "multiple-choice",
      "question": "Case Psi: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? A previous outage showed stale metadata can outlive infrastructure recovery. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Treat profile update worker pool as a reliability-control decision, not an averages-only optimization. \"Bound concurrency at each dependency boundary and reject above safe capacity\" is correct since it mitigates brownout on non-critical features while keeping containment local. The decision remains valid given: A previous outage showed stale metadata can outlive infrastructure recovery.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-024",
      "type": "multiple-choice",
      "question": "Case Omega: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? On-call needs mitigation that is observable by explicit metrics.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Apply brownout toggles to disable expensive non-critical work during stress."
      ],
      "correct": 3,
      "explanation": "For search fanout coordinator, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Apply brownout toggles to disable expensive non-critical work during stress\" outperforms the alternatives because it targets circuit thrashing due to poor thresholds and preserves safe recovery behavior. It is also the most compatible with On-call needs mitigation that is observable by explicit metrics.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-025",
      "type": "multiple-choice",
      "question": "Case Atlas: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? A recent dependency upgrade introduced unknown failure semantics.",
      "options": [
        "Separate queues by workload class to prevent priority inversion.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, feature-flag control API fails mainly through priority inversion under shared queue. The best choice is \"Separate queues by workload class to prevent priority inversion\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A recent dependency upgrade introduced unknown failure semantics.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-026",
      "type": "multiple-choice",
      "question": "Case Nova: document conversion service. Primary reliability risk is head-of-line blocking in dependency pool. Which next move is strongest? Business impact is highest in the top 5% of critical flows.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Document conversion service should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" is strongest because it directly addresses head-of-line blocking in dependency pool and improves repeatability under stress. This aligns with the extra condition (Business impact is highest in the top 5% of critical flows).",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prioritize the approach that best protects reliability objectives under stated failure conditions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 5 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-027",
      "type": "multiple-choice",
      "question": "Case Orion: billing reconciliation job. Primary reliability risk is slow drain after overload event. Which next move is strongest? Regional failover is possible but expensive if used prematurely.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "Treat billing reconciliation job as a reliability-control decision, not an averages-only optimization. \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" is correct since it mitigates slow drain after overload event while keeping containment local. The decision remains valid given: Regional failover is possible but expensive if used prematurely.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-028",
      "type": "multiple-choice",
      "question": "Case Vega: image processing pipeline. Primary reliability risk is token bucket mis-sized for burst profile. Which next move is strongest? A hot tenant currently consumes disproportionate worker capacity.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
      ],
      "correct": 3,
      "explanation": "For image processing pipeline, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" outperforms the alternatives because it targets token bucket mis-sized for burst profile and preserves safe recovery behavior. It is also the most compatible with A hot tenant currently consumes disproportionate worker capacity.",
      "detailedExplanation": "Generalize from image processing pipeline to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-029",
      "type": "multiple-choice",
      "question": "Case Helios: chat history service. Primary reliability risk is degraded dependency still accepted at full rate. Which next move is strongest? The immediate goal is to shrink blast radius while maintaining service.",
      "options": [
        "Precompute fallback responses for degraded mode to reduce dependency calls.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, chat history service fails mainly through degraded dependency still accepted at full rate. The best choice is \"Precompute fallback responses for degraded mode to reduce dependency calls\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The immediate goal is to shrink blast radius while maintaining service.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-030",
      "type": "multiple-choice",
      "question": "Case Aurora: analytics ingestion endpoint. Primary reliability risk is overload spillover across tenants. Which next move is strongest? Queue age is rising even though average CPU appears normal.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Treat analytics ingestion endpoint as a reliability-control decision, not an averages-only optimization. \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" is correct since it mitigates overload spillover across tenants while keeping containment local. The decision remains valid given: Queue age is rising even though average CPU appears normal.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: checkout aggregation tier. Primary reliability risk is queue growth to unbounded latency. Which next move is strongest? A control-plane API is healthy but data-plane errors are increasing.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Use per-priority admission control and shed low-value traffic before core flows.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "For checkout aggregation tier, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Use per-priority admission control and shed low-value traffic before core flows\" outperforms the alternatives because it targets queue growth to unbounded latency and preserves safe recovery behavior. It is also the most compatible with A control-plane API is healthy but data-plane errors are increasing.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-032",
      "type": "multiple-choice",
      "question": "Case Pulse: recommendation API. Primary reliability risk is thread pool exhaustion under burst. Which next move is strongest? Different teams currently use conflicting reliability vocabulary.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves.",
        "Configure circuit breakers with stable windows and explicit half-open probe limits."
      ],
      "correct": 3,
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, recommendation API fails mainly through thread pool exhaustion under burst. The best choice is \"Configure circuit breakers with stable windows and explicit half-open probe limits\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Different teams currently use conflicting reliability vocabulary.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-033",
      "type": "multiple-choice",
      "question": "Case Forge: profile update worker pool. Primary reliability risk is brownout on non-critical features. Which next move is strongest? Legal/compliance constraints require explicit behavior in degraded mode. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Bound concurrency at each dependency boundary and reject above safe capacity.",
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 0,
      "explanation": "Profile update worker pool should be solved at the failure boundary named in Circuit Breakers, Load Shedding & Admission Control. \"Bound concurrency at each dependency boundary and reject above safe capacity\" is strongest because it directly addresses brownout on non-critical features and improves repeatability under stress. This aligns with the extra condition (Legal/compliance constraints require explicit behavior in degraded mode).",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-034",
      "type": "multiple-choice",
      "question": "Case Harbor: search fanout coordinator. Primary reliability risk is circuit thrashing due to poor thresholds. Which next move is strongest? Past incidents show this failure mode recurs every quarter.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Apply brownout toggles to disable expensive non-critical work during stress.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 1,
      "explanation": "Treat search fanout coordinator as a reliability-control decision, not an averages-only optimization. \"Apply brownout toggles to disable expensive non-critical work during stress\" is correct since it mitigates circuit thrashing due to poor thresholds while keeping containment local. The decision remains valid given: Past incidents show this failure mode recurs every quarter.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that directly address failure mode, recovery path, and blast radius. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-035",
      "type": "multiple-choice",
      "question": "Case Vector: feature-flag control API. Primary reliability risk is priority inversion under shared queue. Which next move is strongest? User trust impact is tied to visible inconsistency, not only downtime.",
      "options": [
        "Prioritize median-latency improvements first and defer domain-boundary controls until tail-latency regressions become sustained.",
        "Standardize one baseline policy across workloads first, then add boundary-specific guardrails after rollout telemetry stabilizes.",
        "Separate queues by workload class to prevent priority inversion.",
        "Keep incident response mostly human-led with stronger runbooks, and delay deeper automation until alert quality improves."
      ],
      "correct": 2,
      "explanation": "For feature-flag control API, prefer the option that prevents reoccurrence in Circuit Breakers, Load Shedding & Admission Control. \"Separate queues by workload class to prevent priority inversion\" outperforms the alternatives because it targets priority inversion under shared queue and preserves safe recovery behavior. It is also the most compatible with User trust impact is tied to visible inconsistency, not only downtime.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. The on-call report includes repeated occurrences across multiple weeks. What is the primary diagnosis?",
          "options": [
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Checkout aggregation tier is a two-step reliability decision. At stage 1, \"The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around circuit thrashing due to poor thresholds.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for checkout aggregation tier:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. The same alert pattern appeared during the last failover drill. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents\" best matches recommendation API by targeting priority inversion under shared queue and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for recommendation API: signal\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident diagnosis for recommendation API: signal\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from circuit Breakers, Load Shedding & Admission Control to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. A recent release changed timeout and queue settings simultaneously. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for profile update worker pool, \"The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents\" is correct because it addresses head-of-line blocking in dependency pool and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident diagnosis for profile update worker pool:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" best matches After diagnosing \"incident diagnosis for profile update worker pool:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. Regional traffic shifted unexpectedly due to external dependency issues. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents\". It is the option most directly aligned to slow drain after overload event while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident diagnosis for search fanout coordinator: signal points to slow drain after to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for search fanout coordinator:\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for In the \"incident diagnosis for search fanout coordinator:\" scenario, which next step is strongest under current constraints, \"Precompute fallback responses for degraded mode to reduce dependency calls\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. Customer-support tickets show concentrated failures for premium tenants. What is the primary diagnosis?",
          "options": [
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents\" best matches feature-flag control API by targeting token bucket mis-sized for burst profile and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for feature-flag control API: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "Given the diagnosis in \"incident diagnosis for feature-flag control API: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for document conversion service: signal points to degraded dependency still accepted at full rate. The service map reveals one overloaded shared subdependency. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for document conversion service, \"The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents\" is correct because it addresses degraded dependency still accepted at full rate and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for document conversion service:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Use per-priority admission control and shed low-value traffic before core flows.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use per-priority admission control and shed low-value traffic before core flows\" best matches With diagnosis confirmed in \"incident diagnosis for document conversion service:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for billing reconciliation job: signal points to overload spillover across tenants. Recent postmortems flagged unclear ownership boundaries. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents\". It is the option most directly aligned to overload spillover across tenants while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for billing reconciliation job:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Configure circuit breakers with stable windows and explicit half-open probe limits."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for Using the diagnosis from \"incident diagnosis for billing reconciliation job:\", what first move gives the best reliability impact, \"Configure circuit breakers with stable windows and explicit half-open probe limits\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from circuit Breakers, Load Shedding & Admission Control to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for image processing pipeline: signal points to queue growth to unbounded latency. Saturation appears before autoscaling can react effectively. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Image processing pipeline is a two-step reliability decision. At stage 1, \"The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around queue growth to unbounded latency.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for image processing pipeline:\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Bound concurrency at each dependency boundary and reject above safe capacity.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Bound concurrency at each dependency boundary and reject above safe capacity\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat history service: signal points to thread pool exhaustion under burst. The team needs a mitigation that is safe to canary first. What is the primary diagnosis?",
          "options": [
            "The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents\" best matches chat history service by targeting thread pool exhaustion under burst and lowering repeat risk.",
          "detailedExplanation": "Generalize from incident diagnosis for chat history service: signal points to thread pool exhaustion to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident diagnosis for chat history service: signal\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Apply brownout toggles to disable expensive non-critical work during stress.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In the \"incident diagnosis for chat history service: signal\" scenario, what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Apply brownout toggles to disable expensive non-critical work during stress\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for analytics ingestion endpoint: signal points to brownout on non-critical features. A stale state window has already produced duplicate operations. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for analytics ingestion endpoint, \"The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents\" is correct because it addresses brownout on non-critical features and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for analytics ingestion endpoint:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Separate queues by workload class to prevent priority inversion.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Separate queues by workload class to prevent priority inversion\" best matches After diagnosing \"incident diagnosis for analytics ingestion endpoint:\", what first move gives the best reliability impact by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. A planned migration starts next week, raising risk tolerance questions. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents\". It is the option most directly aligned to circuit thrashing due to poor thresholds while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for checkout aggregation tier:\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for With root cause identified for \"incident diagnosis for checkout aggregation tier:\", which next change should be prioritized first, \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. Current dashboards lack one key domain-segmented signal. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "Recommendation API is a two-step reliability decision. At stage 1, \"The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around priority inversion under shared queue.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident diagnosis for recommendation API: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. Two related services apply inconsistent retry or failover policies. What is the primary diagnosis?",
          "options": [
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents\" best matches profile update worker pool by targeting head-of-line blocking in dependency pool and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for profile update worker pool:\", which next change should be prioritized first?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "Given the diagnosis in \"incident diagnosis for profile update worker pool:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. Error budget burn is now in the red for this service. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for search fanout coordinator, \"The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents\" is correct because it addresses slow drain after overload event and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for search fanout coordinator:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Precompute fallback responses for degraded mode to reduce dependency calls\" best matches With diagnosis confirmed in \"incident diagnosis for search fanout coordinator:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. An executive incident review requests explicit long-term hardening. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Feature-flag control API is a two-step reliability decision. At stage 1, \"The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around token bucket mis-sized for burst profile.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident diagnosis for feature-flag control API: signal\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for document conversion service: signal points to degraded dependency still accepted at full rate. This path is business-critical during a recurring daily peak. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for document conversion service is mismatched to degraded dependency still accepted at full rate, creating repeat reliability incidents\" best matches document conversion service by targeting degraded dependency still accepted at full rate and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for document conversion service:\", what is the highest-leverage change to make now?",
          "options": [
            "Use per-priority admission control and shed low-value traffic before core flows.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Using the diagnosis from \"incident diagnosis for document conversion service:\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Use per-priority admission control and shed low-value traffic before core flows\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Generalize from circuit Breakers, Load Shedding & Admission Control to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for billing reconciliation job: signal points to overload spillover across tenants. Previous fixes optimized throughput but missed correctness controls. What is the primary diagnosis?",
          "options": [
            "The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for billing reconciliation job, \"The design for billing reconciliation job is mismatched to overload spillover across tenants, creating repeat reliability incidents\" is correct because it addresses overload spillover across tenants and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident diagnosis for billing reconciliation job:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Configure circuit breakers with stable windows and explicit half-open probe limits.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Configure circuit breakers with stable windows and explicit half-open probe limits\" best matches With diagnosis confirmed in \"incident diagnosis for billing reconciliation job:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for image processing pipeline: signal points to queue growth to unbounded latency. The incident is now affecting one zone and spreading slowly. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for image processing pipeline is mismatched to queue growth to unbounded latency, creating repeat reliability incidents\". It is the option most directly aligned to queue growth to unbounded latency while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident diagnosis for image processing pipeline:\", what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Bound concurrency at each dependency boundary and reject above safe capacity.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for Given the diagnosis in \"incident diagnosis for image processing pipeline:\", what should change first before wider rollout, \"Bound concurrency at each dependency boundary and reject above safe capacity\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for chat history service: signal points to thread pool exhaustion under burst. Traffic mix changed after a mobile-app release. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Chat history service is a two-step reliability decision. At stage 1, \"The design for chat history service is mismatched to thread pool exhaustion under burst, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around thread pool exhaustion under burst.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident diagnosis for chat history service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Apply brownout toggles to disable expensive non-critical work during stress."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Apply brownout toggles to disable expensive non-critical work during stress\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for analytics ingestion endpoint: signal points to brownout on non-critical features. A backup path exists but has not been validated this month. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for analytics ingestion endpoint is mismatched to brownout on non-critical features, creating repeat reliability incidents\" best matches analytics ingestion endpoint by targeting brownout on non-critical features and lowering repeat risk.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for analytics ingestion endpoint:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Separate queues by workload class to prevent priority inversion.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "With root cause identified for \"incident diagnosis for analytics ingestion endpoint:\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Separate queues by workload class to prevent priority inversion\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for checkout aggregation tier: signal points to circuit thrashing due to poor thresholds. The team can deploy one targeted policy update in under an hour. What is the primary diagnosis?",
          "options": [
            "The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Circuit Breakers, Load Shedding & Admission Control: for checkout aggregation tier, \"The design for checkout aggregation tier is mismatched to circuit thrashing due to poor thresholds, creating repeat reliability incidents\" is correct because it addresses circuit thrashing due to poor thresholds and improves controllability.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident diagnosis for checkout aggregation tier:\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Use token-bucket guarding with burst-aware refill tuned to backend recovery rates.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use token-bucket guarding with burst-aware refill tuned to backend recovery rates\" best matches After diagnosing \"incident diagnosis for checkout aggregation tier:\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for recommendation API: signal points to priority inversion under shared queue. A synthetic probe confirms inconsistent behavior across fault domains. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for recommendation API is mismatched to priority inversion under shared queue, creating repeat reliability incidents\". It is the option most directly aligned to priority inversion under shared queue while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize from incident diagnosis for recommendation API: signal points to priority inversion under to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident diagnosis for recommendation API: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Fail closed on invariant-critical paths and fail open only where stale data is acceptable.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for In the \"incident diagnosis for recommendation API: signal\" scenario, what should change first before wider rollout, \"Fail closed on invariant-critical paths and fail open only where stale data is acceptable\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for profile update worker pool: signal points to head-of-line blocking in dependency pool. The top failure class now accounts for more than half of incidents. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 2,
          "explanation": "Profile update worker pool is a two-step reliability decision. At stage 1, \"The design for profile update worker pool is mismatched to head-of-line blocking in dependency pool, creating repeat reliability incidents\" wins because it balances immediate containment with long-term prevention around head-of-line blocking in dependency pool.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident diagnosis for profile update worker pool:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes.",
            "Tie shedding policy to SLO/error-budget burn rather than raw CPU only."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"Tie shedding policy to SLO/error-budget burn rather than raw CPU only\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for search fanout coordinator: signal points to slow drain after overload event. There is pressure to avoid broad architecture rewrites during business hours. What is the primary diagnosis?",
          "options": [
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves.",
            "The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The design for search fanout coordinator is mismatched to slow drain after overload event, creating repeat reliability incidents\" best matches search fanout coordinator by targeting slow drain after overload event and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident diagnosis for search fanout coordinator:\", which next step is strongest under current constraints?",
          "options": [
            "Precompute fallback responses for degraded mode to reduce dependency calls.",
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 0,
          "explanation": "Using the diagnosis from \"incident diagnosis for search fanout coordinator:\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Precompute fallback responses for degraded mode to reduce dependency calls\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Generalize from circuit Breakers, Load Shedding & Admission Control to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident diagnosis for feature-flag control API: signal points to token bucket mis-sized for burst profile. Audit stakeholders require clear traceability for mitigation decisions. What is the primary diagnosis?",
          "options": [
            "The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents.",
            "Favor short-term retry/backoff tuning first and defer deeper diagnosis unless symptoms persist across multiple release cycles.",
            "Classify the event as transient variance and confirm with one more observation window before adding persistent controls.",
            "Prioritize telemetry refinement first and postpone architecture or policy changes until observability confidence improves."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Circuit Breakers, Load Shedding & Admission Control, the best answer is \"The design for feature-flag control API is mismatched to token bucket mis-sized for burst profile, creating repeat reliability incidents\". It is the option most directly aligned to token bucket mis-sized for burst profile while preserving safe follow-on actions.",
          "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident diagnosis for feature-flag control API: signal\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily loosen protection limits to drain backlog faster, then re-tighten controls once steady-state risk is verified.",
            "Add overload telemetry that exposes dropped-by-class and queue-age behavior.",
            "Continue the current runbook and postpone policy-level changes until after one more full incident review cycle.",
            "Increase traffic gradually to improve confidence in current behavior before making major control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Circuit Breakers, Load Shedding & Admission Control: for With root cause identified for \"incident diagnosis for feature-flag control API: signal\", which next step is strongest under current constraints, \"Add overload telemetry that exposes dropped-by-class and queue-age behavior\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-061",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which indicators most directly reveal cross-domain blast radius.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which indicators most directly reveal cross-domain blast radius needs layered controls, not one silver bullet. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-062",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which controls reduce hidden single points of failure.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Choose every valid option for this prompt: which controls reduce hidden single points of failure is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-063",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: during partial failures, which practices improve diagnosis quality.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-064",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what belongs in a useful dependency failure taxonomy.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Choose every valid option for this prompt: what belongs in a useful dependency failure taxonomy, the highest-signal answer is a bundle of controls. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Generalize from belongs in a useful dependency failure taxonomy? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-065",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which patterns limit correlated failures across zones.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which patterns limit correlated failures across zones needs layered controls, not one silver bullet. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-066",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which runbook elements increase incident execution reliability.",
      "options": [
        "Write fencing during failback",
        "Rollback checkpoints in runbooks",
        "Promote any available replica immediately",
        "Freshness checks before promotion"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Choose every valid option for this prompt: which runbook elements increase incident execution reliability is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Write fencing during failback, Rollback checkpoints in runbooks, and Freshness checks before promotion. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-067",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which signals should trigger graceful isolation first.",
      "options": [
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation",
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Blast-radius mapping for shared services, Error/latency spikes correlated by fault domain, and Dependency saturation by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-068",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which architectural choices help contain tenant-induced overload.",
      "options": [
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria",
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Choose every valid option for this prompt: which architectural choices help contain tenant-induced overload, the highest-signal answer is a bundle of controls. The correct combination is Explicit runbooks with abort criteria, Guardrails for degraded modes, and Dependency budgets for critical paths. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-069",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: for reliability policies, which items should be explicit per endpoint.",
      "options": [
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries",
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: for reliability policies, which items should be explicit per endpoint needs layered controls, not one silver bullet. The correct combination is Priority-aware admission controls, Clear fail-open/fail-closed boundaries, and Per-domain isolation of shared dependencies. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-070",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which anti-patterns commonly enlarge outage blast radius. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior",
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Validation drills for mitigation changes, Updated contracts for degraded behavior, and Postmortem actions tracked to closure. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Evaluate each candidate approach independently under the same constraints. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-071",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what improves confidence in failover assumptions. (Circuit Breakers, Load Shedding & Admission Control context)",
      "options": [
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop",
        "Canary failover tests by zone",
        "Independent control-plane dependencies"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Choose every valid option for this prompt: what improves confidence in failover assumptions, the highest-signal answer is a bundle of controls. The correct combination is Per-tenant isolation limits, Canary failover tests by zone, and Independent control-plane dependencies. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-072",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which data is essential when classifying partial vs fail-stop incidents.",
      "options": [
        "Promote any available replica immediately",
        "Freshness checks before promotion",
        "Write fencing during failback",
        "Rollback checkpoints in runbooks"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which data is essential when classifying partial vs fail-stop incidents needs layered controls, not one silver bullet. The correct combination is Freshness checks before promotion, Write fencing during failback, and Rollback checkpoints in runbooks. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-073",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which controls improve safety when control-plane health is uncertain.",
      "options": [
        "Error/latency spikes correlated by fault domain",
        "Dependency saturation by priority class",
        "Blast-radius mapping for shared services",
        "Single global average latency without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Choose every valid option for this prompt: which controls improve safety when control-plane health is uncertain is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Error/latency spikes correlated by fault domain, Dependency saturation by priority class, and Blast-radius mapping for shared services. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize from controls improve safety when control-plane health is uncertain? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-074",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: for critical writes, which guardrails reduce corruption risk under faults.",
      "options": [
        "Guardrails for degraded modes",
        "Dependency budgets for critical paths",
        "Unbounded retries as a universal fix",
        "Explicit runbooks with abort criteria"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Circuit Breakers, Load Shedding & Admission Control: The correct combination is Guardrails for degraded modes, Dependency budgets for critical paths, and Explicit runbooks with abort criteria. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-075",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which recurring reviews keep reliability boundaries accurate over time.",
      "options": [
        "Per-domain isolation of shared dependencies",
        "Bulk traffic expansion before root-cause triage",
        "Priority-aware admission controls",
        "Clear fail-open/fail-closed boundaries"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Choose every valid option for this prompt: which recurring reviews keep reliability boundaries accurate over time, the highest-signal answer is a bundle of controls. The correct combination is Per-domain isolation of shared dependencies, Priority-aware admission controls, and Clear fail-open/fail-closed boundaries. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-076",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which decisions help teams align on reliability trade-offs during incidents.",
      "options": [
        "Relying on tribal knowledge without documentation",
        "Postmortem actions tracked to closure",
        "Validation drills for mitigation changes",
        "Updated contracts for degraded behavior"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Circuit Breakers, Load Shedding & Admission Control, Choose every valid option for this prompt: which decisions help teams align on reliability trade-offs during incidents needs layered controls, not one silver bullet. The correct combination is Postmortem actions tracked to closure, Validation drills for mitigation changes, and Updated contracts for degraded behavior. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-077",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what evidence best shows a mitigation reduced recurrence risk.",
      "options": [
        "Canary failover tests by zone",
        "Independent control-plane dependencies",
        "Per-tenant isolation limits",
        "Assuming all failures are fail-stop"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Choose every valid option for this prompt: what evidence best shows a mitigation reduced recurrence risk is intentionally multi-dimensional in Circuit Breakers, Load Shedding & Admission Control. The correct combination is Canary failover tests by zone, Independent control-plane dependencies, and Per-tenant isolation limits. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-078",
      "type": "numeric-input",
      "question": "Based on a service processes 4,200,000 requests/day and 0.22% violate reliability SLO, how many violations/day?",
      "answer": 9240,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for Based on a service processes 4,200,000 requests/day and 0 gives 9240 requests. In interview pacing, hitting this value within +/-3% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 4,200 and 000 in aligned units before deciding on an implementation approach. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-079",
      "type": "numeric-input",
      "question": "Based on incident queue receives 1,800 items/min and drains 2,050 items/min, net drain rate?",
      "answer": 250,
      "unit": "items/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Based on incident queue receives 1,800 items/min and drains 2,050 items/min, net drain rate: 250 items/min. Answers within +/-0% show correct directional reasoning for Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 1,800 and 2,050 in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-080",
      "type": "numeric-input",
      "question": "Based on retry policy adds 0.35 extra attempts per request at 60,000 req/sec, effective attempts/sec?",
      "answer": 81000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Circuit Breakers, Load Shedding & Admission Control expects quick quantitative triage: Based on retry policy adds 0 evaluates to 81000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 0.35 and 60,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-081",
      "type": "numeric-input",
      "question": "Based on failover takes 18 seconds and happens 21 times/day, total failover seconds/day?",
      "answer": 378,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for Based on failover takes 18 seconds and happens 21 times/day, total failover seconds/day gives 378 seconds. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. Tie decisions to concrete operational outcomes, not abstract reliability language. Keep quantities like 18 seconds and 21 in aligned units before deciding on an implementation approach. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-082",
      "type": "numeric-input",
      "question": "Based on target p99 latency is 700ms; observed p99 is 980ms, percent over target?",
      "answer": 40,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Based on target p99 latency is 700ms; observed p99 is 980ms, percent over target: 40 %. Answers within +/-30% show correct directional reasoning for Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 700ms and 980ms appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-083",
      "type": "numeric-input",
      "question": "For this case, if 31% of 120,000 requests/min are critical-path, how many critical requests/min?",
      "answer": 37200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For this case, if 31% of 120,000 requests/min are critical-path, how many critical requests/min, the computed target in Circuit Breakers, Load Shedding & Admission Control is 37200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 31 and 120,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-084",
      "type": "numeric-input",
      "question": "Based on error rate drops from 1.2% to 0.3%, percent reduction?",
      "answer": 75,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Circuit Breakers, Load Shedding & Admission Control expects quick quantitative triage: Based on error rate drops from 1 evaluates to 75 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep every transformation in one unit system and check order of magnitude at the end. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 1.2 and 0.3 in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-085",
      "type": "numeric-input",
      "question": "Based on a 7-node quorum system requires majority writes, minimum acknowledgements required?",
      "answer": 4,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for Based on a 7-node quorum system requires majority writes, minimum acknowledgements required gives 4 acks. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 7 and 4 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-086",
      "type": "numeric-input",
      "question": "Based on backlog is 48,000 tasks and net drain is 320 tasks/min, minutes to clear backlog?",
      "answer": 150,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Based on backlog is 48,000 tasks and net drain is 320 tasks/min, minutes to clear backlog: 150 minutes. Answers within +/-0% show correct directional reasoning for Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "Generalize from backlog is 48,000 tasks and net drain is 320 tasks/min to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie decisions to concrete operational outcomes, not abstract reliability language. Numbers such as 48,000 and 320 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-087",
      "type": "numeric-input",
      "question": "Based on a system with 14 zones has 2 unavailable, what percent remain available?",
      "answer": 85.71,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Based on a system with 14 zones has 2 unavailable, what percent remain available, the computed target in Circuit Breakers, Load Shedding & Admission Control is 85.71 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Normalize units before computing so conversion mistakes do not propagate. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. If values like 14 and 2 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-088",
      "type": "numeric-input",
      "question": "Based on mTTR improved from 45 min to 30 min, percent reduction?",
      "answer": 33.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Circuit Breakers, Load Shedding & Admission Control expects quick quantitative triage: Based on mTTR improved from 45 min to 30 min, percent reduction evaluates to 33.33 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 45 min and 30 min should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-089",
      "type": "numeric-input",
      "question": "For this case, if 9% of 2,500,000 daily operations need manual recovery checks, checks/day?",
      "answer": 225000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for this case, if 9% of 2,500,000 daily operations need manual recovery checks, checks/day gives 225000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 9 and 2,500 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-090",
      "type": "ordering",
      "question": "Order a reliability response lifecycle. Focus on circuit breakers, load shedding & admission control tradeoffs.",
      "items": [
        "Detect and scope affected fault domains",
        "Contain blast radius with safe controls",
        "Apply targeted root-cause mitigation",
        "Validate recovery and harden recurrence defenses"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Circuit Breakers, Load Shedding & Admission Control should start with Detect and scope affected fault domains and end with Validate recovery and harden recurrence defenses. Order a reliability response lifecycle rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-091",
      "type": "ordering",
      "question": "Order from lowest to highest reliability risk. Use a circuit breakers, load shedding & admission control perspective.",
      "items": [
        "Isolated dependency with fallback and budget",
        "Shared dependency with guardrails",
        "Shared dependency without domain limits",
        "Implicit dependency with no failure policy"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Isolated dependency with fallback and budget must happen before Implicit dependency with no failure policy. That ordering matches incident-safe flow in Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Place obvious extremes first, then sort the middle by pairwise comparison. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-092",
      "type": "ordering",
      "question": "Order failover safety steps. (circuit breakers, load shedding & admission control lens)",
      "items": [
        "Verify candidate health and freshness",
        "Fence stale writers and freeze unsafe paths",
        "Shift critical traffic gradually",
        "Run failback readiness checks before restoration"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Circuit Breakers, Load Shedding & Admission Control emphasizes safe recovery order. Beginning at Verify candidate health and freshness and finishing at Run failback readiness checks before restoration keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-093",
      "type": "ordering",
      "question": "From a circuit breakers, load shedding & admission control viewpoint, order by increasing overload-protection strength.",
      "items": [
        "No admission limits",
        "Global static request cap",
        "Priority-aware shedding",
        "Priority-aware shedding plus per-domain concurrency bounds"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a circuit breakers, load shedding & admission control viewpoint, order by increasing overload-protection strength, the correct ordering runs from No admission limits to Priority-aware shedding plus per-domain concurrency bounds. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Order by relative scale and bottleneck effect, then validate neighboring items. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-094",
      "type": "ordering",
      "question": "Considering circuit breakers, load shedding & admission control, order data recovery execution.",
      "items": [
        "Select recovery point by RPO target",
        "Restore into validation environment",
        "Verify integrity and reconcile diffs",
        "Promote and re-enable writes with monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Circuit Breakers, Load Shedding & Admission Control should start with Select recovery point by RPO target and end with Promote and re-enable writes with monitoring. Considering circuit breakers, load shedding & admission control, order data recovery execution rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Place obvious extremes first, then sort the middle by pairwise comparison. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-095",
      "type": "ordering",
      "question": "In this circuit breakers, load shedding & admission control context, order reliability operations loop.",
      "items": [
        "Define SLIs tied to user impact",
        "Set SLO and error-budget policy",
        "Operate alerts/runbooks against policy",
        "Review incidents and close corrective actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Define SLIs tied to user impact must happen before Review incidents and close corrective actions. That ordering matches incident-safe flow in Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "Generalize from order reliability operations loop to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-096",
      "type": "ordering",
      "question": "Sort these in ascending blast-radius impact. (Circuit Breakers, Load Shedding & Admission Control context)",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Circuit Breakers, Load Shedding & Admission Control emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Build the rank from biggest differences first, then refine with adjacent checks. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-097",
      "type": "ordering",
      "question": "For circuit breakers, load shedding & admission control, order retry-policy maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential backoff",
        "Capped backoff with jitter",
        "Jittered backoff with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For circuit breakers, load shedding & admission control, order retry-policy maturity, the correct ordering runs from Fixed immediate retries to Jittered backoff with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "In interviews and real systems work, anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Place obvious extremes first, then sort the middle by pairwise comparison. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-098",
      "type": "ordering",
      "question": "Order degradation sophistication. Focus on circuit breakers, load shedding & admission control tradeoffs.",
      "items": [
        "Undocumented ad hoc fallback",
        "Manual kill switch only",
        "Documented fallback tiers per endpoint",
        "Automated policy-driven degradation with user semantics"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Circuit Breakers, Load Shedding & Admission Control should start with Undocumented ad hoc fallback and end with Automated policy-driven degradation with user semantics. Order degradation sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "In interviews and real systems work, begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Place obvious extremes first, then sort the middle by pairwise comparison. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-099",
      "type": "ordering",
      "question": "Order incident command rigor. Use a circuit breakers, load shedding & admission control perspective.",
      "items": [
        "Ad hoc responders with no roles",
        "Named incident commander only",
        "Commander plus role-defined operations",
        "Role-defined operations plus decision log and action tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Ad hoc responders with no roles must happen before Role-defined operations plus decision log and action tracking. That ordering matches incident-safe flow in Circuit Breakers, Load Shedding & Admission Control.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "rel-ca-100",
      "type": "ordering",
      "question": "Order reliability validation confidence. (circuit breakers, load shedding & admission control lens)",
      "items": [
        "Single success in staging",
        "Limited production canary success",
        "Sustained SLO recovery in production",
        "Sustained recovery plus recurrence drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Circuit Breakers, Load Shedding & Admission Control emphasizes safe recovery order. Beginning at Single success in staging and finishing at Sustained recovery plus recurrence drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Build the rank from biggest differences first, then refine with adjacent checks. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "reliability",
        "circuit-breakers-load-shedding-and-admission-control"
      ],
      "difficulty": "staff-level"
    }
  ]
}
