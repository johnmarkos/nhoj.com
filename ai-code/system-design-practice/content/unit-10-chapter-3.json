{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 3,
  "chapterTitle": "URL Shortener Core Architecture",
  "chapterDescription": "Decompose short-link creation and redirect serving paths with key generation, correctness, and safety constraints.",
  "problems": [
    {
      "id": "cd-us-001",
      "type": "multiple-choice",
      "question": "Case Alpha: short-link creation API. Dominant risk is key collision under concurrent create requests. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Use collision-resistant IDs with retry-safe uniqueness checks at write time.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat short-link creation API as a reliability-control decision, not an averages-only optimization. \"Use collision-resistant IDs with retry-safe uniqueness checks at write time\" is correct since it mitigates key collision under concurrent create requests while keeping containment local. The decision remains valid given: The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "The core signal here is \"short-link creation API\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-002",
      "type": "multiple-choice",
      "question": "Case Beta: key generation service. Dominant risk is redirect latency spikes from cold metadata fetches. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Cache redirect metadata at edge with short TTL and explicit invalidation hooks.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For key generation service, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Cache redirect metadata at edge with short TTL and explicit invalidation hooks\" outperforms the alternatives because it targets redirect latency spikes from cold metadata fetches and preserves safe recovery behavior. It is also the most compatible with Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "Use \"key generation service\" as your starting point, then verify tradeoffs carefully. Prefer the choice that remains viable across the planning horizon, not just today. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: forecasting volume without resource thresholds.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-003",
      "type": "multiple-choice",
      "question": "Case Gamma: redirect resolver. Dominant risk is abuse links bypassing validation path. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Run synchronous abuse/security guardrails on creation before key persistence.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In URL Shortener Core Architecture, redirect resolver fails mainly through abuse links bypassing validation path. The best choice is \"Run synchronous abuse/security guardrails on creation before key persistence\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "detailedExplanation": "This prompt is really about \"redirect resolver\". Prioritize the option that best protects the reliability objective under the stated failure conditions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-004",
      "type": "multiple-choice",
      "question": "Case Delta: alias reservation path. Dominant risk is expired links served due to stale edge cache. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Separate hot-link routing from long-tail storage reads using tiered cache strategy."
      ],
      "correct": 3,
      "explanation": "Alias reservation path should be solved at the failure boundary named in URL Shortener Core Architecture. \"Separate hot-link routing from long-tail storage reads using tiered cache strategy\" is strongest because it directly addresses expired links served due to stale edge cache and improves repeatability under stress. This aligns with the extra condition (A previous rollback fixed averages but not tail impact).",
      "detailedExplanation": "The decision turns on \"alias reservation path\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: link metadata store. Dominant risk is hot-key traffic on viral links overloading origin. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Enforce alias reservation with transactional uniqueness and ownership verification.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat link metadata store as a reliability-control decision, not an averages-only optimization. \"Enforce alias reservation with transactional uniqueness and ownership verification\" is correct since it mitigates hot-key traffic on viral links overloading origin while keeping containment local. The decision remains valid given: User trust risk is highest on this path.",
      "detailedExplanation": "Read this as a scenario about \"link metadata store\". Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-006",
      "type": "multiple-choice",
      "question": "Case Zeta: abuse detection pre-check. Dominant risk is alias race conditions causing ownership conflicts. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Decouple analytics writes from redirect critical path with async durable queue.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For abuse detection pre-check, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Decouple analytics writes from redirect critical path with async durable queue\" outperforms the alternatives because it targets alias race conditions causing ownership conflicts and preserves safe recovery behavior. It is also the most compatible with A shared dependency has uncertain health right now.",
      "detailedExplanation": "The key clue in this question is \"abuse detection pre-check\". Prefer the answer that survives a sanity check against known anchor numbers. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-007",
      "type": "multiple-choice",
      "question": "Case Eta: TTL/expiration scheduler. Dominant risk is write path blocking on slow analytics dependencies. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Provide read-after-write guarantees for create-to-first-redirect user flow.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In URL Shortener Core Architecture, TTL/expiration scheduler fails mainly through write path blocking on slow analytics dependencies. The best choice is \"Provide read-after-write guarantees for create-to-first-redirect user flow\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The change must preserve cost discipline during peak.",
      "detailedExplanation": "Start from \"tTL/expiration scheduler\", then pressure-test the result against the options. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-008",
      "type": "multiple-choice",
      "question": "Case Theta: global redirect edge layer. Dominant risk is eventual consistency causing transient 404 after creation. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Shard key generation and avoid single sequencer dependence for global scale."
      ],
      "correct": 3,
      "explanation": "Global redirect edge layer should be solved at the failure boundary named in URL Shortener Core Architecture. \"Shard key generation and avoid single sequencer dependence for global scale\" is strongest because it directly addresses eventual consistency causing transient 404 after creation and improves repeatability under stress. This aligns with the extra condition (Telemetry shows risk concentrated in one partition class).",
      "detailedExplanation": "If you keep \"global redirect edge layer\" in view, the correct answer separates faster. Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Keep quantities like 404 in aligned units before selecting an answer. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "cd-us-009",
      "type": "multiple-choice",
      "question": "Case Iota: custom domain mapping service. Dominant risk is counter-based key service becoming single bottleneck. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Treat expiration/deletion updates as high-priority cache invalidation events.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat custom domain mapping service as a reliability-control decision, not an averages-only optimization. \"Treat expiration/deletion updates as high-priority cache invalidation events\" is correct since it mitigates counter-based key service becoming single bottleneck while keeping containment local. The decision remains valid given: The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "The core signal here is \"custom domain mapping service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-010",
      "type": "multiple-choice",
      "question": "Case Kappa: link ownership management path. Dominant risk is inconsistent policy enforcement across custom domains. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Define policy engine contracts consistently across default and custom domains.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In URL Shortener Core Architecture, link ownership management path fails mainly through inconsistent policy enforcement across custom domains. The best choice is \"Define policy engine contracts consistently across default and custom domains\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Current runbooks are missing explicit ownership for this boundary.",
      "detailedExplanation": "This prompt is really about \"link ownership management path\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-011",
      "type": "multiple-choice",
      "question": "Case Lambda: short-link creation API. Dominant risk is key collision under concurrent create requests. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use collision-resistant IDs with retry-safe uniqueness checks at write time.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Short-link creation API should be solved at the failure boundary named in URL Shortener Core Architecture. \"Use collision-resistant IDs with retry-safe uniqueness checks at write time\" is strongest because it directly addresses key collision under concurrent create requests and improves repeatability under stress. This aligns with the extra condition (A cross-region path recently changed behavior after migration).",
      "detailedExplanation": "Use \"short-link creation API\" as your starting point, then verify tradeoffs carefully. Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-012",
      "type": "multiple-choice",
      "question": "Case Mu: key generation service. Dominant risk is redirect latency spikes from cold metadata fetches. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Cache redirect metadata at edge with short TTL and explicit invalidation hooks."
      ],
      "correct": 3,
      "explanation": "Treat key generation service as a reliability-control decision, not an averages-only optimization. \"Cache redirect metadata at edge with short TTL and explicit invalidation hooks\" is correct since it mitigates redirect latency spikes from cold metadata fetches while keeping containment local. The decision remains valid given: Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "The core signal here is \"key generation service\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-013",
      "type": "multiple-choice",
      "question": "Case Nu: redirect resolver. Dominant risk is abuse links bypassing validation path. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Run synchronous abuse/security guardrails on creation before key persistence.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For redirect resolver, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Run synchronous abuse/security guardrails on creation before key persistence\" outperforms the alternatives because it targets abuse links bypassing validation path and preserves safe recovery behavior. It is also the most compatible with Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "If you keep \"redirect resolver\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-014",
      "type": "multiple-choice",
      "question": "Case Xi: alias reservation path. Dominant risk is expired links served due to stale edge cache. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Separate hot-link routing from long-tail storage reads using tiered cache strategy.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In URL Shortener Core Architecture, alias reservation path fails mainly through expired links served due to stale edge cache. The best choice is \"Separate hot-link routing from long-tail storage reads using tiered cache strategy\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A partial failure is masking itself as success in metrics.",
      "detailedExplanation": "Start from \"alias reservation path\", then pressure-test the result against the options. Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-015",
      "type": "multiple-choice",
      "question": "Case Omicron: link metadata store. Dominant risk is hot-key traffic on viral links overloading origin. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Enforce alias reservation with transactional uniqueness and ownership verification.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Link metadata store should be solved at the failure boundary named in URL Shortener Core Architecture. \"Enforce alias reservation with transactional uniqueness and ownership verification\" is strongest because it directly addresses hot-key traffic on viral links overloading origin and improves repeatability under stress. This aligns with the extra condition (This fix must hold under celebrity or campaign spike conditions).",
      "detailedExplanation": "The key clue in this question is \"link metadata store\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-016",
      "type": "multiple-choice",
      "question": "Case Pi: abuse detection pre-check. Dominant risk is alias race conditions causing ownership conflicts. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Decouple analytics writes from redirect critical path with async durable queue."
      ],
      "correct": 3,
      "explanation": "Treat abuse detection pre-check as a reliability-control decision, not an averages-only optimization. \"Decouple analytics writes from redirect critical path with async durable queue\" is correct since it mitigates alias race conditions causing ownership conflicts while keeping containment local. The decision remains valid given: SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "Read this as a scenario about \"abuse detection pre-check\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-017",
      "type": "multiple-choice",
      "question": "Case Rho: TTL/expiration scheduler. Dominant risk is write path blocking on slow analytics dependencies. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Provide read-after-write guarantees for create-to-first-redirect user flow.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For TTL/expiration scheduler, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Provide read-after-write guarantees for create-to-first-redirect user flow\" outperforms the alternatives because it targets write path blocking on slow analytics dependencies and preserves safe recovery behavior. It is also the most compatible with On-call requested a reversible operational first step.",
      "detailedExplanation": "The decision turns on \"tTL/expiration scheduler\". Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-018",
      "type": "multiple-choice",
      "question": "Case Sigma: global redirect edge layer. Dominant risk is eventual consistency causing transient 404 after creation. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Shard key generation and avoid single sequencer dependence for global scale.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In URL Shortener Core Architecture, global redirect edge layer fails mainly through eventual consistency causing transient 404 after creation. The best choice is \"Shard key generation and avoid single sequencer dependence for global scale\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The system mixes strict and eventual paths with unclear contracts.",
      "detailedExplanation": "This prompt is really about \"global redirect edge layer\". Discard choices that violate required invariants during concurrent or failed states. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. If values like 404 appear, convert them into one unit basis before comparison. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-019",
      "type": "multiple-choice",
      "question": "Case Tau: custom domain mapping service. Dominant risk is counter-based key service becoming single bottleneck. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Treat expiration/deletion updates as high-priority cache invalidation events.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Custom domain mapping service should be solved at the failure boundary named in URL Shortener Core Architecture. \"Treat expiration/deletion updates as high-priority cache invalidation events\" is strongest because it directly addresses counter-based key service becoming single bottleneck and improves repeatability under stress. This aligns with the extra condition (A hot-key pattern is likely from real traffic skew).",
      "detailedExplanation": "Use \"custom domain mapping service\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: link ownership management path. Dominant risk is inconsistent policy enforcement across custom domains. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Define policy engine contracts consistently across default and custom domains."
      ],
      "correct": 3,
      "explanation": "For link ownership management path, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Define policy engine contracts consistently across default and custom domains\" outperforms the alternatives because it targets inconsistent policy enforcement across custom domains and preserves safe recovery behavior. It is also the most compatible with The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "The key clue in this question is \"link ownership management path\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-021",
      "type": "multiple-choice",
      "question": "Case Phi: short-link creation API. Dominant risk is key collision under concurrent create requests. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Use collision-resistant IDs with retry-safe uniqueness checks at write time.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In URL Shortener Core Architecture, short-link creation API fails mainly through key collision under concurrent create requests. The best choice is \"Use collision-resistant IDs with retry-safe uniqueness checks at write time\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Compliance requires explicit behavior for edge-case failures.",
      "detailedExplanation": "Start from \"short-link creation API\", then pressure-test the result against the options. Prefer the choice that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-022",
      "type": "multiple-choice",
      "question": "Case Chi: key generation service. Dominant risk is redirect latency spikes from cold metadata fetches. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Cache redirect metadata at edge with short TTL and explicit invalidation hooks.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Key generation service should be solved at the failure boundary named in URL Shortener Core Architecture. \"Cache redirect metadata at edge with short TTL and explicit invalidation hooks\" is strongest because it directly addresses redirect latency spikes from cold metadata fetches and improves repeatability under stress. This aligns with the extra condition (This boundary has failed during the last two game days).",
      "detailedExplanation": "The decision turns on \"key generation service\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-023",
      "type": "multiple-choice",
      "question": "Case Psi: redirect resolver. Dominant risk is abuse links bypassing validation path. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Run synchronous abuse/security guardrails on creation before key persistence.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat redirect resolver as a reliability-control decision, not an averages-only optimization. \"Run synchronous abuse/security guardrails on creation before key persistence\" is correct since it mitigates abuse links bypassing validation path while keeping containment local. The decision remains valid given: A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "Read this as a scenario about \"redirect resolver\". Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-024",
      "type": "multiple-choice",
      "question": "Case Omega: alias reservation path. Dominant risk is expired links served due to stale edge cache. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Separate hot-link routing from long-tail storage reads using tiered cache strategy."
      ],
      "correct": 3,
      "explanation": "For alias reservation path, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Separate hot-link routing from long-tail storage reads using tiered cache strategy\" outperforms the alternatives because it targets expired links served due to stale edge cache and preserves safe recovery behavior. It is also the most compatible with Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "Use \"alias reservation path\" as your starting point, then verify tradeoffs carefully. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-025",
      "type": "multiple-choice",
      "question": "Case Atlas: link metadata store. Dominant risk is hot-key traffic on viral links overloading origin. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Enforce alias reservation with transactional uniqueness and ownership verification.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In URL Shortener Core Architecture, link metadata store fails mainly through hot-key traffic on viral links overloading origin. The best choice is \"Enforce alias reservation with transactional uniqueness and ownership verification\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The fix should avoid broad architectural rewrites this quarter.",
      "detailedExplanation": "This prompt is really about \"link metadata store\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-026",
      "type": "multiple-choice",
      "question": "Case Nova: abuse detection pre-check. Dominant risk is alias race conditions causing ownership conflicts. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Decouple analytics writes from redirect critical path with async durable queue.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Abuse detection pre-check should be solved at the failure boundary named in URL Shortener Core Architecture. \"Decouple analytics writes from redirect critical path with async durable queue\" is strongest because it directly addresses alias race conditions causing ownership conflicts and improves repeatability under stress. This aligns with the extra condition (Current metrics hide per-tenant variance that matters).",
      "detailedExplanation": "If you keep \"abuse detection pre-check\" in view, the correct answer separates faster. Prefer the answer that survives a sanity check against known anchor numbers. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: skipping anchor checks against known scale.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-027",
      "type": "multiple-choice",
      "question": "Case Orion: TTL/expiration scheduler. Dominant risk is write path blocking on slow analytics dependencies. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Provide read-after-write guarantees for create-to-first-redirect user flow.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat TTL/expiration scheduler as a reliability-control decision, not an averages-only optimization. \"Provide read-after-write guarantees for create-to-first-redirect user flow\" is correct since it mitigates write path blocking on slow analytics dependencies while keeping containment local. The decision remains valid given: The fallback path is under-tested in production-like load.",
      "detailedExplanation": "The core signal here is \"tTL/expiration scheduler\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-us-028",
      "type": "multiple-choice",
      "question": "Case Vega: global redirect edge layer. Dominant risk is eventual consistency causing transient 404 after creation. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Shard key generation and avoid single sequencer dependence for global scale."
      ],
      "correct": 3,
      "explanation": "For global redirect edge layer, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Shard key generation and avoid single sequencer dependence for global scale\" outperforms the alternatives because it targets eventual consistency causing transient 404 after creation and preserves safe recovery behavior. It is also the most compatible with A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "The key clue in this question is \"global redirect edge layer\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Consistency decisions should be explicit about which conflicts are acceptable and why. Keep quantities like 404 in aligned units before selecting an answer. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ]
    },
    {
      "id": "cd-us-029",
      "type": "multiple-choice",
      "question": "Case Helios: custom domain mapping service. Dominant risk is counter-based key service becoming single bottleneck. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Treat expiration/deletion updates as high-priority cache invalidation events.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In URL Shortener Core Architecture, custom domain mapping service fails mainly through counter-based key service becoming single bottleneck. The best choice is \"Treat expiration/deletion updates as high-priority cache invalidation events\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The system must preserve critical events over bulk traffic.",
      "detailedExplanation": "Start from \"custom domain mapping service\", then pressure-test the result against the options. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-030",
      "type": "multiple-choice",
      "question": "Case Aurora: link ownership management path. Dominant risk is inconsistent policy enforcement across custom domains. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Define policy engine contracts consistently across default and custom domains.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat link ownership management path as a reliability-control decision, not an averages-only optimization. \"Define policy engine contracts consistently across default and custom domains\" is correct since it mitigates inconsistent policy enforcement across custom domains while keeping containment local. The decision remains valid given: Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "The core signal here is \"link ownership management path\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: short-link creation API. Dominant risk is key collision under concurrent create requests. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use collision-resistant IDs with retry-safe uniqueness checks at write time.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For short-link creation API, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Use collision-resistant IDs with retry-safe uniqueness checks at write time\" outperforms the alternatives because it targets key collision under concurrent create requests and preserves safe recovery behavior. It is also the most compatible with The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "If you keep \"short-link creation API\" in view, the correct answer separates faster. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-032",
      "type": "multiple-choice",
      "question": "Case Pulse: key generation service. Dominant risk is redirect latency spikes from cold metadata fetches. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Cache redirect metadata at edge with short TTL and explicit invalidation hooks."
      ],
      "correct": 3,
      "explanation": "In URL Shortener Core Architecture, key generation service fails mainly through redirect latency spikes from cold metadata fetches. The best choice is \"Cache redirect metadata at edge with short TTL and explicit invalidation hooks\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Operational complexity is rising faster than team onboarding.",
      "detailedExplanation": "This prompt is really about \"key generation service\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-033",
      "type": "multiple-choice",
      "question": "Case Forge: redirect resolver. Dominant risk is abuse links bypassing validation path. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Run synchronous abuse/security guardrails on creation before key persistence.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Redirect resolver should be solved at the failure boundary named in URL Shortener Core Architecture. \"Run synchronous abuse/security guardrails on creation before key persistence\" is strongest because it directly addresses abuse links bypassing validation path and improves repeatability under stress. This aligns with the extra condition (Stakeholders need clear trade-off rationale in the postmortem).",
      "detailedExplanation": "Use \"redirect resolver\" as your starting point, then verify tradeoffs carefully. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-034",
      "type": "multiple-choice",
      "question": "Case Harbor: alias reservation path. Dominant risk is expired links served due to stale edge cache. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Separate hot-link routing from long-tail storage reads using tiered cache strategy.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat alias reservation path as a reliability-control decision, not an averages-only optimization. \"Separate hot-link routing from long-tail storage reads using tiered cache strategy\" is correct since it mitigates expired links served due to stale edge cache while keeping containment local. The decision remains valid given: A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "Read this as a scenario about \"alias reservation path\". Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-035",
      "type": "multiple-choice",
      "question": "Case Vector: link metadata store. Dominant risk is hot-key traffic on viral links overloading origin. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Enforce alias reservation with transactional uniqueness and ownership verification.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For link metadata store, prefer the option that prevents reoccurrence in URL Shortener Core Architecture. \"Enforce alias reservation with transactional uniqueness and ownership verification\" outperforms the alternatives because it targets hot-key traffic on viral links overloading origin and preserves safe recovery behavior. It is also the most compatible with Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "The decision turns on \"link metadata store\". Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for short-link creation API: signal points to expired links served due to stale edge cache. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around short-link creation API mismatches expired links served due to stale edge cache, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for short-link creation API: signal points to expired links served due to stale edge cache is a two-step reliability decision. At stage 1, \"The current decomposition around short-link creation API mismatches expired links served due to stale edge cache, creating repeated failures\" wins because it balances immediate containment with long-term prevention around expired links served due to stale edge cache.",
          "detailedExplanation": "This prompt is really about \"incident review for short-link creation API: signal points to expired links served due\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for short-link creation API: signal\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Decouple analytics writes from redirect critical path with async durable queue.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in URL Shortener Core Architecture, the best answer is \"Decouple analytics writes from redirect critical path with async durable queue\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"uRL Shortener Core Architecture\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for key generation service: signal points to hot-key traffic on viral links overloading origin. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around key generation service mismatches hot-key traffic on viral links overloading origin, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around key generation service mismatches hot-key traffic on viral links overloading origin, creating repeated failures\" best matches Incident review for key generation service: signal points to hot-key traffic on viral links overloading origin by targeting hot-key traffic on viral links overloading origin and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident review for key generation service: signal points to hot-key traffic on viral\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for key generation service: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Provide read-after-write guarantees for create-to-first-redirect user flow.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Using the diagnosis from \"incident review for key generation service: signal\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Provide read-after-write guarantees for create-to-first-redirect user flow\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect resolver: signal points to alias race conditions causing ownership conflicts. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around redirect resolver mismatches alias race conditions causing ownership conflicts, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in URL Shortener Core Architecture: for Incident review for redirect resolver: signal points to alias race conditions causing ownership conflicts, \"The current decomposition around redirect resolver mismatches alias race conditions causing ownership conflicts, creating repeated failures\" is correct because it addresses alias race conditions causing ownership conflicts and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident review for redirect resolver: signal points to alias race conditions causing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for redirect resolver: signal points to\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Shard key generation and avoid single sequencer dependence for global scale."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Shard key generation and avoid single sequencer dependence for global scale\" best matches After diagnosing \"incident review for redirect resolver: signal points to\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"uRL Shortener Core Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for alias reservation path: signal points to write path blocking on slow analytics dependencies. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around alias reservation path mismatches write path blocking on slow analytics dependencies, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in URL Shortener Core Architecture, the best answer is \"The current decomposition around alias reservation path mismatches write path blocking on slow analytics dependencies, creating repeated failures\". It is the option most directly aligned to write path blocking on slow analytics dependencies while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"incident review for alias reservation path: signal points to write path blocking on\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for alias reservation path: signal\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Treat expiration/deletion updates as high-priority cache invalidation events.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in URL Shortener Core Architecture: for In the \"incident review for alias reservation path: signal\" scenario, which next step is strongest under current constraints, \"Treat expiration/deletion updates as high-priority cache invalidation events\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"uRL Shortener Core Architecture\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for link metadata store: signal points to eventual consistency causing transient 404 after creation. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around link metadata store mismatches eventual consistency causing transient 404 after creation, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around link metadata store mismatches eventual consistency causing transient 404 after creation, creating repeated failures\" best matches Incident review for link metadata store: signal points to eventual consistency causing transient 404 after creation by targeting eventual consistency causing transient 404 after creation and lowering repeat risk.",
          "detailedExplanation": "Use \"incident review for link metadata store: signal points to eventual consistency causing\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 404 in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for link metadata store: signal points\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Define policy engine contracts consistently across default and custom domains.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Given the diagnosis in \"incident review for link metadata store: signal points\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Define policy engine contracts consistently across default and custom domains\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"uRL Shortener Core Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for abuse detection pre-check: signal points to counter-based key service becoming single bottleneck. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around abuse detection pre-check mismatches counter-based key service becoming single bottleneck, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Stage 1 in URL Shortener Core Architecture: for Incident review for abuse detection pre-check: signal points to counter-based key service becoming single bottleneck, \"The current decomposition around abuse detection pre-check mismatches counter-based key service becoming single bottleneck, creating repeated failures\" is correct because it addresses counter-based key service becoming single bottleneck and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident review for abuse detection pre-check: signal points to counter-based key\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for abuse detection pre-check: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Use collision-resistant IDs with retry-safe uniqueness checks at write time.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use collision-resistant IDs with retry-safe uniqueness checks at write time\" best matches With diagnosis confirmed in \"incident review for abuse detection pre-check: signal\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"uRL Shortener Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for TTL/expiration scheduler: signal points to inconsistent policy enforcement across custom domains. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around TTL/expiration scheduler mismatches inconsistent policy enforcement across custom domains, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in URL Shortener Core Architecture, the best answer is \"The current decomposition around TTL/expiration scheduler mismatches inconsistent policy enforcement across custom domains, creating repeated failures\". It is the option most directly aligned to inconsistent policy enforcement across custom domains while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"incident review for TTL/expiration scheduler: signal points to inconsistent policy\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for TTL/expiration scheduler: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Cache redirect metadata at edge with short TTL and explicit invalidation hooks."
          ],
          "correct": 3,
          "explanation": "Stage 2 in URL Shortener Core Architecture: for Using the diagnosis from \"incident review for TTL/expiration scheduler: signal\", which next change should be prioritized first, \"Cache redirect metadata at edge with short TTL and explicit invalidation hooks\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for global redirect edge layer: signal points to key collision under concurrent create requests. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around global redirect edge layer mismatches key collision under concurrent create requests, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for global redirect edge layer: signal points to key collision under concurrent create requests is a two-step reliability decision. At stage 1, \"The current decomposition around global redirect edge layer mismatches key collision under concurrent create requests, creating repeated failures\" wins because it balances immediate containment with long-term prevention around key collision under concurrent create requests.",
          "detailedExplanation": "This prompt is really about \"incident review for global redirect edge layer: signal points to key collision under\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for global redirect edge layer: signal\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Run synchronous abuse/security guardrails on creation before key persistence.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in URL Shortener Core Architecture, the best answer is \"Run synchronous abuse/security guardrails on creation before key persistence\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Start from \"uRL Shortener Core Architecture\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for custom domain mapping service: signal points to redirect latency spikes from cold metadata fetches. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around custom domain mapping service mismatches redirect latency spikes from cold metadata fetches, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around custom domain mapping service mismatches redirect latency spikes from cold metadata fetches, creating repeated failures\" best matches Incident review for custom domain mapping service: signal points to redirect latency spikes from cold metadata fetches by targeting redirect latency spikes from cold metadata fetches and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"incident review for custom domain mapping service: signal points to redirect latency\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for custom domain mapping service:\" scenario, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Separate hot-link routing from long-tail storage reads using tiered cache strategy.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "In the \"incident review for custom domain mapping service:\" scenario, what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Separate hot-link routing from long-tail storage reads using tiered cache strategy\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"uRL Shortener Core Architecture\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for link ownership management path: signal points to abuse links bypassing validation path. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around link ownership management path mismatches abuse links bypassing validation path, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Stage 1 in URL Shortener Core Architecture: for Incident review for link ownership management path: signal points to abuse links bypassing validation path, \"The current decomposition around link ownership management path mismatches abuse links bypassing validation path, creating repeated failures\" is correct because it addresses abuse links bypassing validation path and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident review for link ownership management path: signal points to abuse links\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for link ownership management path:\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Enforce alias reservation with transactional uniqueness and ownership verification.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Enforce alias reservation with transactional uniqueness and ownership verification\" best matches After diagnosing \"incident review for link ownership management path:\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for short-link creation API: signal points to expired links served due to stale edge cache. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around short-link creation API mismatches expired links served due to stale edge cache, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "For stage 1 in URL Shortener Core Architecture, the best answer is \"The current decomposition around short-link creation API mismatches expired links served due to stale edge cache, creating repeated failures\". It is the option most directly aligned to expired links served due to stale edge cache while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"incident review for short-link creation API: signal points to expired links served due\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for short-link creation API: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Decouple analytics writes from redirect critical path with async durable queue."
          ],
          "correct": 3,
          "explanation": "Stage 2 in URL Shortener Core Architecture: for With root cause identified for \"incident review for short-link creation API: signal\", which immediate adjustment best addresses the risk, \"Decouple analytics writes from redirect critical path with async durable queue\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"uRL Shortener Core Architecture\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for key generation service: signal points to hot-key traffic on viral links overloading origin. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around key generation service mismatches hot-key traffic on viral links overloading origin, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for key generation service: signal points to hot-key traffic on viral links overloading origin is a two-step reliability decision. At stage 1, \"The current decomposition around key generation service mismatches hot-key traffic on viral links overloading origin, creating repeated failures\" wins because it balances immediate containment with long-term prevention around hot-key traffic on viral links overloading origin.",
          "detailedExplanation": "Start from \"incident review for key generation service: signal points to hot-key traffic on viral\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for key generation service: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Provide read-after-write guarantees for create-to-first-redirect user flow.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For stage 2 in URL Shortener Core Architecture, the best answer is \"Provide read-after-write guarantees for create-to-first-redirect user flow\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect resolver: signal points to alias race conditions causing ownership conflicts. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around redirect resolver mismatches alias race conditions causing ownership conflicts, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around redirect resolver mismatches alias race conditions causing ownership conflicts, creating repeated failures\" best matches Incident review for redirect resolver: signal points to alias race conditions causing ownership conflicts by targeting alias race conditions causing ownership conflicts and lowering repeat risk.",
          "detailedExplanation": "Use \"incident review for redirect resolver: signal points to alias race conditions causing\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for redirect resolver: signal points to\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Shard key generation and avoid single sequencer dependence for global scale.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Given the diagnosis in \"incident review for redirect resolver: signal points to\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Shard key generation and avoid single sequencer dependence for global scale\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The decision turns on \"uRL Shortener Core Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for alias reservation path: signal points to write path blocking on slow analytics dependencies. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around alias reservation path mismatches write path blocking on slow analytics dependencies, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Stage 1 in URL Shortener Core Architecture: for Incident review for alias reservation path: signal points to write path blocking on slow analytics dependencies, \"The current decomposition around alias reservation path mismatches write path blocking on slow analytics dependencies, creating repeated failures\" is correct because it addresses write path blocking on slow analytics dependencies and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident review for alias reservation path: signal points to write path blocking on\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for alias reservation path: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Treat expiration/deletion updates as high-priority cache invalidation events.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Treat expiration/deletion updates as high-priority cache invalidation events\" best matches With diagnosis confirmed in \"incident review for alias reservation path: signal\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for link metadata store: signal points to eventual consistency causing transient 404 after creation. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around link metadata store mismatches eventual consistency causing transient 404 after creation, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Incident review for link metadata store: signal points to eventual consistency causing transient 404 after creation is a two-step reliability decision. At stage 1, \"The current decomposition around link metadata store mismatches eventual consistency causing transient 404 after creation, creating repeated failures\" wins because it balances immediate containment with long-term prevention around eventual consistency causing transient 404 after creation.",
          "detailedExplanation": "This prompt is really about \"incident review for link metadata store: signal points to eventual consistency causing\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 404 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for link metadata store: signal points\" is diagnosed, which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Define policy engine contracts consistently across default and custom domains."
          ],
          "correct": 3,
          "explanation": "For stage 2 in URL Shortener Core Architecture, the best answer is \"Define policy engine contracts consistently across default and custom domains\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Start from \"uRL Shortener Core Architecture\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for abuse detection pre-check: signal points to counter-based key service becoming single bottleneck. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around abuse detection pre-check mismatches counter-based key service becoming single bottleneck, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around abuse detection pre-check mismatches counter-based key service becoming single bottleneck, creating repeated failures\" best matches Incident review for abuse detection pre-check: signal points to counter-based key service becoming single bottleneck by targeting counter-based key service becoming single bottleneck and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident review for abuse detection pre-check: signal points to counter-based key\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for abuse detection pre-check: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Use collision-resistant IDs with retry-safe uniqueness checks at write time.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Using the diagnosis from \"incident review for abuse detection pre-check: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Use collision-resistant IDs with retry-safe uniqueness checks at write time\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for TTL/expiration scheduler: signal points to inconsistent policy enforcement across custom domains. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around TTL/expiration scheduler mismatches inconsistent policy enforcement across custom domains, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Stage 1 in URL Shortener Core Architecture: for Incident review for TTL/expiration scheduler: signal points to inconsistent policy enforcement across custom domains, \"The current decomposition around TTL/expiration scheduler mismatches inconsistent policy enforcement across custom domains, creating repeated failures\" is correct because it addresses inconsistent policy enforcement across custom domains and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"incident review for TTL/expiration scheduler: signal points to inconsistent policy\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for TTL/expiration scheduler: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Cache redirect metadata at edge with short TTL and explicit invalidation hooks.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Cache redirect metadata at edge with short TTL and explicit invalidation hooks\" best matches With diagnosis confirmed in \"incident review for TTL/expiration scheduler: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for global redirect edge layer: signal points to key collision under concurrent create requests. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around global redirect edge layer mismatches key collision under concurrent create requests, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "For stage 1 in URL Shortener Core Architecture, the best answer is \"The current decomposition around global redirect edge layer mismatches key collision under concurrent create requests, creating repeated failures\". It is the option most directly aligned to key collision under concurrent create requests while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"incident review for global redirect edge layer: signal points to key collision under\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Given the diagnosis in \"incident review for global redirect edge layer: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Run synchronous abuse/security guardrails on creation before key persistence.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Stage 2 in URL Shortener Core Architecture: for Given the diagnosis in \"incident review for global redirect edge layer: signal\", which next change should be prioritized first, \"Run synchronous abuse/security guardrails on creation before key persistence\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"uRL Shortener Core Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for custom domain mapping service: signal points to redirect latency spikes from cold metadata fetches. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around custom domain mapping service mismatches redirect latency spikes from cold metadata fetches, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Incident review for custom domain mapping service: signal points to redirect latency spikes from cold metadata fetches is a two-step reliability decision. At stage 1, \"The current decomposition around custom domain mapping service mismatches redirect latency spikes from cold metadata fetches, creating repeated failures\" wins because it balances immediate containment with long-term prevention around redirect latency spikes from cold metadata fetches.",
          "detailedExplanation": "Start from \"incident review for custom domain mapping service: signal points to redirect latency\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for custom domain mapping service:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Separate hot-link routing from long-tail storage reads using tiered cache strategy."
          ],
          "correct": 3,
          "explanation": "For stage 2 in URL Shortener Core Architecture, the best answer is \"Separate hot-link routing from long-tail storage reads using tiered cache strategy\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "This prompt is really about \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for link ownership management path: signal points to abuse links bypassing validation path. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around link ownership management path mismatches abuse links bypassing validation path, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around link ownership management path mismatches abuse links bypassing validation path, creating repeated failures\" best matches Incident review for link ownership management path: signal points to abuse links bypassing validation path by targeting abuse links bypassing validation path and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident review for link ownership management path: signal points to abuse links\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for link ownership management path:\", which next step is strongest under current constraints?",
          "options": [
            "Enforce alias reservation with transactional uniqueness and ownership verification.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "With root cause identified for \"incident review for link ownership management path:\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Enforce alias reservation with transactional uniqueness and ownership verification\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Use \"uRL Shortener Core Architecture\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for short-link creation API: signal points to expired links served due to stale edge cache. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around short-link creation API mismatches expired links served due to stale edge cache, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Stage 1 in URL Shortener Core Architecture: for Incident review for short-link creation API: signal points to expired links served due to stale edge cache, \"The current decomposition around short-link creation API mismatches expired links served due to stale edge cache, creating repeated failures\" is correct because it addresses expired links served due to stale edge cache and improves controllability.",
          "detailedExplanation": "The core signal here is \"incident review for short-link creation API: signal points to expired links served due\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for short-link creation API: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Decouple analytics writes from redirect critical path with async durable queue.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Decouple analytics writes from redirect critical path with async durable queue\" best matches After diagnosing \"incident review for short-link creation API: signal\", what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"uRL Shortener Core Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for key generation service: signal points to hot-key traffic on viral links overloading origin. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around key generation service mismatches hot-key traffic on viral links overloading origin, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "For stage 1 in URL Shortener Core Architecture, the best answer is \"The current decomposition around key generation service mismatches hot-key traffic on viral links overloading origin, creating repeated failures\". It is the option most directly aligned to hot-key traffic on viral links overloading origin while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"incident review for key generation service: signal points to hot-key traffic on viral\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for key generation service: signal\" scenario, what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Provide read-after-write guarantees for create-to-first-redirect user flow.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Stage 2 in URL Shortener Core Architecture: for In the \"incident review for key generation service: signal\" scenario, what first move gives the best reliability impact, \"Provide read-after-write guarantees for create-to-first-redirect user flow\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"uRL Shortener Core Architecture\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for redirect resolver: signal points to alias race conditions causing ownership conflicts. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around redirect resolver mismatches alias race conditions causing ownership conflicts, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Incident review for redirect resolver: signal points to alias race conditions causing ownership conflicts is a two-step reliability decision. At stage 1, \"The current decomposition around redirect resolver mismatches alias race conditions causing ownership conflicts, creating repeated failures\" wins because it balances immediate containment with long-term prevention around alias race conditions causing ownership conflicts.",
          "detailedExplanation": "This prompt is really about \"incident review for redirect resolver: signal points to alias race conditions causing\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for redirect resolver: signal points to\" is diagnosed, what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Shard key generation and avoid single sequencer dependence for global scale."
          ],
          "correct": 3,
          "explanation": "For stage 2 in URL Shortener Core Architecture, the best answer is \"Shard key generation and avoid single sequencer dependence for global scale\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"uRL Shortener Core Architecture\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for alias reservation path: signal points to write path blocking on slow analytics dependencies. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around alias reservation path mismatches write path blocking on slow analytics dependencies, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around alias reservation path mismatches write path blocking on slow analytics dependencies, creating repeated failures\" best matches Incident review for alias reservation path: signal points to write path blocking on slow analytics dependencies by targeting write path blocking on slow analytics dependencies and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident review for alias reservation path: signal points to write path blocking on\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for alias reservation path: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Treat expiration/deletion updates as high-priority cache invalidation events.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Using the diagnosis from \"incident review for alias reservation path: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Treat expiration/deletion updates as high-priority cache invalidation events\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"uRL Shortener Core Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for link metadata store: signal points to eventual consistency causing transient 404 after creation. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around link metadata store mismatches eventual consistency causing transient 404 after creation, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "For stage 1 in URL Shortener Core Architecture, the best answer is \"The current decomposition around link metadata store mismatches eventual consistency causing transient 404 after creation, creating repeated failures\". It is the option most directly aligned to eventual consistency causing transient 404 after creation while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"incident review for link metadata store: signal points to eventual consistency causing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 404 appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for link metadata store: signal points\", what first move gives the best reliability impact?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Define policy engine contracts consistently across default and custom domains.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Stage 2 in URL Shortener Core Architecture: for With root cause identified for \"incident review for link metadata store: signal points\", what first move gives the best reliability impact, \"Define policy engine contracts consistently across default and custom domains\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"uRL Shortener Core Architecture\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-061",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In URL Shortener Core Architecture, Choose every valid option for this prompt: which signals best identify decomposition boundary mistakes needs layered controls, not one silver bullet. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "This prompt is really about \"signals best identify decomposition boundary mistakes? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-062",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Choose every valid option for this prompt: which controls improve safety on critical write paths is intentionally multi-dimensional in URL Shortener Core Architecture. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "If you keep \"controls improve safety on critical write paths? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-063",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for URL Shortener Core Architecture: The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The core signal here is \"practices reduce hot-key or hot-partition impact? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-us-064",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Choose every valid option for this prompt: what improves reliability when mixing sync and async paths, the highest-signal answer is a bundle of controls. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The key clue in this question is \"improves reliability when mixing sync and async paths? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-us-065",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In URL Shortener Core Architecture, Choose every valid option for this prompt: which choices usually lower operational risk at scale needs layered controls, not one silver bullet. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"choices usually lower operational risk at scale? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-066",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Choose every valid option for this prompt: what should be explicit in API/service contracts for this design is intentionally multi-dimensional in URL Shortener Core Architecture. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The decision turns on \"be explicit in API/service contracts for this design? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-067",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for URL Shortener Core Architecture: The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Read this as a scenario about \"anti-patterns often cause incident recurrence? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-068",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Choose every valid option for this prompt: what increases confidence before broad traffic rollout, the highest-signal answer is a bundle of controls. The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Use \"increases confidence before broad traffic rollout? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-069",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In URL Shortener Core Architecture, Choose every valid option for this prompt: which controls protect high-priority traffic during spikes needs layered controls, not one silver bullet. The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "This prompt is really about \"controls protect high-priority traffic during spikes? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-070",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for URL Shortener Core Architecture: The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Read this as a scenario about \"telemetry dimensions are most actionable for design triage? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-071",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Choose every valid option for this prompt: which governance actions improve cross-team reliability ownership, the highest-signal answer is a bundle of controls. The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The decision turns on \"governance actions improve cross-team reliability ownership? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-072",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In URL Shortener Core Architecture, Choose every valid option for this prompt: what helps prevent retry amplification cascades needs layered controls, not one silver bullet. The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Start from \"helps prevent retry amplification cascades? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-us-073",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Choose every valid option for this prompt: which fallback strategies are strong when dependencies degrade is intentionally multi-dimensional in URL Shortener Core Architecture. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The key clue in this question is \"fallback strategies are strong when dependencies degrade? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-074",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for URL Shortener Core Architecture: The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "The core signal here is \"reduces data-quality regressions in eventual pipelines? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-075",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Choose every valid option for this prompt: which runbook components improve incident execution quality, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "If you keep \"runbook components improve incident execution quality? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-076",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In URL Shortener Core Architecture, Choose every valid option for this prompt: which architecture choices improve blast-radius containment needs layered controls, not one silver bullet. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "This prompt is really about \"architecture choices improve blast-radius containment? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-077",
      "type": "multi-select",
      "question": "Choose every valid option for this prompt: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Choose every valid option for this prompt: what evidence demonstrates a fix worked beyond short-term recovery is intentionally multi-dimensional in URL Shortener Core Architecture. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Use \"evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-078",
      "type": "numeric-input",
      "question": "Based on a critical path handles 5,400,000 requests/day and 0.18% fail SLO, failures/day?",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for Based on a critical path handles 5,400,000 requests/day and 0 gives 9720 requests. In interview pacing, hitting this value within +/-3% is the pass condition.",
      "detailedExplanation": "Read this as a scenario about \"critical path handles 5,400,000 requests/day and 0\". Keep every transformation in one unit system and check order of magnitude at the end. Tie the decision to concrete operational outcomes, not abstract reliability language. Keep quantities like 5,400 and 000 in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-079",
      "type": "numeric-input",
      "question": "Based on queue ingest is 2,200 events/min and drain is 2,530 events/min, net drain rate?",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Based on queue ingest is 2,200 events/min and drain is 2,530 events/min, net drain rate: 330 events/min. Answers within +/-0% show correct directional reasoning for URL Shortener Core Architecture.",
      "detailedExplanation": "The decision turns on \"queue ingest is 2,200 events/min and drain is 2,530 events/min\". Normalize units before computing so conversion mistakes do not propagate. A good message-system answer defines guarantees clearly for both producer and consumer paths. If values like 2,200 and 2,530 appear, convert them into one unit basis before comparison. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-080",
      "type": "numeric-input",
      "question": "Based on retries add 0.28 extra attempts at 75,000 req/sec, effective attempts/sec?",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "URL Shortener Core Architecture expects quick quantitative triage: Based on retries add 0 evaluates to 96000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "If you keep \"retries add 0\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 0.28 and 75,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-081",
      "type": "numeric-input",
      "question": "Based on failover takes 16s and occurs 24 times/day, total failover seconds/day?",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for Based on failover takes 16s and occurs 24 times/day, total failover seconds/day gives 384 seconds. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "The core signal here is \"failover takes 16s and occurs 24 times/day\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 16s and 24 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-082",
      "type": "numeric-input",
      "question": "Based on target p99 is 650ms; observed p99 is 845ms, percent over target?",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Based on target p99 is 650ms; observed p99 is 845ms, percent over target: 30 %. Answers within +/-30% show correct directional reasoning for URL Shortener Core Architecture.",
      "detailedExplanation": "Use \"target p99 is 650ms\" as your starting point, then verify tradeoffs carefully. Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 650ms and 845ms in aligned units before selecting an answer. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-083",
      "type": "numeric-input",
      "question": "For this case, if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For For this case, if 34% of 130,000 req/min are high-priority, how many high-priority req/min, the computed target in URL Shortener Core Architecture is 44200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "This prompt is really about \"if 34% of 130,000 req/min are high-priority, how many high-priority req/min\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 34 and 130,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-084",
      "type": "numeric-input",
      "question": "Based on error rate drops from 1.0% to 0.22%, percent reduction?",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "URL Shortener Core Architecture expects quick quantitative triage: Based on error rate drops from 1 evaluates to 78 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "The decision turns on \"error rate drops from 1\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 1.0 and 0.22 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-085",
      "type": "numeric-input",
      "question": "Based on a 9-node quorum cluster requires majority writes, minimum acknowledgements?",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for Based on a 9-node quorum cluster requires majority writes, minimum acknowledgements gives 5 acks. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Read this as a scenario about \"9-node quorum cluster requires majority writes\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Numbers such as 9 and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-086",
      "type": "numeric-input",
      "question": "Based on backlog is 56,000 tasks with net drain 350 tasks/min, minutes to clear?",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Based on backlog is 56,000 tasks with net drain 350 tasks/min, minutes to clear: 160 minutes. Answers within +/-0% show correct directional reasoning for URL Shortener Core Architecture.",
      "detailedExplanation": "The key clue in this question is \"backlog is 56,000 tasks with net drain 350 tasks/min\". Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 56,000 and 350 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-087",
      "type": "numeric-input",
      "question": "Based on a fleet has 18 zones and 3 are unavailable, percent remaining available?",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Based on a fleet has 18 zones and 3 are unavailable, percent remaining available, the computed target in URL Shortener Core Architecture is 83.33 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "Start from \"fleet has 18 zones and 3 are unavailable\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 18 and 3 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-088",
      "type": "numeric-input",
      "question": "Based on mTTR improved from 52 min to 34 min, percent reduction?",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "URL Shortener Core Architecture expects quick quantitative triage: Based on mTTR improved from 52 min to 34 min, percent reduction evaluates to 34.62 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "If you keep \"mTTR improved from 52 min to 34 min\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Keep quantities like 52 min and 34 min in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-089",
      "type": "numeric-input",
      "question": "For this case, if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for For this case, if 11% of 2,800,000 daily ops need manual checks, checks/day gives 308000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "The core signal here is \"if 11% of 2,800,000 daily ops need manual checks, checks/day\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 11 and 2,800 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-090",
      "type": "ordering",
      "question": "Considering url shortener core architecture, order a classic-design decomposition workflow.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in URL Shortener Core Architecture should start with Identify critical user journey and invariants and end with Validate with load/failure drills and refine. Considering url shortener core architecture, order a classic-design decomposition workflow rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "This prompt is really about \"order a classic-design decomposition workflow\". Build the rank from biggest differences first, then refine with adjacent checks. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-091",
      "type": "ordering",
      "question": "In this url shortener core architecture context, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Explicit boundaries with contracts must happen before Implicit coupling with no ownership. That ordering matches incident-safe flow in URL Shortener Core Architecture.",
      "detailedExplanation": "Use \"order by increasing design risk\" as your starting point, then verify tradeoffs carefully. Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-092",
      "type": "ordering",
      "question": "Within url shortener core architecture, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "URL Shortener Core Architecture emphasizes safe recovery order. Beginning at Scope blast radius and affected flows and finishing at Run recurrence checks and hardening actions keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The core signal here is \"order safe incident mitigation steps\". Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-us-093",
      "type": "ordering",
      "question": "For url shortener core architecture, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For For url shortener core architecture, order by increasing retry-control maturity, the correct ordering runs from Fixed immediate retries to Jittered retries with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "If you keep \"order by increasing retry-control maturity\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "cd-us-094",
      "type": "ordering",
      "question": "Order fallback sophistication. Focus on url shortener core architecture tradeoffs.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in URL Shortener Core Architecture should start with Implicit fallback behavior and end with Policy-driven automated fallback with tests. Order fallback sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Start from \"order fallback sophistication\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-095",
      "type": "ordering",
      "question": "Order failover validation rigor. Use a url shortener core architecture perspective.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Host health check only must happen before Staged shift plus failback rehearsal and rollback gates. That ordering matches incident-safe flow in URL Shortener Core Architecture.",
      "detailedExplanation": "The key clue in this question is \"order failover validation rigor\". Order by relative scale and bottleneck effect, then validate neighboring items. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-096",
      "type": "ordering",
      "question": "Sort these in ascending blast-radius impact.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "URL Shortener Core Architecture emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Read this as a scenario about \"order by increasing blast radius\". Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-097",
      "type": "ordering",
      "question": "From a url shortener core architecture viewpoint, order data-path durability confidence.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For From a url shortener core architecture viewpoint, order data-path durability confidence, the correct ordering runs from In-memory only acknowledgment to Replicated durable write plus replay/integrity verification. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The decision turns on \"order data-path durability confidence\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-098",
      "type": "ordering",
      "question": "Considering url shortener core architecture, order by increasing operational discipline.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in URL Shortener Core Architecture should start with Ad hoc incident response and end with Role-based response plus action closure tracking. Considering url shortener core architecture, order by increasing operational discipline rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "This prompt is really about \"order by increasing operational discipline\". Order by relative scale and bottleneck effect, then validate neighboring items. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-099",
      "type": "ordering",
      "question": "In this url shortener core architecture context, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Canary small cohort must happen before Finalize runbook and ownership updates. That ordering matches incident-safe flow in URL Shortener Core Architecture.",
      "detailedExplanation": "Use \"order rollout safety for major design changes\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-us-100",
      "type": "ordering",
      "question": "Within url shortener core architecture, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "URL Shortener Core Architecture emphasizes safe recovery order. Beginning at Single successful test run and finishing at Sustained recovery plus failure-drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "The core signal here is \"order evidence strength for fix success\". Place obvious extremes first, then sort the middle by pairwise comparison. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    }
  ]
}
