{
  "unit": 2,
  "unitTitle": "Data Modeling",
  "chapter": 7,
  "chapterTitle": "Schema Evolution",
  "chapterDescription": "Migrations, backward compatibility, and evolving schemas without downtime.",
  "problems": [
    {
      "id": "evo-001",
      "type": "multiple-choice",
      "question": "What is a 'schema migration'?",
      "options": [
        "Moving a database to a new server",
        "A versioned change to the database schema (adding/altering/dropping tables or columns) applied in a controlled, repeatable way",
        "Converting data from one format to another",
        "Migrating from one database vendor to another"
      ],
      "correct": 1,
      "explanation": "Schema migrations are versioned scripts that modify the database structure. They're applied in order (v001, v002, ...) and tracked so you know which migrations have been applied. Tools: Flyway, Liquibase, Rails migrations, Alembic.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-002",
      "type": "ordering",
      "question": "Rank these migration steps in the correct order:",
      "items": [
        "Write the migration script (SQL/code)",
        "Test the migration against a copy of production data",
        "Apply the migration to production",
        "Verify the application works with the new schema"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Write → test on production-like data (catch data-dependent failures) → apply to production → verify. Never skip testing against realistic data — migrations that work on empty dev databases can fail on real data.",
      "detailedExplanation": "Compare relative scale first, then confirm neighboring items pairwise to lock in the order. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-003",
      "type": "multiple-choice",
      "question": "What does 'backward compatible' mean for schema changes?",
      "options": [
        "The change works on older database versions",
        "The old version of the application can still work correctly after the schema change — both old and new code can coexist during deployment",
        "The migration can be rolled back",
        "The schema matches a previous version"
      ],
      "correct": 1,
      "explanation": "Backward compatible schema changes don't break existing code. During rolling deployments, old and new application versions run simultaneously. If the schema change breaks old code, you get errors during the rollout.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-004",
      "type": "multi-select",
      "question": "Which schema changes are backward compatible (safe during rolling deployment)?",
      "options": [
        "Adding a new nullable column",
        "Renaming an existing column",
        "Adding a new table",
        "Dropping a column that old code reads from"
      ],
      "correctIndices": [0, 2],
      "explanation": "Adding nullable columns: old code ignores them. Adding tables: old code doesn't reference them. Renaming breaks old code's column references. Dropping a column breaks old code that reads it.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-005",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to rename column 'name' to 'display_name' on the Users table. The app is deployed via rolling updates (old and new versions run simultaneously). What's the safe approach?",
          "options": [
            "ALTER TABLE users RENAME COLUMN name TO display_name (one step)",
            "Multi-step: (1) Add display_name column, (2) backfill data, (3) update app to read/write both columns, (4) stop writing to name, (5) drop name column",
            "Create a view that aliases the column",
            "Don't rename — use the old name forever"
          ],
          "correct": 1,
          "explanation": "Multi-step expand-and-contract: add the new column, double-write during transition, migrate reads to the new column, then drop the old one. Each step is backward compatible. A direct rename breaks old code instantly.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "This multi-step approach is called?",
          "options": [
            "Blue-green deployment",
            "Expand and contract (also called parallel change)",
            "Schema versioning",
            "Hot migration"
          ],
          "correct": 1,
          "explanation": "Expand and contract: 'expand' the schema (add the new column alongside the old), transition code, then 'contract' (remove the old column). Each step is independently safe and deployable.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-006",
      "type": "ordering",
      "question": "Rank the steps of an expand-and-contract column rename (name → display_name) in correct order:",
      "items": [
        "Add display_name column (nullable)",
        "Backfill: UPDATE users SET display_name = name WHERE display_name IS NULL",
        "Deploy app to read display_name (fallback to name) and write both",
        "Drop the name column"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Add column → backfill existing data → deploy code that uses new column (with fallback) → once all code reads new column, drop old column. Each step is a separate, safe deployment.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-007",
      "type": "multiple-choice",
      "question": "Why is ALTER TABLE ... ADD COLUMN with a DEFAULT value potentially dangerous on large tables in older PostgreSQL versions (< 11)?",
      "options": [
        "It creates a new table",
        "It rewrites every row in the table to add the default value, locking the table for the duration — potentially minutes to hours on large tables",
        "It changes the column type",
        "It drops existing indexes"
      ],
      "correct": 1,
      "explanation": "Before PostgreSQL 11, ADD COLUMN with a non-null DEFAULT rewrote every row. On a 100M row table, this could take hours with the table locked. PostgreSQL 11+ stores the default in the catalog instead — instant ADD COLUMN.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-008",
      "type": "multi-select",
      "question": "Which ALTER TABLE operations typically require a table rewrite (slow, locking)?",
      "options": [
        "Changing a column's data type (e.g., INT to BIGINT)",
        "Adding a nullable column with no default",
        "Adding a NOT NULL constraint with a default (pre-PG11)",
        "Adding a new index (in databases without CONCURRENTLY)"
      ],
      "correctIndices": [0, 2],
      "explanation": "Changing data types requires converting every row's value — table rewrite. Adding NOT NULL + DEFAULT (pre-PG11) rewrites all rows. Adding a nullable column is metadata-only (instant). Index creation doesn't rewrite the table (it reads it).",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-009",
      "type": "multiple-choice",
      "question": "What does CREATE INDEX CONCURRENTLY do in PostgreSQL?",
      "options": [
        "Creates the index faster using multiple CPUs",
        "Builds the index without holding a lock that blocks writes — allows reads and writes to continue during index creation",
        "Creates multiple indexes at once",
        "Creates the index in a background thread"
      ],
      "correct": 1,
      "explanation": "Regular CREATE INDEX holds a lock that blocks writes for the duration. CONCURRENTLY builds the index in multiple passes without blocking writes. It takes longer but doesn't cause downtime. Essential for production.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-010",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to add an index to a 500M row table in production. CREATE INDEX takes 30 minutes and locks the table. What should you do?",
          "options": [
            "Run it during low traffic (still locks writes)",
            "Use CREATE INDEX CONCURRENTLY — no write locks",
            "Create the index on a replica and promote it",
            "Don't add the index"
          ],
          "correct": 1,
          "explanation": "CREATE INDEX CONCURRENTLY: builds the index without blocking writes. Takes longer (two table passes instead of one) but zero downtime. This is the standard approach for production index creation.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "CREATE INDEX CONCURRENTLY can fail partway through. What happens?",
          "options": [
            "The table is corrupted",
            "The index is left in an INVALID state — you must DROP it and retry",
            "The partially built index still works",
            "The database rolls back automatically"
          ],
          "correct": 1,
          "explanation": "If CONCURRENTLY fails (e.g., unique constraint violation, out of disk space), the index is marked INVALID. It exists but isn't used by the query planner. You must DROP INDEX and retry.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-011",
      "type": "multiple-choice",
      "question": "What is a 'zero-downtime migration'?",
      "options": [
        "A migration that takes zero time to run",
        "A schema change applied while the application continues serving traffic without interruption or errors",
        "A migration run at midnight when no one is online",
        "A migration that doesn't change anything"
      ],
      "correct": 1,
      "explanation": "Zero-downtime migrations change the schema while the application stays live. This requires backward-compatible changes, online DDL operations (no long locks), and careful sequencing of schema + code changes.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-012",
      "type": "ordering",
      "question": "For a zero-downtime column addition with NOT NULL constraint, rank these steps in order:",
      "items": [
        "Add the column as nullable",
        "Backfill existing rows with the desired default value",
        "Deploy code that always writes the new column",
        "Add the NOT NULL constraint (with a default)"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "Add nullable (instant, no lock) → deploy code that writes to it (new rows get values) → backfill old rows → add NOT NULL constraint (now safe — all rows have values). Each step is safe independently.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-013",
      "type": "multi-select",
      "question": "Which are 'online DDL' operations in MySQL 8.0 (executed without blocking reads/writes)?",
      "options": [
        "Adding a column",
        "Adding an index (ALGORITHM=INPLACE)",
        "Changing a column from VARCHAR(100) to VARCHAR(200)",
        "Changing a column from INT to VARCHAR"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "MySQL 8.0 supports online DDL for many operations: ADD COLUMN, ADD INDEX (INPLACE), and increasing VARCHAR length (within limits). Changing column type fundamentally (INT to VARCHAR) requires a table copy.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-014",
      "type": "multiple-choice",
      "question": "What is a 'migration rollback' and when is it needed?",
      "options": [
        "Restoring from a database backup",
        "Reversing a schema migration by applying a 'down' migration that undoes the changes — needed when a migration causes issues in production",
        "Rolling back a Git commit",
        "Restarting the database server"
      ],
      "correct": 1,
      "explanation": "A rollback applies the reverse migration (DROP what was ADDed, re-ADD what was DROPped). Needed when a migration causes application errors or performance issues. Not all migrations are easily reversible (e.g., data-destructive changes).",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-015",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration drops the 'middle_name' column from the Users table. After deployment, a bug is discovered and you need to rollback. Can you restore the column?",
          "options": [
            "Yes — just re-add the column",
            "You can re-add the column, but the DATA is lost forever — you can't restore the values that were in the dropped column",
            "No — dropped columns can't be re-added",
            "The database keeps a backup automatically"
          ],
          "correct": 1,
          "explanation": "You can ADD COLUMN middle_name back, but all the data that was in it is gone. Dropping a column is a data-destructive operation. This is why careful teams mark columns as 'deprecated' before dropping them.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "What's the safer approach for removing a column?",
          "options": [
            "Drop it immediately",
            "Multi-step: (1) Stop writing to the column, (2) Stop reading the column, (3) Wait a safe period, (4) Drop the column — with a backup/export of the data before dropping",
            "Rename it to 'deprecated_middle_name'",
            "Never drop columns"
          ],
          "correct": 1,
          "explanation": "Stop using the column in code first. Wait until you're confident it's not needed. Export/backup the data. Then drop. The waiting period gives you time to catch unexpected dependencies before the data is gone.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-016",
      "type": "multiple-choice",
      "question": "What is 'schema versioning' in the context of migration tools?",
      "options": [
        "Versioning the database software",
        "Tracking which migrations have been applied by storing version numbers in a metadata table (e.g., schema_migrations)",
        "Creating a new database for each version",
        "Versioning the SQL syntax used"
      ],
      "correct": 1,
      "explanation": "Migration tools (Flyway, Alembic, etc.) store applied migration IDs in a table (e.g., flyway_schema_history). This lets the tool know which migrations are pending and prevents re-applying already-applied changes.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-017",
      "type": "multi-select",
      "question": "Which are features of modern migration tools (Flyway, Alembic, Rails Migrations)?",
      "options": [
        "Ordered, versioned migration scripts",
        "Tracking of applied migrations in a metadata table",
        "Automatic rollback on failure",
        "Generating migrations from model/schema diffs"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Migration tools provide: versioned ordering, tracking applied migrations, and some can auto-generate migrations from model changes (Alembic autogenerate, Rails). Automatic rollback isn't guaranteed — many DDL operations aren't transactional.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-018",
      "type": "ordering",
      "question": "Rank these migration file naming conventions by what they encode, from LEAST to MOST information:",
      "items": [
        "001_add_users.sql (sequential number)",
        "20240115_add_users.sql (date)",
        "20240115143022_add_users.sql (timestamp)",
        "V1.2.3__add_users.sql (semantic version)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sequential: just ordering. Date: when it was created. Timestamp: precise creation time (avoids collisions when multiple developers create migrations on the same day). Semantic version: ties to release version.",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-019",
      "type": "multiple-choice",
      "question": "Two developers on different branches both create migration V005. When both branches merge to main, what happens?",
      "options": [
        "One migration overwrites the other",
        "A migration version conflict — both claim V005, and the tool may refuse to apply them or apply them in unpredictable order",
        "They automatically merge",
        "The database handles it"
      ],
      "correct": 1,
      "explanation": "Version collision: two different V005 scripts. Sequential numbering is prone to this. Timestamp-based naming (20240115143022) virtually eliminates collisions. Some tools (Flyway) detect and reject duplicate versions.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-020",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration needs to update 50M rows: UPDATE orders SET status_code = status WHERE status_code IS NULL. Running this in one transaction takes 45 minutes and locks the table. How do you make this safe for production?",
          "options": [
            "Run it as-is during maintenance window",
            "Batch the update: process 10,000 rows at a time with small sleeps between batches",
            "Skip the backfill — leave NULLs",
            "Use a stored procedure"
          ],
          "correct": 1,
          "explanation": "Batch the update: UPDATE orders SET status_code = status WHERE status_code IS NULL AND id BETWEEN ? AND ? LIMIT 10000. Small transactions, minimal lock duration, and pauses between batches to avoid overwhelming the database.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why is batching safer than a single large UPDATE?",
          "options": [
            "Batches are faster overall",
            "Each batch is a small, short transaction — minimal lock time, bounded WAL/undo log usage, and can be paused or stopped without losing all progress",
            "Batches use less disk space",
            "Batches don't use transactions"
          ],
          "correct": 1,
          "explanation": "Small transactions: short locks (milliseconds not minutes). Bounded log usage: won't fill up WAL or transaction log. Stoppable: if issues arise, stop after the current batch — already-updated rows keep their values.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-021",
      "type": "numeric-input",
      "question": "A backfill migration updates 10M rows in batches of 5,000, with a 100ms sleep between batches. Approximately how long will the migration take (ignoring query execution time)?",
      "answer": 200,
      "unit": "seconds",
      "tolerance": 0.1,
      "explanation": "10,000,000 / 5,000 = 2,000 batches. 2,000 × 100ms = 200,000ms = 200 seconds (~3.3 minutes). Plus actual query execution time per batch. Much better than a single 45-minute locked transaction.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-022",
      "type": "multiple-choice",
      "question": "What is a 'ghost table' migration pattern (used by tools like gh-ost)?",
      "options": [
        "A table that doesn't show up in queries",
        "Create a new table with the desired schema, copy data in the background, then atomically swap the new table into the old table's name",
        "A temporary table for staging data",
        "A table marked for deletion"
      ],
      "correct": 1,
      "explanation": "gh-ost (GitHub Online Schema Tool): creates a 'ghost' table with the new schema, streams changes via binlog, copies data in the background, and atomically renames tables. Zero-downtime schema changes for MySQL without metadata locks.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-023",
      "type": "multi-select",
      "question": "Which tools help perform online schema changes in MySQL?",
      "options": [
        "gh-ost (GitHub's Online Schema Translator)",
        "pt-online-schema-change (Percona Toolkit)",
        "MySQL's built-in Online DDL (ALGORITHM=INPLACE)",
        "pg_repack (PostgreSQL tool)"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "gh-ost, pt-online-schema-change, and MySQL Online DDL all perform schema changes without blocking writes. pg_repack is a PostgreSQL tool, not MySQL.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-024",
      "type": "ordering",
      "question": "Rank these schema changes from SAFEST (least risk of downtime) to MOST DANGEROUS:",
      "items": [
        "Add a nullable column (no default)",
        "Add a new index (CONCURRENTLY)",
        "Change a column's data type (table rewrite)",
        "Drop a table"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Add nullable column: metadata-only, instant. Add index concurrently: no locks but takes time. Change type: table rewrite, potential long lock. Drop table: permanent data loss, no recovery without backup.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-025",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to change a column from INT to BIGINT on a 200M row table because values are approaching INT's max (2.1 billion). In PostgreSQL, ALTER COLUMN TYPE requires a table rewrite. How do you do this with zero downtime?",
          "options": [
            "ALTER TABLE orders ALTER COLUMN id TYPE BIGINT (simple but locks table)",
            "Expand-and-contract: add a new BIGINT column, dual-write, backfill, swap reads, drop old column",
            "Use a sequence that wraps around",
            "Switch to UUIDs"
          ],
          "correct": 1,
          "explanation": "Expand-and-contract avoids the table rewrite. Add id_new BIGINT, dual-write to both, backfill old rows, swap application reads to id_new, then drop old column. Each step is non-blocking.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "What's the trickiest part of this migration?",
          "options": [
            "Adding the new column",
            "Keeping id_new in sync during the transition — new inserts must write both columns, and the backfill must handle concurrent modifications without conflicts",
            "Dropping the old column",
            "Updating indexes"
          ],
          "correct": 1,
          "explanation": "The dual-write period is complex: inserts write both columns, updates must propagate, and the backfill must not overwrite concurrently-written values. Usually solved with triggers or application-level double-writes.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-026",
      "type": "multiple-choice",
      "question": "What is 'schema drift'?",
      "options": [
        "Schema changes that happen gradually",
        "When the actual database schema diverges from what the migration tool or application code expects — usually from manual changes or failed migrations",
        "A database optimization technique",
        "Schema changes that drift across replicas"
      ],
      "correct": 1,
      "explanation": "Schema drift occurs when someone applies manual DDL changes (hotfixes, console patches) outside the migration tool. The tool's state says the schema is at V10, but extra columns or indexes exist. This causes deployment failures and confusion.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-027",
      "type": "multi-select",
      "question": "How do you prevent and detect schema drift?",
      "options": [
        "All schema changes go through migration scripts — no manual DDL in production",
        "Automated schema comparison between expected and actual (CI/CD check)",
        "Restrict direct DDL access in production",
        "Only allow one developer to make schema changes"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Prevention: all changes via migrations (no manual DDL). Detection: automated comparison tools. Access control: restrict production DDL to the migration tool. Restricting to one developer doesn't help if they still make manual changes.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-028",
      "type": "multiple-choice",
      "question": "What is a 'feature flag' approach to schema evolution?",
      "options": [
        "A flag column in the schema table",
        "Using feature flags in application code to gradually switch between old and new schema usage — allowing safe rollback without a schema change",
        "Marking columns as deprecated",
        "A database flag that enables new features"
      ],
      "correct": 1,
      "explanation": "Feature flags let you deploy code that reads from the new schema structure, but flag it off initially. Gradually enable it for a percentage of users. If issues arise, flip the flag — no schema rollback needed. The schema stays unchanged; code behavior changes.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-029",
      "type": "two-stage",
      "stages": [
        {
          "question": "A legacy system stores addresses as a single 'address' TEXT column. The new design needs structured fields: street, city, state, zip. How do you migrate without downtime?",
          "options": [
            "Replace the address column with 4 new columns in one step",
            "Add the 4 new columns alongside the old one, parse and populate them, then transition reads to the new columns",
            "Create a new Addresses table and reference it",
            "Keep the TEXT column and parse it at read time"
          ],
          "correct": 1,
          "explanation": "Add new columns alongside the old one (expand). Parse the existing TEXT field to populate them (backfill). Update the app to read/write the new structured columns. Eventually drop the old TEXT column (contract).",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "During the transition, where should the application read address data from?",
          "options": [
            "Always the old TEXT column",
            "New structured columns if populated, falling back to the old TEXT column — a dual-read strategy",
            "Always the new columns (even if empty)",
            "A separate cache"
          ],
          "correct": 1,
          "explanation": "Dual-read: prefer new columns, fall back to old column. This handles the transition period where some rows are backfilled and others aren't. Once backfill is complete, remove the fallback and drop the old column.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-030",
      "type": "multiple-choice",
      "question": "What is the 'dark launch' pattern in schema evolution?",
      "options": [
        "Deploying at night",
        "Writing to the new schema in production alongside the old one, but only reading from the old — validating the new schema with production traffic before switching reads",
        "Hiding schema changes from the team",
        "Using dark mode in the database GUI"
      ],
      "correct": 1,
      "explanation": "Dark launch: write to both old and new schema paths. Reads still use the old path. Compare results to validate the new schema handles real data correctly. Once confident, switch reads to the new path. Low risk — the old path is always available.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-031",
      "type": "ordering",
      "question": "Rank these schema evolution strategies from MOST CONSERVATIVE (lowest risk) to MOST AGGRESSIVE:",
      "items": [
        "Dark launch (dual write, validate, then switch)",
        "Expand and contract (add new, migrate, remove old)",
        "Direct ALTER TABLE in production",
        "Blue-green database deployment (full schema swap)"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Dark launch: validate before switching, lowest risk. Expand-and-contract: gradual, backward compatible. Blue-green DB: full swap with fallback. Direct ALTER: immediate, no safety net, highest risk.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-032",
      "type": "multi-select",
      "question": "Which schema changes require data backfill?",
      "options": [
        "Adding a new column that should have a value for existing rows",
        "Splitting one column into two (e.g., full_name → first_name + last_name)",
        "Adding a new empty table",
        "Changing a column from nullable to NOT NULL"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "New column needing values: backfill from existing data or a default. Split column: parse and populate new columns. NOT NULL constraint: all existing NULLs must be filled first. Empty table: nothing to backfill.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-033",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration splits the Users table: User(id, name, email, bio, avatar_url, preferences_json) into User(id, name, email) and UserProfile(user_id, bio, avatar_url, preferences_json). What must happen before old code stops being deployed?",
          "options": [
            "Nothing — the split is transparent",
            "A transition period where code reads from both tables: User for core fields, UserProfile for extended fields — with fallback to the old User table",
            "All data must be migrated instantly",
            "The old User table must be dropped immediately"
          ],
          "correct": 1,
          "explanation": "During transition: new code reads from both tables (or a JOIN). Old code still reads from the original User table (which still has all columns). The old columns are only removed after all code is updated.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "What's the correct order? (A) Create UserProfile table. (B) Backfill UserProfile from User. (C) Deploy code reading from UserProfile. (D) Drop bio/avatar/preferences from User.",
          "options": ["A, B, C, D", "C, A, B, D", "A, C, B, D", "D, A, B, C"],
          "correct": 0,
          "explanation": "Create table → backfill data → deploy code that reads from new table → drop old columns. Each step depends on the previous one. Dropping columns before migrating reads would break the application.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-034",
      "type": "multiple-choice",
      "question": "What is 'idempotent migration'?",
      "options": [
        "A migration that runs exactly once",
        "A migration that can be safely run multiple times with the same result — running it twice doesn't cause errors or duplicate changes",
        "A migration that identifies itself",
        "A migration that can't be rolled back"
      ],
      "correct": 1,
      "explanation": "Idempotent migrations use guards: CREATE TABLE IF NOT EXISTS, ADD COLUMN IF NOT EXISTS, etc. If accidentally re-run (e.g., migration state is lost), they complete successfully without errors or double-applying changes.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-035",
      "type": "multi-select",
      "question": "Which SQL constructs make migrations idempotent?",
      "options": [
        "CREATE TABLE IF NOT EXISTS",
        "ALTER TABLE ADD COLUMN IF NOT EXISTS (PostgreSQL 9.6+)",
        "DROP TABLE IF EXISTS",
        "INSERT ... ON CONFLICT DO NOTHING"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All use conditional guards: IF NOT EXISTS / IF EXISTS prevents errors on re-run. ON CONFLICT DO NOTHING prevents duplicate inserts. These make migrations safe to re-run without checking state.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-036",
      "type": "multiple-choice",
      "question": "What is 'forward-only migration'?",
      "options": [
        "Migrations that only run on the latest database version",
        "A philosophy where rollback scripts aren't written — instead, fix-forward by deploying a new migration that corrects the issue",
        "Migrations that can only be run in production",
        "Migrations applied in chronological order"
      ],
      "correct": 1,
      "explanation": "Forward-only: don't write DOWN migrations. If something goes wrong, write a new UP migration that fixes the issue. Rationale: rollbacks are rarely tested, often fail, and some changes (data deletion) can't be rolled back anyway.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team deploys a migration that adds a column with a CHECK constraint. The constraint rejects 5% of incoming writes (valid data that doesn't match the constraint). What's the fix-forward approach?",
          "options": [
            "Roll back the migration",
            "Deploy a new migration that relaxes or removes the CHECK constraint",
            "Ask users to send valid data",
            "Restart the database"
          ],
          "correct": 1,
          "explanation": "Fix forward: deploy a new migration that corrects the constraint (ALTER TABLE ... DROP CONSTRAINT, or ALTER CONSTRAINT). No rollback needed — a new forward migration resolves the issue. This is why forward-only is pragmatic.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why is fix-forward often safer than rollback?",
          "options": [
            "Fix-forward is faster",
            "Rollback scripts may be untested, may not handle data written since the migration, and some changes (backfills, data transforms) can't be undone without data loss",
            "Databases don't support rollbacks",
            "There's no difference"
          ],
          "correct": 1,
          "explanation": "Rollback scripts assume the database state hasn't changed since the migration. But new data has been written, constraints may have been enforced, and destructive changes (drops, deletes) can't be undone. Fix-forward operates on the current state.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-038",
      "type": "ordering",
      "question": "Rank these approaches to handling a broken production migration from FASTEST to MOST THOROUGH:",
      "items": [
        "Revert the application code (if migration was backward compatible)",
        "Deploy a fix-forward migration",
        "Roll back the migration (if a rollback script exists and is tested)",
        "Restore from a pre-migration database backup"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Code revert: fastest if schema is backward compatible — deploy old code, schema stays. Fix-forward: write and deploy a new migration. Rollback script: apply the reverse migration. Backup restore: slowest, nuclear option — hours of downtime and data loss.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-039",
      "type": "multiple-choice",
      "question": "What is 'schema-on-read' vs 'schema-on-write'?",
      "options": [
        "Reading the schema vs writing the schema",
        "Schema-on-write enforces structure at write time (relational); schema-on-read interprets structure at read time (schemaless/document) — data is stored as-is and parsed when queried",
        "Reading during migrations vs writing during migrations",
        "Schemas for read replicas vs primary"
      ],
      "correct": 1,
      "explanation": "Schema-on-write (SQL): data must conform to the schema when inserted. Schema-on-read (MongoDB, data lakes): data is stored without enforcement; the application interprets structure when reading. Schema evolution is easier with schema-on-read but harder to enforce consistency.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-040",
      "type": "multi-select",
      "question": "What are advantages of schema-on-read for schema evolution?",
      "options": [
        "No migration scripts needed — just store data in the new format",
        "Old and new data formats coexist naturally",
        "Application code must handle multiple data formats",
        "Schema changes are instant (no ALTER TABLE)"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Schema-on-read: no ALTER TABLE, old/new formats coexist, instant 'migration.' But application code must handle all historical formats — the complexity shifts from the database to the application. Option C is a disadvantage, not an advantage.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-041",
      "type": "multiple-choice",
      "question": "In a document database (MongoDB), how do you 'migrate' from schema V1 (name as a string) to V2 (name as {first, last})?",
      "options": [
        "ALTER TABLE — wait, there's no ALTER TABLE",
        "Write new documents in V2 format; update the application to handle both formats at read time; optionally backfill old documents lazily or via batch",
        "Convert all documents in one transaction",
        "Create a new collection"
      ],
      "correct": 1,
      "explanation": "In document DBs, schema evolution is lazy: new writes use the new format. Old documents keep the old format. The app handles both. Optionally backfill old documents, but it's not required — dual-format reads handle the transition.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "MongoDB's 'lazy migration' pattern: old documents have {name: 'John Doe'}, new documents have {name: {first: 'John', last: 'Doe'}}. What does the application read logic look like?",
          "options": [
            "Always expect the new format",
            "Check the type: if string, parse it; if object, use it directly — handle both formats gracefully",
            "Always expect the old format",
            "Reject old-format documents"
          ],
          "correct": 1,
          "explanation": "The app checks the type of the name field. String → split to get first/last. Object → use directly. This is the cost of schema-on-read: the application must handle format heterogeneity.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Optionally, when reading an old-format document, the app can update it to the new format (lazy backfill). What's the benefit?",
          "options": [
            "Faster reads forever after",
            "Over time, old-format documents are gradually converted — the migration completes organically without a dedicated batch job",
            "It's required for MongoDB to work",
            "No benefit — it just wastes writes"
          ],
          "correct": 1,
          "explanation": "Lazy migration: each read of an old document converts it. High-traffic documents migrate quickly (read often = converted soon). Low-traffic documents migrate eventually. No batch job needed — migration happens naturally with usage.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-043",
      "type": "multiple-choice",
      "question": "What is 'event sourcing' schema evolution?",
      "options": [
        "Migrating events from one queue to another",
        "Events are immutable — you don't change old events. Instead, you version event schemas and teach the application to handle all historical event versions",
        "Sourcing schema changes from events",
        "Using events to trigger migrations"
      ],
      "correct": 1,
      "explanation": "In event sourcing, events are the source of truth and are immutable. V1 events stay as V1 forever. New events use V2 schema. The application must 'upcast' old event formats when replaying. Schema evolution means evolving event format versions.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-044",
      "type": "ordering",
      "question": "Rank these data serialization formats from WEAKEST to STRONGEST schema evolution support:",
      "items": [
        "Raw JSON (no schema enforcement)",
        "JSON Schema (validated but manual evolution)",
        "Protocol Buffers (compiled schemas with field numbering)",
        "Apache Avro (schema registry, automatic compatibility checks)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Raw JSON: no schema at all. JSON Schema: validation but manual compatibility management. Protobuf: field numbers enable adding/removing fields safely. Avro + Schema Registry: automated backward/forward compatibility verification.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-045",
      "type": "multi-select",
      "question": "In Protocol Buffers, which schema changes are backward compatible?",
      "options": [
        "Adding a new optional field with a new field number",
        "Removing a field (old messages still have it, new code ignores unknown fields)",
        "Changing a field's type (e.g., int32 to string)",
        "Renaming a field (field numbers stay the same)"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Protobuf identifies fields by number, not name. Adding fields: old code ignores new numbers. Removing: new code ignores old numbers. Renaming: field numbers unchanged, no effect on serialization. Changing type breaks binary compatibility.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-046",
      "type": "multiple-choice",
      "question": "What is a 'schema registry' (e.g., Confluent Schema Registry)?",
      "options": [
        "A database of database schemas",
        "A service that stores versioned schemas for serialized data (Avro, Protobuf) and enforces compatibility rules — producers can't publish incompatible schema changes",
        "A registry of database users",
        "A DNS-like service for databases"
      ],
      "correct": 1,
      "explanation": "Schema Registry stores versioned schemas and enforces compatibility (backward, forward, full). When a producer registers a new schema version, the registry checks compatibility against previous versions and rejects breaking changes.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "evo-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "What does 'backward compatible' mean for a schema in a schema registry?",
          "options": [
            "New code can read old data",
            "Consumers using the OLD schema can still read data produced with the NEW schema — new data is readable by old consumers",
            "The schema works with older database versions",
            "Old migrations still run"
          ],
          "correct": 0,
          "explanation": "Backward compatible: NEW code (consumers) can read OLD data (messages). This means adding optional fields and removing required fields is OK. Consumers with the latest schema can process all historical messages.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "What does 'forward compatible' mean?",
          "options": [
            "New data can be read by new code",
            "OLD code (consumers) can read NEW data (messages) — old consumers can handle messages with the new schema",
            "The schema works with future database versions",
            "Migrations run in forward order"
          ],
          "correct": 1,
          "explanation": "Forward compatible: OLD consumers can read NEW messages (they ignore unknown fields). This means you can deploy the producer (new schema) before updating consumers. Add optional fields, don't remove fields old code uses.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-048",
      "type": "ordering",
      "question": "Rank these compatibility levels from LEAST RESTRICTIVE to MOST RESTRICTIVE:",
      "items": [
        "None (any change allowed)",
        "Backward compatible (new code reads old data)",
        "Forward compatible (old code reads new data)",
        "Full compatible (both backward and forward)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "None: anything goes. Backward: only consumers must handle old data. Forward: only producers can add fields (consumers must ignore unknowns). Full: both constraints — only safe changes (adding optional fields) allowed.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-049",
      "type": "multiple-choice",
      "question": "A Kafka topic has messages in Avro format. The schema evolves from V1 to V5 over 2 years. Consumers joining now need to read all historical messages. What compatibility mode is needed?",
      "options": [
        "Forward compatible",
        "Backward compatible — new consumers (with V5 schema) must be able to read V1-V5 messages",
        "Full compatible",
        "None — just use the latest schema"
      ],
      "correct": 1,
      "explanation": "New consumers need to read ALL messages (V1 through V5). This requires backward compatibility — each new schema version must be able to decode data from all previous versions. The schema registry enforces this.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "evo-050",
      "type": "multi-select",
      "question": "Which are challenges of schema evolution in distributed systems?",
      "options": [
        "Different services may be on different schema versions simultaneously",
        "Cached data may be in old schema format",
        "Message queues contain messages with mixed schema versions",
        "All services must be updated simultaneously"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Distributed schema evolution means: services deploy independently (mixed versions). Caches hold old-format data. Queues have mixed-version messages. The whole point is to avoid simultaneous updates (option D) — that's the problem you're solving.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-051",
      "type": "multiple-choice",
      "question": "What is 'API versioning' and how does it relate to schema evolution?",
      "options": [
        "Versioning the database API",
        "Maintaining multiple API versions (v1/users, v2/users) so clients using older versions continue to work while the underlying schema evolves",
        "Versioning the SQL syntax",
        "Using different APIs for different tables"
      ],
      "correct": 1,
      "explanation": "API versioning decouples external contracts from internal schema. The v1 API returns {name: 'John'}, v2 returns {first: 'John', last: 'Doe'}. The schema can evolve freely; each API version maps to the current schema through an adapter layer.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "An API returns user.address as a string in v1. In v2, address is an object {street, city, state, zip}. The database has been migrated to structured fields. How does the v1 API still work?",
          "options": [
            "It doesn't — v1 clients must upgrade",
            "An adapter layer concatenates the structured fields back into a string for v1 responses: `${street}, ${city}, ${state} ${zip}`",
            "v1 reads from a separate old database",
            "Return the raw database row"
          ],
          "correct": 1,
          "explanation": "The v1 serializer/adapter maps the new schema to the old response format. The database schema evolves independently. Multiple API versions can coexist, each mapping to the current schema differently.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "When can the v1 API be retired?",
          "options": [
            "Immediately after v2 is released",
            "When monitoring shows no v1 clients remain, after a deprecation period with migration guides",
            "Never — v1 must be supported forever",
            "After 30 days automatically"
          ],
          "correct": 1,
          "explanation": "Deprecation lifecycle: announce v1 deprecation → provide migration guides → monitor v1 usage → set a sunset date → retire when usage drops to zero (or accept breaking the remaining stragglers).",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-053",
      "type": "multiple-choice",
      "question": "What is the 'strangler fig' pattern for legacy database migration?",
      "options": [
        "Wrapping the old database in a new interface",
        "Gradually replacing a legacy database by building new functionality on the new system while routing legacy operations through an adapter, until the old system can be decommissioned",
        "Strangling the database connections",
        "A load balancing pattern"
      ],
      "correct": 1,
      "explanation": "Strangler fig: new features use the new database. Old features gradually migrate over. An adapter/proxy routes queries to old or new system based on the data. Over time, the new system handles everything and the old one is turned off.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-054",
      "type": "ordering",
      "question": "Rank the phases of a strangler fig database migration in order:",
      "items": [
        "Build new system alongside old, with data sync",
        "Route new features to the new system",
        "Gradually migrate existing features to the new system",
        "Decommission the old system"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Build new + sync → new features on new system → migrate old features one by one → decommission old system. Each phase is incremental and reversible. No big-bang cutover.",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-055",
      "type": "multi-select",
      "question": "Which are risks of 'big bang' database migrations (switch everything at once)?",
      "options": [
        "Long downtime during migration",
        "Data loss or corruption if the migration fails partway",
        "No rollback path if the new system has issues",
        "Impossible to test with real production traffic beforehand"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "Big bang risks: extended downtime (hours), partial failure = inconsistency, hard to rollback (old system may be out of sync), and can't test incrementally with production traffic. Incremental migration (strangler fig) avoids all of these.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "A company is migrating from MySQL to PostgreSQL. The application uses 200 queries. What's the incremental approach?",
          "options": [
            "Rewrite all 200 queries and switch at once",
            "Set up dual-write to both databases, migrate one feature (e.g., 20 queries) at a time to PostgreSQL, validate, repeat until all features are migrated",
            "Run both databases forever",
            "Export all data and import into PostgreSQL"
          ],
          "correct": 1,
          "explanation": "Dual-write: both databases receive writes. Migrate features one at a time (20 queries → PostgreSQL). Validate each batch reads correctly from the new database. This takes longer but is dramatically safer.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "During dual-write, which database is the source of truth?",
          "options": [
            "Both equally",
            "The old database (MySQL) remains the source of truth until all reads are migrated — if PostgreSQL data diverges, MySQL is correct",
            "The new database (PostgreSQL)",
            "Neither — use a separate system"
          ],
          "correct": 1,
          "explanation": "The old database is the source of truth throughout the migration. If the new database has issues, the old one is authoritative. Only after all reads are verified on the new database do you switch the source of truth.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-057",
      "type": "multiple-choice",
      "question": "What is 'schema-as-code' (also called 'declarative schema management')?",
      "options": [
        "Writing schemas in a programming language",
        "Defining the desired schema state in code/config, and a tool computes and applies the diff to reach that state — instead of writing sequential migration scripts",
        "Embedding SQL in application code",
        "Generating code from the database schema"
      ],
      "correct": 1,
      "explanation": "Declarative schema: define what the schema should look like. The tool (e.g., Atlas, Skeema, Terraform for DBs) computes the migration needed to get from current state to desired state. Contrast with imperative migrations (sequential scripts).",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-058",
      "type": "multi-select",
      "question": "What are advantages of declarative schema management over sequential migration scripts?",
      "options": [
        "The current schema is readable in one file (not spread across 200 migration files)",
        "No need to track migration history",
        "Automatic diff generation between current and desired state",
        "Always safe for production — no human review needed"
      ],
      "correctIndices": [0, 2],
      "explanation": "Declarative schemas are readable (one file = current schema) and auto-generate diffs. You still need migration tracking (to know what's applied). Human review is still essential — auto-generated diffs can produce dangerous operations.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-059",
      "type": "numeric-input",
      "question": "A project has been running for 5 years with sequential migrations. At 2 migrations per week on average, approximately how many migration files exist?",
      "answer": 520,
      "unit": "files",
      "tolerance": 0.1,
      "explanation": "5 years × 52 weeks × 2 migrations/week = 520 migration files. Understanding the current schema requires mentally replaying all 520. This is a practical argument for declarative schema management.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-060",
      "type": "multiple-choice",
      "question": "What is 'data contract' in the context of schema evolution?",
      "options": [
        "A legal agreement about data usage",
        "A formal specification of a data schema that producers and consumers agree on — changes to the contract follow versioning and compatibility rules",
        "A database EULA",
        "A data retention policy"
      ],
      "correct": 1,
      "explanation": "Data contracts define: what fields exist, their types, which are required, acceptable values. Producers must conform when writing. Consumers can depend on the contract when reading. Schema evolution follows the contract's compatibility rules.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-061",
      "type": "two-stage",
      "stages": [
        {
          "question": "An analytics pipeline consumes data from 10 producer services. Producer A changes their schema (removes a field). The pipeline breaks. What should have prevented this?",
          "options": [
            "Better monitoring",
            "A data contract with backward compatibility enforcement — Producer A's change would be rejected by the schema registry for removing a required field",
            "More comprehensive testing",
            "The pipeline should handle missing fields"
          ],
          "correct": 1,
          "explanation": "A schema registry with backward compatibility enforcement would have rejected Producer A's breaking change before it reached production. The contract ensures producers can't make incompatible changes unilaterally.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "What process should Producer A follow to remove the field?",
          "options": [
            "Just remove it and fix consumers later",
            "Announce deprecation → consumers stop depending on the field → mark field optional → remove after confirmation that no consumers need it",
            "Ask the DBA to remove it",
            "Add a new field instead"
          ],
          "correct": 1,
          "explanation": "Deprecation lifecycle: announce → consumers migrate away → mark optional (schema change) → verify no consumers read it → remove. This is the safe, contract-respecting process for removing fields.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-062",
      "type": "ordering",
      "question": "Rank these schema evolution complexity levels from SIMPLEST to MOST COMPLEX:",
      "items": [
        "Single application, single database",
        "Multiple microservices, shared database",
        "Multiple microservices, each with their own database",
        "Event-driven microservices with async messaging and schema registry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Single app/DB: change both together. Shared DB: coordinate across services. Own DBs: each service evolves independently, but cross-service data sync is needed. Event-driven + schema registry: full distributed schema evolution.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-063",
      "type": "multiple-choice",
      "question": "What does 'database seeding' mean in the context of schema evolution?",
      "options": [
        "Planting database servers in data centers",
        "Populating a database with initial data (reference data, default configurations, test data) as part of the migration/setup process",
        "Generating random data for testing",
        "Creating database backups"
      ],
      "correct": 1,
      "explanation": "Seeding: inserting initial data that the application needs to function — roles (admin, user), country codes, default settings. Often done in migration scripts alongside schema changes. Different from backfill (transforming existing data).",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-064",
      "type": "multi-select",
      "question": "Which are risks of mixing data seeding with schema migrations?",
      "options": [
        "Seeds may fail if the data already exists (non-idempotent)",
        "Seeds tie environment-specific data to schema versions",
        "Rolling back the schema doesn't roll back the seeded data",
        "Seeds in migrations are harder to customize per environment (dev vs prod)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are real risks. Seeds should be idempotent (INSERT ... ON CONFLICT DO NOTHING). Environment-specific data shouldn't be in migrations. Rollbacks don't undo INSERTs (unless wrapped in a transaction). Keep seeds separate from schema DDL when possible.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-065",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration adds an 'enum' column: status VARCHAR CHECK (status IN ('active', 'inactive')). Six months later, a new status 'suspended' is needed. What's the migration?",
          "options": [
            "ALTER TABLE users DROP CONSTRAINT ... ADD CONSTRAINT ... CHECK (status IN ('active', 'inactive', 'suspended'))",
            "Delete the column and recreate it",
            "Add a new column for the new status",
            "Use an integer instead of varchar"
          ],
          "correct": 0,
          "explanation": "Drop the old CHECK constraint, add a new one with the additional value. In PostgreSQL: ALTER TABLE users DROP CONSTRAINT users_status_check, ADD CONSTRAINT users_status_check CHECK (status IN ('active', 'inactive', 'suspended')).",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why do some teams prefer a separate status lookup table over CHECK constraints for enums?",
          "options": [
            "Lookup tables are faster",
            "Adding a new status is just an INSERT into the lookup table — no schema migration needed",
            "CHECK constraints don't work in MySQL",
            "Lookup tables use less storage"
          ],
          "correct": 1,
          "explanation": "A Statuses lookup table: adding 'suspended' is INSERT INTO statuses (name) VALUES ('suspended'). No DDL change, no migration, no deployment. CHECK constraints require schema changes for each new value. The tradeoff: joins vs simpler schema.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-066",
      "type": "multiple-choice",
      "question": "What is a 'transactional migration'?",
      "options": [
        "A migration related to financial transactions",
        "A migration wrapped in a database transaction — if any statement fails, the entire migration is rolled back to a clean state",
        "A migration that processes transactions",
        "A paid migration service"
      ],
      "correct": 1,
      "explanation": "Transactional migrations: BEGIN → DDL statements → COMMIT (or ROLLBACK on failure). PostgreSQL supports transactional DDL. MySQL does NOT — each DDL statement auto-commits. If a MySQL migration fails partway, you have a partially applied state.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-067",
      "type": "multi-select",
      "question": "Which databases support transactional DDL (wrapping CREATE/ALTER/DROP in a transaction)?",
      "options": ["PostgreSQL", "MySQL/InnoDB", "SQLite", "SQL Server"],
      "correctIndices": [0, 2, 3],
      "explanation": "PostgreSQL, SQLite, and SQL Server support transactional DDL — DDL statements can be rolled back. MySQL auto-commits each DDL statement (implicit COMMIT), making transactional DDL impossible.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-068",
      "type": "two-stage",
      "stages": [
        {
          "question": "A MySQL migration has 3 DDL statements. Statement 2 fails. What is the database state?",
          "options": [
            "All 3 changes are rolled back",
            "Statement 1 is applied (committed), statement 2 failed, statement 3 didn't run — the schema is in a partially migrated state",
            "All 3 are applied despite the error",
            "The database is corrupted"
          ],
          "correct": 1,
          "explanation": "MySQL auto-commits each DDL. Statement 1 is permanently applied. Statement 2 failed. Statement 3 never ran. You're stuck in an intermediate state. This is why MySQL schema changes need careful ordering and idempotent scripts.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "How do you recover from a partially-applied MySQL migration?",
          "options": [
            "Restore from backup",
            "Manually fix the schema to either complete the migration or revert statement 1, then update the migration version tracking",
            "Restart MySQL",
            "Re-run the migration (it will fail on statement 1 again)"
          ],
          "correct": 1,
          "explanation": "Manually assess: complete the remaining statements (if safe) or revert the applied ones. Update the migration tracking table to reflect the actual state. This is tedious — idempotent migrations prevent the 're-run failure' problem.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-069",
      "type": "multiple-choice",
      "question": "What is the 'advisory lock' pattern in migration tools?",
      "options": [
        "A lock that advises users not to modify the schema",
        "A database-level lock that prevents multiple instances from running migrations simultaneously — ensuring only one migration runner is active",
        "A lock on advisory tables",
        "An optional lock that can be overridden"
      ],
      "correct": 1,
      "explanation": "Migration tools (Flyway, Alembic) acquire an advisory lock before running migrations. If two app instances start simultaneously, only one gets the lock and runs migrations. The other waits. Prevents duplicate or conflicting migrations.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-070",
      "type": "numeric-input",
      "question": "A deployment has 5 application replicas, each running migration checks at startup. Without an advisory lock, how many could attempt to run the same migration simultaneously?",
      "answer": 5,
      "unit": "replicas",
      "tolerance": "exact",
      "explanation": "All 5 replicas check for pending migrations, all 5 see the same pending migration, all 5 try to run it. With an advisory lock, only 1 runs it; the others wait and find no pending migrations.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-071",
      "type": "ordering",
      "question": "Rank these migration testing strategies from LEAST REALISTIC to MOST REALISTIC:",
      "items": [
        "Run migration on empty database",
        "Run migration on a small dataset (100 rows)",
        "Run migration on a copy of production data (anonymized)",
        "Run migration on a staging environment with production-scale data"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Empty DB: finds syntax errors only. Small dataset: finds basic logic errors. Production copy: finds data-dependent issues. Production-scale staging: finds performance issues (lock duration, backfill time). Each level catches different failures.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-072",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration adds a UNIQUE constraint to an email column. It passes on dev (10 rows) but fails on production (5M rows). Why?",
          "options": [
            "Production has more data",
            "Production has duplicate emails that violate the UNIQUE constraint — a data-dependent failure not caught by testing on clean dev data",
            "The production server is slower",
            "UNIQUE constraints don't work on large tables"
          ],
          "correct": 1,
          "explanation": "Dev data has no duplicates. Production has users who registered with the same email (perhaps allowed by old code). Adding UNIQUE fails because duplicates exist. Always test migrations against production-like data.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "How do you safely add a UNIQUE constraint when duplicates exist?",
          "options": [
            "Force the constraint and lose duplicate rows",
            "First deduplicate: identify and resolve duplicate emails (merge accounts, pick one), then add the constraint after cleanup",
            "Add the constraint as non-enforced",
            "Use a non-unique index instead"
          ],
          "correct": 1,
          "explanation": "Step 1: Find duplicates (SELECT email, COUNT(*) ... HAVING COUNT(*) > 1). Step 2: Resolve them (merge, deactivate, or assign unique emails). Step 3: Verify no duplicates remain. Step 4: Add UNIQUE constraint. The constraint must be earned.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Use the core equation with explicit unit tracking before committing to an answer. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-073",
      "type": "multiple-choice",
      "question": "What is 'schema linting'?",
      "options": [
        "Cleaning the database schema",
        "Automated analysis of migration scripts for common problems — dangerous operations (locks, rewrites), missing indexes, naming convention violations",
        "Formatting SQL code",
        "Checking for SQL syntax errors"
      ],
      "correct": 1,
      "explanation": "Schema linters (squawk, strong_migrations, sqlfluff) analyze migrations for: operations that lock tables, missing CONCURRENTLY on index creation, adding NOT NULL without a default, etc. They catch dangerous operations before they reach production.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-074",
      "type": "multi-select",
      "question": "Which issues can schema linting catch in CI?",
      "options": [
        "Adding a column with NOT NULL and no default (table lock in older PG)",
        "Creating an index without CONCURRENTLY on a large table",
        "Irreversible operations (DROP COLUMN) without explicit confirmation",
        "Business logic errors in the application"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Schema linters catch DDL-level risks: lock-causing operations, non-concurrent index creation, destructive changes. They can't catch business logic errors — that's application-level testing.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-075",
      "type": "ordering",
      "question": "Rank these migration safety practices from BASIC (minimum) to ADVANCED:",
      "items": [
        "Version-controlled migration scripts",
        "Test migrations on production-like data before deploying",
        "Schema linting in CI to catch dangerous operations",
        "Automated canary deployments with schema change monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Version control: bare minimum. Production-like testing: catches data-dependent issues. CI linting: automated safety checks. Canary deployment + monitoring: detects issues in production with minimal blast radius.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "evo-076",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team needs to shard a monolithic Users table across 4 database servers by user_id. What's the migration challenge?",
          "options": [
            "Sharding is a simple ALTER TABLE",
            "Moving data to the correct shard, updating the application to route queries by shard, and handling cross-shard queries — a fundamental architectural change, not just a schema change",
            "Just add more indexes",
            "Create 4 copies of the same database"
          ],
          "correct": 1,
          "explanation": "Sharding is one of the most complex schema evolutions. It changes: data distribution (move rows to correct shards), application logic (route by shard key), query patterns (no cross-shard joins), and operational complexity (4x the databases).",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "How do you migrate data from a single database to 4 shards without downtime?",
          "options": [
            "Copy all data at once and switch",
            "Dual-write to both old and new locations during migration; gradually redirect reads to shards; once validated, stop writing to the old database",
            "Export to CSV and import to each shard",
            "Use database replication to all 4 shards"
          ],
          "correct": 1,
          "explanation": "Dual-write: every write goes to the monolith AND the correct shard. Backfill existing data to shards. Gradually redirect reads to shards (shard by shard or feature by feature). Once all reads use shards, stop writing to the monolith.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Use a formula-first approach with explicit units to avoid hidden conversion mistakes. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-077",
      "type": "multiple-choice",
      "question": "What is a 'canary migration'?",
      "options": [
        "A migration for canary deployment environments",
        "Applying a schema change to a small subset of production (one shard, one replica) first, monitoring for issues, then rolling out to all",
        "A migration that detects problems",
        "A test migration that doesn't make real changes"
      ],
      "correct": 1,
      "explanation": "Canary migration: apply to one shard or partition first. Monitor query performance, error rates, application behavior. If issues arise, only one shard is affected. If clean, roll out to the rest. Reduces blast radius.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "evo-078",
      "type": "multi-select",
      "question": "What should you monitor after applying a schema migration?",
      "options": [
        "Query latency (did any queries get slower?)",
        "Error rates (are queries failing due to schema changes?)",
        "Lock wait times (is the migration holding locks?)",
        "Disk usage (did the migration consume unexpected space?)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All four: latency changes indicate plan regression. Error rates catch incompatible code. Lock waits indicate blocking DDL. Disk usage catches unexpected table rewrites or index bloat.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ]
    },
    {
      "id": "evo-079",
      "type": "multiple-choice",
      "question": "What is the 'blue-green deployment' approach to database migrations?",
      "options": [
        "Painting servers blue or green",
        "Maintaining two database instances: 'blue' (current) and 'green' (new schema). Switch traffic from blue to green after migration. Roll back by switching back to blue.",
        "Running migrations on green servers only",
        "Color-coded migration scripts"
      ],
      "correct": 1,
      "explanation": "Blue-green: two full databases. Migrate 'green' while 'blue' serves traffic. Switch when ready. Rollback = switch back to 'blue'. Challenge: keeping both databases in sync during the transition period.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-080",
      "type": "two-stage",
      "stages": [
        {
          "question": "Blue-green database deployment: 'blue' is live, 'green' has the new schema. During switchover, writes happen on both. What's the biggest challenge?",
          "options": [
            "Network bandwidth",
            "Data synchronization — writes to 'blue' during switchover must be replicated to 'green', and vice versa, to avoid data loss",
            "Color management",
            "Server naming"
          ],
          "correct": 1,
          "explanation": "During switchover, both databases may receive writes. These must be synchronized to prevent data loss or divergence. Change Data Capture or dual-write patterns help, but it's operationally complex.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why is blue-green less common for databases than for application deployments?",
          "options": [
            "Databases are more expensive to duplicate",
            "Databases hold state — synchronizing two full database copies with different schemas while maintaining consistency is far harder than deploying stateless application servers",
            "Databases don't support blue-green",
            "No one has tried it"
          ],
          "correct": 1,
          "explanation": "Applications are (mostly) stateless — blue-green is easy, just route traffic. Databases are stateful — duplicating the full dataset, keeping both in sync during schema changes, and handling writes to both is extremely complex and expensive.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Start from the governing formula, keep units visible, and validate the final magnitude. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-081",
      "type": "ordering",
      "question": "Rank these schema change types from FASTEST to SLOWEST on a 100M row PostgreSQL table:",
      "items": [
        "ADD COLUMN (nullable, no default)",
        "ADD COLUMN with DEFAULT (PG 11+)",
        "CREATE INDEX CONCURRENTLY",
        "ALTER COLUMN TYPE (e.g., INT to BIGINT — table rewrite)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Add nullable column: instant (metadata only). Add with default (PG 11+): instant (stored in catalog). Create index concurrently: minutes (scans table twice but no lock). Alter type: rewrite entire table — potentially hours.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-082",
      "type": "multiple-choice",
      "question": "Why should migration scripts be immutable (never modified after applied)?",
      "options": [
        "Database tools forbid changes",
        "The migration represents a historical change — modifying it creates divergence between databases that applied the old version and new ones that apply the modified version",
        "Immutable scripts run faster",
        "Version control requires it"
      ],
      "correct": 1,
      "explanation": "If migration V005 is modified after some databases applied the original, those databases have a different schema than databases applying the modified V005. This creates silent schema drift. New changes should be new migration files.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-083",
      "type": "multi-select",
      "question": "Which are valid strategies for handling long-running data migrations in production?",
      "options": [
        "Run as a background job separate from the DDL migration",
        "Batch updates with throttling (sleep between batches)",
        "Use online migration tools (gh-ost, LHM) for table transformations",
        "Run the entire migration in a single transaction for atomicity"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Background jobs: decouple data migration from schema change. Batched updates: controlled resource usage. Online tools: handle large table changes. Single huge transaction: locks resources for the entire duration — dangerous.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-084",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team uses a ORM (like Django or Rails) that auto-generates migrations from model changes. A developer changes a model field type from String(50) to String(200). What migration is generated?",
          "options": [
            "DROP and recreate the column",
            "ALTER TABLE ... ALTER COLUMN ... TYPE VARCHAR(200) — increasing length, typically a safe online operation",
            "No migration — the ORM handles it at runtime",
            "A complete table rebuild"
          ],
          "correct": 1,
          "explanation": "ORM migration generators produce ALTER COLUMN TYPE. Increasing VARCHAR length is generally safe (metadata change in most databases). Decreasing length would be risky — could truncate existing data.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "Why should auto-generated migrations be reviewed before applying?",
          "options": [
            "They might have syntax errors",
            "The generator may produce inefficient or dangerous operations — e.g., column type changes that trigger table rewrites, or operations that should use CONCURRENTLY but don't",
            "They're always wrong",
            "ORM bugs are common"
          ],
          "correct": 1,
          "explanation": "Auto-generated migrations are correct SQL but may not be production-safe. They don't add CONCURRENTLY for index creation, may generate table rewrites for type changes, or produce unnecessary operations. Human review ensures safety.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Follow the canonical calculation path and check both units and magnitude before finalizing. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-085",
      "type": "numeric-input",
      "question": "A 500GB table needs a column type change (table rewrite) at a disk write speed of 500MB/sec. Approximately how long will the rewrite take (ignoring overhead)?",
      "answer": 1000,
      "unit": "seconds",
      "tolerance": 0.1,
      "explanation": "500 GB = 500,000 MB. At 500 MB/sec, rewrite takes 500,000/500 = 1,000 seconds ≈ 17 minutes. With overhead (WAL writes, index rebuilds), expect 2-3x longer. During this time, the table may be locked.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ]
    },
    {
      "id": "evo-086",
      "type": "multiple-choice",
      "question": "What is 'schema archaeology' and when is it needed?",
      "options": [
        "Studying ancient database schemas",
        "Reverse-engineering the history and purpose of schema decisions in a legacy system — understanding why tables, columns, and constraints exist",
        "Digging through database backups",
        "Studying database internals"
      ],
      "correct": 1,
      "explanation": "Schema archaeology: when joining a legacy project, understanding why the schema is the way it is. Why is this column VARCHAR(37)? Why does this table exist? Migration history, commit messages, and documentation provide clues.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-087",
      "type": "multi-select",
      "question": "Which practices help future developers understand schema evolution decisions?",
      "options": [
        "Descriptive migration filenames (add_user_preferences_table)",
        "Comments in migration scripts explaining WHY (not just what)",
        "A schema changelog documenting major design decisions",
        "Automated migration from migration scripts to documentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Descriptive names: quick understanding. Comments: explain the reasoning. Changelogs: high-level decision history. All reduce schema archaeology effort for future developers.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-088",
      "type": "ordering",
      "question": "Rank the phases of safely adding a NOT NULL constraint to an existing column with NULLs:",
      "items": [
        "Deploy code that always writes a value to the column",
        "Backfill existing NULL values with a default",
        "Validate no NULLs remain (SELECT COUNT(*) WHERE col IS NULL)",
        "Add NOT NULL constraint"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stop writing NULLs first (code change). Then backfill existing NULLs. Verify none remain. Only then add the constraint. If you add NOT NULL first, writes without the column fail and existing NULLs cause the ALTER to fail.",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-089",
      "type": "two-stage",
      "stages": [
        {
          "question": "PostgreSQL 12+ supports adding NOT NULL constraints with NOT VALID. What does this do?",
          "options": [
            "Adds the constraint without checking existing rows — new writes are enforced, but existing rows aren't validated",
            "Makes the constraint optional",
            "Adds the constraint only during business hours",
            "Creates a non-valid (fake) constraint"
          ],
          "correct": 0,
          "explanation": "NOT VALID: the constraint is enforced on new writes immediately, but existing rows aren't checked. This is fast (no full table scan). You validate existing rows separately with VALIDATE CONSTRAINT.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "What is the benefit of the two-step (ADD CONSTRAINT NOT VALID → VALIDATE CONSTRAINT) approach?",
          "options": [
            "It's faster overall",
            "ADD CONSTRAINT NOT VALID is instant (no lock), and VALIDATE CONSTRAINT only takes a ShareUpdateExclusiveLock (doesn't block writes) — zero downtime",
            "It skips validation entirely",
            "It only works on small tables"
          ],
          "correct": 1,
          "explanation": "Step 1 (NOT VALID): instant, lightweight lock. Step 2 (VALIDATE): scans the table but uses a weaker lock that doesn't block INSERT/UPDATE/DELETE. Total: zero downtime. Without NOT VALID, adding the constraint locks the table during the full scan.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        }
      ],
      "detailedExplanation": "Apply the main relationship stepwise and verify unit consistency at each step. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-090",
      "type": "multiple-choice",
      "question": "What is 'soft delete' and how does it affect schema evolution?",
      "options": [
        "Deleting data softly",
        "Marking rows as deleted (deleted_at timestamp or is_deleted flag) instead of physically removing them — affects queries (must filter deleted rows) and schema evolution (column still has data)",
        "A gentle way to drop tables",
        "Deleting with a lower priority"
      ],
      "correct": 1,
      "explanation": "Soft delete: rows aren't physically removed, just flagged. Schema impact: all queries need WHERE deleted_at IS NULL. Column changes must consider 'deleted' rows too. UNIQUE constraints need to exclude soft-deleted rows (partial indexes).",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-091",
      "type": "multi-select",
      "question": "What schema implications does soft delete have?",
      "options": [
        "UNIQUE constraints must account for soft-deleted rows (use partial unique index excluding deleted rows)",
        "Tables grow larger over time (deleted rows accumulate)",
        "Foreign key relationships to soft-deleted rows must be handled",
        "Queries must always include a deleted_at filter"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are real implications. UNIQUE must be partial (WHERE deleted_at IS NULL). Tables bloat (need periodic purging). FK references to deleted rows create orphans. Every query needs the filter (use default scopes in ORMs).",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-092",
      "type": "two-stage",
      "stages": [
        {
          "question": "A table has a UNIQUE constraint on (email). The system uses soft delete. User A (deleted) has email 'a@b.com'. User B wants to register with 'a@b.com'. What happens?",
          "options": [
            "User B registers successfully",
            "UNIQUE violation — 'a@b.com' exists in the table (soft-deleted row), blocking User B",
            "The soft-deleted row is automatically removed",
            "The database ignores soft-deleted rows"
          ],
          "correct": 1,
          "explanation": "The UNIQUE constraint applies to ALL rows, including soft-deleted ones. User A's row still exists with email 'a@b.com'. User B's INSERT violates the constraint. The database doesn't know about soft delete semantics.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "How do you fix this?",
          "options": [
            "Remove the UNIQUE constraint entirely",
            "Use a partial unique index: CREATE UNIQUE INDEX ... ON users(email) WHERE deleted_at IS NULL — uniqueness only among active rows",
            "Hard-delete the row instead",
            "Use a different email for User B"
          ],
          "correct": 1,
          "explanation": "A partial unique index enforces uniqueness only among non-deleted rows. Soft-deleted rows are excluded from the uniqueness check. User B can register with the same email as a soft-deleted user.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Anchor on the base formula, preserve unit integrity, and then run a reasonableness check. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-093",
      "type": "multiple-choice",
      "question": "What is a 'migration dependency graph'?",
      "options": [
        "A visual representation of table relationships",
        "The ordering of migrations where some depend on others — migration B requires migration A's table to exist, creating a dependency",
        "A graph database for tracking migrations",
        "The relationship between database versions"
      ],
      "correct": 1,
      "explanation": "Migration B (add foreign key to Users) depends on Migration A (create Users table). In branching workflows, dependencies can create conflicts. Some tools support explicit dependency declarations instead of linear ordering.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-094",
      "type": "ordering",
      "question": "Rank the safety of these approaches for renaming a table from SAFEST to RISKIEST:",
      "items": [
        "Create new table, migrate data, create view with old name pointing to new table, update code, drop view",
        "Create new table, dual-write, migrate reads, drop old table",
        "ALTER TABLE RENAME (direct rename)",
        "Create a synonym/alias for the new name"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "View aliasing: safest, old name still works. Dual-write: safe, gradual transition. Synonym: DB-level alias, limited support. Direct rename: instant but breaks all code referencing the old name simultaneously.",
      "detailedExplanation": "Order by relative impact rather than exact values, then verify the sequence one boundary at a time. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-095",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration needs to merge two tables: Accounts and Profiles (1:1 relationship) into a single Users table. What's the safe approach?",
          "options": [
            "DROP both tables and CREATE Users",
            "Create Users table, backfill from both tables, deploy code using Users, create views for Accounts and Profiles pointing to Users for backward compatibility, eventually drop views",
            "UNION the two tables",
            "Rename Accounts to Users and copy Profile columns"
          ],
          "correct": 1,
          "explanation": "Create the target table, migrate data, deploy code incrementally, and use views for backward compatibility. Old code referencing Accounts or Profiles sees views that map to Users. Drop views when all code is updated.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        },
        {
          "question": "What's the trickiest part of merging two tables?",
          "options": [
            "Creating the new table",
            "Handling writes during the transition — writes to old tables must be reflected in the new table, and vice versa, until the transition is complete",
            "Choosing the table name",
            "Copying the data"
          ],
          "correct": 1,
          "explanation": "During transition, writes may target old tables (via views or old code) or the new table (new code). All write paths must keep data consistent. This usually requires either triggers on views or application-level dual-write logic.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Compute from the primary formula first, then sanity-check the scale of the result. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-096",
      "type": "multi-select",
      "question": "Which are signs that a schema evolution process is mature?",
      "options": [
        "All changes go through versioned, reviewed migration scripts",
        "Migrations are tested against production-like data before deployment",
        "Schema linting runs in CI",
        "Developers apply DDL directly in production consoles"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Mature process: versioned scripts, production-like testing, automated linting. Direct DDL in production consoles bypasses all safety nets — a sign of an immature or emergency-driven process.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-097",
      "type": "numeric-input",
      "question": "A team performs schema migrations via a CI/CD pipeline. The pipeline takes 3 minutes. They deploy 10 times per day. How many minutes per day are spent on migration pipeline runs?",
      "answer": 30,
      "unit": "minutes",
      "tolerance": "exact",
      "explanation": "10 deployments × 3 minutes = 30 minutes/day. This is the overhead of automated safety. Most of these deployments have no migrations (the check is fast), but the pipeline runs regardless to verify.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-098",
      "type": "multiple-choice",
      "question": "What is 'database state machine' thinking for schema evolution?",
      "options": [
        "Using a state machine library for migrations",
        "Treating the database schema as a finite state machine where each migration is a transition from one valid state to another — the sequence of transitions must always leave the schema in a consistent, valid state",
        "Automating database decisions",
        "Running migrations in a specific state"
      ],
      "correct": 1,
      "explanation": "Each migration transitions: Schema V1 → Schema V2 → ... Each intermediate state must be valid (application can run). No migration should leave the schema in a state where the application breaks. Every step is a safe state transition.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-099",
      "type": "two-stage",
      "stages": [
        {
          "question": "An application needs to support both Schema V5 (current) and V6 (new) simultaneously during rolling deployment. What design principle ensures this?",
          "options": [
            "Schema V6 must be backward compatible — V5 code can still work with V6 schema",
            "V6 must be a superset of V5",
            "Both schemas must be identical",
            "V5 code must be rewritten for V6"
          ],
          "correct": 0,
          "explanation": "Backward compatibility: V6 schema works with both V5 code (old replicas) and V6 code (new replicas). During rolling deployment, both versions run simultaneously. The schema must not break either version.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly."
        },
        {
          "question": "After all replicas are on V6 code, is the old V5 compatibility still needed?",
          "options": [
            "Yes — always maintain backward compatibility with all previous versions",
            "No — once V5 code is fully decommissioned, V5 compatibility can be dropped in the next schema version (V7)",
            "Only for one more version",
            "It depends on the database"
          ],
          "correct": 1,
          "explanation": "You only need N and N-1 compatibility (current and previous). Once V5 code is fully gone, V7 can drop V5-specific compatibility (remove deprecated columns, etc.). You don't need to support all historical versions forever.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "detailedExplanation": "Work through the core math with unit labels attached, then verify scale plausibility. Modeling choices should tie directly to query paths, write amplification, and index/storage overhead to make tradeoffs measurable.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    },
    {
      "id": "evo-100",
      "type": "ordering",
      "question": "Rank these schema evolution anti-patterns from MOST COMMON to LEAST COMMON:",
      "items": [
        "Applying DDL directly in production without migration scripts",
        "Not testing migrations against production-scale data",
        "Missing backward compatibility during rolling deployments",
        "Modifying already-applied migration files"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Direct DDL: extremely common, especially in early startups. Skipping production-scale testing: common (dev data is always small). Missing backward compat: common in non-continuous deployment shops. Modifying applied migrations: less common (tools often checksum-verify).",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ]
    }
  ]
}
