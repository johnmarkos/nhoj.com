{
  "unit": 2,
  "unitTitle": "Data Modeling",
  "chapter": 7,
  "chapterTitle": "Schema Evolution",
  "chapterDescription": "Migrations, backward compatibility, and evolving schemas without downtime.",
  "problems": [
    {
      "id": "evo-001",
      "type": "multiple-choice",
      "question": "What is a 'schema migration'?",
      "options": [
        "Moving a database to a new server",
        "A versioned change to the database schema (adding/altering/dropping tables or columns) applied in a controlled, repeatable way",
        "Converting data from one format to another",
        "Migrating from one database vendor to another"
      ],
      "correct": 1,
      "explanation": "Schema migrations are versioned scripts that modify the database structure. They're applied in order (v001, v002, ...) and tracked so you know which migrations have been applied. Tools: Flyway, Liquibase, Rails migrations, Alembic.",
      "detailedExplanation": "If you keep \"a 'schema migration'\" in view, the correct answer separates faster. Prefer the schema/index decision that minimizes query and write amplification for this workload. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-002",
      "type": "ordering",
      "question": "Rank these migration steps in the correct order:",
      "items": [
        "Write the migration script (SQL/code)",
        "Test the migration against a copy of production data",
        "Apply the migration to production",
        "Verify the application works with the new schema"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Write → test on production-like data (catch data-dependent failures) → apply to production → verify. Never skip testing against realistic data — migrations that work on empty dev databases can fail on real data.",
      "detailedExplanation": "This prompt is really about \"rank these migration steps in the correct order:\". Order by relative scale and bottleneck effect, then validate neighboring items. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-003",
      "type": "multiple-choice",
      "question": "What does 'backward compatible' mean for schema changes?",
      "options": [
        "The change works on older database versions",
        "The old version of the application can still work correctly after the schema change — both old and new code can coexist during deployment",
        "The migration can be rolled back",
        "The schema matches a previous version"
      ],
      "correct": 1,
      "explanation": "Backward compatible schema changes don't break existing code. During rolling deployments, old and new application versions run simultaneously. If the schema change breaks old code, you get errors during the rollout.",
      "detailedExplanation": "Use \"'backward compatible' mean for schema changes\" as your starting point, then verify tradeoffs carefully. Prefer the schema/index decision that minimizes query and write amplification for this workload. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-004",
      "type": "multi-select",
      "question": "Which schema changes are backward compatible (safe during rolling deployment)?",
      "options": [
        "Adding a new nullable column",
        "Renaming an existing column",
        "Adding a new table",
        "Dropping a column that old code reads from"
      ],
      "correctIndices": [0, 2],
      "explanation": "Adding nullable columns: old code ignores them. Adding tables: old code doesn't reference them. Renaming breaks old code's column references. Dropping a column breaks old code that reads it.",
      "detailedExplanation": "Read this as a scenario about \"schema changes are backward compatible (safe during rolling deployment)\". Treat every option as a separate true/false test under the same constraints. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-005",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to rename column 'name' to 'display_name' on the Users table. The app is deployed via rolling updates (old and new versions run simultaneously). What's the safe approach?",
          "options": [
            "ALTER TABLE users RENAME COLUMN name TO display_name (one step)",
            "Multi-step: (1) Add display_name column, (2) backfill data, (3) update app to read/write both columns, (4) stop writing to name, (5) drop name column",
            "Create a view that aliases the column",
            "Don't rename — use the old name forever"
          ],
          "correct": 1,
          "explanation": "Multi-step expand-and-contract: add the new column, double-write during transition, migrate reads to the new column, then drop the old one. Each step is backward compatible. A direct rename breaks old code instantly.",
          "detailedExplanation": "Use \"you need to rename column 'name' to 'display_name' on the Users table\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "This multi-step approach is called?",
          "options": [
            "Blue-green deployment",
            "Expand and contract (also called parallel change)",
            "Schema versioning",
            "Hot migration"
          ],
          "correct": 1,
          "explanation": "Expand and contract: 'expand' the schema (add the new column alongside the old), transition code, then 'contract' (remove the old column). Each step is independently safe and deployable.",
          "detailedExplanation": "The core signal here is \"this multi-step approach is called\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        }
      ],
      "detailedExplanation": "The decision turns on \"schema Evolution\". Do not reset assumptions between stages; carry forward prior constraints directly. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-006",
      "type": "ordering",
      "question": "Rank the steps of an expand-and-contract column rename (name → display_name) in correct order:",
      "items": [
        "Add display_name column (nullable)",
        "Backfill: UPDATE users SET display_name = name WHERE display_name IS NULL",
        "Deploy app to read display_name (fallback to name) and write both",
        "Drop the name column"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Add column → backfill existing data → deploy code that uses new column (with fallback) → once all code reads new column, drop old column. Each step is a separate, safe deployment.",
      "detailedExplanation": "Start from \"rank the steps of an expand-and-contract column rename (name → display_name) in correct\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-007",
      "type": "multiple-choice",
      "question": "Why is ALTER TABLE ... ADD COLUMN with a DEFAULT value potentially dangerous on large tables in older PostgreSQL versions (< 11)?",
      "options": [
        "It creates a new table",
        "It rewrites every row in the table to add the default value, locking the table for the duration — potentially minutes to hours on large tables",
        "It changes the column type",
        "It drops existing indexes"
      ],
      "correct": 1,
      "explanation": "Before PostgreSQL 11, ADD COLUMN with a non-null DEFAULT rewrote every row. On a 100M row table, this could take hours with the table locked. PostgreSQL 11+ stores the default in the catalog instead — instant ADD COLUMN.",
      "detailedExplanation": "The key clue in this question is \"aLTER TABLE\". Discard modeling choices that look clean but perform poorly for the target queries. Schema and index choices should follow access patterns and write/read amplification constraints. Numbers such as 11 and 100M should be normalized first so downstream reasoning stays consistent. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-008",
      "type": "multi-select",
      "question": "Which ALTER TABLE operations typically require a table rewrite (slow, locking)?",
      "options": [
        "Changing a column's data type (e.g., INT to BIGINT)",
        "Adding a nullable column with no default",
        "Adding a NOT NULL constraint with a default (pre-PG11)",
        "Adding a new index (in databases without CONCURRENTLY)"
      ],
      "correctIndices": [0, 2],
      "explanation": "Changing data types requires converting every row's value — table rewrite. Adding NOT NULL + DEFAULT (pre-PG11) rewrites all rows. Adding a nullable column is metadata-only (instant). Index creation doesn't rewrite the table (it reads it).",
      "detailedExplanation": "The core signal here is \"aLTER TABLE operations typically require a table rewrite (slow, locking)\". Validate each option independently; do not select statements that are only partially true. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-009",
      "type": "multiple-choice",
      "question": "What does CREATE INDEX CONCURRENTLY do in PostgreSQL?",
      "options": [
        "Creates the index faster using multiple CPUs",
        "Builds the index without holding a lock that blocks writes — allows reads and writes to continue during index creation",
        "Creates multiple indexes at once",
        "Creates the index in a background thread"
      ],
      "correct": 1,
      "explanation": "Regular CREATE INDEX holds a lock that blocks writes for the duration. CONCURRENTLY builds the index in multiple passes without blocking writes. It takes longer but doesn't cause downtime. Essential for production.",
      "detailedExplanation": "If you keep \"cREATE INDEX CONCURRENTLY do in PostgreSQL\" in view, the correct answer separates faster. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-010",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to add an index to a 500M row table in production. CREATE INDEX takes 30 minutes and locks the table. What should you do?",
          "options": [
            "Run it during low traffic (still locks writes)",
            "Use CREATE INDEX CONCURRENTLY — no write locks",
            "Create the index on a replica and promote it",
            "Don't add the index"
          ],
          "correct": 1,
          "explanation": "CREATE INDEX CONCURRENTLY: builds the index without blocking writes. Takes longer (two table passes instead of one) but zero downtime. This is the standard approach for production index creation.",
          "detailedExplanation": "If you keep \"you need to add an index to a 500M row table in production\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 500M and 30 minutes should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "CREATE INDEX CONCURRENTLY can fail partway through. What happens?",
          "options": [
            "The table is corrupted",
            "The index is left in an INVALID state — you must DROP it and retry",
            "The partially built index still works",
            "The database rolls back automatically"
          ],
          "correct": 1,
          "explanation": "If CONCURRENTLY fails (e.g., unique constraint violation, out of disk space), the index is marked INVALID. It exists but isn't used by the query planner. You must DROP INDEX and retry.",
          "detailedExplanation": "This prompt is really about \"cREATE INDEX CONCURRENTLY can fail partway through\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"schema Evolution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-011",
      "type": "multiple-choice",
      "question": "What is a 'zero-downtime migration'?",
      "options": [
        "A migration that takes zero time to run",
        "A schema change applied while the application continues serving traffic without interruption or errors",
        "A migration run at midnight when no one is online",
        "A migration that doesn't change anything"
      ],
      "correct": 1,
      "explanation": "Zero-downtime migrations change the schema while the application stays live. This requires backward-compatible changes, online DDL operations (no long locks), and careful sequencing of schema + code changes.",
      "detailedExplanation": "Start from \"a 'zero-downtime migration'\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-012",
      "type": "ordering",
      "question": "For a zero-downtime column addition with NOT NULL constraint, rank these steps in order:",
      "items": [
        "Add the column as nullable",
        "Backfill existing rows with the desired default value",
        "Deploy code that always writes the new column",
        "Add the NOT NULL constraint (with a default)"
      ],
      "correctOrder": [0, 2, 1, 3],
      "explanation": "Add nullable (instant, no lock) → deploy code that writes to it (new rows get values) → backfill old rows → add NOT NULL constraint (now safe — all rows have values). Each step is safe independently.",
      "detailedExplanation": "The decision turns on \"for a zero-downtime column addition with NOT NULL constraint, rank these steps in order:\". Order by relative scale and bottleneck effect, then validate neighboring items. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-013",
      "type": "multi-select",
      "question": "Which are 'online DDL' operations in MySQL 8.0 (executed without blocking reads/writes)?",
      "options": [
        "Adding a column",
        "Adding an index (ALGORITHM=INPLACE)",
        "Changing a column from VARCHAR(100) to VARCHAR(200)",
        "Changing a column from INT to VARCHAR"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "MySQL 8.0 supports online DDL for many operations: ADD COLUMN, ADD INDEX (INPLACE), and increasing VARCHAR length (within limits). Changing column type fundamentally (INT to VARCHAR) requires a table copy.",
      "detailedExplanation": "Read this as a scenario about \"'online DDL' operations in MySQL 8\". Validate each option independently; do not select statements that are only partially true. Schema and index choices should follow access patterns and write/read amplification constraints. If values like 8.0 appear, convert them into one unit basis before comparison. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-014",
      "type": "multiple-choice",
      "question": "What is a 'migration rollback' and when is it needed?",
      "options": [
        "Restoring from a database backup",
        "Reversing a schema migration by applying a 'down' migration that undoes the changes — needed when a migration causes issues in production",
        "Rolling back a Git commit",
        "Restarting the database server"
      ],
      "correct": 1,
      "explanation": "A rollback applies the reverse migration (DROP what was ADDed, re-ADD what was DROPped). Needed when a migration causes application errors or performance issues. Not all migrations are easily reversible (e.g., data-destructive changes).",
      "detailedExplanation": "Use \"a 'migration rollback' and when is it needed\" as your starting point, then verify tradeoffs carefully. Reject options that conflict with the primary access pattern or index strategy. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-015",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration drops the 'middle_name' column from the Users table. After deployment, a bug is discovered and you need to rollback. Can you restore the column?",
          "options": [
            "Yes — just re-add the column",
            "You can re-add the column, but the DATA is lost forever — you can't restore the values that were in the dropped column",
            "No — dropped columns can't be re-added",
            "The database keeps a backup automatically"
          ],
          "correct": 1,
          "explanation": "You can ADD COLUMN middle_name back, but all the data that was in it is gone. Dropping a column is a data-destructive operation. This is why careful teams mark columns as 'deprecated' before dropping them.",
          "detailedExplanation": "Start from \"migration drops the 'middle_name' column from the Users table\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: unbounded cardinality in joins or fan-out."
        },
        {
          "question": "What's the safer approach for removing a column?",
          "options": [
            "Drop it immediately",
            "Multi-step: (1) Stop writing to the column, (2) Stop reading the column, (3) Wait a safe period, (4) Drop the column — with a backup/export of the data before dropping",
            "Rename it to 'deprecated_middle_name'",
            "Never drop columns"
          ],
          "correct": 1,
          "explanation": "Stop using the column in code first. Wait until you're confident it's not needed. Export/backup the data. Then drop. The waiting period gives you time to catch unexpected dependencies before the data is gone.",
          "detailedExplanation": "The decision turns on \"what's the safer approach for removing a column\". Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"schema Evolution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-016",
      "type": "multiple-choice",
      "question": "What is 'schema versioning' in the context of migration tools?",
      "options": [
        "Versioning the database software",
        "Tracking which migrations have been applied by storing version numbers in a metadata table (e.g., schema_migrations)",
        "Creating a new database for each version",
        "Versioning the SQL syntax used"
      ],
      "correct": 1,
      "explanation": "Migration tools (Flyway, Alembic, etc.) store applied migration IDs in a table (e.g., flyway_schema_history). This lets the tool know which migrations are pending and prevents re-applying already-applied changes.",
      "detailedExplanation": "If you keep \"'schema versioning' in the context of migration tools\" in view, the correct answer separates faster. Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-017",
      "type": "multi-select",
      "question": "Which are features of modern migration tools (Flyway, Alembic, Rails Migrations)?",
      "options": [
        "Ordered, versioned migration scripts",
        "Tracking of applied migrations in a metadata table",
        "Automatic rollback on failure",
        "Generating migrations from model/schema diffs"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Migration tools provide: versioned ordering, tracking applied migrations, and some can auto-generate migrations from model changes (Alembic autogenerate, Rails). Automatic rollback isn't guaranteed — many DDL operations aren't transactional.",
      "detailedExplanation": "The core signal here is \"features of modern migration tools (Flyway, Alembic, Rails Migrations)\". Validate each option independently; do not select statements that are only partially true. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-018",
      "type": "ordering",
      "question": "Rank these migration file naming conventions by what they encode, from LEAST to MOST information:",
      "items": [
        "001_add_users.sql (sequential number)",
        "20240115_add_users.sql (date)",
        "20240115143022_add_users.sql (timestamp)",
        "V1.2.3__add_users.sql (semantic version)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Sequential: just ordering. Date: when it was created. Timestamp: precise creation time (avoids collisions when multiple developers create migrations on the same day). Semantic version: ties to release version.",
      "detailedExplanation": "The key clue in this question is \"rank these migration file naming conventions by what they encode, from LEAST to MOST\". Build the rank from biggest differences first, then refine with adjacent checks. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-019",
      "type": "multiple-choice",
      "question": "Two developers on different branches both create migration V005. When both branches merge to main, what happens?",
      "options": [
        "One migration overwrites the other",
        "A migration version conflict — both claim V005, and the tool may refuse to apply them or apply them in unpredictable order",
        "They automatically merge",
        "The database handles it"
      ],
      "correct": 1,
      "explanation": "Version collision: two different V005 scripts. Sequential numbering is prone to this. Timestamp-based naming (20240115143022) virtually eliminates collisions. Some tools (Flyway) detect and reject duplicate versions.",
      "detailedExplanation": "Start from \"two developers on different branches both create migration V005\", then pressure-test the result against the options. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. If values like 20240115143022 appear, convert them into one unit basis before comparison. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-020",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration needs to update 50M rows: UPDATE orders SET status_code = status WHERE status_code IS NULL. Running this in one transaction takes 45 minutes and locks the table. How do you make this safe for production?",
          "options": [
            "Run it as-is during maintenance window",
            "Batch the update: process 10,000 rows at a time with small sleeps between batches",
            "Skip the backfill — leave NULLs",
            "Use a stored procedure"
          ],
          "correct": 1,
          "explanation": "Batch the update: UPDATE orders SET status_code = status WHERE status_code IS NULL AND id BETWEEN ? AND ? LIMIT 10000. Small transactions, minimal lock duration, and pauses between batches to avoid overwhelming the database.",
          "detailedExplanation": "Start from \"migration needs to update 50M rows: UPDATE orders SET status_code = status WHERE\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Modeling quality is measured by query fit, cardinality behavior, and operational cost. If values like 50M and 45 minutes appear, convert them into one unit basis before comparison. Common pitfall: schema optimized for entities instead of queries."
        },
        {
          "question": "Why is batching safer than a single large UPDATE?",
          "options": [
            "Batches are faster overall",
            "Each batch is a small, short transaction — minimal lock time, bounded WAL/undo log usage, and can be paused or stopped without losing all progress",
            "Batches use less disk space",
            "Batches don't use transactions"
          ],
          "correct": 1,
          "explanation": "Small transactions: short locks (milliseconds not minutes). Bounded log usage: won't fill up WAL or transaction log. Stoppable: if issues arise, stop after the current batch — already-updated rows keep their values.",
          "detailedExplanation": "The decision turns on \"batching safer than a single large UPDATE\". Do not reset assumptions between stages; carry forward prior constraints directly. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out."
        }
      ],
      "detailedExplanation": "This prompt is really about \"schema Evolution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-021",
      "type": "numeric-input",
      "question": "A backfill migration updates 10M rows in batches of 5,000, with a 100ms sleep between batches. Approximately how long will the migration take (ignoring query execution time)?",
      "answer": 200,
      "unit": "seconds",
      "tolerance": 0.1,
      "explanation": "10,000,000 / 5,000 = 2,000 batches. 2,000 × 100ms = 200,000ms = 200 seconds (~3.3 minutes). Plus actual query execution time per batch. Much better than a single 45-minute locked transaction.",
      "detailedExplanation": "Use \"backfill migration updates 10M rows in batches of 5,000, with a 100ms sleep between\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Choose data shape based on workload paths, not on normalization dogma alone. Numbers such as 10M and 5,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-022",
      "type": "multiple-choice",
      "question": "What is a 'ghost table' migration pattern (used by tools like gh-ost)?",
      "options": [
        "A table that doesn't show up in queries",
        "Create a new table with the desired schema, copy data in the background, then atomically swap the new table into the old table's name",
        "A temporary table for staging data",
        "A table marked for deletion"
      ],
      "correct": 1,
      "explanation": "gh-ost (GitHub Online Schema Tool): creates a 'ghost' table with the new schema, streams changes via binlog, copies data in the background, and atomically renames tables. Zero-downtime schema changes for MySQL without metadata locks.",
      "detailedExplanation": "The core signal here is \"a 'ghost table' migration pattern (used by tools like gh-ost)\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-023",
      "type": "multi-select",
      "question": "Which tools help perform online schema changes in MySQL?",
      "options": [
        "gh-ost (GitHub's Online Schema Translator)",
        "pt-online-schema-change (Percona Toolkit)",
        "MySQL's built-in Online DDL (ALGORITHM=INPLACE)",
        "pg_repack (PostgreSQL tool)"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "gh-ost, pt-online-schema-change, and MySQL Online DDL all perform schema changes without blocking writes. pg_repack is a PostgreSQL tool, not MySQL.",
      "detailedExplanation": "If you keep \"tools help perform online schema changes in MySQL\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-024",
      "type": "ordering",
      "question": "Rank these schema changes from SAFEST (least risk of downtime) to MOST DANGEROUS:",
      "items": [
        "Add a nullable column (no default)",
        "Add a new index (CONCURRENTLY)",
        "Change a column's data type (table rewrite)",
        "Drop a table"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Add nullable column: metadata-only, instant. Add index concurrently: no locks but takes time. Change type: table rewrite, potential long lock. Drop table: permanent data loss, no recovery without backup.",
      "detailedExplanation": "Start from \"rank these schema changes from SAFEST (least risk of downtime) to MOST DANGEROUS:\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-025",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need to change a column from INT to BIGINT on a 200M row table because values are approaching INT's max (2.1 billion). In PostgreSQL, ALTER COLUMN TYPE requires a table rewrite. How do you do this with zero downtime?",
          "options": [
            "ALTER TABLE orders ALTER COLUMN id TYPE BIGINT (simple but locks table)",
            "Expand-and-contract: add a new BIGINT column, dual-write, backfill, swap reads, drop old column",
            "Use a sequence that wraps around",
            "Switch to UUIDs"
          ],
          "correct": 1,
          "explanation": "Expand-and-contract avoids the table rewrite. Add id_new BIGINT, dual-write to both, backfill old rows, swap application reads to id_new, then drop old column. Each step is non-blocking.",
          "detailedExplanation": "If you keep \"you need to change a column from INT to BIGINT on a 200M row table because values are\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. If values like 200M and 2.1 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "What's the trickiest part of this migration?",
          "options": [
            "Adding the new column",
            "Keeping id_new in sync during the transition — new inserts must write both columns, and the backfill must handle concurrent modifications without conflicts",
            "Dropping the old column",
            "Updating indexes"
          ],
          "correct": 1,
          "explanation": "The dual-write period is complex: inserts write both columns, updates must propagate, and the backfill must not overwrite concurrently-written values. Usually solved with triggers or application-level double-writes.",
          "detailedExplanation": "This prompt is really about \"what's the trickiest part of this migration\". Do not reset assumptions between stages; carry forward prior constraints directly. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"schema Evolution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-026",
      "type": "multiple-choice",
      "question": "What is 'schema drift'?",
      "options": [
        "Schema changes that happen gradually",
        "When the actual database schema diverges from what the migration tool or application code expects — usually from manual changes or failed migrations",
        "A database optimization technique",
        "Schema changes that drift across replicas"
      ],
      "correct": 1,
      "explanation": "Schema drift occurs when someone applies manual DDL changes (hotfixes, console patches) outside the migration tool. The tool's state says the schema is at V10, but extra columns or indexes exist. This causes deployment failures and confusion.",
      "detailedExplanation": "Read this as a scenario about \"'schema drift'\". Reject options that conflict with the primary access pattern or index strategy. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-027",
      "type": "multi-select",
      "question": "How do you prevent and detect schema drift?",
      "options": [
        "All schema changes go through migration scripts — no manual DDL in production",
        "Automated schema comparison between expected and actual (CI/CD check)",
        "Restrict direct DDL access in production",
        "Only allow one developer to make schema changes"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Prevention: all changes via migrations (no manual DDL). Detection: automated comparison tools. Access control: restrict production DDL to the migration tool. Restricting to one developer doesn't help if they still make manual changes.",
      "detailedExplanation": "The decision turns on \"you prevent and detect schema drift\". Treat every option as a separate true/false test under the same constraints. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-028",
      "type": "multiple-choice",
      "question": "What is a 'feature flag' approach to schema evolution?",
      "options": [
        "A flag column in the schema table",
        "Using feature flags in application code to gradually switch between old and new schema usage — allowing safe rollback without a schema change",
        "Marking columns as deprecated",
        "A database flag that enables new features"
      ],
      "correct": 1,
      "explanation": "Feature flags let you deploy code that reads from the new schema structure, but flag it off initially. Gradually enable it for a percentage of users. If issues arise, flip the flag — no schema rollback needed. The schema stays unchanged; code behavior changes.",
      "detailedExplanation": "This prompt is really about \"a 'feature flag' approach to schema evolution\". Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-029",
      "type": "two-stage",
      "stages": [
        {
          "question": "A legacy system stores addresses as a single 'address' TEXT column. The new design needs structured fields: street, city, state, zip. How do you migrate without downtime?",
          "options": [
            "Replace the address column with 4 new columns in one step",
            "Add the 4 new columns alongside the old one, parse and populate them, then transition reads to the new columns",
            "Create a new Addresses table and reference it",
            "Keep the TEXT column and parse it at read time"
          ],
          "correct": 1,
          "explanation": "Add new columns alongside the old one (expand). Parse the existing TEXT field to populate them (backfill). Update the app to read/write the new structured columns. Eventually drop the old TEXT column (contract).",
          "detailedExplanation": "The decision turns on \"legacy system stores addresses as a single 'address' TEXT column\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "During the transition, where should the application read address data from?",
          "options": [
            "Always the old TEXT column",
            "New structured columns if populated, falling back to the old TEXT column — a dual-read strategy",
            "Always the new columns (even if empty)",
            "A separate cache"
          ],
          "correct": 1,
          "explanation": "Dual-read: prefer new columns, fall back to old column. This handles the transition period where some rows are backfilled and others aren't. Once backfill is complete, remove the fallback and drop the old column.",
          "detailedExplanation": "Start from \"during the transition, where should the application read address data from\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes."
        }
      ],
      "detailedExplanation": "Use \"schema Evolution\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-030",
      "type": "multiple-choice",
      "question": "What is the 'dark launch' pattern in schema evolution?",
      "options": [
        "Deploying at night",
        "Writing to the new schema in production alongside the old one, but only reading from the old — validating the new schema with production traffic before switching reads",
        "Hiding schema changes from the team",
        "Using dark mode in the database GUI"
      ],
      "correct": 1,
      "explanation": "Dark launch: write to both old and new schema paths. Reads still use the old path. Compare results to validate the new schema handles real data correctly. Once confident, switch reads to the new path. Low risk — the old path is always available.",
      "detailedExplanation": "If you keep \"the 'dark launch' pattern in schema evolution\" in view, the correct answer separates faster. Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-031",
      "type": "ordering",
      "question": "Rank these schema evolution strategies from MOST CONSERVATIVE (lowest risk) to MOST AGGRESSIVE:",
      "items": [
        "Dark launch (dual write, validate, then switch)",
        "Expand and contract (add new, migrate, remove old)",
        "Direct ALTER TABLE in production",
        "Blue-green database deployment (full schema swap)"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Dark launch: validate before switching, lowest risk. Expand-and-contract: gradual, backward compatible. Blue-green DB: full swap with fallback. Direct ALTER: immediate, no safety net, highest risk.",
      "detailedExplanation": "The core signal here is \"rank these schema evolution strategies from MOST CONSERVATIVE (lowest risk) to MOST\". Place obvious extremes first, then sort the middle by pairwise comparison. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-032",
      "type": "multi-select",
      "question": "Which schema changes require data backfill?",
      "options": [
        "Adding a new column that should have a value for existing rows",
        "Splitting one column into two (e.g., full_name → first_name + last_name)",
        "Adding a new empty table",
        "Changing a column from nullable to NOT NULL"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "New column needing values: backfill from existing data or a default. Split column: parse and populate new columns. NOT NULL constraint: all existing NULLs must be filled first. Empty table: nothing to backfill.",
      "detailedExplanation": "Use \"schema changes require data backfill\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-033",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration splits the Users table: User(id, name, email, bio, avatar_url, preferences_json) into User(id, name, email) and UserProfile(user_id, bio, avatar_url, preferences_json). What must happen before old code stops being deployed?",
          "options": [
            "Nothing — the split is transparent",
            "A transition period where code reads from both tables: User for core fields, UserProfile for extended fields — with fallback to the old User table",
            "All data must be migrated instantly",
            "The old User table must be dropped immediately"
          ],
          "correct": 1,
          "explanation": "During transition: new code reads from both tables (or a JOIN). Old code still reads from the original User table (which still has all columns). The old columns are only removed after all code is updated.",
          "detailedExplanation": "Start from \"migration splits the Users table: User(id, name, email, bio, avatar_url,\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes."
        },
        {
          "question": "What's the correct order? (A) Create UserProfile table. (B) Backfill UserProfile from User. (C) Deploy code reading from UserProfile. (D) Drop bio/avatar/preferences from User.",
          "options": ["A, B, C, D", "C, A, B, D", "A, C, B, D", "D, A, B, C"],
          "correct": 0,
          "explanation": "Create table → backfill data → deploy code that reads from new table → drop old columns. Each step depends on the previous one. Dropping columns before migrating reads would break the application.",
          "detailedExplanation": "The decision turns on \"what's the correct order? (A) Create UserProfile table\". Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes."
        }
      ],
      "detailedExplanation": "This prompt is really about \"schema Evolution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-034",
      "type": "multiple-choice",
      "question": "What is 'idempotent migration'?",
      "options": [
        "A migration that runs exactly once",
        "A migration that can be safely run multiple times with the same result — running it twice doesn't cause errors or duplicate changes",
        "A migration that identifies itself",
        "A migration that can't be rolled back"
      ],
      "correct": 1,
      "explanation": "Idempotent migrations use guards: CREATE TABLE IF NOT EXISTS, ADD COLUMN IF NOT EXISTS, etc. If accidentally re-run (e.g., migration state is lost), they complete successfully without errors or double-applying changes.",
      "detailedExplanation": "The decision turns on \"'idempotent migration'\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-035",
      "type": "multi-select",
      "question": "Which SQL constructs make migrations idempotent?",
      "options": [
        "CREATE TABLE IF NOT EXISTS",
        "ALTER TABLE ADD COLUMN IF NOT EXISTS (PostgreSQL 9.6+)",
        "DROP TABLE IF EXISTS",
        "INSERT ... ON CONFLICT DO NOTHING"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All use conditional guards: IF NOT EXISTS / IF EXISTS prevents errors on re-run. ON CONFLICT DO NOTHING prevents duplicate inserts. These make migrations safe to re-run without checking state.",
      "detailedExplanation": "Read this as a scenario about \"sQL constructs make migrations idempotent\". Validate each option independently; do not select statements that are only partially true. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-036",
      "type": "multiple-choice",
      "question": "What is 'forward-only migration'?",
      "options": [
        "Migrations that only run on the latest database version",
        "A philosophy where rollback scripts aren't written — instead, fix-forward by deploying a new migration that corrects the issue",
        "Migrations that can only be run in production",
        "Migrations applied in chronological order"
      ],
      "correct": 1,
      "explanation": "Forward-only: don't write DOWN migrations. If something goes wrong, write a new UP migration that fixes the issue. Rationale: rollbacks are rarely tested, often fail, and some changes (data deletion) can't be rolled back anyway.",
      "detailedExplanation": "The key clue in this question is \"'forward-only migration'\". Discard modeling choices that look clean but perform poorly for the target queries. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team deploys a migration that adds a column with a CHECK constraint. The constraint rejects 5% of incoming writes (valid data that doesn't match the constraint). What's the fix-forward approach?",
          "options": [
            "Roll back the migration",
            "Deploy a new migration that relaxes or removes the CHECK constraint",
            "Ask users to send valid data",
            "Restart the database"
          ],
          "correct": 1,
          "explanation": "Fix forward: deploy a new migration that corrects the constraint (ALTER TABLE ... DROP CONSTRAINT, or ALTER CONSTRAINT). No rollback needed — a new forward migration resolves the issue. This is why forward-only is pragmatic.",
          "detailedExplanation": "This prompt is really about \"team deploys a migration that adds a column with a CHECK constraint\". Solve this as chained reasoning where stage two must respect stage one assumptions. Choose data shape based on workload paths, not on normalization dogma alone. If values like 5 appear, convert them into one unit basis before comparison. Common pitfall: indexing that over-amplifies writes."
        },
        {
          "question": "Why is fix-forward often safer than rollback?",
          "options": [
            "Fix-forward is faster",
            "Rollback scripts may be untested, may not handle data written since the migration, and some changes (backfills, data transforms) can't be undone without data loss",
            "Databases don't support rollbacks",
            "There's no difference"
          ],
          "correct": 1,
          "explanation": "Rollback scripts assume the database state hasn't changed since the migration. But new data has been written, constraints may have been enforced, and destructive changes (drops, deletes) can't be undone. Fix-forward operates on the current state.",
          "detailedExplanation": "If you keep \"fix-forward often safer than rollback\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries."
        }
      ],
      "detailedExplanation": "Start from \"schema Evolution\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-038",
      "type": "ordering",
      "question": "Rank these approaches to handling a broken production migration from FASTEST to MOST THOROUGH:",
      "items": [
        "Revert the application code (if migration was backward compatible)",
        "Deploy a fix-forward migration",
        "Roll back the migration (if a rollback script exists and is tested)",
        "Restore from a pre-migration database backup"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Code revert: fastest if schema is backward compatible — deploy old code, schema stays. Fix-forward: write and deploy a new migration. Rollback script: apply the reverse migration. Backup restore: slowest, nuclear option — hours of downtime and data loss.",
      "detailedExplanation": "If you keep \"rank these approaches to handling a broken production migration from FASTEST to MOST\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-039",
      "type": "multiple-choice",
      "question": "What is 'schema-on-read' vs 'schema-on-write'?",
      "options": [
        "Reading the schema vs writing the schema",
        "Schema-on-write enforces structure at write time (relational); schema-on-read interprets structure at read time (schemaless/document) — data is stored as-is and parsed when queried",
        "Reading during migrations vs writing during migrations",
        "Schemas for read replicas vs primary"
      ],
      "correct": 1,
      "explanation": "Schema-on-write (SQL): data must conform to the schema when inserted. Schema-on-read (MongoDB, data lakes): data is stored without enforcement; the application interprets structure when reading. Schema evolution is easier with schema-on-read but harder to enforce consistency.",
      "detailedExplanation": "The core signal here is \"'schema-on-read' vs 'schema-on-write'\". Prefer the option that preserves correctness guarantees for the stated consistency boundary. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-040",
      "type": "multi-select",
      "question": "What are advantages of schema-on-read for schema evolution?",
      "options": [
        "No migration scripts needed — just store data in the new format",
        "Old and new data formats coexist naturally",
        "Application code must handle multiple data formats",
        "Schema changes are instant (no ALTER TABLE)"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Schema-on-read: no ALTER TABLE, old/new formats coexist, instant 'migration.' But application code must handle all historical formats — the complexity shifts from the database to the application. Option C is a disadvantage, not an advantage.",
      "detailedExplanation": "Read this as a scenario about \"advantages of schema-on-read for schema evolution\". Treat every option as a separate true/false test under the same constraints. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-041",
      "type": "multiple-choice",
      "question": "In a document database (MongoDB), how do you 'migrate' from schema V1 (name as a string) to V2 (name as {first, last})?",
      "options": [
        "ALTER TABLE — wait, there's no ALTER TABLE",
        "Write new documents in V2 format; update the application to handle both formats at read time; optionally backfill old documents lazily or via batch",
        "Convert all documents in one transaction",
        "Create a new collection"
      ],
      "correct": 1,
      "explanation": "In document DBs, schema evolution is lazy: new writes use the new format. Old documents keep the old format. The app handles both. Optionally backfill old documents, but it's not required — dual-format reads handle the transition.",
      "detailedExplanation": "The decision turns on \"in a document database (MongoDB), how do you 'migrate' from schema V1 (name as a\". Discard modeling choices that look clean but perform poorly for the target queries. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "MongoDB's 'lazy migration' pattern: old documents have {name: 'John Doe'}, new documents have {name: {first: 'John', last: 'Doe'}}. What does the application read logic look like?",
          "options": [
            "Always expect the new format",
            "Check the type: if string, parse it; if object, use it directly — handle both formats gracefully",
            "Always expect the old format",
            "Reject old-format documents"
          ],
          "correct": 1,
          "explanation": "The app checks the type of the name field. String → split to get first/last. Object → use directly. This is the cost of schema-on-read: the application must handle format heterogeneity.",
          "detailedExplanation": "This prompt is really about \"mongoDB's 'lazy migration' pattern: old documents have {name: 'John Doe'}, new\". Do not reset assumptions between stages; carry forward prior constraints directly. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries."
        },
        {
          "question": "Optionally, when reading an old-format document, the app can update it to the new format (lazy backfill). What's the benefit?",
          "options": [
            "Faster reads forever after",
            "Over time, old-format documents are gradually converted — the migration completes organically without a dedicated batch job",
            "It's required for MongoDB to work",
            "No benefit — it just wastes writes"
          ],
          "correct": 1,
          "explanation": "Lazy migration: each read of an old document converts it. High-traffic documents migrate quickly (read often = converted soon). Low-traffic documents migrate eventually. No batch job needed — migration happens naturally with usage.",
          "detailedExplanation": "If you keep \"optionally, when reading an old-format document, the app can update it to the new\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out."
        }
      ],
      "detailedExplanation": "Start from \"schema Evolution\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-043",
      "type": "multiple-choice",
      "question": "What is 'event sourcing' schema evolution?",
      "options": [
        "Migrating events from one queue to another",
        "Events are immutable — you don't change old events. Instead, you version event schemas and teach the application to handle all historical event versions",
        "Sourcing schema changes from events",
        "Using events to trigger migrations"
      ],
      "correct": 1,
      "explanation": "In event sourcing, events are the source of truth and are immutable. V1 events stay as V1 forever. New events use V2 schema. The application must 'upcast' old event formats when replaying. Schema evolution means evolving event format versions.",
      "detailedExplanation": "The key clue in this question is \"'event sourcing' schema evolution\". Eliminate options that ignore delivery semantics or backpressure behavior. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-044",
      "type": "ordering",
      "question": "Rank these data serialization formats from WEAKEST to STRONGEST schema evolution support:",
      "items": [
        "Raw JSON (no schema enforcement)",
        "JSON Schema (validated but manual evolution)",
        "Protocol Buffers (compiled schemas with field numbering)",
        "Apache Avro (schema registry, automatic compatibility checks)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Raw JSON: no schema at all. JSON Schema: validation but manual compatibility management. Protobuf: field numbers enable adding/removing fields safely. Avro + Schema Registry: automated backward/forward compatibility verification.",
      "detailedExplanation": "The core signal here is \"rank these data serialization formats from WEAKEST to STRONGEST schema evolution\". Build the rank from biggest differences first, then refine with adjacent checks. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-045",
      "type": "multi-select",
      "question": "In Protocol Buffers, which schema changes are backward compatible?",
      "options": [
        "Adding a new optional field with a new field number",
        "Removing a field (old messages still have it, new code ignores unknown fields)",
        "Changing a field's type (e.g., int32 to string)",
        "Renaming a field (field numbers stay the same)"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Protobuf identifies fields by number, not name. Adding fields: old code ignores new numbers. Removing: new code ignores old numbers. Renaming: field numbers unchanged, no effect on serialization. Changing type breaks binary compatibility.",
      "detailedExplanation": "If you keep \"in Protocol Buffers, which schema changes are backward compatible\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-046",
      "type": "multiple-choice",
      "question": "What is a 'schema registry' (e.g., Confluent Schema Registry)?",
      "options": [
        "A database of database schemas",
        "A service that stores versioned schemas for serialized data (Avro, Protobuf) and enforces compatibility rules — producers can't publish incompatible schema changes",
        "A registry of database users",
        "A DNS-like service for databases"
      ],
      "correct": 1,
      "explanation": "Schema Registry stores versioned schemas and enforces compatibility (backward, forward, full). When a producer registers a new schema version, the registry checks compatibility against previous versions and rejects breaking changes.",
      "detailedExplanation": "This prompt is really about \"a 'schema registry' (e\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "What does 'backward compatible' mean for a schema in a schema registry?",
          "options": [
            "New code can read old data",
            "Consumers using the OLD schema can still read data produced with the NEW schema — new data is readable by old consumers",
            "The schema works with older database versions",
            "Old migrations still run"
          ],
          "correct": 0,
          "explanation": "Backward compatible: NEW code (consumers) can read OLD data (messages). This means adding optional fields and removing required fields is OK. Consumers with the latest schema can process all historical messages.",
          "detailedExplanation": "The decision turns on \"'backward compatible' mean for a schema in a schema registry\". Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries."
        },
        {
          "question": "What does 'forward compatible' mean?",
          "options": [
            "New data can be read by new code",
            "OLD code (consumers) can read NEW data (messages) — old consumers can handle messages with the new schema",
            "The schema works with future database versions",
            "Migrations run in forward order"
          ],
          "correct": 1,
          "explanation": "Forward compatible: OLD consumers can read NEW messages (they ignore unknown fields). This means you can deploy the producer (new schema) before updating consumers. Add optional fields, don't remove fields old code uses.",
          "detailedExplanation": "Start from \"'forward compatible' mean\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "Use \"schema Evolution\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-048",
      "type": "ordering",
      "question": "Rank these compatibility levels from LEAST RESTRICTIVE to MOST RESTRICTIVE:",
      "items": [
        "None (any change allowed)",
        "Backward compatible (new code reads old data)",
        "Forward compatible (old code reads new data)",
        "Full compatible (both backward and forward)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "None: anything goes. Backward: only consumers must handle old data. Forward: only producers can add fields (consumers must ignore unknowns). Full: both constraints — only safe changes (adding optional fields) allowed.",
      "detailedExplanation": "Read this as a scenario about \"rank these compatibility levels from LEAST RESTRICTIVE to MOST RESTRICTIVE:\". Order by relative scale and bottleneck effect, then validate neighboring items. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-049",
      "type": "multiple-choice",
      "question": "A Kafka topic has messages in Avro format. The schema evolves from V1 to V5 over 2 years. Consumers joining now need to read all historical messages. What compatibility mode is needed?",
      "options": [
        "Forward compatible",
        "Backward compatible — new consumers (with V5 schema) must be able to read V1-V5 messages",
        "Full compatible",
        "None — just use the latest schema"
      ],
      "correct": 1,
      "explanation": "New consumers need to read ALL messages (V1 through V5). This requires backward compatibility — each new schema version must be able to decode data from all previous versions. The schema registry enforces this.",
      "detailedExplanation": "The decision turns on \"kafka topic has messages in Avro format\". Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Numbers such as 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-050",
      "type": "multi-select",
      "question": "Which are challenges of schema evolution in distributed systems?",
      "options": [
        "Different services may be on different schema versions simultaneously",
        "Cached data may be in old schema format",
        "Message queues contain messages with mixed schema versions",
        "All services must be updated simultaneously"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Distributed schema evolution means: services deploy independently (mixed versions). Caches hold old-format data. Queues have mixed-version messages. The whole point is to avoid simultaneous updates (option D) — that's the problem you're solving.",
      "detailedExplanation": "Use \"challenges of schema evolution in distributed systems\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-051",
      "type": "multiple-choice",
      "question": "What is 'API versioning' and how does it relate to schema evolution?",
      "options": [
        "Versioning the database API",
        "Maintaining multiple API versions (v1/users, v2/users) so clients using older versions continue to work while the underlying schema evolves",
        "Versioning the SQL syntax",
        "Using different APIs for different tables"
      ],
      "correct": 1,
      "explanation": "API versioning decouples external contracts from internal schema. The v1 API returns {name: 'John'}, v2 returns {first: 'John', last: 'Doe'}. The schema can evolve freely; each API version maps to the current schema through an adapter layer.",
      "detailedExplanation": "This prompt is really about \"'API versioning' and how does it relate to schema evolution\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "An API returns user.address as a string in v1. In v2, address is an object {street, city, state, zip}. The database has been migrated to structured fields. How does the v1 API still work?",
          "options": [
            "It doesn't — v1 clients must upgrade",
            "An adapter layer concatenates the structured fields back into a string for v1 responses: `${street}, ${city}, ${state} ${zip}`",
            "v1 reads from a separate old database",
            "Return the raw database row"
          ],
          "correct": 1,
          "explanation": "The v1 serializer/adapter maps the new schema to the old response format. The database schema evolves independently. Multiple API versions can coexist, each mapping to the current schema differently.",
          "detailedExplanation": "The key clue in this question is \"aPI returns user\". Solve this as chained reasoning where stage two must respect stage one assumptions. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes."
        },
        {
          "question": "When can the v1 API be retired?",
          "options": [
            "Immediately after v2 is released",
            "When monitoring shows no v1 clients remain, after a deprecation period with migration guides",
            "Never — v1 must be supported forever",
            "After 30 days automatically"
          ],
          "correct": 1,
          "explanation": "Deprecation lifecycle: announce v1 deprecation → provide migration guides → monitor v1 usage → set a sunset date → retire when usage drops to zero (or accept breaking the remaining stragglers).",
          "detailedExplanation": "Read this as a scenario about \"the v1 API be retired\". Do not reset assumptions between stages; carry forward prior constraints directly. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "If you keep \"schema Evolution\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-053",
      "type": "multiple-choice",
      "question": "What is the 'strangler fig' pattern for legacy database migration?",
      "options": [
        "Wrapping the old database in a new interface",
        "Gradually replacing a legacy database by building new functionality on the new system while routing legacy operations through an adapter, until the old system can be decommissioned",
        "Strangling the database connections",
        "A load balancing pattern"
      ],
      "correct": 1,
      "explanation": "Strangler fig: new features use the new database. Old features gradually migrate over. An adapter/proxy routes queries to old or new system based on the data. Over time, the new system handles everything and the old one is turned off.",
      "detailedExplanation": "The core signal here is \"the 'strangler fig' pattern for legacy database migration\". Prefer the schema/index decision that minimizes query and write amplification for this workload. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-054",
      "type": "ordering",
      "question": "Rank the phases of a strangler fig database migration in order:",
      "items": [
        "Build new system alongside old, with data sync",
        "Route new features to the new system",
        "Gradually migrate existing features to the new system",
        "Decommission the old system"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Build new + sync → new features on new system → migrate old features one by one → decommission old system. Each phase is incremental and reversible. No big-bang cutover.",
      "detailedExplanation": "The key clue in this question is \"rank the phases of a strangler fig database migration in order:\". Place obvious extremes first, then sort the middle by pairwise comparison. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-055",
      "type": "multi-select",
      "question": "Which are risks of 'big bang' database migrations (switch everything at once)?",
      "options": [
        "Long downtime during migration",
        "Data loss or corruption if the migration fails partway",
        "No rollback path if the new system has issues",
        "Impossible to test with real production traffic beforehand"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "Big bang risks: extended downtime (hours), partial failure = inconsistency, hard to rollback (old system may be out of sync), and can't test incrementally with production traffic. Incremental migration (strangler fig) avoids all of these.",
      "detailedExplanation": "Start from \"risks of 'big bang' database migrations (switch everything at once)\", then pressure-test the result against the options. Avoid pattern guessing and evaluate each candidate directly against the scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "A company is migrating from MySQL to PostgreSQL. The application uses 200 queries. What's the incremental approach?",
          "options": [
            "Rewrite all 200 queries and switch at once",
            "Set up dual-write to both databases, migrate one feature (e.g., 20 queries) at a time to PostgreSQL, validate, repeat until all features are migrated",
            "Run both databases forever",
            "Export all data and import into PostgreSQL"
          ],
          "correct": 1,
          "explanation": "Dual-write: both databases receive writes. Migrate features one at a time (20 queries → PostgreSQL). Validate each batch reads correctly from the new database. This takes longer but is dramatically safer.",
          "detailedExplanation": "Use \"company is migrating from MySQL to PostgreSQL\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Schema and index choices should follow access patterns and write/read amplification constraints. Keep quantities like 200 and 20 in aligned units before selecting an answer. Common pitfall: indexing that over-amplifies writes."
        },
        {
          "question": "During dual-write, which database is the source of truth?",
          "options": [
            "Both equally",
            "The old database (MySQL) remains the source of truth until all reads are migrated — if PostgreSQL data diverges, MySQL is correct",
            "The new database (PostgreSQL)",
            "Neither — use a separate system"
          ],
          "correct": 1,
          "explanation": "The old database is the source of truth throughout the migration. If the new database has issues, the old one is authoritative. Only after all reads are verified on the new database do you switch the source of truth.",
          "detailedExplanation": "The core signal here is \"during dual-write, which database is the source of truth\". Do not reset assumptions between stages; carry forward prior constraints directly. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries."
        }
      ],
      "detailedExplanation": "The decision turns on \"schema Evolution\". Do not reset assumptions between stages; carry forward prior constraints directly. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-057",
      "type": "multiple-choice",
      "question": "What is 'schema-as-code' (also called 'declarative schema management')?",
      "options": [
        "Writing schemas in a programming language",
        "Defining the desired schema state in code/config, and a tool computes and applies the diff to reach that state — instead of writing sequential migration scripts",
        "Embedding SQL in application code",
        "Generating code from the database schema"
      ],
      "correct": 1,
      "explanation": "Declarative schema: define what the schema should look like. The tool (e.g., Atlas, Skeema, Terraform for DBs) computes the migration needed to get from current state to desired state. Contrast with imperative migrations (sequential scripts).",
      "detailedExplanation": "Read this as a scenario about \"'schema-as-code' (also called 'declarative schema management')\". Reject options that conflict with the primary access pattern or index strategy. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-058",
      "type": "multi-select",
      "question": "What are advantages of declarative schema management over sequential migration scripts?",
      "options": [
        "The current schema is readable in one file (not spread across 200 migration files)",
        "No need to track migration history",
        "Automatic diff generation between current and desired state",
        "Always safe for production — no human review needed"
      ],
      "correctIndices": [0, 2],
      "explanation": "Declarative schemas are readable (one file = current schema) and auto-generate diffs. You still need migration tracking (to know what's applied). Human review is still essential — auto-generated diffs can produce dangerous operations.",
      "detailedExplanation": "Use \"advantages of declarative schema management over sequential migration scripts\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-059",
      "type": "numeric-input",
      "question": "A project has been running for 5 years with sequential migrations. At 2 migrations per week on average, approximately how many migration files exist?",
      "answer": 520,
      "unit": "files",
      "tolerance": 0.1,
      "explanation": "5 years × 52 weeks × 2 migrations/week = 520 migration files. Understanding the current schema requires mentally replaying all 520. This is a practical argument for declarative schema management.",
      "detailedExplanation": "This prompt is really about \"project has been running for 5 years with sequential migrations\". Keep every transformation in one unit system and check order of magnitude at the end. Choose data shape based on workload paths, not on normalization dogma alone. Keep quantities like 5 and 2 in aligned units before selecting an answer. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-060",
      "type": "multiple-choice",
      "question": "What is 'data contract' in the context of schema evolution?",
      "options": [
        "A legal agreement about data usage",
        "A formal specification of a data schema that producers and consumers agree on — changes to the contract follow versioning and compatibility rules",
        "A database EULA",
        "A data retention policy"
      ],
      "correct": 1,
      "explanation": "Data contracts define: what fields exist, their types, which are required, acceptable values. Producers must conform when writing. Consumers can depend on the contract when reading. Schema evolution follows the contract's compatibility rules.",
      "detailedExplanation": "Start from \"'data contract' in the context of schema evolution\", then pressure-test the result against the options. Prefer the choice that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-061",
      "type": "two-stage",
      "stages": [
        {
          "question": "An analytics pipeline consumes data from 10 producer services. Producer A changes their schema (removes a field). The pipeline breaks. What should have prevented this?",
          "options": [
            "Better monitoring",
            "A data contract with backward compatibility enforcement — Producer A's change would be rejected by the schema registry for removing a required field",
            "More comprehensive testing",
            "The pipeline should handle missing fields"
          ],
          "correct": 1,
          "explanation": "A schema registry with backward compatibility enforcement would have rejected Producer A's breaking change before it reached production. The contract ensures producers can't make incompatible changes unilaterally.",
          "detailedExplanation": "If you keep \"analytics pipeline consumes data from 10 producer services\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 10 should be normalized first so downstream reasoning stays consistent. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "What process should Producer A follow to remove the field?",
          "options": [
            "Just remove it and fix consumers later",
            "Announce deprecation → consumers stop depending on the field → mark field optional → remove after confirmation that no consumers need it",
            "Ask the DBA to remove it",
            "Add a new field instead"
          ],
          "correct": 1,
          "explanation": "Deprecation lifecycle: announce → consumers migrate away → mark optional (schema change) → verify no consumers read it → remove. This is the safe, contract-respecting process for removing fields.",
          "detailedExplanation": "This prompt is really about \"process should Producer A follow to remove the field\". Do not reset assumptions between stages; carry forward prior constraints directly. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"schema Evolution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-062",
      "type": "ordering",
      "question": "Rank these schema evolution complexity levels from SIMPLEST to MOST COMPLEX:",
      "items": [
        "Single application, single database",
        "Multiple microservices, shared database",
        "Multiple microservices, each with their own database",
        "Event-driven microservices with async messaging and schema registry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Single app/DB: change both together. Shared DB: coordinate across services. Own DBs: each service evolves independently, but cross-service data sync is needed. Event-driven + schema registry: full distributed schema evolution.",
      "detailedExplanation": "Read this as a scenario about \"rank these schema evolution complexity levels from SIMPLEST to MOST COMPLEX:\". Order by relative scale and bottleneck effect, then validate neighboring items. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-063",
      "type": "multiple-choice",
      "question": "What does 'database seeding' mean in the context of schema evolution?",
      "options": [
        "Planting database servers in data centers",
        "Populating a database with initial data (reference data, default configurations, test data) as part of the migration/setup process",
        "Generating random data for testing",
        "Creating database backups"
      ],
      "correct": 1,
      "explanation": "Seeding: inserting initial data that the application needs to function — roles (admin, user), country codes, default settings. Often done in migration scripts alongside schema changes. Different from backfill (transforming existing data).",
      "detailedExplanation": "The decision turns on \"'database seeding' mean in the context of schema evolution\". Discard modeling choices that look clean but perform poorly for the target queries. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-064",
      "type": "multi-select",
      "question": "Which are risks of mixing data seeding with schema migrations?",
      "options": [
        "Seeds may fail if the data already exists (non-idempotent)",
        "Seeds tie environment-specific data to schema versions",
        "Rolling back the schema doesn't roll back the seeded data",
        "Seeds in migrations are harder to customize per environment (dev vs prod)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are real risks. Seeds should be idempotent (INSERT ... ON CONFLICT DO NOTHING). Environment-specific data shouldn't be in migrations. Rollbacks don't undo INSERTs (unless wrapped in a transaction). Keep seeds separate from schema DDL when possible.",
      "detailedExplanation": "This prompt is really about \"risks of mixing data seeding with schema migrations\". Treat every option as a separate true/false test under the same constraints. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-065",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration adds an 'enum' column: status VARCHAR CHECK (status IN ('active', 'inactive')). Six months later, a new status 'suspended' is needed. What's the migration?",
          "options": [
            "ALTER TABLE users DROP CONSTRAINT ... ADD CONSTRAINT ... CHECK (status IN ('active', 'inactive', 'suspended'))",
            "Delete the column and recreate it",
            "Add a new column for the new status",
            "Use an integer instead of varchar"
          ],
          "correct": 0,
          "explanation": "Drop the old CHECK constraint, add a new one with the additional value. In PostgreSQL: ALTER TABLE users DROP CONSTRAINT users_status_check, ADD CONSTRAINT users_status_check CHECK (status IN ('active', 'inactive', 'suspended')).",
          "detailedExplanation": "The decision turns on \"migration adds an 'enum' column: status VARCHAR CHECK (status IN ('active', 'inactive'))\". Do not reset assumptions between stages; carry forward prior constraints directly. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: indexing that over-amplifies writes."
        },
        {
          "question": "Why do some teams prefer a separate status lookup table over CHECK constraints for enums?",
          "options": [
            "Lookup tables are faster",
            "Adding a new status is just an INSERT into the lookup table — no schema migration needed",
            "CHECK constraints don't work in MySQL",
            "Lookup tables use less storage"
          ],
          "correct": 1,
          "explanation": "A Statuses lookup table: adding 'suspended' is INSERT INTO statuses (name) VALUES ('suspended'). No DDL change, no migration, no deployment. CHECK constraints require schema changes for each new value. The tradeoff: joins vs simpler schema.",
          "detailedExplanation": "Start from \"some teams prefer a separate status lookup table over CHECK constraints for enums\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries."
        }
      ],
      "detailedExplanation": "Use \"schema Evolution\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-066",
      "type": "multiple-choice",
      "question": "What is a 'transactional migration'?",
      "options": [
        "A migration related to financial transactions",
        "A migration wrapped in a database transaction — if any statement fails, the entire migration is rolled back to a clean state",
        "A migration that processes transactions",
        "A paid migration service"
      ],
      "correct": 1,
      "explanation": "Transactional migrations: BEGIN → DDL statements → COMMIT (or ROLLBACK on failure). PostgreSQL supports transactional DDL. MySQL does NOT — each DDL statement auto-commits. If a MySQL migration fails partway, you have a partially applied state.",
      "detailedExplanation": "The core signal here is \"a 'transactional migration'\". Reject options that conflict with the primary access pattern or index strategy. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-067",
      "type": "multi-select",
      "question": "Which databases support transactional DDL (wrapping CREATE/ALTER/DROP in a transaction)?",
      "options": ["PostgreSQL", "MySQL/InnoDB", "SQLite", "SQL Server"],
      "correctIndices": [0, 2, 3],
      "explanation": "PostgreSQL, SQLite, and SQL Server support transactional DDL — DDL statements can be rolled back. MySQL auto-commits each DDL statement (implicit COMMIT), making transactional DDL impossible.",
      "detailedExplanation": "If you keep \"databases support transactional DDL (wrapping CREATE/ALTER/DROP in a transaction)\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-068",
      "type": "two-stage",
      "stages": [
        {
          "question": "A MySQL migration has 3 DDL statements. Statement 2 fails. What is the database state?",
          "options": [
            "All 3 changes are rolled back",
            "Statement 1 is applied (committed), statement 2 failed, statement 3 didn't run — the schema is in a partially migrated state",
            "All 3 are applied despite the error",
            "The database is corrupted"
          ],
          "correct": 1,
          "explanation": "MySQL auto-commits each DDL. Statement 1 is permanently applied. Statement 2 failed. Statement 3 never ran. You're stuck in an intermediate state. This is why MySQL schema changes need careful ordering and idempotent scripts.",
          "detailedExplanation": "This prompt is really about \"mySQL migration has 3 DDL statements\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. If values like 3 and 2 appear, convert them into one unit basis before comparison. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "How do you recover from a partially-applied MySQL migration?",
          "options": [
            "Restore from backup",
            "Manually fix the schema to either complete the migration or revert statement 1, then update the migration version tracking",
            "Restart MySQL",
            "Re-run the migration (it will fail on statement 1 again)"
          ],
          "correct": 1,
          "explanation": "Manually assess: complete the remaining statements (if safe) or revert the applied ones. Update the migration tracking table to reflect the actual state. This is tedious — idempotent migrations prevent the 're-run failure' problem.",
          "detailedExplanation": "If you keep \"you recover from a partially-applied MySQL migration\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation."
        }
      ],
      "detailedExplanation": "Start from \"schema Evolution\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-069",
      "type": "multiple-choice",
      "question": "What is the 'advisory lock' pattern in migration tools?",
      "options": [
        "A lock that advises users not to modify the schema",
        "A database-level lock that prevents multiple instances from running migrations simultaneously — ensuring only one migration runner is active",
        "A lock on advisory tables",
        "An optional lock that can be overridden"
      ],
      "correct": 1,
      "explanation": "Migration tools (Flyway, Alembic) acquire an advisory lock before running migrations. If two app instances start simultaneously, only one gets the lock and runs migrations. The other waits. Prevents duplicate or conflicting migrations.",
      "detailedExplanation": "The key clue in this question is \"the 'advisory lock' pattern in migration tools\". Discard modeling choices that look clean but perform poorly for the target queries. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-070",
      "type": "numeric-input",
      "question": "A deployment has 5 application replicas, each running migration checks at startup. Without an advisory lock, how many could attempt to run the same migration simultaneously?",
      "answer": 5,
      "unit": "replicas",
      "tolerance": "exact",
      "explanation": "All 5 replicas check for pending migrations, all 5 see the same pending migration, all 5 try to run it. With an advisory lock, only 1 runs it; the others wait and find no pending migrations.",
      "detailedExplanation": "The decision turns on \"deployment has 5 application replicas, each running migration checks at startup\". Keep every transformation in one unit system and check order of magnitude at the end. Schema and index choices should follow access patterns and write/read amplification constraints. Keep quantities like 5 and 1 in aligned units before selecting an answer. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-071",
      "type": "ordering",
      "question": "Rank these migration testing strategies from LEAST REALISTIC to MOST REALISTIC:",
      "items": [
        "Run migration on empty database",
        "Run migration on a small dataset (100 rows)",
        "Run migration on a copy of production data (anonymized)",
        "Run migration on a staging environment with production-scale data"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Empty DB: finds syntax errors only. Small dataset: finds basic logic errors. Production copy: finds data-dependent issues. Production-scale staging: finds performance issues (lock duration, backfill time). Each level catches different failures.",
      "detailedExplanation": "Read this as a scenario about \"rank these migration testing strategies from LEAST REALISTIC to MOST REALISTIC:\". Place obvious extremes first, then sort the middle by pairwise comparison. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-072",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration adds a UNIQUE constraint to an email column. It passes on dev (10 rows) but fails on production (5M rows). Why?",
          "options": [
            "Production has more data",
            "Production has duplicate emails that violate the UNIQUE constraint — a data-dependent failure not caught by testing on clean dev data",
            "The production server is slower",
            "UNIQUE constraints don't work on large tables"
          ],
          "correct": 1,
          "explanation": "Dev data has no duplicates. Production has users who registered with the same email (perhaps allowed by old code). Adding UNIQUE fails because duplicates exist. Always test migrations against production-like data.",
          "detailedExplanation": "If you keep \"migration adds a UNIQUE constraint to an email column\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Choose data shape based on workload paths, not on normalization dogma alone. If values like 10 and 5M appear, convert them into one unit basis before comparison. Common pitfall: schema optimized for entities instead of queries."
        },
        {
          "question": "How do you safely add a UNIQUE constraint when duplicates exist?",
          "options": [
            "Force the constraint and lose duplicate rows",
            "First deduplicate: identify and resolve duplicate emails (merge accounts, pick one), then add the constraint after cleanup",
            "Add the constraint as non-enforced",
            "Use a non-unique index instead"
          ],
          "correct": 1,
          "explanation": "Step 1: Find duplicates (SELECT email, COUNT(*) ... HAVING COUNT(*) > 1). Step 2: Resolve them (merge, deactivate, or assign unique emails). Step 3: Verify no duplicates remain. Step 4: Add UNIQUE constraint. The constraint must be earned.",
          "detailedExplanation": "This prompt is really about \"you safely add a UNIQUE constraint when duplicates exist\". Do not reset assumptions between stages; carry forward prior constraints directly. Choose data shape based on workload paths, not on normalization dogma alone. If values like 1 and 2 appear, convert them into one unit basis before comparison. Common pitfall: indexing that over-amplifies writes."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"schema Evolution\". Do not reset assumptions between stages; carry forward prior constraints directly. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-073",
      "type": "multiple-choice",
      "question": "What is 'schema linting'?",
      "options": [
        "Cleaning the database schema",
        "Automated analysis of migration scripts for common problems — dangerous operations (locks, rewrites), missing indexes, naming convention violations",
        "Formatting SQL code",
        "Checking for SQL syntax errors"
      ],
      "correct": 1,
      "explanation": "Schema linters (squawk, strong_migrations, sqlfluff) analyze migrations for: operations that lock tables, missing CONCURRENTLY on index creation, adding NOT NULL without a default, etc. They catch dangerous operations before they reach production.",
      "detailedExplanation": "Start from \"'schema linting'\", then pressure-test the result against the options. Prefer the schema/index decision that minimizes query and write amplification for this workload. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-074",
      "type": "multi-select",
      "question": "Which issues can schema linting catch in CI?",
      "options": [
        "Adding a column with NOT NULL and no default (table lock in older PG)",
        "Creating an index without CONCURRENTLY on a large table",
        "Irreversible operations (DROP COLUMN) without explicit confirmation",
        "Business logic errors in the application"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Schema linters catch DDL-level risks: lock-causing operations, non-concurrent index creation, destructive changes. They can't catch business logic errors — that's application-level testing.",
      "detailedExplanation": "If you keep \"issues can schema linting catch in CI\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-075",
      "type": "ordering",
      "question": "Rank these migration safety practices from BASIC (minimum) to ADVANCED:",
      "items": [
        "Version-controlled migration scripts",
        "Test migrations on production-like data before deploying",
        "Schema linting in CI to catch dangerous operations",
        "Automated canary deployments with schema change monitoring"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Version control: bare minimum. Production-like testing: catches data-dependent issues. CI linting: automated safety checks. Canary deployment + monitoring: detects issues in production with minimal blast radius.",
      "detailedExplanation": "The core signal here is \"rank these migration safety practices from BASIC (minimum) to ADVANCED:\". Build the rank from biggest differences first, then refine with adjacent checks. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-076",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team needs to shard a monolithic Users table across 4 database servers by user_id. What's the migration challenge?",
          "options": [
            "Sharding is a simple ALTER TABLE",
            "Moving data to the correct shard, updating the application to route queries by shard, and handling cross-shard queries — a fundamental architectural change, not just a schema change",
            "Just add more indexes",
            "Create 4 copies of the same database"
          ],
          "correct": 1,
          "explanation": "Sharding is one of the most complex schema evolutions. It changes: data distribution (move rows to correct shards), application logic (route by shard key), query patterns (no cross-shard joins), and operational complexity (4x the databases).",
          "detailedExplanation": "The decision turns on \"team needs to shard a monolithic Users table across 4 database servers by user_id\". Solve this as chained reasoning where stage two must respect stage one assumptions. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Numbers such as 4 and 4x should be normalized first so downstream reasoning stays consistent. Common pitfall: indexing that over-amplifies writes."
        },
        {
          "question": "How do you migrate data from a single database to 4 shards without downtime?",
          "options": [
            "Copy all data at once and switch",
            "Dual-write to both old and new locations during migration; gradually redirect reads to shards; once validated, stop writing to the old database",
            "Export to CSV and import to each shard",
            "Use database replication to all 4 shards"
          ],
          "correct": 1,
          "explanation": "Dual-write: every write goes to the monolith AND the correct shard. Backfill existing data to shards. Gradually redirect reads to shards (shard by shard or feature by feature). Once all reads use shards, stop writing to the monolith.",
          "detailedExplanation": "Start from \"you migrate data from a single database to 4 shards without downtime\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. If values like 4 appear, convert them into one unit basis before comparison. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"schema Evolution\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-077",
      "type": "multiple-choice",
      "question": "What is a 'canary migration'?",
      "options": [
        "A migration for canary deployment environments",
        "Applying a schema change to a small subset of production (one shard, one replica) first, monitoring for issues, then rolling out to all",
        "A migration that detects problems",
        "A test migration that doesn't make real changes"
      ],
      "correct": 1,
      "explanation": "Canary migration: apply to one shard or partition first. Monitor query performance, error rates, application behavior. If issues arise, only one shard is affected. If clean, roll out to the rest. Reduces blast radius.",
      "detailedExplanation": "This prompt is really about \"a 'canary migration'\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-078",
      "type": "multi-select",
      "question": "What should you monitor after applying a schema migration?",
      "options": [
        "Query latency (did any queries get slower?)",
        "Error rates (are queries failing due to schema changes?)",
        "Lock wait times (is the migration holding locks?)",
        "Disk usage (did the migration consume unexpected space?)"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All four: latency changes indicate plan regression. Error rates catch incompatible code. Lock waits indicate blocking DDL. Disk usage catches unexpected table rewrites or index bloat.",
      "detailedExplanation": "The decision turns on \"you monitor after applying a schema migration\". Treat every option as a separate true/false test under the same constraints. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-079",
      "type": "multiple-choice",
      "question": "What is the 'blue-green deployment' approach to database migrations?",
      "options": [
        "Painting servers blue or green",
        "Maintaining two database instances: 'blue' (current) and 'green' (new schema). Switch traffic from blue to green after migration. Roll back by switching back to blue.",
        "Running migrations on green servers only",
        "Color-coded migration scripts"
      ],
      "correct": 1,
      "explanation": "Blue-green: two full databases. Migrate 'green' while 'blue' serves traffic. Switch when ready. Rollback = switch back to 'blue'. Challenge: keeping both databases in sync during the transition period.",
      "detailedExplanation": "Read this as a scenario about \"the 'blue-green deployment' approach to database migrations\". Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-080",
      "type": "two-stage",
      "stages": [
        {
          "question": "Blue-green database deployment: 'blue' is live, 'green' has the new schema. During switchover, writes happen on both. What's the biggest challenge?",
          "options": [
            "Network bandwidth",
            "Data synchronization — writes to 'blue' during switchover must be replicated to 'green', and vice versa, to avoid data loss",
            "Color management",
            "Server naming"
          ],
          "correct": 1,
          "explanation": "During switchover, both databases may receive writes. These must be synchronized to prevent data loss or divergence. Change Data Capture or dual-write patterns help, but it's operationally complex.",
          "detailedExplanation": "The core signal here is \"blue-green database deployment: 'blue' is live, 'green' has the new schema\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out."
        },
        {
          "question": "Why is blue-green less common for databases than for application deployments?",
          "options": [
            "Databases are more expensive to duplicate",
            "Databases hold state — synchronizing two full database copies with different schemas while maintaining consistency is far harder than deploying stateless application servers",
            "Databases don't support blue-green",
            "No one has tried it"
          ],
          "correct": 1,
          "explanation": "Applications are (mostly) stateless — blue-green is easy, just route traffic. Databases are stateful — duplicating the full dataset, keeping both in sync during schema changes, and handling writes to both is extremely complex and expensive.",
          "detailedExplanation": "Use \"blue-green less common for databases than for application deployments\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out."
        }
      ],
      "detailedExplanation": "The core signal here is \"schema Evolution\". Do not reset assumptions between stages; carry forward prior constraints directly. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-081",
      "type": "ordering",
      "question": "Rank these schema change types from FASTEST to SLOWEST on a 100M row PostgreSQL table:",
      "items": [
        "ADD COLUMN (nullable, no default)",
        "ADD COLUMN with DEFAULT (PG 11+)",
        "CREATE INDEX CONCURRENTLY",
        "ALTER COLUMN TYPE (e.g., INT to BIGINT — table rewrite)"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Add nullable column: instant (metadata only). Add with default (PG 11+): instant (stored in catalog). Create index concurrently: minutes (scans table twice but no lock). Alter type: rewrite entire table — potentially hours.",
      "detailedExplanation": "If you keep \"rank these schema change types from FASTEST to SLOWEST on a 100M row PostgreSQL table:\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. Choose data shape based on workload paths, not on normalization dogma alone. If values like 100M and 11 appear, convert them into one unit basis before comparison. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-082",
      "type": "multiple-choice",
      "question": "Why should migration scripts be immutable (never modified after applied)?",
      "options": [
        "Database tools forbid changes",
        "The migration represents a historical change — modifying it creates divergence between databases that applied the old version and new ones that apply the modified version",
        "Immutable scripts run faster",
        "Version control requires it"
      ],
      "correct": 1,
      "explanation": "If migration V005 is modified after some databases applied the original, those databases have a different schema than databases applying the modified V005. This creates silent schema drift. New changes should be new migration files.",
      "detailedExplanation": "This prompt is really about \"migration scripts be immutable (never modified after applied)\". Prefer the schema/index decision that minimizes query and write amplification for this workload. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-083",
      "type": "multi-select",
      "question": "Which are valid strategies for handling long-running data migrations in production?",
      "options": [
        "Run as a background job separate from the DDL migration",
        "Batch updates with throttling (sleep between batches)",
        "Use online migration tools (gh-ost, LHM) for table transformations",
        "Run the entire migration in a single transaction for atomicity"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Background jobs: decouple data migration from schema change. Batched updates: controlled resource usage. Online tools: handle large table changes. Single huge transaction: locks resources for the entire duration — dangerous.",
      "detailedExplanation": "Use \"valid strategies for handling long-running data migrations in production\" as your starting point, then verify tradeoffs carefully. Validate each option independently; do not select statements that are only partially true. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-084",
      "type": "two-stage",
      "stages": [
        {
          "question": "A team uses a ORM (like Django or Rails) that auto-generates migrations from model changes. A developer changes a model field type from String(50) to String(200). What migration is generated?",
          "options": [
            "DROP and recreate the column",
            "ALTER TABLE ... ALTER COLUMN ... TYPE VARCHAR(200) — increasing length, typically a safe online operation",
            "No migration — the ORM handles it at runtime",
            "A complete table rebuild"
          ],
          "correct": 1,
          "explanation": "ORM migration generators produce ALTER COLUMN TYPE. Increasing VARCHAR length is generally safe (metadata change in most databases). Decreasing length would be risky — could truncate existing data.",
          "detailedExplanation": "Read this as a scenario about \"team uses a ORM (like Django or Rails) that auto-generates migrations from model changes\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Schema and index choices should follow access patterns and write/read amplification constraints. Keep quantities like 50 and 200 in aligned units before selecting an answer. Common pitfall: indexing that over-amplifies writes."
        },
        {
          "question": "Why should auto-generated migrations be reviewed before applying?",
          "options": [
            "They might have syntax errors",
            "The generator may produce inefficient or dangerous operations — e.g., column type changes that trigger table rewrites, or operations that should use CONCURRENTLY but don't",
            "They're always wrong",
            "ORM bugs are common"
          ],
          "correct": 1,
          "explanation": "Auto-generated migrations are correct SQL but may not be production-safe. They don't add CONCURRENTLY for index creation, may generate table rewrites for type changes, or produce unnecessary operations. Human review ensures safety.",
          "detailedExplanation": "The key clue in this question is \"auto-generated migrations be reviewed before applying\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"schema Evolution\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-085",
      "type": "numeric-input",
      "question": "A 500GB table needs a column type change (table rewrite) at a disk write speed of 500MB/sec. Approximately how long will the rewrite take (ignoring overhead)?",
      "answer": 1000,
      "unit": "seconds",
      "tolerance": 0.1,
      "explanation": "500 GB = 500,000 MB. At 500 MB/sec, rewrite takes 500,000/500 = 1,000 seconds ≈ 17 minutes. With overhead (WAL writes, index rebuilds), expect 2-3x longer. During this time, the table may be locked.",
      "detailedExplanation": "The decision turns on \"500GB table needs a column type change (table rewrite) at a disk write speed of\". Normalize units before computing so conversion mistakes do not propagate. Schema and index choices should follow access patterns and write/read amplification constraints. If values like 500GB and 500MB appear, convert them into one unit basis before comparison. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-086",
      "type": "multiple-choice",
      "question": "What is 'schema archaeology' and when is it needed?",
      "options": [
        "Studying ancient database schemas",
        "Reverse-engineering the history and purpose of schema decisions in a legacy system — understanding why tables, columns, and constraints exist",
        "Digging through database backups",
        "Studying database internals"
      ],
      "correct": 1,
      "explanation": "Schema archaeology: when joining a legacy project, understanding why the schema is the way it is. Why is this column VARCHAR(37)? Why does this table exist? Migration history, commit messages, and documentation provide clues.",
      "detailedExplanation": "Start from \"'schema archaeology' and when is it needed\", then pressure-test the result against the options. Reject options that conflict with the primary access pattern or index strategy. Schema and index choices should follow access patterns and write/read amplification constraints. Numbers such as 37 should be normalized first so downstream reasoning stays consistent. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-087",
      "type": "multi-select",
      "question": "Which practices help future developers understand schema evolution decisions?",
      "options": [
        "Descriptive migration filenames (add_user_preferences_table)",
        "Comments in migration scripts explaining WHY (not just what)",
        "A schema changelog documenting major design decisions",
        "Automated migration from migration scripts to documentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Descriptive names: quick understanding. Comments: explain the reasoning. Changelogs: high-level decision history. All reduce schema archaeology effort for future developers.",
      "detailedExplanation": "The key clue in this question is \"practices help future developers understand schema evolution decisions\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-088",
      "type": "ordering",
      "question": "Rank the phases of safely adding a NOT NULL constraint to an existing column with NULLs:",
      "items": [
        "Deploy code that always writes a value to the column",
        "Backfill existing NULL values with a default",
        "Validate no NULLs remain (SELECT COUNT(*) WHERE col IS NULL)",
        "Add NOT NULL constraint"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Stop writing NULLs first (code change). Then backfill existing NULLs. Verify none remain. Only then add the constraint. If you add NOT NULL first, writes without the column fail and existing NULLs cause the ALTER to fail.",
      "detailedExplanation": "The core signal here is \"rank the phases of safely adding a NOT NULL constraint to an existing column with NULLs:\". Build the rank from biggest differences first, then refine with adjacent checks. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-089",
      "type": "two-stage",
      "stages": [
        {
          "question": "PostgreSQL 12+ supports adding NOT NULL constraints with NOT VALID. What does this do?",
          "options": [
            "Adds the constraint without checking existing rows — new writes are enforced, but existing rows aren't validated",
            "Makes the constraint optional",
            "Adds the constraint only during business hours",
            "Creates a non-valid (fake) constraint"
          ],
          "correct": 0,
          "explanation": "NOT VALID: the constraint is enforced on new writes immediately, but existing rows aren't checked. This is fast (no full table scan). You validate existing rows separately with VALIDATE CONSTRAINT.",
          "detailedExplanation": "The key clue in this question is \"postgreSQL 12+ supports adding NOT NULL constraints with NOT VALID\". Do not reset assumptions between stages; carry forward prior constraints directly. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Numbers such as 12 should be normalized first so downstream reasoning stays consistent. Common pitfall: unbounded cardinality in joins or fan-out."
        },
        {
          "question": "What is the benefit of the two-step (ADD CONSTRAINT NOT VALID → VALIDATE CONSTRAINT) approach?",
          "options": [
            "It's faster overall",
            "ADD CONSTRAINT NOT VALID is instant (no lock), and VALIDATE CONSTRAINT only takes a ShareUpdateExclusiveLock (doesn't block writes) — zero downtime",
            "It skips validation entirely",
            "It only works on small tables"
          ],
          "correct": 1,
          "explanation": "Step 1 (NOT VALID): instant, lightweight lock. Step 2 (VALIDATE): scans the table but uses a weaker lock that doesn't block INSERT/UPDATE/DELETE. Total: zero downtime. Without NOT VALID, adding the constraint locks the table during the full scan.",
          "detailedExplanation": "Read this as a scenario about \"the benefit of the two-step (ADD CONSTRAINT NOT VALID → VALIDATE CONSTRAINT) approach\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "If you keep \"schema Evolution\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-090",
      "type": "multiple-choice",
      "question": "What is 'soft delete' and how does it affect schema evolution?",
      "options": [
        "Deleting data softly",
        "Marking rows as deleted (deleted_at timestamp or is_deleted flag) instead of physically removing them — affects queries (must filter deleted rows) and schema evolution (column still has data)",
        "A gentle way to drop tables",
        "Deleting with a lower priority"
      ],
      "correct": 1,
      "explanation": "Soft delete: rows aren't physically removed, just flagged. Schema impact: all queries need WHERE deleted_at IS NULL. Column changes must consider 'deleted' rows too. UNIQUE constraints need to exclude soft-deleted rows (partial indexes).",
      "detailedExplanation": "The key clue in this question is \"'soft delete' and how does it affect schema evolution\". Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-091",
      "type": "multi-select",
      "question": "What schema implications does soft delete have?",
      "options": [
        "UNIQUE constraints must account for soft-deleted rows (use partial unique index excluding deleted rows)",
        "Tables grow larger over time (deleted rows accumulate)",
        "Foreign key relationships to soft-deleted rows must be handled",
        "Queries must always include a deleted_at filter"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All are real implications. UNIQUE must be partial (WHERE deleted_at IS NULL). Tables bloat (need periodic purging). FK references to deleted rows create orphans. Every query needs the filter (use default scopes in ORMs).",
      "detailedExplanation": "Start from \"schema implications does soft delete have\", then pressure-test the result against the options. Avoid pattern guessing and evaluate each candidate directly against the scenario. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-092",
      "type": "two-stage",
      "stages": [
        {
          "question": "A table has a UNIQUE constraint on (email). The system uses soft delete. User A (deleted) has email 'a@b.com'. User B wants to register with 'a@b.com'. What happens?",
          "options": [
            "User B registers successfully",
            "UNIQUE violation — 'a@b.com' exists in the table (soft-deleted row), blocking User B",
            "The soft-deleted row is automatically removed",
            "The database ignores soft-deleted rows"
          ],
          "correct": 1,
          "explanation": "The UNIQUE constraint applies to ALL rows, including soft-deleted ones. User A's row still exists with email 'a@b.com'. User B's INSERT violates the constraint. The database doesn't know about soft delete semantics.",
          "detailedExplanation": "Use \"table has a UNIQUE constraint on (email)\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: unbounded cardinality in joins or fan-out."
        },
        {
          "question": "How do you fix this?",
          "options": [
            "Remove the UNIQUE constraint entirely",
            "Use a partial unique index: CREATE UNIQUE INDEX ... ON users(email) WHERE deleted_at IS NULL — uniqueness only among active rows",
            "Hard-delete the row instead",
            "Use a different email for User B"
          ],
          "correct": 1,
          "explanation": "A partial unique index enforces uniqueness only among non-deleted rows. Soft-deleted rows are excluded from the uniqueness check. User B can register with the same email as a soft-deleted user.",
          "detailedExplanation": "The core signal here is \"you fix this\". Solve this as chained reasoning where stage two must respect stage one assumptions. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out."
        }
      ],
      "detailedExplanation": "The decision turns on \"schema Evolution\". Do not reset assumptions between stages; carry forward prior constraints directly. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-093",
      "type": "multiple-choice",
      "question": "What is a 'migration dependency graph'?",
      "options": [
        "A visual representation of table relationships",
        "The ordering of migrations where some depend on others — migration B requires migration A's table to exist, creating a dependency",
        "A graph database for tracking migrations",
        "The relationship between database versions"
      ],
      "correct": 1,
      "explanation": "Migration B (add foreign key to Users) depends on Migration A (create Users table). In branching workflows, dependencies can create conflicts. Some tools support explicit dependency declarations instead of linear ordering.",
      "detailedExplanation": "Read this as a scenario about \"a 'migration dependency graph'\". Discard modeling choices that look clean but perform poorly for the target queries. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-094",
      "type": "ordering",
      "question": "Rank the safety of these approaches for renaming a table from SAFEST to RISKIEST:",
      "items": [
        "Create new table, migrate data, create view with old name pointing to new table, update code, drop view",
        "Create new table, dual-write, migrate reads, drop old table",
        "ALTER TABLE RENAME (direct rename)",
        "Create a synonym/alias for the new name"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "View aliasing: safest, old name still works. Dual-write: safe, gradual transition. Synonym: DB-level alias, limited support. Direct rename: instant but breaks all code referencing the old name simultaneously.",
      "detailedExplanation": "Use \"rank the safety of these approaches for renaming a table from SAFEST to RISKIEST:\" as your starting point, then verify tradeoffs carefully. Order by relative scale and bottleneck effect, then validate neighboring items. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-095",
      "type": "two-stage",
      "stages": [
        {
          "question": "A migration needs to merge two tables: Accounts and Profiles (1:1 relationship) into a single Users table. What's the safe approach?",
          "options": [
            "DROP both tables and CREATE Users",
            "Create Users table, backfill from both tables, deploy code using Users, create views for Accounts and Profiles pointing to Users for backward compatibility, eventually drop views",
            "UNION the two tables",
            "Rename Accounts to Users and copy Profile columns"
          ],
          "correct": 1,
          "explanation": "Create the target table, migrate data, deploy code incrementally, and use views for backward compatibility. Old code referencing Accounts or Profiles sees views that map to Users. Drop views when all code is updated.",
          "detailedExplanation": "Start from \"migration needs to merge two tables: Accounts and Profiles (1:1 relationship) into a\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Choose data shape based on workload paths, not on normalization dogma alone. If values like 1 appear, convert them into one unit basis before comparison. Common pitfall: unbounded cardinality in joins or fan-out."
        },
        {
          "question": "What's the trickiest part of merging two tables?",
          "options": [
            "Creating the new table",
            "Handling writes during the transition — writes to old tables must be reflected in the new table, and vice versa, until the transition is complete",
            "Choosing the table name",
            "Copying the data"
          ],
          "correct": 1,
          "explanation": "During transition, writes may target old tables (via views or old code) or the new table (new code). All write paths must keep data consistent. This usually requires either triggers on views or application-level dual-write logic.",
          "detailedExplanation": "The decision turns on \"what's the trickiest part of merging two tables\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries."
        }
      ],
      "detailedExplanation": "This prompt is really about \"schema Evolution\". Solve this as chained reasoning where stage two must respect stage one assumptions. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: indexing that over-amplifies writes.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-096",
      "type": "multi-select",
      "question": "Which are signs that a schema evolution process is mature?",
      "options": [
        "All changes go through versioned, reviewed migration scripts",
        "Migrations are tested against production-like data before deployment",
        "Schema linting runs in CI",
        "Developers apply DDL directly in production consoles"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Mature process: versioned scripts, production-like testing, automated linting. Direct DDL in production consoles bypasses all safety nets — a sign of an immature or emergency-driven process.",
      "detailedExplanation": "If you keep \"signs that a schema evolution process is mature\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Choose data shape based on workload paths, not on normalization dogma alone. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-097",
      "type": "numeric-input",
      "question": "A team performs schema migrations via a CI/CD pipeline. The pipeline takes 3 minutes. They deploy 10 times per day. How many minutes per day are spent on migration pipeline runs?",
      "answer": 30,
      "unit": "minutes",
      "tolerance": "exact",
      "explanation": "10 deployments × 3 minutes = 30 minutes/day. This is the overhead of automated safety. Most of these deployments have no migrations (the check is fast), but the pipeline runs regardless to verify.",
      "detailedExplanation": "The core signal here is \"team performs schema migrations via a CI/CD pipeline\". Keep every transformation in one unit system and check order of magnitude at the end. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Keep quantities like 3 minutes and 10 in aligned units before selecting an answer. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-098",
      "type": "multiple-choice",
      "question": "What is 'database state machine' thinking for schema evolution?",
      "options": [
        "Using a state machine library for migrations",
        "Treating the database schema as a finite state machine where each migration is a transition from one valid state to another — the sequence of transitions must always leave the schema in a consistent, valid state",
        "Automating database decisions",
        "Running migrations in a specific state"
      ],
      "correct": 1,
      "explanation": "Each migration transitions: Schema V1 → Schema V2 → ... Each intermediate state must be valid (application can run). No migration should leave the schema in a state where the application breaks. Every step is a safe state transition.",
      "detailedExplanation": "The key clue in this question is \"'database state machine' thinking for schema evolution\". Prefer the schema/index decision that minimizes query and write amplification for this workload. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-099",
      "type": "two-stage",
      "stages": [
        {
          "question": "An application needs to support both Schema V5 (current) and V6 (new) simultaneously during rolling deployment. What design principle ensures this?",
          "options": [
            "Schema V6 must be backward compatible — V5 code can still work with V6 schema",
            "V6 must be a superset of V5",
            "Both schemas must be identical",
            "V5 code must be rewritten for V6"
          ],
          "correct": 0,
          "explanation": "Backward compatibility: V6 schema works with both V5 code (old replicas) and V6 code (new replicas). During rolling deployment, both versions run simultaneously. The schema must not break either version.",
          "detailedExplanation": "This prompt is really about \"application needs to support both Schema V5 (current) and V6 (new) simultaneously\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After all replicas are on V6 code, is the old V5 compatibility still needed?",
          "options": [
            "Yes — always maintain backward compatibility with all previous versions",
            "No — once V5 code is fully decommissioned, V5 compatibility can be dropped in the next schema version (V7)",
            "Only for one more version",
            "It depends on the database"
          ],
          "correct": 1,
          "explanation": "You only need N and N-1 compatibility (current and previous). Once V5 code is fully gone, V7 can drop V5-specific compatibility (remove deprecated columns, etc.). You don't need to support all historical versions forever.",
          "detailedExplanation": "If you keep \"after all replicas are on V6 code, is the old V5 compatibility still needed\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Choose data shape based on workload paths, not on normalization dogma alone. Keep quantities like 1 in aligned units before selecting an answer. Common pitfall: unbounded cardinality in joins or fan-out."
        }
      ],
      "detailedExplanation": "Start from \"schema Evolution\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Schema and index choices should follow access patterns and write/read amplification constraints. Common pitfall: schema optimized for entities instead of queries.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    },
    {
      "id": "evo-100",
      "type": "ordering",
      "question": "Rank these schema evolution anti-patterns from MOST COMMON to LEAST COMMON:",
      "items": [
        "Applying DDL directly in production without migration scripts",
        "Not testing migrations against production-scale data",
        "Missing backward compatibility during rolling deployments",
        "Modifying already-applied migration files"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Direct DDL: extremely common, especially in early startups. Skipping production-scale testing: common (dev data is always small). Missing backward compat: common in non-continuous deployment shops. Modifying applied migrations: less common (tools often checksum-verify).",
      "detailedExplanation": "If you keep \"rank these schema evolution anti-patterns from MOST COMMON to LEAST COMMON:\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. Modeling quality is measured by query fit, cardinality behavior, and operational cost. Common pitfall: unbounded cardinality in joins or fan-out.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        }
      ],
      "tags": ["data-modeling", "schema-evolution"],
      "difficulty": "senior"
    }
  ]
}
