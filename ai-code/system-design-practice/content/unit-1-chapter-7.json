{
  "unit": 1,
  "unitTitle": "Estimation",
  "chapter": 7,
  "chapterTitle": "Sanity Checks",
  "chapterDescription": "Validating estimates, spotting red flags, and checking calculations against real-world constraints",
  "problems": [
    {
      "id": "sanity-001",
      "type": "multiple-choice",
      "question": "A teammate estimates their new service will need 1 PB of RAM. What's the most likely issue?",
      "options": ["Confusing RAM with disk storage", "Correct for a large-scale service", "Underestimating by 10x", "Using wrong units (should be EB)"],
      "correct": 0,
      "explanation": "1 PB of RAM would cost ~$10B and require thousands of servers. Even Google Search doesn't use this much RAM for a single service. They likely mean disk storage."
    },
    {
      "id": "sanity-002",
      "type": "multiple-choice",
      "question": "Estimate says a single MySQL instance will handle 1M QPS. Is this reasonable?",
      "options": ["Yes, with proper indexing", "No, typical limit is 10K-50K QPS", "No, typical limit is 100-1K QPS", "Yes, with read replicas"],
      "correct": 1,
      "explanation": "A well-tuned MySQL instance handles 10K-50K QPS for simple queries. 1M QPS requires sharding across many instances. This estimate is off by ~20-100x."
    },
    {
      "id": "sanity-003",
      "type": "multiple-choice",
      "question": "Design doc claims 99.999% availability with a single-region deployment. Red flag?",
      "options": ["No, achievable with redundant servers", "Yes, single region can't survive regional outages", "No, cloud providers guarantee this", "Depends on the database choice"],
      "correct": 1,
      "explanation": "Five 9s (5.26 min/year downtime) requires surviving regional failures. A single region deployment has a single point of failure for the entire region. Multi-region is required."
    },
    {
      "id": "sanity-004",
      "type": "multiple-choice",
      "question": "Estimate: 'We'll store 1 billion user profiles at 10 KB each = 10 TB.' Sanity check?",
      "options": ["Math is correct", "Off by 1000x (should be 10 PB)", "Off by 10x (should be 100 TB)", "Off by 1000x (should be 10 GB)"],
      "correct": 0,
      "explanation": "1B × 10 KB = 10B KB = 10 TB. The math checks out. This is a reasonable size for a large user database."
    },
    {
      "id": "sanity-005",
      "type": "multiple-choice",
      "question": "Team claims their API p99 latency is 1ms with a cross-country database call. Issue?",
      "options": ["Plausible with edge caching", "Impossible - network RTT alone is 30-70ms", "Correct if using TCP fast open", "Plausible with connection pooling"],
      "correct": 1,
      "explanation": "Cross-country (e.g., NYC to LA) network RTT is 30-70ms minimum due to speed of light. 1ms p99 is physically impossible without local caching."
    },
    {
      "id": "sanity-006",
      "type": "multiple-choice",
      "question": "Capacity plan shows 100 servers handling 10M QPS total. Per-server load reasonable?",
      "options": ["Yes, 100K QPS per server is typical", "No, that's 100K QPS per server - too high for most apps", "No, that's only 1K QPS per server - underutilized", "Depends entirely on the workload"],
      "correct": 0,
      "explanation": "10M ÷ 100 = 100K QPS per server. For simple stateless web servers with fast backends, this is achievable. For complex processing, it might be high. Context matters, but it's in the reasonable range."
    },
    {
      "id": "sanity-007",
      "type": "multiple-choice",
      "question": "Estimate claims 1 TB/second of network throughput from a single server. Valid?",
      "options": ["Yes, with 100 Gbps NIC", "No, fastest NICs are ~400 Gbps = 50 GB/s", "Yes, with NVMe storage", "No, but achievable with kernel bypass"],
      "correct": 1,
      "explanation": "1 TB/s = 8 Tbps. The fastest commodity NICs are 400 Gbps (~50 GB/s). 1 TB/s would need 20+ of the fastest NICs. This is off by ~20x."
    },
    {
      "id": "sanity-008",
      "type": "multiple-choice",
      "question": "Design proposes storing 10 years of logs at 1 GB/day. Total storage estimate of 3.6 TB. Check?",
      "options": ["Correct", "Wrong - should be ~36 TB", "Wrong - should be ~365 GB", "Wrong - should be ~3.6 PB"],
      "correct": 0,
      "explanation": "10 years × 365 days × 1 GB = 3,650 GB ≈ 3.6 TB. The estimate is correct."
    },
    {
      "id": "sanity-009",
      "type": "multiple-choice",
      "question": "Proposal: 'Each user uploads 1 photo/day. 100M users = 100M photos/day = 1,157 photos/second.' Sanity check?",
      "options": ["Math is correct", "Wrong - should be ~11,574/second", "Wrong - should be ~115/second", "Wrong - should be ~1.16M/second"],
      "correct": 0,
      "explanation": "100M ÷ 86,400 seconds = 1,157 photos/second. The math checks out."
    },
    {
      "id": "sanity-010",
      "type": "multiple-choice",
      "question": "Teammate says Redis will handle their 10 TB dataset in memory. Concern?",
      "options": ["No concern, Redis scales horizontally", "Major concern - largest instances are ~500 GB RAM", "No concern with Redis Cluster", "Minor concern - may need 2-3 instances"],
      "correct": 1,
      "explanation": "10 TB in-memory requires enormous hardware. The largest cloud instances have ~500 GB-1 TB RAM. You'd need 10-20+ of the largest instances. Consider if all data needs to be in memory."
    },
    {
      "id": "sanity-011",
      "type": "multiple-choice",
      "question": "Estimate: 'P99 latency of 500ms is fine for our real-time chat app.' Red flag?",
      "options": ["Acceptable for non-critical features", "Major red flag - users expect <100ms for chat", "Fine if most requests are faster", "Acceptable for mobile users"],
      "correct": 1,
      "explanation": "Real-time chat users expect near-instant delivery. 500ms p99 means 1% of messages take half a second - very noticeable. Target should be <100ms p99."
    },
    {
      "id": "sanity-012",
      "type": "multiple-choice",
      "question": "Design claims a single Kafka broker will handle 10M messages/second. Reasonable?",
      "options": ["Yes, Kafka is designed for this scale", "No, typical limit is 100K-500K msg/sec per broker", "No, typical limit is 1K-10K msg/sec per broker", "Yes, with proper partitioning"],
      "correct": 1,
      "explanation": "A single Kafka broker typically handles 100K-500K messages/second depending on message size. 10M msg/sec needs 20-100 brokers. Off by ~20-100x."
    },
    {
      "id": "sanity-013",
      "type": "multiple-choice",
      "question": "Cost estimate: '$0.10/GB storage × 1 PB = $100,000/month.' Check the math.",
      "options": ["Correct", "Wrong - should be $100M/month", "Wrong - should be $10,000/month", "Wrong - should be $1M/month"],
      "correct": 0,
      "explanation": "1 PB = 1,000,000 GB. $0.10 × 1,000,000 = $100,000/month. Math is correct. (Though $0.10/GB is high - S3 standard is ~$0.023/GB.)"
    },
    {
      "id": "sanity-014",
      "type": "multiple-choice",
      "question": "Architect claims their service processes 1 request in 10 nanoseconds. Believable?",
      "options": ["Yes, with optimized code", "No, a single L1 cache access takes ~1ns", "Yes, with FPGA acceleration", "No, context switches alone take longer"],
      "correct": 1,
      "explanation": "10ns is only ~30 CPU cycles. Even reading from L1 cache takes 1ns, L2 takes 4ns. A complete request/response with any I/O or logic is impossible in 10ns. Minimum realistic is microseconds."
    },
    {
      "id": "sanity-015",
      "type": "multiple-choice",
      "question": "Proposal estimates 10 engineers can build and operate a global distributed database in 6 months. Realistic?",
      "options": ["Realistic for experienced team", "Unrealistic - takes 50+ engineers years", "Realistic if using existing frameworks", "Depends on feature set"],
      "correct": 1,
      "explanation": "Production-grade distributed databases (CockroachDB, Spanner, etc.) took massive teams many years. Even with shortcuts, a reliable global database is a multi-year, large-team effort. Use existing solutions."
    },
    {
      "id": "sanity-016",
      "type": "multiple-choice",
      "question": "Estimate: 'Our 50-node cluster has 50 × 16 cores = 800 cores, so we can handle 800 concurrent requests.' Flaw?",
      "options": ["No flaw, math is correct", "Flaw: most requests involve I/O waiting, not CPU", "Flaw: should account for hyperthreading (1600)", "Flaw: need to subtract OS overhead"],
      "correct": 1,
      "explanation": "Concurrent requests ≠ CPU cores. Most web requests spend time waiting on I/O (database, network). A single core can handle hundreds or thousands of concurrent I/O-bound requests with async processing."
    },
    {
      "id": "sanity-017",
      "type": "multiple-choice",
      "question": "Team proposes 100% cache hit rate as their target. Issue?",
      "options": ["Achievable with enough cache", "Unrealistic - cold starts, cache eviction, new data", "Achievable with predictive caching", "Fine as an aspirational goal"],
      "correct": 1,
      "explanation": "100% hit rate is impossible: new/updated data isn't cached, cold starts after deployments, cache eviction under memory pressure, and long-tail requests for rarely-accessed data. Typical targets are 90-99%."
    },
    {
      "id": "sanity-018",
      "type": "multiple-choice",
      "question": "Design doc: 'With 1M users and 10 friends average, we'll have 10M friend relationships.' Missing factor?",
      "options": ["No missing factor, math is correct", "Friendships are bidirectional - actually 5M edges", "Should account for follow asymmetry", "Missing power-law distribution"],
      "correct": 1,
      "explanation": "If friendships are mutual (A friends B means B friends A), each friendship is counted twice in '10 friends average.' Actual edges = 1M × 10 ÷ 2 = 5M relationships."
    },
    {
      "id": "sanity-019",
      "type": "multiple-choice",
      "question": "Estimate claims SSD random read latency of 1 microsecond. Accurate?",
      "options": ["Yes, modern NVMe SSDs achieve this", "No, typical is 50-100 microseconds", "No, typical is 10-20 milliseconds", "Yes, with Intel Optane"],
      "correct": 1,
      "explanation": "NVMe SSD random read latency is typically 50-100 microseconds. Intel Optane is faster (~10μs) but still not 1μs. 1μs would be closer to RAM access speeds."
    },
    {
      "id": "sanity-020",
      "type": "multiple-choice",
      "question": "Load test shows server handling 50K QPS at 10% CPU utilization. Extrapolation to 100% CPU = 500K QPS. Valid?",
      "options": ["Valid linear extrapolation", "Invalid - performance degrades non-linearly at high utilization", "Valid if using autoscaling", "Invalid - should use 80% as max"],
      "correct": 1,
      "explanation": "CPU utilization doesn't scale linearly. At high utilization, context switching, lock contention, and queueing cause degradation. Never plan for 100% utilization. Use 70-80% as practical max."
    },
    {
      "id": "sanity-021",
      "type": "multiple-choice",
      "question": "Estimate: 'Twitter has 500M users tweeting 10x/day = 5B tweets/day = 58K tweets/second.' Reality check?",
      "options": ["Reasonable estimate", "Too high - actual is ~6K tweets/second", "Too low - actual is ~500K tweets/second", "Can't verify without internal data"],
      "correct": 1,
      "explanation": "Not all 500M users are daily active, and most users read more than write. Actual tweet rate is ~500M tweets/day = ~6K/second. The 10x/day assumption is too high for average users."
    },
    {
      "id": "sanity-022",
      "type": "multiple-choice",
      "question": "Proposal to process 1M images/second with 10 GPU servers. Each image takes 100ms on GPU. Feasible?",
      "options": ["Feasible with batching", "Infeasible - 10 GPUs × 10/sec = 100 images/sec max", "Feasible with model optimization", "Infeasible without TPUs"],
      "correct": 1,
      "explanation": "If each image takes 100ms, one GPU processes 10 images/second. 10 GPUs = 100 images/second. For 1M/sec, you'd need 100,000 GPUs. Off by 10,000x."
    },
    {
      "id": "sanity-023",
      "type": "multiple-choice",
      "question": "Cost projection: 'AWS bill will stay flat as we 10x users because we'll optimize.' Realistic?",
      "options": ["Realistic with good engineering", "Unrealistic - costs scale with usage", "Realistic if already over-provisioned", "Depends on pricing model"],
      "correct": 1,
      "explanation": "While optimization helps, 10x users means ~10x compute, storage, and bandwidth. You might achieve 3-5x efficiency gains, but flat costs while 10x-ing is unrealistic without fundamental architecture changes."
    },
    {
      "id": "sanity-024",
      "type": "multiple-choice",
      "question": "Design assumes 0ms network latency between microservices 'because they're in the same datacenter.' Problem?",
      "options": ["No problem, same-DC latency is negligible", "Problem: same-DC RTT is 0.5-2ms, adds up with many hops", "No problem with service mesh", "Problem only at very high scale"],
      "correct": 1,
      "explanation": "Same-datacenter RTT is 0.5-2ms, not 0. With 10 microservice hops, that's 5-20ms just in network latency. This matters for latency-sensitive applications and is often underestimated."
    },
    {
      "id": "sanity-025",
      "type": "multiple-choice",
      "question": "Teammate's estimate: '1 billion rows × 100 bytes = 100 GB, easily fits in RAM.' What's missing?",
      "options": ["Nothing missing, estimate is complete", "Index overhead (often 2-3x data size)", "Missing replication factor", "Should use compression ratio"],
      "correct": 1,
      "explanation": "Database indexes often add 1-3x the raw data size. B-tree indexes, secondary indexes, and internal metadata mean 100 GB of data might need 200-400 GB total. Always account for index overhead."
    },
    {
      "id": "sanity-026",
      "type": "multiple-choice",
      "question": "SLA promises 99.99% availability. System has 10 components in series, each 99.99% available. Actual availability?",
      "options": ["99.99% (matches SLA)", "99.9% (10x worse)", "99% (100x worse)", "~99.9% (close but misses SLA)"],
      "correct": 3,
      "explanation": "Serial availability: 0.9999^10 = 0.999 = 99.9%. Each additional component in the critical path reduces overall availability. To achieve 99.99% overall, individual components need higher availability."
    },
    {
      "id": "sanity-027",
      "type": "multiple-choice",
      "question": "Estimate: 'Video transcoding takes 1 hour for 1 hour of video on 1 server. For 1M hours/day, we need 1M servers.' Flaw?",
      "options": ["No flaw, math is correct", "Flaw: can parallelize transcoding of single video", "Flaw: 1M servers is cost-prohibitive, need queueing", "Flaw: should use 1 hour = 1 server-hour, so 1M server-hours/day ÷ 24 = 42K servers"],
      "correct": 3,
      "explanation": "1M hours of video needs 1M server-hours of work. Spread over 24 hours: 1M ÷ 24 = 41,667 servers running continuously. The original estimate doesn't account for time distribution."
    },
    {
      "id": "sanity-028",
      "type": "multiple-choice",
      "question": "Design shows read/write ratio of 1:1 for a social media feed. Suspicious?",
      "options": ["Normal for social apps", "Suspicious - reads typically 100-1000x writes", "Suspicious - writes typically 10x reads", "Depends on the feature"],
      "correct": 1,
      "explanation": "Social feeds are read-heavy: users scroll through many posts but create few. Typical ratio is 100:1 to 1000:1 reads:writes. 1:1 suggests misunderstanding the access pattern or wrong metrics."
    },
    {
      "id": "sanity-029",
      "type": "multiple-choice",
      "question": "Capacity plan: 'We need 1 TB of bandwidth per second for 1 billion video views per day.' Check?",
      "options": ["Reasonable for short-form video", "Too high by ~100x", "Too low by ~10x", "Need more info about video quality"],
      "correct": 3,
      "explanation": "1B views/day = 11,574 views/sec. If each view streams 10 MB (short video), that's 115 GB/s, not 1 TB/s. For 2-hour 4K movies at 20 GB each, concurrent streams matter more than daily views. Need video duration/quality."
    },
    {
      "id": "sanity-030",
      "type": "multiple-choice",
      "question": "Startup claims their ML model achieves 99.9% accuracy on fraud detection. Red flag?",
      "options": ["Impressive but achievable", "Red flag - likely overfit or wrong metric", "Normal for mature ML systems", "Depends on the dataset"],
      "correct": 1,
      "explanation": "99.9% accuracy on imbalanced data (fraud is rare) is often meaningless - predicting 'not fraud' always achieves >99% accuracy. Need precision/recall/F1. Also suggests possible overfitting on test data."
    },
    {
      "id": "sanity-031",
      "type": "multiple-choice",
      "question": "Estimate: 'Compression will reduce our 100 TB to 10 TB (10x ratio).' Reasonable for JSON logs?",
      "options": ["Reasonable, JSON compresses well", "Too optimistic - expect 3-5x for JSON", "Too pessimistic - JSON compresses 20x+", "Depends on compression algorithm"],
      "correct": 0,
      "explanation": "JSON logs compress very well due to repetitive keys and structure. 10x compression (90% reduction) is achievable with gzip/zstd for typical log data. Some achieve even higher ratios."
    },
    {
      "id": "sanity-032",
      "type": "multiple-choice",
      "question": "Design assumes all 1M users are online simultaneously for capacity planning. Issue?",
      "options": ["Correct for worst-case planning", "Over-provisioned - typical concurrency is 1-10% of total users", "Under-provisioned - should plan for 2M", "Depends on application type"],
      "correct": 1,
      "explanation": "Simultaneous online users are typically 1-10% of total users, depending on app type. 1M total users might mean 10K-100K concurrent. Planning for 100% concurrency wastes resources."
    },
    {
      "id": "sanity-033",
      "type": "multiple-choice",
      "question": "Latency budget: 'Database: 50ms, API processing: 50ms, Network: 0ms, Total: 100ms.' Missing component?",
      "options": ["Serialization/deserialization overhead", "All components accounted for", "Should include retry latency", "Missing database connection time"],
      "correct": 0,
      "explanation": "Serialization (JSON/protobuf encoding/decoding) often adds 1-10ms. Network is never 0ms (even localhost has overhead). Queue wait times, GC pauses, and connection overhead are also often missed."
    },
    {
      "id": "sanity-034",
      "type": "multiple-choice",
      "question": "Security estimate: 'Bcrypt password hashing at 100ms/hash, 1000 logins/second needs 100 seconds of CPU.' Implication?",
      "options": ["Need 100+ dedicated CPU cores for auth", "Can handle with 10 cores", "Should switch to faster hashing", "Async processing solves this"],
      "correct": 0,
      "explanation": "1000 logins/sec × 100ms/hash = 100 CPU-seconds of work per real second. You need 100+ cores just for password hashing. This is a real constraint that affects architecture (dedicated auth servers, rate limiting)."
    },
    {
      "id": "sanity-035",
      "type": "multiple-choice",
      "question": "Team says 'We'll just cache everything in Redis' for their 50 TB dataset. Problem?",
      "options": ["No problem with Redis Cluster", "Problem: cost-prohibitive, RAM is 10-30x more expensive than SSD", "No problem with proper eviction policy", "Problem only if data changes frequently"],
      "correct": 1,
      "explanation": "50 TB in RAM costs $500K-$1.5M in hardware vs $5-15K for SSD. Also requires massive cluster management. Cache the hot subset (typically 10-20% of data handles 80%+ of requests)."
    },
    {
      "id": "sanity-036",
      "type": "multiple-choice",
      "question": "Estimate: 'Our API gateway adds 0.1ms latency.' After adding auth, rate limiting, logging, and metrics. Realistic?",
      "options": ["Realistic for optimized gateways", "Unrealistic - each feature adds latency, expect 5-20ms", "Realistic with async logging", "Depends on implementation"],
      "correct": 1,
      "explanation": "Auth (JWT validation, token lookup), rate limiting (Redis call), logging, and metrics each add latency. A full-featured API gateway typically adds 5-20ms. 0.1ms is only achievable with minimal processing."
    },
    {
      "id": "sanity-037",
      "type": "multiple-choice",
      "question": "Design: 'We'll use eventual consistency and guarantee updates propagate in under 100ms globally.' Conflict?",
      "options": ["No conflict, modern systems achieve this", "Conflict: global propagation takes 100-300ms minimum (speed of light)", "No conflict with edge caching", "Conflict only for writes"],
      "correct": 1,
      "explanation": "Speed of light limits global RTT to 100-300ms minimum. 'Eventual' consistency that's guaranteed under 100ms globally is physically impossible. Either relax the time guarantee or use synchronous replication."
    },
    {
      "id": "sanity-038",
      "type": "multiple-choice",
      "question": "Cost estimate: '100 engineers × $200K salary = $20M/year in engineering costs.' What's missing?",
      "options": ["Nothing, salary is total cost", "Benefits, equipment, office space add 30-50%", "Should include management overhead", "Should subtract equity compensation"],
      "correct": 1,
      "explanation": "Total employee cost includes benefits (health, 401k), payroll taxes, equipment, office space, software licenses, and overhead. Rule of thumb: multiply salary by 1.3-1.5 for true cost."
    },
    {
      "id": "sanity-039",
      "type": "multiple-choice",
      "question": "Architect proposes synchronous writes to 5 datacenters for strong consistency. Latency concern?",
      "options": ["No concern with fast networks", "Major concern: must wait for slowest DC, likely 100-300ms", "Minor concern, can use quorum writes", "No concern if DCs are in same region"],
      "correct": 1,
      "explanation": "Synchronous writes wait for ALL replicas. With 5 global DCs, you wait for the slowest (likely 100-300ms RTT away). This makes sub-100ms writes impossible. Use quorum writes or async replication."
    },
    {
      "id": "sanity-040",
      "type": "multiple-choice",
      "question": "Load test shows 1ms average latency at 1K QPS. Team extrapolates to 1ms at 100K QPS. Valid?",
      "options": ["Valid if system is stateless", "Invalid - latency increases with load due to queueing", "Valid with horizontal scaling", "Invalid only if approaching capacity"],
      "correct": 1,
      "explanation": "Latency increases with load due to queueing theory (Little's Law). As utilization rises, queue wait times increase non-linearly. Must load test at target QPS, not extrapolate from low load."
    },
    {
      "id": "sanity-041",
      "type": "multiple-choice",
      "question": "Estimate: 'UUID is 32 characters = 32 bytes.' Correct?",
      "options": ["Correct", "Wrong - UUID is 36 characters with hyphens, but 16 bytes binary", "Wrong - UUID is 128 bytes", "Depends on encoding"],
      "correct": 1,
      "explanation": "UUID string is 36 characters (32 hex + 4 hyphens) = 36 bytes as string. But a UUID is fundamentally 128 bits = 16 bytes in binary. Store as binary (16 bytes) not string (36 bytes) when possible."
    },
    {
      "id": "sanity-042",
      "type": "multiple-choice",
      "question": "Team plans to retry failed requests 10 times with no backoff. During an outage, what happens?",
      "options": ["System recovers faster", "Retry storm amplifies load 10x, delays recovery", "Normal behavior for resilient systems", "Only problematic at scale"],
      "correct": 1,
      "explanation": "10 retries without backoff means each failed request generates 10x the load. During outages, this creates a 'retry storm' that overwhelms the recovering system. Always use exponential backoff with jitter."
    },
    {
      "id": "sanity-043",
      "type": "multiple-choice",
      "question": "Design assumes 'stateless' services can be scaled infinitely. What's the hidden bottleneck?",
      "options": ["No bottleneck if truly stateless", "Database becomes the bottleneck", "Network bandwidth limits", "CPU always limits eventually"],
      "correct": 1,
      "explanation": "Stateless services still depend on stateful backends (databases, caches). Scaling 100 stateless servers that all hit one database just moves the bottleneck. Must scale data tier too."
    },
    {
      "id": "sanity-044",
      "type": "multiple-choice",
      "question": "Estimate: 'S3 GET request cost is $0.0004. At 1B requests/month = $400K/month.' Is there a cheaper option?",
      "options": ["No, S3 is already cheapest", "Yes, CloudFront reduces GET costs with caching", "No, all object stores have similar pricing", "Yes, switch to blob storage"],
      "correct": 1,
      "explanation": "CDN caching (CloudFront, etc.) serves repeated requests from edge at lower cost (~$0.01/GB vs per-request). For cacheable content with repeated access, CDN dramatically reduces costs."
    },
    {
      "id": "sanity-045",
      "type": "multiple-choice",
      "question": "Proposal: 'Our ML pipeline processes data in real-time with 50ms latency.' Pipeline has 20 sequential steps. Feasible?",
      "options": ["Feasible with optimization", "Infeasible - 50ms ÷ 20 steps = 2.5ms/step is very tight", "Feasible with parallel processing", "Depends on step complexity"],
      "correct": 1,
      "explanation": "20 sequential steps in 50ms means 2.5ms average per step. Even without any I/O, this is very tight. Any single slow step breaks the budget. Consider parallelizing or reducing steps."
    },
    {
      "id": "sanity-046",
      "type": "multiple-choice",
      "question": "Design claims 'zero downtime deployments' with a single database server. Gap?",
      "options": ["No gap with blue-green deployment", "Gap: database migrations can require downtime", "No gap with rolling deployments", "Gap only for schema changes"],
      "correct": 1,
      "explanation": "Zero downtime requires handling database schema changes without locking. Single DB is a bottleneck: ALTER TABLE can lock tables, breaking zero-downtime claims. Need online schema migration tools."
    },
    {
      "id": "sanity-047",
      "type": "multiple-choice",
      "question": "Cost model: 'Serverless at $0.20/million invocations beats servers at $100/month.' At what volume does this flip?",
      "options": ["Never flips, serverless always cheaper", "~500M invocations/month", "~50M invocations/month", "~5M invocations/month"],
      "correct": 1,
      "explanation": "$100/month ÷ $0.20/million = 500M invocations breakeven. Below 500M/month, serverless is cheaper. Above, dedicated servers win. But also consider compute time costs, not just invocations."
    },
    {
      "id": "sanity-048",
      "type": "multiple-choice",
      "question": "Teammate estimates 'JSON parsing is essentially free.' Their service parses 100K JSON docs/second, each 10 KB. Concern?",
      "options": ["No concern, JSON parsing is fast", "Concern: 1 GB/sec of parsing has real CPU cost", "No concern with modern hardware", "Concern only for nested JSON"],
      "correct": 1,
      "explanation": "100K × 10 KB = 1 GB/second of JSON parsing. Fast parsers achieve 1-2 GB/sec per core. This could consume an entire CPU core just for parsing. Not free at scale."
    },
    {
      "id": "sanity-049",
      "type": "multiple-choice",
      "question": "Plan: 'We'll handle 100K WebSocket connections per server.' Realistic for commodity hardware?",
      "options": ["Realistic with epoll/kqueue", "Unrealistic - typical limit is 10K per server", "Realistic only with specialized hardware", "Unrealistic - memory alone is prohibitive"],
      "correct": 0,
      "explanation": "With proper async I/O (epoll/kqueue), 100K-1M connections per server is achievable. Each connection uses ~10-20 KB RAM (socket buffers). 100K × 20 KB = 2 GB RAM - manageable."
    },
    {
      "id": "sanity-050",
      "type": "multiple-choice",
      "question": "Estimate assumes 'database read replicas have zero replication lag.' When does this break down?",
      "options": ["Only under extreme write load", "Always - async replication inherently has lag (typically ms-seconds)", "Never with synchronous replication", "Only during network partitions"],
      "correct": 1,
      "explanation": "Async replication always has some lag (often ms, sometimes seconds during load). Read-after-write consistency requires reading from primary or accepting stale reads. 'Zero lag' is misleading."
    },
    {
      "id": "sanity-051",
      "type": "multiple-choice",
      "question": "Capacity plan for image CDN: '1 PB storage, 10 Gbps bandwidth.' If average image is 100 KB, what's daily serve capacity?",
      "options": ["~1 billion images/day", "~10 billion images/day", "~100 million images/day", "Need concurrent connection info"],
      "correct": 0,
      "explanation": "10 Gbps = 1.25 GB/s. At 100 KB/image: 1,250 MB/s ÷ 0.1 MB = 12,500 images/second = 1.08B images/day. Storage of 1 PB holds 10B images. Bandwidth is likely the constraint."
    },
    {
      "id": "sanity-052",
      "type": "multiple-choice",
      "question": "Design doc claims 'our service is horizontally scalable - just add more servers.' What needs verification?",
      "options": ["Nothing, horizontal scaling is automatic", "Whether shared state exists (database, cache)", "Only network topology", "Only load balancer capacity"],
      "correct": 1,
      "explanation": "True horizontal scaling requires no shared mutable state between servers. Verify: Is there a shared database? Shared cache? Session affinity? Distributed locks? Any of these can bottleneck scaling."
    },
    {
      "id": "sanity-053",
      "type": "multiple-choice",
      "question": "Estimate: 'gRPC will be faster than REST because binary protocol.' Always true?",
      "options": ["Yes, binary is always faster than text", "Not always - small payloads show minimal difference", "Yes, but only for streaming", "Not always - REST over HTTP/2 closes the gap"],
      "correct": 1,
      "explanation": "For small payloads, protobuf vs JSON difference is minimal. HTTP/2 provides multiplexing for both. gRPC shines for: streaming, large payloads, strict schemas. Small request/response? Difference is often negligible."
    },
    {
      "id": "sanity-054",
      "type": "multiple-choice",
      "question": "Team plans 30-day data retention. Deletes run weekly. What's the actual retention?",
      "options": ["30 days as planned", "30-37 days depending on timing", "23-30 days depending on timing", "Exactly 28 days (4 weeks)"],
      "correct": 1,
      "explanation": "Weekly deletion of 30-day old data means data survives 30-37 days. Data created right after a deletion run survives until next week's run: 30 days + up to 7 days = 37 days max."
    },
    {
      "id": "sanity-055",
      "type": "multiple-choice",
      "question": "Load balancer health check: ping every 10 seconds, mark unhealthy after 3 failures. How long until unhealthy server is removed?",
      "options": ["~10 seconds", "~30 seconds", "~60 seconds", "~90 seconds"],
      "correct": 1,
      "explanation": "3 failures × 10 seconds between checks = 30 seconds minimum to detect failure. Add time for actual removal and propagation. Meanwhile, traffic continues to failing server. Consider faster checks."
    },
    {
      "id": "sanity-056",
      "type": "multiple-choice",
      "question": "Estimate: 'Our caching layer has 99% hit rate, so origin handles 1% of traffic.' At 10M QPS total, origin load is:",
      "options": ["100 QPS", "1,000 QPS", "10,000 QPS", "100,000 QPS"],
      "correct": 3,
      "explanation": "1% of 10M = 100,000 QPS to origin. Even with 99% cache hit rate, 100K QPS is substantial load. High cache hit rates at scale still mean significant origin traffic."
    },
    {
      "id": "sanity-057",
      "type": "multiple-choice",
      "question": "Proposal: 'We'll use microservices for our 3-person team.' Concern?",
      "options": ["Good architecture for any team size", "Overhead may exceed benefits for small teams", "Only concerning without DevOps expertise", "No concern with managed Kubernetes"],
      "correct": 1,
      "explanation": "Microservices add deployment, monitoring, and debugging complexity. A 3-person team may spend more time on infrastructure than features. Monolith is often better until team/scale justify the split."
    },
    {
      "id": "sanity-058",
      "type": "multiple-choice",
      "question": "Design assumes 'network is reliable' for distributed transactions. What's the actual packet loss rate in datacenters?",
      "options": ["Effectively 0% in modern datacenters", "0.01-0.1% is typical", "1-5% is typical", "Varies too much to estimate"],
      "correct": 1,
      "explanation": "Even in well-run datacenters, packet loss is 0.01-0.1%. At 10K QPS, that's 1-10 failures/second. Design must handle network failures, timeouts, and retries. 'Reliable network' is a distributed systems fallacy."
    },
    {
      "id": "sanity-059",
      "type": "multiple-choice",
      "question": "Monitoring plan: 'We'll sample 1% of requests for distributed tracing.' At 100K QPS, traces per second?",
      "options": ["10 traces/second", "100 traces/second", "1,000 traces/second", "10,000 traces/second"],
      "correct": 2,
      "explanation": "1% of 100K = 1,000 traces/second. Each trace has multiple spans. At 10 spans/trace, that's 10K spans/second to store and query. Ensure tracing backend can handle this volume."
    },
    {
      "id": "sanity-060",
      "type": "multiple-choice",
      "question": "Cost estimate: 'Free tier covers our needs.' Product has 100K users sending 100 requests/day each. Red flag for which service?",
      "options": ["Compute (likely within free tier)", "Database queries (likely within free tier)", "Email notifications (likely exceeds free tier)", "All likely within free tier"],
      "correct": 2,
      "explanation": "Email services typically have 100-1000 emails/day free tier. 100K users × even 1 email/day = 100K emails. Most free tiers cap at 100-1000/day. Email, SMS, and push notifications often exceed free tiers."
    },
    {
      "id": "sanity-061",
      "type": "multiple-choice",
      "question": "Teammate says 'Kubernetes will handle our scaling automatically.' What does Kubernetes NOT handle?",
      "options": ["Pod autoscaling", "Load balancing", "Database scaling", "Container health checks"],
      "correct": 2,
      "explanation": "Kubernetes autoscales stateless pods well. But databases, caches, and stateful services require manual capacity planning. You can't just 'add pods' to PostgreSQL. Stateful scaling remains hard."
    },
    {
      "id": "sanity-062",
      "type": "multiple-choice",
      "question": "Estimate: 'Base64 encoding adds 33% overhead.' When storing 1 GB of binary data as Base64, total size?",
      "options": ["~1 GB (negligible overhead)", "~1.33 GB", "~2 GB", "Depends on content"],
      "correct": 1,
      "explanation": "Base64 encodes 3 bytes as 4 characters = 33% overhead. 1 GB binary becomes ~1.33 GB Base64. For large binary data, this overhead matters. Consider binary storage or compression."
    },
    {
      "id": "sanity-063",
      "type": "multiple-choice",
      "question": "Design claims 'consistent hashing solves all data distribution problems.' Limitation?",
      "options": ["Only works for key-value data", "Hotspots when keys are skewed", "Requires prime number of nodes", "Only works for reads"],
      "correct": 1,
      "explanation": "Consistent hashing distributes uniformly across nodes IF keys are uniformly distributed. Hot keys (celebrity users, viral content) still cause hotspots regardless of hashing. Need additional strategies."
    },
    {
      "id": "sanity-064",
      "type": "multiple-choice",
      "question": "Proposal: 'We'll achieve sub-millisecond latency by moving to GraphQL.' Likely outcome?",
      "options": ["Significant latency reduction", "Latency unchanged or worse - GraphQL adds parsing overhead", "Slight improvement from reduced payload", "Depends on query complexity"],
      "correct": 1,
      "explanation": "GraphQL adds query parsing, validation, and resolution overhead. It reduces over-fetching but doesn't inherently reduce latency. For latency, database/network dominate. GraphQL is about flexibility, not speed."
    },
    {
      "id": "sanity-065",
      "type": "multiple-choice",
      "question": "Estimate: 'Feature flags are free - just IF statements.' At 10K QPS with 100 flags evaluated per request, what's the load?",
      "options": ["Negligible - just CPU cycles", "1M flag evaluations/second - noticeable if flags hit database", "Only matters for A/B test flags", "Depends on flag complexity"],
      "correct": 1,
      "explanation": "10K × 100 = 1M flag evaluations/second. If flags are in-memory, fast. If they hit a remote service for each evaluation, you've added 1M external calls/second. Ensure flags are cached locally."
    },
    {
      "id": "sanity-066",
      "type": "multiple-choice",
      "question": "Backup strategy: 'Daily backups with 7-day retention.' If corruption is discovered 10 days after it started, recovery options?",
      "options": ["Restore from any backup within 7 days", "No clean backup available - corruption propagated to all backups", "Point-in-time recovery solves this", "Incremental backup helps"],
      "correct": 1,
      "explanation": "If corruption started 10 days ago and backups retain 7 days, ALL backups contain corrupted data. Need longer retention or different backup verification strategy to catch silent corruption."
    },
    {
      "id": "sanity-067",
      "type": "multiple-choice",
      "question": "Design: 'Users can upload files up to 100 GB.' Via standard HTTP upload. Problem?",
      "options": ["No problem with chunked encoding", "Problem: HTTP timeouts, connection resets over long uploads", "No problem with keep-alive", "Problem only on mobile networks"],
      "correct": 1,
      "explanation": "100 GB upload at 10 MB/s = 2.7 hours. Network interruptions, proxy timeouts, and connection limits make single HTTP uploads unreliable for large files. Need resumable/chunked upload protocol."
    },
    {
      "id": "sanity-068",
      "type": "multiple-choice",
      "question": "Estimate: 'Adding an index will speed up all queries on that column.' Counter-example?",
      "options": ["No counter-example, indexes always help reads", "Write-heavy workloads slow down (index maintenance)", "Only fails for very small tables", "Only fails for composite indexes"],
      "correct": 1,
      "explanation": "Indexes speed reads but slow writes. Each INSERT/UPDATE must update all indexes. For write-heavy tables, index overhead can outweigh read benefits. Also, full table scans beat indexes for low selectivity."
    },
    {
      "id": "sanity-069",
      "type": "multiple-choice",
      "question": "Monitoring: 'CPU <50% means we have plenty of headroom.' During incident, CPU is 40% but latency spiked. What else to check?",
      "options": ["Network bandwidth", "Memory/swap usage", "Disk I/O wait", "All of the above"],
      "correct": 3,
      "explanation": "Low CPU doesn't mean no bottleneck. Services can be I/O bound (waiting on disk, network, external services) or memory bound (swapping). Check: disk I/O, network I/O, memory, and external dependencies."
    },
    {
      "id": "sanity-070",
      "type": "multiple-choice",
      "question": "Team estimates 'We need 2x capacity for peak.' Peak is Black Friday at 20x normal. Issue?",
      "options": ["2x is sufficient with autoscaling", "2x is 10x short of 20x peak capacity needed", "2x is reasonable with queuing", "Depends on acceptable degradation"],
      "correct": 1,
      "explanation": "If peak is 20x normal, you need infrastructure that can handle 20x, not 2x. Autoscaling helps but has limits and spin-up time. Either over-provision or explicitly plan for graceful degradation at peak."
    },
    {
      "id": "sanity-071",
      "type": "multiple-choice",
      "question": "Assumption: 'Timestamps are unique identifiers.' When does this fail?",
      "options": ["Never, with nanosecond precision", "Multiple events in same millisecond", "Only with distributed systems", "Only without synchronized clocks"],
      "correct": 1,
      "explanation": "At high throughput, multiple events occur in the same millisecond (or even nanosecond with concurrent threads). Timestamps need additional disambiguation: sequence numbers, random components, or UUIDs."
    },
    {
      "id": "sanity-072",
      "type": "multiple-choice",
      "question": "Estimate: 'With 1000 database connections, we can handle 1000 concurrent queries.' Actual effective parallelism?",
      "options": ["1000 parallel queries", "Limited by CPU cores (typically 8-64)", "500 due to overhead", "Depends on query type"],
      "correct": 1,
      "explanation": "Database can only execute as many queries in parallel as it has CPU cores. 1000 connections mostly means 1000 queries can wait simultaneously, not execute simultaneously. IO-bound queries may achieve more concurrency."
    },
    {
      "id": "sanity-073",
      "type": "multiple-choice",
      "question": "Cost projection: 'Traffic grows 10%/month, costs grow 10%/month.' Hidden assumption?",
      "options": ["Correct proportional scaling", "Assumes no efficiency improvements", "Assumes no volume discounts", "Assumes no tier pricing (costs often sublinear with scale)"],
      "correct": 3,
      "explanation": "Cloud pricing often has tiers: cost/unit decreases at higher volumes. Also, fixed costs (baseline infrastructure) don't scale with traffic. Actual cost growth is often sublinear with traffic growth."
    },
    {
      "id": "sanity-074",
      "type": "multiple-choice",
      "question": "Design: 'Saga pattern for distributed transactions with 10 services.' Failure at step 7 requires:",
      "options": ["Rollback steps 1-7", "Compensating transactions for steps 1-6", "Only retry step 7", "No action needed with eventual consistency"],
      "correct": 1,
      "explanation": "Sagas use compensating transactions, not rollback. Steps 1-6 completed and committed. Step 7 failure requires running compensation for 6, 5, 4, 3, 2, 1 in reverse. Complex to implement correctly."
    },
    {
      "id": "sanity-075",
      "type": "multiple-choice",
      "question": "Estimate: 'Encrypting at rest is free with modern CPUs (AES-NI).' Storage performance impact?",
      "options": ["Near zero (<1%)", "~5-10% overhead", "~25-50% overhead", "Depends on block size"],
      "correct": 0,
      "explanation": "AES-NI achieves ~5-10 GB/s encryption, faster than most storage. Encryption at rest with modern CPUs adds <1% overhead for storage-bound workloads. But key management adds operational complexity."
    },
    {
      "id": "sanity-076",
      "type": "multiple-choice",
      "question": "QA asks: 'Why does staging not match production?' Staging has 10% of production data. Missing concern?",
      "options": ["Missing data variety", "Database query plans differ with different data sizes", "Missing edge cases", "All of the above"],
      "correct": 3,
      "explanation": "10% data affects: query plans (optimizer chooses differently), caching behavior, pagination edge cases, and missing data variety/edge cases. Performance tests on small data don't reflect production."
    },
    {
      "id": "sanity-077",
      "type": "multiple-choice",
      "question": "Team plans to store 'unlimited message history.' At 1M users × 100 messages/day × 1 KB each, daily growth?",
      "options": ["~100 MB", "~1 GB", "~100 GB", "~1 TB"],
      "correct": 2,
      "explanation": "1M × 100 × 1 KB = 100M KB = 100 GB/day = 36 TB/year. 'Unlimited' history at this rate becomes expensive fast. Consider archival tiers, compression, or retention limits."
    },
    {
      "id": "sanity-078",
      "type": "multiple-choice",
      "question": "Design: 'We'll use UUIDs as primary keys for global uniqueness.' Performance consideration?",
      "options": ["No concerns with UUIDs", "Random UUIDs cause index fragmentation and slower inserts", "UUIDs are slower to compare", "UUID storage size is the only concern"],
      "correct": 1,
      "explanation": "Random UUIDs (v4) scatter inserts across index pages, causing fragmentation and slower writes. Solutions: UUID v7 (time-sorted), ULID, or sequential IDs with sharding for global uniqueness."
    },
    {
      "id": "sanity-079",
      "type": "multiple-choice",
      "question": "Estimate: 'Moving to ARM instances saves 40% on compute costs.' When might savings not materialize?",
      "options": ["Always saves money if workload runs on ARM", "When dependencies don't support ARM", "When compute isn't the primary cost", "Both B and C"],
      "correct": 3,
      "explanation": "ARM savings require: 1) all dependencies have ARM builds, 2) compute is a significant cost (if database or bandwidth dominates, 40% compute savings is minor). Rebuild/testing effort also has costs."
    },
    {
      "id": "sanity-080",
      "type": "multiple-choice",
      "question": "Load test: 'System handles 10K QPS with 50ms p50 latency.' What additional metric is critical for capacity?",
      "options": ["p99 latency", "Error rate", "CPU utilization", "All of the above"],
      "correct": 3,
      "explanation": "p50 hides tail latency (p99 might be 5 seconds). Error rate shows if 10K QPS is actually successful. CPU utilization shows headroom. All three are needed to assess true capacity."
    },
    {
      "id": "sanity-081",
      "type": "multiple-choice",
      "question": "Plan: 'Shard by user_id for even distribution.' User with 1M followers posts. Impact?",
      "options": ["Even distribution handles this fine", "Fan-out to 1M follower shards creates hotspot", "No impact with good sharding", "Impacts only write performance"],
      "correct": 1,
      "explanation": "User-ID sharding distributes users evenly, but a celebrity post fans out to millions of followers across many shards. This creates read/write amplification and hotspots in the fan-out path."
    },
    {
      "id": "sanity-082",
      "type": "multiple-choice",
      "question": "Estimate: 'Connection pool of 100 handles all load.' Each request holds connection for 50ms. Max sustainable QPS?",
      "options": ["100 QPS", "1,000 QPS", "2,000 QPS", "10,000 QPS"],
      "correct": 2,
      "explanation": "Each connection handles 1000ms/50ms = 20 requests/second. 100 connections × 20 req/sec = 2,000 QPS maximum. Beyond this, requests wait for connections, increasing latency."
    },
    {
      "id": "sanity-083",
      "type": "multiple-choice",
      "question": "Design assumes 'DynamoDB handles any scale.' What requires additional planning?",
      "options": ["Nothing, DynamoDB auto-scales", "Hot partitions still cause throttling", "Only write capacity needs planning", "Only read capacity needs planning"],
      "correct": 1,
      "explanation": "DynamoDB auto-scales total capacity but has per-partition limits (~3K RCU, 1K WCU). Hot keys hitting one partition get throttled regardless of total capacity. Partition key design is critical."
    },
    {
      "id": "sanity-084",
      "type": "multiple-choice",
      "question": "Test environment has 100% automated test coverage. 'No bugs will reach production.' Issue with this claim?",
      "options": ["No issue with 100% coverage", "Coverage measures lines, not logic paths or edge cases", "Only integration tests matter", "Manual testing is still needed"],
      "correct": 1,
      "explanation": "100% line coverage doesn't mean 100% tested scenarios. Edge cases, timing issues, integration problems, and unexpected inputs aren't caught by coverage metrics. Coverage is necessary but not sufficient."
    },
    {
      "id": "sanity-085",
      "type": "multiple-choice",
      "question": "Estimate: 'JWT tokens are stateless, so auth scales infinitely.' Hidden state?",
      "options": ["No hidden state with JWTs", "Token revocation requires state (blocklist)", "Only refresh tokens have state", "State is in the token itself"],
      "correct": 1,
      "explanation": "JWTs are stateless for validation, but revocation requires checking a blocklist. Logout, password change, or security incidents need immediate invalidation, requiring shared state (Redis blocklist, etc.)."
    },
    {
      "id": "sanity-086",
      "type": "multiple-choice",
      "question": "Design: 'Multi-tenant with shared database, isolated by tenant_id column.' Security concern?",
      "options": ["No concern with proper access control", "One missing WHERE clause exposes all tenants", "Only performance is concerning", "Only backup/restore is affected"],
      "correct": 1,
      "explanation": "Shared-database multi-tenancy relies on every query having correct tenant_id filter. One bug (missing WHERE, wrong join) can expose or corrupt other tenants' data. Defense in depth recommended."
    },
    {
      "id": "sanity-087",
      "type": "multiple-choice",
      "question": "Teammate: 'We don't need rate limiting, we trust our clients.' Service is internal API. Gap?",
      "options": ["No gap for internal services", "Bugs/loops in clients can create self-inflicted DDoS", "Rate limiting only needed externally", "Trust but verify is overkill internally"],
      "correct": 1,
      "explanation": "Internal clients have bugs: infinite loops, retry storms, batch jobs gone wrong. Rate limiting protects against both malicious and accidental overload. Internal != safe."
    },
    {
      "id": "sanity-088",
      "type": "multiple-choice",
      "question": "Estimate: 'Switching from PostgreSQL to MongoDB will improve performance.' When is this likely false?",
      "options": ["When data is relational with complex joins", "When transactions span multiple documents", "When current issues are query optimization, not database type", "All of the above"],
      "correct": 3,
      "explanation": "Database switches rarely fix performance issues caused by bad schema design, missing indexes, or unoptimized queries. The same problems often follow. Identify root cause before switching databases."
    },
    {
      "id": "sanity-089",
      "type": "multiple-choice",
      "question": "Plan: 'Use client-side caching to reduce server load by 80%.' What happens during cache stampede?",
      "options": ["80% reduction maintained", "All clients simultaneously hit server when cache expires", "Cache naturally spreads load", "Server handles gracefully"],
      "correct": 1,
      "explanation": "If all clients cache the same data with same TTL, cache expires simultaneously for everyone. Instant spike to server. Use jitter in TTL, staggered refresh, or lock-based refresh strategies."
    },
    {
      "id": "sanity-090",
      "type": "multiple-choice",
      "question": "Capacity: 'Peak is 3x average, so provision for 3x.' Average is 10K QPS. What's wrong with provisioning for exactly 30K QPS?",
      "options": ["Nothing wrong, correct calculation", "No headroom for growth or unexpected spikes", "3x is usually overprovisioned", "Should use p99 not average"],
      "correct": 1,
      "explanation": "Provisioning exactly at expected peak leaves no margin for: growth, unexpected spikes beyond forecast, or handling failover from other instances. Typically provision for 1.5-2x expected peak."
    },
    {
      "id": "sanity-091",
      "type": "multiple-choice",
      "question": "Design: 'Async processing via job queue. Jobs complete in ~1 minute.' Queue depth alert set at 1,000 jobs. At 100 jobs/second ingest, how fast does alert trigger?",
      "options": ["~10 seconds", "~100 seconds", "Never triggers if processing keeps up", "Depends on worker count"],
      "correct": 3,
      "explanation": "If workers process 100 jobs/sec (matching ingest), queue stays near zero. If workers process 90 jobs/sec, queue grows 10 jobs/sec, hitting 1000 in 100 seconds. Worker capacity determines queue growth."
    },
    {
      "id": "sanity-092",
      "type": "multiple-choice",
      "question": "Estimate: 'Feature takes 1 sprint to build, 0 sprints to maintain.' Realistic for a new microservice?",
      "options": ["Realistic for simple features", "Unrealistic - maintenance is ongoing (20-40% of dev time)", "Realistic with good testing", "Depends on service complexity"],
      "correct": 1,
      "explanation": "Maintenance is never zero: bug fixes, dependency updates, scaling, on-call support, documentation. Industry average is 20-40% of engineering time on maintenance. Plan for it."
    },
    {
      "id": "sanity-093",
      "type": "multiple-choice",
      "question": "Teammate claims: 'Our p50 latency is 20ms, so user experience is good.' What's missing?",
      "options": ["p50 is the right metric for UX", "p95/p99 - half of users experience worse than p50", "Only average matters for UX", "Need p50 for each endpoint"],
      "correct": 1,
      "explanation": "p50 means 50% of requests are faster. But 50% are slower - potentially much slower. Users notice the slow requests. p99 (99th percentile) better represents worst-case user experience."
    },
    {
      "id": "sanity-094",
      "type": "multiple-choice",
      "question": "Design: 'Store images as base64 in database for simplicity.' At 1M images × 1 MB each, base64 storage?",
      "options": ["~1 TB", "~1.33 TB", "~4 TB (with DB overhead)", "~10 TB"],
      "correct": 2,
      "explanation": "1 MB × 1.33 (base64) × 1M = 1.33 TB raw. But databases add overhead: row storage, indexes, WAL logs, TOAST tables. Actual DB footprint can be 2-3x. Also kills query performance. Use object storage."
    },
    {
      "id": "sanity-095",
      "type": "multiple-choice",
      "question": "Reliability: 'We have 3 replicas, so we can lose 2 and stay up.' With consensus (Raft/Paxos), what's actually tolerable?",
      "options": ["Lose 2, keep 1 running", "Lose 1, need 2 for quorum", "Lose any number, 1 is enough", "Depends on configuration"],
      "correct": 1,
      "explanation": "Consensus requires majority quorum. With 3 nodes, quorum is 2. Lose 1 node: 2 remain (can reach consensus). Lose 2 nodes: 1 remains (no quorum, cluster unavailable). For 2 failure tolerance, need 5 nodes."
    },
    {
      "id": "sanity-096",
      "type": "multiple-choice",
      "question": "Cost model: 'Reserved instances save 60%, so reserve everything.' Downside?",
      "options": ["No downside if you can predict usage", "Locked to instance type; can't optimize later", "Locked to capacity even if needs decrease", "Both B and C"],
      "correct": 3,
      "explanation": "Reserved instances commit you to specific instance types and capacity. If better instances launch, needs change, or architecture shifts, you're stuck paying for unused capacity. Reserve baseline, use on-demand for variable."
    },
    {
      "id": "sanity-097",
      "type": "multiple-choice",
      "question": "Design: 'Daily batch job at midnight.' Global users. Issue?",
      "options": ["No issue, midnight is low-traffic", "Midnight differs by timezone - always peak somewhere", "Only affects reporting freshness", "Batch timing doesn't impact users"],
      "correct": 1,
      "explanation": "For global services, there's no universal 'off-hours.' Midnight PST is 8AM in London, 5PM in Tokyo. Batch jobs may hit users' peak hours. Also, batch completion time affects different regions differently."
    },
    {
      "id": "sanity-098",
      "type": "multiple-choice",
      "question": "Estimate: 'We need 99.99% availability. Budget: $10K/month for infrastructure.' Achievable?",
      "options": ["Achievable with smart architecture", "Likely insufficient - 4 nines requires significant redundancy", "Achievable with managed services", "Depends entirely on traffic"],
      "correct": 1,
      "explanation": "99.99% (52 min/year downtime) needs: multi-AZ/region redundancy, automated failover, redundant databases, monitoring, and on-call support. $10K/month is tight for meaningful redundancy at most scales."
    },
    {
      "id": "sanity-099",
      "type": "multiple-choice",
      "question": "Log analysis: 'Error rate is 0.01%, system is healthy.' 10M requests/day. Daily error count?",
      "options": ["~10 errors", "~100 errors", "~1,000 errors", "~10,000 errors"],
      "correct": 2,
      "explanation": "0.01% of 10M = 1,000 errors/day. That's 1000 potentially frustrated users or hidden bugs. Low percentage can still mean high absolute numbers. Always look at both rate and count."
    },
    {
      "id": "sanity-100",
      "type": "multiple-choice",
      "question": "Final check: teammate's estimate has 10 assumptions, each 90% likely to be correct. Probability all are correct?",
      "options": ["~90%", "~65%", "~35%", "~50%"],
      "correct": 2,
      "explanation": "0.9^10 = 0.349 ≈ 35%. With 10 independent assumptions at 90% confidence each, you have only 35% confidence in the combined estimate. Small uncertainties compound. Validate critical assumptions."
    }
  ]
}
