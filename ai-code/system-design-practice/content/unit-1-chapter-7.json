{
  "unit": 1,
  "unitTitle": "Estimation",
  "chapter": 7,
  "chapterTitle": "Reasonableness Checks",
  "chapterDescription": "Validating estimates, spotting red flags, and checking calculations against real-world constraints",
  "problems": [
    {
      "id": "sanity-001",
      "type": "multiple-choice",
      "question": "A teammate estimates their new service will need 1 PB of RAM. What's the most likely issue?",
      "options": [
        "Confusing RAM with disk storage",
        "Correct for a large-scale service",
        "Underestimating by 10x",
        "Using wrong units (should be EB)"
      ],
      "correct": 0,
      "explanation": "1 PB of RAM would cost ~$10B and require thousands of servers. Even Google Search doesn't use this much RAM for a single service. They likely mean disk storage.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-002",
      "type": "multiple-choice",
      "question": "Estimate says a single MySQL instance will handle 1M QPS. Is this reasonable?",
      "options": [
        "Yes, with proper indexing",
        "No, typical limit is 10K-50K QPS",
        "No, typical limit is 100-1K QPS",
        "Yes, with read replicas"
      ],
      "correct": 1,
      "explanation": "A well-tuned MySQL instance handles 10K-50K QPS for simple queries. 1M QPS requires sharding across many instances. This estimate is off by ~20-100x.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-003",
      "type": "multiple-choice",
      "question": "Design doc claims 99.999% availability with a single-region deployment. Red flag?",
      "options": [
        "No, achievable with redundant servers",
        "Yes, single region can't survive regional outages",
        "No, cloud providers guarantee this",
        "Depends on the database choice"
      ],
      "correct": 1,
      "explanation": "Five 9s (5.26 min/year downtime) requires surviving regional failures. A single region deployment has a single point of failure for the entire region. Multi-region is required.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-004",
      "type": "multiple-choice",
      "question": "Estimate: 'We'll store 1 billion user profiles at 10 KB each = 10 TB.' Sanity check?",
      "options": [
        "Math is correct",
        "Off by 1000x (should be 10 PB)",
        "Off by 10x (should be 100 TB)",
        "Off by 1000x (should be 10 GB)"
      ],
      "correct": 0,
      "explanation": "1B × 10 KB = 10B KB = 10 TB. The math checks out. This is a reasonable size for a large user database.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-005",
      "type": "multiple-choice",
      "question": "Team claims their API p99 latency is 1ms with a cross-country database call. Issue?",
      "options": [
        "Plausible with edge caching",
        "Impossible - network RTT alone is 30-70ms",
        "Correct if using TCP fast open",
        "Plausible with connection pooling"
      ],
      "correct": 1,
      "explanation": "Cross-country (e.g., NYC to LA) network RTT is 30-70ms minimum due to speed of light. 1ms p99 is physically impossible without local caching.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-006",
      "type": "multiple-choice",
      "question": "Capacity plan shows 100 servers handling 10M QPS total. Per-server load reasonable?",
      "options": [
        "Yes, 100K QPS per server is typical",
        "No, that's 100K QPS per server - too high for most apps",
        "No, that's only 1K QPS per server - underutilized",
        "Depends entirely on the workload"
      ],
      "correct": 0,
      "explanation": "10M ÷ 100 = 100K QPS per server. For simple stateless web servers with fast backends, this is achievable. For complex processing, it might be high. Context matters, but it's in the reasonable range.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-007",
      "type": "multiple-choice",
      "question": "Estimate claims 1 TB/second of network throughput from a single server. Valid?",
      "options": [
        "Yes, with 100 Gbps NIC",
        "No, fastest NICs are ~400 Gbps = 50 GB/s",
        "Yes, with NVMe storage",
        "No, but achievable with kernel bypass"
      ],
      "correct": 1,
      "explanation": "1 TB/s = 8 Tbps. The fastest commodity NICs are 400 Gbps (~50 GB/s). 1 TB/s would need 20+ of the fastest NICs. This is off by ~20x.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "sanity-008",
      "type": "multiple-choice",
      "question": "Design proposes storing 10 years of logs at 1 GB/day. Total storage estimate of 3.6 TB. Check?",
      "options": [
        "Correct",
        "Wrong - should be ~36 TB",
        "Wrong - should be ~365 GB",
        "Wrong - should be ~3.6 PB"
      ],
      "correct": 0,
      "explanation": "10 years × 365 days × 1 GB = 3,650 GB ≈ 3.6 TB. The estimate is correct.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-009",
      "type": "multiple-choice",
      "question": "Proposal: 'Each user uploads 1 photo/day. 100M users = 100M photos/day = 1,157 photos/second.' Sanity check?",
      "options": [
        "Math is correct",
        "Wrong - should be ~11,574/second",
        "Wrong - should be ~115/second",
        "Wrong - should be ~1.16M/second"
      ],
      "correct": 0,
      "explanation": "100M ÷ 86,400 seconds = 1,157 photos/second. The math checks out.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-010",
      "type": "multiple-choice",
      "question": "Teammate says Redis will handle their 10 TB dataset in memory. Concern?",
      "options": [
        "No concern, Redis scales horizontally",
        "Major concern - largest instances are ~500 GB RAM",
        "No concern with Redis Cluster",
        "Minor concern - may need 2-3 instances"
      ],
      "correct": 1,
      "explanation": "10 TB in-memory requires enormous hardware. The largest cloud instances have ~500 GB-1 TB RAM. You'd need 10-20+ of the largest instances. Consider if all data needs to be in memory.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-011",
      "type": "multiple-choice",
      "question": "Estimate: 'P99 latency of 500ms is fine for our real-time chat app.' Red flag?",
      "options": [
        "Acceptable for non-critical features",
        "Major red flag - users expect <100ms for chat",
        "Fine if most requests are faster",
        "Acceptable for mobile users"
      ],
      "correct": 1,
      "explanation": "Real-time chat users expect near-instant delivery. 500ms p99 means 1% of messages take half a second - very noticeable. Target should be <100ms p99.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-012",
      "type": "multiple-choice",
      "question": "Design claims a single Kafka broker will handle 10M messages/second. Reasonable?",
      "options": [
        "Yes, Kafka is designed for this scale",
        "No, typical limit is 100K-500K msg/sec per broker",
        "No, typical limit is 1K-10K msg/sec per broker",
        "Yes, with proper partitioning"
      ],
      "correct": 1,
      "explanation": "A single Kafka broker typically handles 100K-500K messages/second depending on message size. 10M msg/sec needs 20-100 brokers. Off by ~20-100x.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-013",
      "type": "multiple-choice",
      "question": "Cost estimate: '$0.10/GB storage × 1 PB = $100,000/month.' Check the math.",
      "options": [
        "Correct",
        "Wrong - should be $100M/month",
        "Wrong - should be $10,000/month",
        "Wrong - should be $1M/month"
      ],
      "correct": 0,
      "explanation": "1 PB = 1,000,000 GB. $0.10 × 1,000,000 = $100,000/month. Math is correct. (Though $0.10/GB is high - S3 standard is ~$0.023/GB.)",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-014",
      "type": "multiple-choice",
      "question": "Architect claims their service processes 1 request in 10 nanoseconds. Believable?",
      "options": [
        "Yes, with optimized code",
        "No, a single L1 cache access takes ~1ns",
        "Yes, with FPGA acceleration",
        "No, context switches alone take longer"
      ],
      "correct": 1,
      "explanation": "10ns is only ~30 CPU cycles. Even reading from L1 cache takes 1ns, L2 takes 4ns. A complete request/response with any I/O or logic is impossible in 10ns. Minimum realistic is microseconds.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-015",
      "type": "multiple-choice",
      "question": "Proposal estimates 10 engineers can build and operate a global distributed database in 6 months. Realistic?",
      "options": [
        "Realistic for experienced team",
        "Unrealistic - takes 50+ engineers years",
        "Realistic if using existing frameworks",
        "Depends on feature set"
      ],
      "correct": 1,
      "explanation": "Production-grade distributed databases (CockroachDB, Spanner, etc.) took massive teams many years. Even with shortcuts, a reliable global database is a multi-year, large-team effort. Use existing solutions.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-016",
      "type": "multiple-choice",
      "question": "Estimate: 'Our 50-node cluster has 50 × 16 cores = 800 cores, so we can handle 800 concurrent requests.' Flaw?",
      "options": [
        "No flaw, math is correct",
        "Flaw: most requests involve I/O waiting, not CPU",
        "Flaw: should account for hyperthreading (1600)",
        "Flaw: need to subtract OS overhead"
      ],
      "correct": 1,
      "explanation": "Concurrent requests ≠ CPU cores. Most web requests spend time waiting on I/O (database, network). A single core can handle hundreds or thousands of concurrent I/O-bound requests with async processing.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-017",
      "type": "multiple-choice",
      "question": "Team proposes 100% cache hit rate as their target. Issue?",
      "options": [
        "Achievable with enough cache",
        "Unrealistic - cold starts, cache eviction, new data",
        "Achievable with predictive caching",
        "Fine as an aspirational goal"
      ],
      "correct": 1,
      "explanation": "100% hit rate is impossible: new/updated data isn't cached, cold starts after deployments, cache eviction under memory pressure, and long-tail requests for rarely-accessed data. Typical targets are 90-99%.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-018",
      "type": "multiple-choice",
      "question": "Design doc: 'With 1M users and 10 friends average, we'll have 10M friend relationships.' Missing factor?",
      "options": [
        "No missing factor, math is correct",
        "Friendships are bidirectional - actually 5M edges",
        "Should account for follow asymmetry",
        "Missing power-law distribution"
      ],
      "correct": 1,
      "explanation": "If friendships are mutual (A friends B means B friends A), each friendship is counted twice in '10 friends average.' Actual edges = 1M × 10 ÷ 2 = 5M relationships.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-019",
      "type": "multiple-choice",
      "question": "Estimate claims SSD random read latency of 1 microsecond. Accurate?",
      "options": [
        "Yes, modern NVMe SSDs achieve this",
        "No, typical is 50-100 microseconds",
        "No, typical is 10-20 milliseconds",
        "Yes, with Intel Optane"
      ],
      "correct": 1,
      "explanation": "NVMe SSD random read latency is typically 50-100 microseconds. Intel Optane is faster (~10μs) but still not 1μs. 1μs would be closer to RAM access speeds.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-020",
      "type": "multiple-choice",
      "question": "Load test shows server handling 50K QPS at 10% CPU utilization. Extrapolation to 100% CPU = 500K QPS. Valid?",
      "options": [
        "Valid linear extrapolation",
        "Invalid - performance degrades non-linearly at high utilization",
        "Valid if using autoscaling",
        "Invalid - should use 80% as max"
      ],
      "correct": 1,
      "explanation": "CPU utilization doesn't scale linearly. At high utilization, context switching, lock contention, and queueing cause degradation. Never plan for 100% utilization. Use 70-80% as practical max.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-021",
      "type": "multiple-choice",
      "question": "Estimate: 'Twitter has 500M users tweeting 10x/day = 5B tweets/day = 58K tweets/second.' Reality check?",
      "options": [
        "Reasonable estimate",
        "Too high - actual is ~6K tweets/second",
        "Too low - actual is ~500K tweets/second",
        "Can't verify without internal data"
      ],
      "correct": 1,
      "explanation": "Not all 500M users are daily active, and most users read more than write. Actual tweet rate is ~500M tweets/day = ~6K/second. The 10x/day assumption is too high for average users.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-022",
      "type": "multiple-choice",
      "question": "Proposal to process 1M images/second with 10 GPU servers. Each image takes 100ms on GPU. Feasible?",
      "options": [
        "Feasible with batching",
        "Infeasible - 10 GPUs × 10/sec = 100 images/sec max",
        "Feasible with model optimization",
        "Infeasible without TPUs"
      ],
      "correct": 1,
      "explanation": "If each image takes 100ms, one GPU processes 10 images/second. 10 GPUs = 100 images/second. For 1M/sec, you'd need 100,000 GPUs. Off by 10,000x.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-023",
      "type": "multiple-choice",
      "question": "Cost projection: 'AWS bill will stay flat as we 10x users because we'll optimize.' Realistic?",
      "options": [
        "Realistic with good engineering",
        "Unrealistic - costs scale with usage",
        "Realistic if already over-provisioned",
        "Depends on pricing model"
      ],
      "correct": 1,
      "explanation": "While optimization helps, 10x users means ~10x compute, storage, and bandwidth. You might achieve 3-5x efficiency gains, but flat costs while 10x-ing is unrealistic without fundamental architecture changes.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "sanity-024",
      "type": "multiple-choice",
      "question": "Design assumes 0ms network latency between microservices 'because they're in the same datacenter.' Problem?",
      "options": [
        "No problem, same-DC latency is negligible",
        "Problem: same-DC RTT is 0.5-2ms, adds up with many hops",
        "No problem with service mesh",
        "Problem only at very high scale"
      ],
      "correct": 1,
      "explanation": "Same-datacenter RTT is 0.5-2ms, not 0. With 10 microservice hops, that's 5-20ms just in network latency. This matters for latency-sensitive applications and is often underestimated.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-025",
      "type": "multiple-choice",
      "question": "Teammate's estimate: '1 billion rows × 100 bytes = 100 GB, easily fits in RAM.' What's missing?",
      "options": [
        "Nothing missing, estimate is complete",
        "Index overhead (often 2-3x data size)",
        "Missing replication factor",
        "Should use compression ratio"
      ],
      "correct": 1,
      "explanation": "Database indexes often add 1-3x the raw data size. B-tree indexes, secondary indexes, and internal metadata mean 100 GB of data might need 200-400 GB total. Always account for index overhead.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-026",
      "type": "multiple-choice",
      "question": "SLA promises 99.99% availability. System has 10 components in series, each 99.99% available. Actual availability?",
      "options": [
        "99.99% (matches SLA)",
        "99.9% (10x worse)",
        "99% (100x worse)",
        "~99.9% (close but misses SLA)"
      ],
      "correct": 3,
      "explanation": "Serial availability: 0.9999^10 = 0.999 = 99.9%. Each additional component in the critical path reduces overall availability. To achieve 99.99% overall, individual components need higher availability.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-027",
      "type": "multiple-choice",
      "question": "Estimate: 'Video transcoding takes 1 hour for 1 hour of video on 1 server. For 1M hours/day, we need 1M servers.' Flaw?",
      "options": [
        "No flaw, math is correct",
        "Flaw: can parallelize transcoding of single video",
        "Flaw: 1M servers is cost-prohibitive, need queueing",
        "Flaw: should use 1 hour = 1 server-hour, so 1M server-hours/day ÷ 24 = 42K servers"
      ],
      "correct": 3,
      "explanation": "1M hours of video needs 1M server-hours of work. Spread over 24 hours: 1M ÷ 24 = 41,667 servers running continuously. The original estimate doesn't account for time distribution.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-028",
      "type": "multiple-choice",
      "question": "Design shows read/write ratio of 1:1 for a social media feed. Suspicious?",
      "options": [
        "Normal for social apps",
        "Suspicious - reads typically 100-1000x writes",
        "Suspicious - writes typically 10x reads",
        "Depends on the feature"
      ],
      "correct": 1,
      "explanation": "Social feeds are read-heavy: users scroll through many posts but create few. Typical ratio is 100:1 to 1000:1 reads:writes. 1:1 suggests misunderstanding the access pattern or wrong metrics.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-029",
      "type": "multiple-choice",
      "question": "Capacity plan: 'We need 1 TB of bandwidth per second for 1 billion video views per day.' Check?",
      "options": [
        "Reasonable for short-form video",
        "Too high by ~100x",
        "Too low by ~10x",
        "Need more info about video quality"
      ],
      "correct": 3,
      "explanation": "1B views/day = 11,574 views/sec. If each view streams 10 MB (short video), that's 115 GB/s, not 1 TB/s. For 2-hour 4K movies at 20 GB each, concurrent streams matter more than daily views. Need video duration/quality.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "sanity-030",
      "type": "multiple-choice",
      "question": "Startup claims their ML model achieves 99.9% accuracy on fraud detection. Red flag?",
      "options": [
        "Impressive but achievable",
        "Red flag - likely overfit or wrong metric",
        "Normal for mature ML systems",
        "Depends on the dataset"
      ],
      "correct": 1,
      "explanation": "99.9% accuracy on imbalanced data (fraud is rare) is often meaningless - predicting 'not fraud' always achieves >99% accuracy. Need precision/recall/F1. Also suggests possible overfitting on test data.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-031",
      "type": "multiple-choice",
      "question": "Estimate: 'Compression will reduce our 100 TB to 10 TB (10x ratio).' Reasonable for JSON logs?",
      "options": [
        "Reasonable, JSON compresses well",
        "Too optimistic - expect 3-5x for JSON",
        "Too pessimistic - JSON compresses 20x+",
        "Depends on compression algorithm"
      ],
      "correct": 0,
      "explanation": "JSON logs compress very well due to repetitive keys and structure. 10x compression (90% reduction) is achievable with gzip/zstd for typical log data. Some achieve even higher ratios.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-032",
      "type": "multiple-choice",
      "question": "Design assumes all 1M users are online simultaneously for capacity planning. Issue?",
      "options": [
        "Correct for worst-case planning",
        "Over-provisioned - typical concurrency is 1-10% of total users",
        "Under-provisioned - should plan for 2M",
        "Depends on application type"
      ],
      "correct": 1,
      "explanation": "Simultaneous online users are typically 1-10% of total users, depending on app type. 1M total users might mean 10K-100K concurrent. Planning for 100% concurrency wastes resources.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-033",
      "type": "multiple-choice",
      "question": "Latency budget: 'Database: 50ms, API processing: 50ms, Network: 0ms, Total: 100ms.' Missing component?",
      "options": [
        "Serialization/deserialization overhead",
        "All components accounted for",
        "Should include retry latency",
        "Missing database connection time"
      ],
      "correct": 0,
      "explanation": "Serialization (JSON/protobuf encoding/decoding) often adds 1-10ms. Network is never 0ms (even localhost has overhead). Queue wait times, GC pauses, and connection overhead are also often missed.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-034",
      "type": "multiple-choice",
      "question": "Security estimate: 'Bcrypt password hashing at 100ms/hash, 1000 logins/second needs 100 seconds of CPU.' Implication?",
      "options": [
        "Need 100+ dedicated CPU cores for auth",
        "Can handle with 10 cores",
        "Should switch to faster hashing",
        "Async processing solves this"
      ],
      "correct": 0,
      "explanation": "1000 logins/sec × 100ms/hash = 100 CPU-seconds of work per real second. You need 100+ cores just for password hashing. This is a real constraint that affects architecture (dedicated auth servers, rate limiting).",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-035",
      "type": "multiple-choice",
      "question": "Team says 'We'll just cache everything in Redis' for their 50 TB dataset. Problem?",
      "options": [
        "No problem with Redis Cluster",
        "Problem: cost-prohibitive, RAM is 10-30x more expensive than SSD",
        "No problem with proper eviction policy",
        "Problem only if data changes frequently"
      ],
      "correct": 1,
      "explanation": "50 TB in RAM costs $500K-$1.5M in hardware vs $5-15K for SSD. Also requires massive cluster management. Cache the hot subset (typically 10-20% of data handles 80%+ of requests).",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-036",
      "type": "multiple-choice",
      "question": "Estimate: 'Our API gateway adds 0.1ms latency.' After adding auth, rate limiting, logging, and metrics. Realistic?",
      "options": [
        "Realistic for optimized gateways",
        "Unrealistic - each feature adds latency, expect 5-20ms",
        "Realistic with async logging",
        "Depends on implementation"
      ],
      "correct": 1,
      "explanation": "Auth (JWT validation, token lookup), rate limiting (Redis call), logging, and metrics each add latency. A full-featured API gateway typically adds 5-20ms. 0.1ms is only achievable with minimal processing.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-037",
      "type": "multiple-choice",
      "question": "Design: 'We'll use eventual consistency and guarantee updates propagate in under 100ms globally.' Conflict?",
      "options": [
        "No conflict, modern systems achieve this",
        "Conflict: global propagation takes 100-300ms minimum (speed of light)",
        "No conflict with edge caching",
        "Conflict only for writes"
      ],
      "correct": 1,
      "explanation": "Speed of light limits global RTT to 100-300ms minimum. 'Eventual' consistency that's guaranteed under 100ms globally is physically impossible. Either relax the time guarantee or use synchronous replication.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-038",
      "type": "multiple-choice",
      "question": "Cost estimate: '100 engineers × $200K salary = $20M/year in engineering costs.' What's missing?",
      "options": [
        "Nothing, salary is total cost",
        "Benefits, equipment, office space add 30-50%",
        "Should include management overhead",
        "Should subtract equity compensation"
      ],
      "correct": 1,
      "explanation": "Total employee cost includes benefits (health, 401k), payroll taxes, equipment, office space, software licenses, and overhead. Rule of thumb: multiply salary by 1.3-1.5 for true cost.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-039",
      "type": "multiple-choice",
      "question": "Architect proposes synchronous writes to 5 datacenters for strong consistency. Latency concern?",
      "options": [
        "No concern with fast networks",
        "Major concern: must wait for slowest DC, likely 100-300ms",
        "Minor concern, can use quorum writes",
        "No concern if DCs are in same region"
      ],
      "correct": 1,
      "explanation": "Synchronous writes wait for ALL replicas. With 5 global DCs, you wait for the slowest (likely 100-300ms RTT away). This makes sub-100ms writes impossible. Use quorum writes or async replication.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sanity-040",
      "type": "multiple-choice",
      "question": "Load test shows 1ms average latency at 1K QPS. Team extrapolates to 1ms at 100K QPS. Valid?",
      "options": [
        "Valid if system is stateless",
        "Invalid - latency increases with load due to queueing",
        "Valid with horizontal scaling",
        "Invalid only if approaching capacity"
      ],
      "correct": 1,
      "explanation": "Latency increases with load due to queueing theory (Little's Law). As utilization rises, queue wait times increase non-linearly. Must load test at target QPS, not extrapolate from low load.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sanity-041",
      "type": "multiple-choice",
      "question": "Estimate: 'UUID is 32 characters = 32 bytes.' Correct?",
      "options": [
        "Correct",
        "Wrong - UUID is 36 characters with hyphens, but 16 bytes binary",
        "Wrong - UUID is 128 bytes",
        "Depends on encoding"
      ],
      "correct": 1,
      "explanation": "UUID string is 36 characters (32 hex + 4 hyphens) = 36 bytes as string. But a UUID is fundamentally 128 bits = 16 bytes in binary. Store as binary (16 bytes) not string (36 bytes) when possible.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-042",
      "type": "multiple-choice",
      "question": "Team plans to retry failed requests 10 times with no backoff. During an outage, what happens?",
      "options": [
        "System recovers faster",
        "Retry storm amplifies load 10x, delays recovery",
        "Normal behavior for resilient systems",
        "Only problematic at scale"
      ],
      "correct": 1,
      "explanation": "10 retries without backoff means each failed request generates 10x the load. During outages, this creates a 'retry storm' that overwhelms the recovering system. Always use exponential backoff with jitter.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-043",
      "type": "multiple-choice",
      "question": "Design assumes 'stateless' services can be scaled infinitely. What's the hidden bottleneck?",
      "options": [
        "No bottleneck if truly stateless",
        "Database becomes the bottleneck",
        "Network bandwidth limits",
        "CPU always limits eventually"
      ],
      "correct": 1,
      "explanation": "Stateless services still depend on stateful backends (databases, caches). Scaling 100 stateless servers that all hit one database just moves the bottleneck. Must scale data tier too.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-044",
      "type": "multiple-choice",
      "question": "Estimate: 'S3 GET request cost is $0.0004. At 1B requests/month = $400K/month.' Is there a cheaper option?",
      "options": [
        "No, S3 is already cheapest",
        "Yes, CloudFront reduces GET costs with caching",
        "No, all object stores have similar pricing",
        "Yes, switch to blob storage"
      ],
      "correct": 1,
      "explanation": "CDN caching (CloudFront, etc.) serves repeated requests from edge at lower cost (~$0.01/GB vs per-request). For cacheable content with repeated access, CDN dramatically reduces costs.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "sanity-045",
      "type": "multiple-choice",
      "question": "Proposal: 'Our ML pipeline processes data in real-time with 50ms latency.' Pipeline has 20 sequential steps. Feasible?",
      "options": [
        "Feasible with optimization",
        "Infeasible - 50ms ÷ 20 steps = 2.5ms/step is very tight",
        "Feasible with parallel processing",
        "Depends on step complexity"
      ],
      "correct": 1,
      "explanation": "20 sequential steps in 50ms means 2.5ms average per step. Even without any I/O, this is very tight. Any single slow step breaks the budget. Consider parallelizing or reducing steps.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-046",
      "type": "multiple-choice",
      "question": "Design claims 'zero downtime deployments' with a single database server. Gap?",
      "options": [
        "No gap with blue-green deployment",
        "Gap: database migrations can require downtime",
        "No gap with rolling deployments",
        "Gap only for schema changes"
      ],
      "correct": 1,
      "explanation": "Zero downtime requires handling database schema changes without locking. Single DB is a bottleneck: ALTER TABLE can lock tables, breaking zero-downtime claims. Need online schema migration tools.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-047",
      "type": "multiple-choice",
      "question": "Cost model: 'Serverless at $0.20/million invocations beats servers at $100/month.' At what volume does this flip?",
      "options": [
        "Never flips, serverless always cheaper",
        "~500M invocations/month",
        "~50M invocations/month",
        "~5M invocations/month"
      ],
      "correct": 1,
      "explanation": "$100/month ÷ $0.20/million = 500M invocations breakeven. Below 500M/month, serverless is cheaper. Above, dedicated servers win. But also consider compute time costs, not just invocations.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-048",
      "type": "multiple-choice",
      "question": "Teammate estimates 'JSON parsing is essentially free.' Their service parses 100K JSON docs/second, each 10 KB. Concern?",
      "options": [
        "No concern, JSON parsing is fast",
        "Concern: 1 GB/sec of parsing has real CPU cost",
        "No concern with modern hardware",
        "Concern only for nested JSON"
      ],
      "correct": 1,
      "explanation": "100K × 10 KB = 1 GB/second of JSON parsing. Fast parsers achieve 1-2 GB/sec per core. This could consume an entire CPU core just for parsing. Not free at scale.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-049",
      "type": "multiple-choice",
      "question": "Plan: 'We'll handle 100K WebSocket connections per server.' Realistic for commodity hardware?",
      "options": [
        "Realistic with epoll/kqueue",
        "Unrealistic - typical limit is 10K per server",
        "Realistic only with specialized hardware",
        "Unrealistic - memory alone is prohibitive"
      ],
      "correct": 0,
      "explanation": "With proper async I/O (epoll/kqueue), 100K-1M connections per server is achievable. Each connection uses ~10-20 KB RAM (socket buffers). 100K × 20 KB = 2 GB RAM - manageable.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-050",
      "type": "multiple-choice",
      "question": "Estimate assumes 'database read replicas have zero replication lag.' When does this break down?",
      "options": [
        "Only under extreme write load",
        "Always - async replication inherently has lag (typically ms-seconds)",
        "Never with synchronous replication",
        "Only during network partitions"
      ],
      "correct": 1,
      "explanation": "Async replication always has some lag (often ms, sometimes seconds during load). Read-after-write consistency requires reading from primary or accepting stale reads. 'Zero lag' is misleading.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-051",
      "type": "multiple-choice",
      "question": "Capacity plan for image CDN: '1 PB storage, 10 Gbps bandwidth.' If average image is 100 KB, what's daily serve capacity?",
      "options": [
        "~1 billion images/day",
        "~10 billion images/day",
        "~100 million images/day",
        "Need concurrent connection info"
      ],
      "correct": 0,
      "explanation": "10 Gbps = 1.25 GB/s. At 100 KB/image: 1,250 MB/s ÷ 0.1 MB = 12,500 images/second = 1.08B images/day. Storage of 1 PB holds 10B images. Bandwidth is likely the constraint.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "sanity-052",
      "type": "multiple-choice",
      "question": "Design doc claims 'our service is horizontally scalable - just add more servers.' What needs verification?",
      "options": [
        "Nothing, horizontal scaling is automatic",
        "Whether shared state exists (database, cache)",
        "Only network topology",
        "Only load balancer capacity"
      ],
      "correct": 1,
      "explanation": "True horizontal scaling requires no shared mutable state between servers. Verify: Is there a shared database? Shared cache? Session affinity? Distributed locks? Any of these can bottleneck scaling.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-053",
      "type": "multiple-choice",
      "question": "Estimate: 'gRPC will be faster than REST because binary protocol.' Always true?",
      "options": [
        "Yes, binary is always faster than text",
        "Not always - small payloads show minimal difference",
        "Yes, but only for streaming",
        "Not always - REST over HTTP/2 closes the gap"
      ],
      "correct": 1,
      "explanation": "For small payloads, protobuf vs JSON difference is minimal. HTTP/2 provides multiplexing for both. gRPC shines for: streaming, large payloads, strict schemas. Small request/response? Difference is often negligible.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-054",
      "type": "multiple-choice",
      "question": "Team plans 30-day data retention. Deletes run weekly. What's the actual retention?",
      "options": [
        "30 days as planned",
        "30-37 days depending on timing",
        "23-30 days depending on timing",
        "Exactly 28 days (4 weeks)"
      ],
      "correct": 1,
      "explanation": "Weekly deletion of 30-day old data means data survives 30-37 days. Data created right after a deletion run survives until next week's run: 30 days + up to 7 days = 37 days max.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-055",
      "type": "multiple-choice",
      "question": "Load balancer health check: ping every 10 seconds, mark unhealthy after 3 failures. How long until unhealthy server is removed?",
      "options": ["~10 seconds", "~30 seconds", "~60 seconds", "~90 seconds"],
      "correct": 1,
      "explanation": "3 failures × 10 seconds between checks = 30 seconds minimum to detect failure. Add time for actual removal and propagation. Meanwhile, traffic continues to failing server. Consider faster checks.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-056",
      "type": "multiple-choice",
      "question": "Estimate: 'Our caching layer has 99% hit rate, so origin handles 1% of traffic.' At 10M QPS total, origin load is:",
      "options": ["100 QPS", "1,000 QPS", "10,000 QPS", "100,000 QPS"],
      "correct": 3,
      "explanation": "1% of 10M = 100,000 QPS to origin. Even with 99% cache hit rate, 100K QPS is substantial load. High cache hit rates at scale still mean significant origin traffic.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-057",
      "type": "multiple-choice",
      "question": "Proposal: 'We'll use microservices for our 3-person team.' Concern?",
      "options": [
        "Good architecture for any team size",
        "Overhead may exceed benefits for small teams",
        "Only concerning without DevOps expertise",
        "No concern with managed Kubernetes"
      ],
      "correct": 1,
      "explanation": "Microservices add deployment, monitoring, and debugging complexity. A 3-person team may spend more time on infrastructure than features. Monolith is often better until team/scale justify the split.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-058",
      "type": "multiple-choice",
      "question": "Design assumes 'network is reliable' for distributed transactions. What's the actual packet loss rate in datacenters?",
      "options": [
        "Effectively 0% in modern datacenters",
        "0.01-0.1% is typical",
        "1-5% is typical",
        "Varies too much to estimate"
      ],
      "correct": 1,
      "explanation": "Even in well-run datacenters, packet loss is 0.01-0.1%. At 10K QPS, that's 1-10 failures/second. Design must handle network failures, timeouts, and retries. 'Reliable network' is a distributed systems fallacy.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-059",
      "type": "multiple-choice",
      "question": "Monitoring plan: 'We'll sample 1% of requests for distributed tracing.' At 100K QPS, traces per second?",
      "options": [
        "10 traces/second",
        "100 traces/second",
        "1,000 traces/second",
        "10,000 traces/second"
      ],
      "correct": 2,
      "explanation": "1% of 100K = 1,000 traces/second. Each trace has multiple spans. At 10 spans/trace, that's 10K spans/second to store and query. Ensure tracing backend can handle this volume.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-060",
      "type": "multiple-choice",
      "question": "Cost estimate: 'Free tier covers our needs.' Product has 100K users sending 100 requests/day each. Red flag for which service?",
      "options": [
        "Compute (likely within free tier)",
        "Database queries (likely within free tier)",
        "Email notifications (likely exceeds free tier)",
        "All likely within free tier"
      ],
      "correct": 2,
      "explanation": "Email services typically have 100-1000 emails/day free tier. 100K users × even 1 email/day = 100K emails. Most free tiers cap at 100-1000/day. Email, SMS, and push notifications often exceed free tiers.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-061",
      "type": "multiple-choice",
      "question": "Teammate says 'Kubernetes will handle our scaling automatically.' What does Kubernetes NOT handle?",
      "options": [
        "Pod autoscaling",
        "Load balancing",
        "Database scaling",
        "Container health checks"
      ],
      "correct": 2,
      "explanation": "Kubernetes autoscales stateless pods well. But databases, caches, and stateful services require manual capacity planning. You can't just 'add pods' to PostgreSQL. Stateful scaling remains hard.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-062",
      "type": "multiple-choice",
      "question": "Estimate: 'Base64 encoding adds 33% overhead.' When storing 1 GB of binary data as Base64, total size?",
      "options": [
        "~1 GB (negligible overhead)",
        "~1.33 GB",
        "~2 GB",
        "Depends on content"
      ],
      "correct": 1,
      "explanation": "Base64 encodes 3 bytes as 4 characters = 33% overhead. 1 GB binary becomes ~1.33 GB Base64. For large binary data, this overhead matters. Consider binary storage or compression.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-063",
      "type": "multiple-choice",
      "question": "Design claims 'consistent hashing solves all data distribution problems.' Limitation?",
      "options": [
        "Only works for key-value data",
        "Hotspots when keys are skewed",
        "Requires prime number of nodes",
        "Only works for reads"
      ],
      "correct": 1,
      "explanation": "Consistent hashing distributes uniformly across nodes IF keys are uniformly distributed. Hot keys (celebrity users, viral content) still cause hotspots regardless of hashing. Need additional strategies.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-064",
      "type": "multiple-choice",
      "question": "Proposal: 'We'll achieve sub-millisecond latency by moving to GraphQL.' Likely outcome?",
      "options": [
        "Significant latency reduction",
        "Latency unchanged or worse - GraphQL adds parsing overhead",
        "Slight improvement from reduced payload",
        "Depends on query complexity"
      ],
      "correct": 1,
      "explanation": "GraphQL adds query parsing, validation, and resolution overhead. It reduces over-fetching but doesn't inherently reduce latency. For latency, database/network dominate. GraphQL is about flexibility, not speed.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-065",
      "type": "multiple-choice",
      "question": "Estimate: 'Feature flags are free - just IF statements.' At 10K QPS with 100 flags evaluated per request, what's the load?",
      "options": [
        "Negligible - just CPU cycles",
        "1M flag evaluations/second - noticeable if flags hit database",
        "Only matters for A/B test flags",
        "Depends on flag complexity"
      ],
      "correct": 1,
      "explanation": "10K × 100 = 1M flag evaluations/second. If flags are in-memory, fast. If they hit a remote service for each evaluation, you've added 1M external calls/second. Ensure flags are cached locally.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-066",
      "type": "multiple-choice",
      "question": "Backup strategy: 'Daily backups with 7-day retention.' If corruption is discovered 10 days after it started, recovery options?",
      "options": [
        "Restore from any backup within 7 days",
        "No clean backup available - corruption propagated to all backups",
        "Point-in-time recovery solves this",
        "Incremental backup helps"
      ],
      "correct": 1,
      "explanation": "If corruption started 10 days ago and backups retain 7 days, ALL backups contain corrupted data. Need longer retention or different backup verification strategy to catch silent corruption.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-067",
      "type": "multiple-choice",
      "question": "Design: 'Users can upload files up to 100 GB.' Via standard HTTP upload. Problem?",
      "options": [
        "No problem with chunked encoding",
        "Problem: HTTP timeouts, connection resets over long uploads",
        "No problem with keep-alive",
        "Problem only on mobile networks"
      ],
      "correct": 1,
      "explanation": "100 GB upload at 10 MB/s = 2.7 hours. Network interruptions, proxy timeouts, and connection limits make single HTTP uploads unreliable for large files. Need resumable/chunked upload protocol.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "sanity-068",
      "type": "multiple-choice",
      "question": "Estimate: 'Adding an index will speed up all queries on that column.' Counter-example?",
      "options": [
        "No counter-example, indexes always help reads",
        "Write-heavy workloads slow down (index maintenance)",
        "Only fails for very small tables",
        "Only fails for composite indexes"
      ],
      "correct": 1,
      "explanation": "Indexes speed reads but slow writes. Each INSERT/UPDATE must update all indexes. For write-heavy tables, index overhead can outweigh read benefits. Also, full table scans beat indexes for low selectivity.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-069",
      "type": "multiple-choice",
      "question": "Monitoring: 'CPU <50% means we have plenty of headroom.' During incident, CPU is 40% but latency spiked. What else to check?",
      "options": [
        "Network bandwidth",
        "Memory/swap usage",
        "Disk I/O wait",
        "All of the above"
      ],
      "correct": 3,
      "explanation": "Low CPU doesn't mean no bottleneck. Services can be I/O bound (waiting on disk, network, external services) or memory bound (swapping). Check: disk I/O, network I/O, memory, and external dependencies.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "sanity-070",
      "type": "multiple-choice",
      "question": "Team estimates 'We need 2x capacity for peak.' Peak is Black Friday at 20x normal. Issue?",
      "options": [
        "2x is sufficient with autoscaling",
        "2x is 10x short of 20x peak capacity needed",
        "2x is reasonable with queuing",
        "Depends on acceptable degradation"
      ],
      "correct": 1,
      "explanation": "If peak is 20x normal, you need infrastructure that can handle 20x, not 2x. Autoscaling helps but has limits and spin-up time. Either over-provision or explicitly plan for graceful degradation at peak.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-071",
      "type": "multiple-choice",
      "question": "Assumption: 'Timestamps are unique identifiers.' When does this fail?",
      "options": [
        "Never, with nanosecond precision",
        "Multiple events in same millisecond",
        "Only with distributed systems",
        "Only without synchronized clocks"
      ],
      "correct": 1,
      "explanation": "At high throughput, multiple events occur in the same millisecond (or even nanosecond with concurrent threads). Timestamps need additional disambiguation: sequence numbers, random components, or UUIDs.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-072",
      "type": "multiple-choice",
      "question": "Estimate: 'With 1000 database connections, we can handle 1000 concurrent queries.' Actual effective parallelism?",
      "options": [
        "1000 parallel queries",
        "Limited by CPU cores (typically 8-64)",
        "500 due to overhead",
        "Depends on query type"
      ],
      "correct": 1,
      "explanation": "Database can only execute as many queries in parallel as it has CPU cores. 1000 connections mostly means 1000 queries can wait simultaneously, not execute simultaneously. IO-bound queries may achieve more concurrency.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-073",
      "type": "multiple-choice",
      "question": "Cost projection: 'Traffic grows 10%/month, costs grow 10%/month.' Hidden assumption?",
      "options": [
        "Correct proportional scaling",
        "Assumes no efficiency improvements",
        "Assumes no volume discounts",
        "Assumes no tier pricing (costs often sublinear with scale)"
      ],
      "correct": 3,
      "explanation": "Cloud pricing often has tiers: cost/unit decreases at higher volumes. Also, fixed costs (baseline infrastructure) don't scale with traffic. Actual cost growth is often sublinear with traffic growth.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Growth planning should show today vs horizon requirements side by side because compounding makes small percentages material quickly.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-074",
      "type": "multiple-choice",
      "question": "Design: 'Saga pattern for distributed transactions with 10 services.' Failure at step 7 requires:",
      "options": [
        "Rollback steps 1-7",
        "Compensating transactions for steps 1-6",
        "Only retry step 7",
        "No action needed with eventual consistency"
      ],
      "correct": 1,
      "explanation": "Sagas use compensating transactions, not rollback. Steps 1-6 completed and committed. Step 7 failure requires running compensation for 6, 5, 4, 3, 2, 1 in reverse. Complex to implement correctly.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-075",
      "type": "multiple-choice",
      "question": "Estimate: 'Encrypting at rest is free with modern CPUs (AES-NI).' Storage performance impact?",
      "options": [
        "Near zero (<1%)",
        "~5-10% overhead",
        "~25-50% overhead",
        "Depends on block size"
      ],
      "correct": 0,
      "explanation": "AES-NI achieves ~5-10 GB/s encryption, faster than most storage. Encryption at rest with modern CPUs adds <1% overhead for storage-bound workloads. But key management adds operational complexity.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-076",
      "type": "multiple-choice",
      "question": "QA asks: 'Why does staging not match production?' Staging has 10% of production data. Missing concern?",
      "options": [
        "Missing data variety",
        "Database query plans differ with different data sizes",
        "Missing edge cases",
        "All of the above"
      ],
      "correct": 3,
      "explanation": "10% data affects: query plans (optimizer chooses differently), caching behavior, pagination edge cases, and missing data variety/edge cases. Performance tests on small data don't reflect production.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-077",
      "type": "multiple-choice",
      "question": "Team plans to store 'unlimited message history.' At 1M users × 100 messages/day × 1 KB each, daily growth?",
      "options": ["~100 MB", "~1 GB", "~100 GB", "~1 TB"],
      "correct": 2,
      "explanation": "1M × 100 × 1 KB = 100M KB = 100 GB/day = 36 TB/year. 'Unlimited' history at this rate becomes expensive fast. Consider archival tiers, compression, or retention limits.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "sanity-078",
      "type": "multiple-choice",
      "question": "Design: 'We'll use UUIDs as primary keys for global uniqueness.' Performance consideration?",
      "options": [
        "No concerns with UUIDs",
        "Random UUIDs cause index fragmentation and slower inserts",
        "UUIDs are slower to compare",
        "UUID storage size is the only concern"
      ],
      "correct": 1,
      "explanation": "Random UUIDs (v4) scatter inserts across index pages, causing fragmentation and slower writes. Solutions: UUID v7 (time-sorted), ULID, or sequential IDs with sharding for global uniqueness.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-079",
      "type": "multiple-choice",
      "question": "Estimate: 'Moving to ARM instances saves 40% on compute costs.' When might savings not materialize?",
      "options": [
        "Always saves money if workload runs on ARM",
        "When dependencies don't support ARM",
        "When compute isn't the primary cost",
        "Both B and C"
      ],
      "correct": 3,
      "explanation": "ARM savings require: 1) all dependencies have ARM builds, 2) compute is a significant cost (if database or bandwidth dominates, 40% compute savings is minor). Rebuild/testing effort also has costs.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "sanity-080",
      "type": "multiple-choice",
      "question": "Load test: 'System handles 10K QPS with 50ms p50 latency.' What additional metric is critical for capacity?",
      "options": [
        "p99 latency",
        "Error rate",
        "CPU utilization",
        "All of the above"
      ],
      "correct": 3,
      "explanation": "p50 hides tail latency (p99 might be 5 seconds). Error rate shows if 10K QPS is actually successful. CPU utilization shows headroom. All three are needed to assess true capacity.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sanity-081",
      "type": "multiple-choice",
      "question": "Plan: 'Shard by user_id for even distribution.' User with 1M followers posts. Impact?",
      "options": [
        "Even distribution handles this fine",
        "Fan-out to 1M follower shards creates hotspot",
        "No impact with good sharding",
        "Impacts only write performance"
      ],
      "correct": 1,
      "explanation": "User-ID sharding distributes users evenly, but a celebrity post fans out to millions of followers across many shards. This creates read/write amplification and hotspots in the fan-out path.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-082",
      "type": "multiple-choice",
      "question": "Estimate: 'Connection pool of 100 handles all load.' Each request holds connection for 50ms. Max sustainable QPS?",
      "options": ["100 QPS", "1,000 QPS", "2,000 QPS", "10,000 QPS"],
      "correct": 2,
      "explanation": "Each connection handles 1000ms/50ms = 20 requests/second. 100 connections × 20 req/sec = 2,000 QPS maximum. Beyond this, requests wait for connections, increasing latency.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sanity-083",
      "type": "multiple-choice",
      "question": "Design assumes 'DynamoDB handles any scale.' What requires additional planning?",
      "options": [
        "Nothing, DynamoDB auto-scales",
        "Hot partitions still cause throttling",
        "Only write capacity needs planning",
        "Only read capacity needs planning"
      ],
      "correct": 1,
      "explanation": "DynamoDB auto-scales total capacity but has per-partition limits (~3K RCU, 1K WCU). Hot keys hitting one partition get throttled regardless of total capacity. Partition key design is critical.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-084",
      "type": "multiple-choice",
      "question": "Test environment has 100% automated test coverage. 'No bugs will reach production.' Issue with this claim?",
      "options": [
        "No issue with 100% coverage",
        "Coverage measures lines, not logic paths or edge cases",
        "Only integration tests matter",
        "Manual testing is still needed"
      ],
      "correct": 1,
      "explanation": "100% line coverage doesn't mean 100% tested scenarios. Edge cases, timing issues, integration problems, and unexpected inputs aren't caught by coverage metrics. Coverage is necessary but not sufficient.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-085",
      "type": "multiple-choice",
      "question": "Estimate: 'JWT tokens are stateless, so auth scales infinitely.' Hidden state?",
      "options": [
        "No hidden state with JWTs",
        "Token revocation requires state (blocklist)",
        "Only refresh tokens have state",
        "State is in the token itself"
      ],
      "correct": 1,
      "explanation": "JWTs are stateless for validation, but revocation requires checking a blocklist. Logout, password change, or security incidents need immediate invalidation, requiring shared state (Redis blocklist, etc.).",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-086",
      "type": "multiple-choice",
      "question": "Design: 'Multi-tenant with shared database, isolated by tenant_id column.' Security concern?",
      "options": [
        "No concern with proper access control",
        "One missing WHERE clause exposes all tenants",
        "Only performance is concerning",
        "Only backup/restore is affected"
      ],
      "correct": 1,
      "explanation": "Shared-database multi-tenancy relies on every query having correct tenant_id filter. One bug (missing WHERE, wrong join) can expose or corrupt other tenants' data. Defense in depth recommended.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-087",
      "type": "multiple-choice",
      "question": "Teammate: 'We don't need rate limiting, we trust our clients.' Service is internal API. Gap?",
      "options": [
        "No gap for internal services",
        "Bugs/loops in clients can create self-inflicted DDoS",
        "Rate limiting only needed externally",
        "Trust but verify is overkill internally"
      ],
      "correct": 1,
      "explanation": "Internal clients have bugs: infinite loops, retry storms, batch jobs gone wrong. Rate limiting protects against both malicious and accidental overload. Internal != safe.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Token bucket",
          "url": "https://en.wikipedia.org/wiki/Token_bucket"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-088",
      "type": "multiple-choice",
      "question": "Estimate: 'Switching from PostgreSQL to MongoDB will improve performance.' When is this likely false?",
      "options": [
        "When data is relational with complex joins",
        "When transactions span multiple documents",
        "When current issues are query optimization, not database type",
        "All of the above"
      ],
      "correct": 3,
      "explanation": "Database switches rarely fix performance issues caused by bad schema design, missing indexes, or unoptimized queries. The same problems often follow. Identify root cause before switching databases.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-089",
      "type": "multiple-choice",
      "question": "Plan: 'Use client-side caching to reduce server load by 80%.' What happens during cache stampede?",
      "options": [
        "80% reduction maintained",
        "All clients simultaneously hit server when cache expires",
        "Cache naturally spreads load",
        "Server handles gracefully"
      ],
      "correct": 1,
      "explanation": "If all clients cache the same data with same TTL, cache expires simultaneously for everyone. Instant spike to server. Use jitter in TTL, staggered refresh, or lock-based refresh strategies.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-090",
      "type": "multiple-choice",
      "question": "Capacity: 'Peak is 3x average, so provision for 3x.' Average is 10K QPS. What's wrong with provisioning for exactly 30K QPS?",
      "options": [
        "Nothing wrong, correct calculation",
        "No headroom for growth or unexpected spikes",
        "3x is usually overprovisioned",
        "Should use p99 not average"
      ],
      "correct": 1,
      "explanation": "Provisioning exactly at expected peak leaves no margin for: growth, unexpected spikes beyond forecast, or handling failover from other instances. Typically provision for 1.5-2x expected peak.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "sanity-091",
      "type": "multiple-choice",
      "question": "Design: 'Async processing via job queue. Jobs complete in ~1 minute.' Queue depth alert set at 1,000 jobs. At 100 jobs/second ingest, how fast does alert trigger?",
      "options": [
        "~10 seconds",
        "~100 seconds",
        "Never triggers if processing keeps up",
        "Depends on worker count"
      ],
      "correct": 3,
      "explanation": "If workers process 100 jobs/sec (matching ingest), queue stays near zero. If workers process 90 jobs/sec, queue grows 10 jobs/sec, hitting 1000 in 100 seconds. Worker capacity determines queue growth.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "sanity-092",
      "type": "multiple-choice",
      "question": "Estimate: 'Feature takes 1 sprint to build, 0 sprints to maintain.' Realistic for a new microservice?",
      "options": [
        "Realistic for simple features",
        "Unrealistic - maintenance is ongoing (20-40% of dev time)",
        "Realistic with good testing",
        "Depends on service complexity"
      ],
      "correct": 1,
      "explanation": "Maintenance is never zero: bug fixes, dependency updates, scaling, on-call support, documentation. Industry average is 20-40% of engineering time on maintenance. Plan for it.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-093",
      "type": "multiple-choice",
      "question": "Teammate claims: 'Our p50 latency is 20ms, so user experience is good.' What's missing?",
      "options": [
        "p50 is the right metric for UX",
        "p95/p99 - half of users experience worse than p50",
        "Only average matters for UX",
        "Need p50 for each endpoint"
      ],
      "correct": 1,
      "explanation": "p50 means 50% of requests are faster. But 50% are slower - potentially much slower. Users notice the slow requests. p99 (99th percentile) better represents worst-case user experience.",
      "detailedExplanation": "The fastest path is to anchor on the core constraint and eliminate implausible options early. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-094",
      "type": "multiple-choice",
      "question": "Design: 'Store images as base64 in database for simplicity.' At 1M images × 1 MB each, base64 storage?",
      "options": ["~1 TB", "~1.33 TB", "~4 TB (with DB overhead)", "~10 TB"],
      "correct": 2,
      "explanation": "1 MB × 1.33 (base64) × 1M = 1.33 TB raw. But databases add overhead: row storage, indexes, WAL logs, TOAST tables. Actual DB footprint can be 2-3x. Also kills query performance. Use object storage.",
      "detailedExplanation": "Treat this as a constraint-filtering problem: find the bottleneck first, then narrow the choices. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sanity-095",
      "type": "multiple-choice",
      "question": "Reliability: 'We have 3 replicas, so we can lose 2 and stay up.' With consensus (Raft/Paxos), what's actually tolerable?",
      "options": [
        "Lose 2, keep 1 running",
        "Lose 1, need 2 for quorum",
        "Lose any number, 1 is enough",
        "Depends on configuration"
      ],
      "correct": 1,
      "explanation": "Consensus requires majority quorum. With 3 nodes, quorum is 2. Lose 1 node: 2 remain (can reach consensus). Lose 2 nodes: 1 remains (no quorum, cluster unavailable). For 2 failure tolerance, need 5 nodes.",
      "detailedExplanation": "A practical approach here is to lock down the dominant constraint before comparing answer choices. Consistency choices should be tied to concrete invariants and failure modes, then balanced against latency and availability cost.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-096",
      "type": "multiple-choice",
      "question": "Cost model: 'Reserved instances save 60%, so reserve everything.' Downside?",
      "options": [
        "No downside if you can predict usage",
        "Locked to instance type; can't optimize later",
        "Locked to capacity even if needs decrease",
        "Both B and C"
      ],
      "correct": 3,
      "explanation": "Reserved instances commit you to specific instance types and capacity. If better instances launch, needs change, or architecture shifts, you're stuck paying for unused capacity. Reserve baseline, use on-demand for variable.",
      "detailedExplanation": "Start by identifying the limiting factor, then rule out options that violate scale or semantics. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-097",
      "type": "multiple-choice",
      "question": "Design: 'Daily batch job at midnight.' Global users. Issue?",
      "options": [
        "No issue, midnight is low-traffic",
        "Midnight differs by timezone - always peak somewhere",
        "Only affects reporting freshness",
        "Batch timing doesn't impact users"
      ],
      "correct": 1,
      "explanation": "For global services, there's no universal 'off-hours.' Midnight PST is 8AM in London, 5PM in Tokyo. Batch jobs may hit users' peak hours. Also, batch completion time affects different regions differently.",
      "detailedExplanation": "Lead with the key assumption, then remove options that break units, scale, or architecture constraints. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-098",
      "type": "multiple-choice",
      "question": "Estimate: 'We need 99.99% availability. Budget: $10K/month for infrastructure.' Achievable?",
      "options": [
        "Achievable with smart architecture",
        "Likely insufficient - 4 nines requires significant redundancy",
        "Achievable with managed services",
        "Depends entirely on traffic"
      ],
      "correct": 1,
      "explanation": "99.99% (52 min/year downtime) needs: multi-AZ/region redundancy, automated failover, redundant databases, monitoring, and on-call support. $10K/month is tight for meaningful redundancy at most scales.",
      "detailedExplanation": "Frame the problem around the main tradeoff first, then test each option for consistency. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-099",
      "type": "multiple-choice",
      "question": "Log analysis: 'Error rate is 0.01%, system is healthy.' 10M requests/day. Daily error count?",
      "options": [
        "~10 errors",
        "~100 errors",
        "~1,000 errors",
        "~10,000 errors"
      ],
      "correct": 2,
      "explanation": "0.01% of 10M = 1,000 errors/day. That's 1000 potentially frustrated users or hidden bugs. Low percentage can still mean high absolute numbers. Always look at both rate and count.",
      "detailedExplanation": "Solve this by pinning the governing ratio first and using it to discard impossible answers. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "sanity-100",
      "type": "multiple-choice",
      "question": "Final check: teammate's estimate has 10 assumptions, each 90% likely to be correct. Probability all are correct?",
      "options": ["~90%", "~65%", "~35%", "~50%"],
      "correct": 2,
      "explanation": "0.9^10 = 0.349 ≈ 35%. With 10 independent assumptions at 90% confidence each, you have only 35% confidence in the combined estimate. Small uncertainties compound. Validate critical assumptions.",
      "detailedExplanation": "A reliable method is to name the primary boundary condition, then evaluate options against it. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n001",
      "type": "numeric-input",
      "question": "A proposal claims 1 million QPS from a single PostgreSQL instance. Typical single-instance max QPS?",
      "answer": 10000,
      "tolerance": 0.5,
      "explanation": "Single PostgreSQL typically maxes around 5K-15K QPS. 1M QPS is ~100× too high.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-n002",
      "type": "numeric-input",
      "question": "Estimate claims 10 TB/day for 1M users sending 10 messages/day at 100 bytes each. Actual TB/day?",
      "answer": 0.001,
      "unit": "TB",
      "tolerance": 0.2,
      "explanation": "1M × 10 × 100 bytes = 1 billion bytes = 1 GB, not 10 TB. Off by 10,000×.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-n003",
      "type": "numeric-input",
      "question": "Proposal: 99.999% availability with single server, monthly restarts. Realistic availability %?",
      "answer": 99.9,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Monthly restarts (~15 min each) alone consume more than 5.26 min/year budget for 5 nines.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n004",
      "type": "numeric-input",
      "question": "Claim: 100 servers for 100K DAU, 10 requests/user/day. Reasonable server count?",
      "answer": 1,
      "tolerance": 0.5,
      "explanation": "100K × 10 / 86400 ≈ 12 QPS. One server can easily handle this. 100 servers is 100× overprovisioned.",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-n005",
      "type": "numeric-input",
      "question": "Estimate says 1 PB storage for 10M user profiles at 1 KB each. Actual storage in GB?",
      "answer": 10,
      "unit": "GB",
      "tolerance": 0.1,
      "explanation": "10M × 1 KB = 10 GB. 1 PB is 100,000× too high.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-n006",
      "type": "numeric-input",
      "question": "Proposal: 50ms latency for a request making 20 sequential DB queries at 5ms each. Minimum possible latency in ms?",
      "answer": 100,
      "unit": "ms",
      "tolerance": 0.1,
      "explanation": "20 × 5ms = 100ms minimum. 50ms is physically impossible.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n007",
      "type": "numeric-input",
      "question": "Claim: transfer 1 TB in 1 minute over 1 Gbps link. Actual minimum time in minutes?",
      "answer": 133,
      "unit": "minutes",
      "tolerance": 0.15,
      "explanation": "1 Gbps = 125 MB/s. 1 TB / 125 MB/s = 8,000 sec = 133 min. Claim is off by 133×.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "reason-n008",
      "type": "numeric-input",
      "question": "Design proposes 1 billion rows in a 10 GB database at 1 KB per row. Max rows that fit?",
      "answer": 10000000,
      "tolerance": 0.1,
      "explanation": "10 GB / 1 KB = 10 million rows max, not 1 billion.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-n009",
      "type": "numeric-input",
      "question": "Claim: handle 1M WebSocket connections per server. Realistic connections per server (thousands)?",
      "answer": 100,
      "unit": "K",
      "tolerance": 0.5,
      "explanation": "With tuning, 50K-200K connections per server is achievable. 1M per server is aggressive.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n010",
      "type": "numeric-input",
      "question": "Proposal says 10ms RTT for US-to-Europe request. Realistic RTT in ms?",
      "answer": 100,
      "unit": "ms",
      "tolerance": 0.3,
      "explanation": "Speed of light limits: ~80-150ms for transatlantic RTT. 10ms is physically impossible.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n011",
      "type": "numeric-input",
      "question": "Claim: 1 developer can write 10,000 lines of production code per day. Realistic lines/day?",
      "answer": 100,
      "tolerance": 0.5,
      "explanation": "Industry average is 50-200 lines of quality code per day. 10K is unrealistic.",
      "detailedExplanation": "Do the conversion step first and maintain unit labels to prevent silent math errors. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n012",
      "type": "numeric-input",
      "question": "Estimate: 1 GB RAM to cache 100 million 1 KB objects. Required RAM in GB?",
      "answer": 100,
      "unit": "GB",
      "tolerance": 0.1,
      "explanation": "100M × 1 KB = 100 GB. Estimate is off by 100×.",
      "detailedExplanation": "Normalize units before calculating, and keep order-of-magnitude checks explicit throughout. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-n013",
      "type": "numeric-input",
      "question": "Proposal: daily backup of 10 TB in 1-hour window at 100 Mbps. Required bandwidth in Gbps?",
      "answer": 22.2,
      "unit": "Gbps",
      "tolerance": 0.15,
      "explanation": "10 TB / 3600 sec = 2.78 GB/s = 22.2 Gbps. 100 Mbps is ~200× too slow.",
      "detailedExplanation": "Anchor the math in base units and check each transformation to avoid compounding conversion errors. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "reason-n014",
      "type": "numeric-input",
      "question": "Design claims 1μs network latency between servers. Realistic same-rack latency in μs?",
      "answer": 100,
      "unit": "μs",
      "tolerance": 0.5,
      "explanation": "Same-rack: 100-500μs. Same-DC: 500μs-1ms. 1μs is unrealistic except for specialized hardware.",
      "detailedExplanation": "Start with unit normalization, then verify that the final magnitude passes a quick sanity check. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "reason-n015",
      "type": "numeric-input",
      "question": "Estimate: 100 engineers can build and maintain a 10M LOC codebase. Typical LOC per engineer (thousands)?",
      "answer": 10,
      "unit": "K",
      "tolerance": 0.5,
      "explanation": "Industry heuristic: 5-15K LOC per engineer for maintenance. 100K LOC/engineer is too high.",
      "detailedExplanation": "Reduce the problem to base units, compute, and sanity-check the output scale before finalizing. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n016",
      "type": "numeric-input",
      "question": "Proposal: 10ms p99 latency for a service calling 5 downstream services sequentially (each 5ms p99). Minimum p99 in ms?",
      "answer": 25,
      "unit": "ms",
      "tolerance": 0.2,
      "explanation": "5 sequential calls at 5ms each = 25ms minimum. 10ms is impossible.",
      "detailedExplanation": "Make the units explicit at every step, then validate the resulting magnitude against known anchors. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n017",
      "type": "numeric-input",
      "question": "Claim: single SSD can sustain 1 million IOPS. Typical high-end SSD IOPS (thousands)?",
      "answer": 500,
      "unit": "K",
      "tolerance": 0.5,
      "explanation": "High-end NVMe SSDs achieve 200K-800K IOPS. 1M is at the upper extreme.",
      "detailedExplanation": "Convert to base units first, then track powers of ten so arithmetic mistakes are easier to catch. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-n018",
      "type": "numeric-input",
      "question": "Design says 100 Kafka partitions needed for 100 msg/sec. Reasonable partition count?",
      "answer": 1,
      "tolerance": 0.5,
      "explanation": "Single Kafka partition handles 10K+ msg/sec. 100 partitions for 100 msg/sec is massive over-provisioning.",
      "detailedExplanation": "Use base-unit arithmetic plus a magnitude check to keep the estimate robust under time pressure. Message design should specify delivery guarantees, ordering scope, and backpressure behavior under failure, not just happy-path throughput.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-o001",
      "type": "ordering",
      "question": "Rank these estimates by how overestimated they are (least to most).",
      "items": [
        "10 servers for 1000 QPS",
        "100 GB for 1M user profiles",
        "1ms cross-DC latency",
        "1B rows in 10 GB"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "10 servers (2-5× over) < 100GB (10× over) < 1ms latency (50× under) < 1B rows (100× over).",
      "detailedExplanation": "Prioritize ratio-based comparisons and validate each neighboring step to avoid inversion mistakes. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-o002",
      "type": "ordering",
      "question": "Rank by plausibility for a startup MVP (most to least plausible).",
      "items": [
        "99.9% availability",
        "10ms global latency",
        "1M QPS day one",
        "10K daily active users"
      ],
      "correctOrder": [3, 0, 1, 2],
      "explanation": "10K DAU is realistic. 99.9% is achievable. 10ms global and 1M QPS are unrealistic for MVP.",
      "detailedExplanation": "Establish the extremes first and fill the middle with pairwise comparisons. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-o003",
      "type": "ordering",
      "question": "Rank these latencies by how realistic they are (most to least realistic).",
      "items": [
        "100ms API response",
        "1ms database query",
        "10μs network hop",
        "1ns disk read"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "100ms API is normal. 1ms DB query is fast but possible. 10μs network is aggressive. 1ns disk is impossible.",
      "detailedExplanation": "Use relative magnitude to draft the order and confirm it with local adjacency checks. The key API design signal is tradeoff clarity: client ergonomics, backward compatibility, and evolvability should be justified explicitly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "reason-o004",
      "type": "ordering",
      "question": "Rank by accuracy of estimate for 1M users (most to least accurate).",
      "items": [
        "1 TB photo storage",
        "100 GB metadata",
        "10 servers",
        "1 PB total storage"
      ],
      "correctOrder": [1, 2, 0, 3],
      "explanation": "100 GB metadata (~100B/user) reasonable. 10 servers reasonable. 1 TB photos (1 MB/user) low. 1 PB way too high.",
      "detailedExplanation": "Compare relative scale first, then confirm neighboring items pairwise to lock in the order. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-o005",
      "type": "ordering",
      "question": "Rank these claims by red-flag severity (least to most concerning).",
      "items": [
        "Linear scaling to 10×",
        "Zero downtime deployments",
        "1μs P99 latency",
        "Infinite horizontal scaling"
      ],
      "correctOrder": [1, 0, 2, 3],
      "explanation": "Zero-downtime is achievable. Linear 10× is optimistic but possible. 1μs P99 is unrealistic. Infinite scaling is impossible.",
      "detailedExplanation": "Order by relative impact rather than exact values, then verify the sequence one boundary at a time. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-o006",
      "type": "ordering",
      "question": "Rank by how much the estimate misses by (smallest to largest miss).",
      "items": [
        "'1 server for 1M users'",
        "'10ms cross-continent'",
        "'1 GB for 1B records'",
        "'100% cache hit rate'"
      ],
      "correctOrder": [1, 0, 3, 2],
      "explanation": "10ms cross-continent (10× off). 1 server/1M users (10-100× off). 100% cache (impossible). 1 GB/1B records (1000× off).",
      "detailedExplanation": "Build the ordering from major scale differences first, then refine with adjacent comparisons. Caching answers should define invalidation behavior and staleness boundaries; hit rate alone is not a correctness guarantee.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-o007",
      "type": "ordering",
      "question": "Rank infrastructure costs by how underestimated they typically are (least to most underestimated).",
      "items": [
        "Compute costs",
        "Data transfer costs",
        "Storage costs",
        "Operational costs"
      ],
      "correctOrder": [2, 0, 1, 3],
      "explanation": "Storage is predictable. Compute is understood. Data transfer surprises people. Operations (people, tooling) often massively underestimated.",
      "detailedExplanation": "Rank by dominant bottleneck or magnitude, then validate adjacent transitions for consistency. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "reason-o008",
      "type": "ordering",
      "question": "Rank by typical accuracy of back-of-envelope estimates (most to least accurate).",
      "items": [
        "Storage calculations",
        "Latency estimates",
        "Cost projections",
        "Availability targets"
      ],
      "correctOrder": [0, 1, 3, 2],
      "explanation": "Storage math is straightforward. Latency can be measured. Availability is complex. Costs have many hidden factors.",
      "detailedExplanation": "Start with the clear smallest/largest anchors, then place intermediate items by pairwise checks. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        }
      ]
    },
    {
      "id": "reason-m001",
      "type": "multi-select",
      "question": "Which estimates are reasonable for a social media app with 10M DAU?",
      "options": [
        "100 TB storage",
        "10,000 QPS average",
        "99.9% availability",
        "1 engineer team"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "100 TB (10KB/user), 10K QPS (86 req/user/day), 99.9% all reasonable. 1 engineer can't maintain at this scale.",
      "detailedExplanation": "Assess each option separately and keep answers that hold across the full problem context. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "reason-m002",
      "type": "multi-select",
      "question": "Which are red flags in a system design proposal?",
      "options": [
        "Ignoring network latency",
        "Assuming 100% cache hit rate",
        "Planning for 2× peak capacity",
        "Using eventually consistent storage"
      ],
      "correctIndices": [0, 1],
      "explanation": "Ignoring latency and 100% cache are unrealistic. 2× headroom and eventual consistency are valid choices.",
      "detailedExplanation": "Check every option on its own merits and reject statements that are only true under hidden assumptions. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-m003",
      "type": "multi-select",
      "question": "Which claims should trigger sanity check questions?",
      "options": [
        "1ms response time for any request",
        "Horizontal scaling to any load",
        "Zero data loss guarantee",
        "All operations are idempotent"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All of these are extreme claims that need verification. Even idempotency should be confirmed.",
      "detailedExplanation": "Evaluate each option independently against the constraint instead of looking for a pattern across choices. A good sanity check compares against anchor numbers and names which assumption must change if the result looks implausible.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-m004",
      "type": "multi-select",
      "question": "Which are physically plausible for a single server?",
      "options": [
        "500K WebSocket connections",
        "1 million IOPS",
        "10 Gbps network throughput",
        "1 TB RAM"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "500K connections (with tuning), 10 Gbps NIC, 1 TB RAM are possible. 1M IOPS is at the extreme limit.",
      "detailedExplanation": "Score each option independently and keep only those that remain valid under the stated constraints. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        }
      ]
    },
    {
      "id": "reason-m005",
      "type": "multi-select",
      "question": "An estimate claims 1 PB/year data growth. Which user bases could justify this?",
      "options": [
        "1M users, text only",
        "10M users with photos",
        "100M users with video",
        "1B users, text only"
      ],
      "correctIndices": [2, 3],
      "explanation": "1 PB/year ≈ 3 TB/day. 100M × 30KB/day ✓ or 1B × 3KB/day ✓. 1M text or 10M photos is way under.",
      "detailedExplanation": "Treat each candidate as a separate true/false check against the same governing requirement. Growth planning should show today vs horizon requirements side by side because compounding makes small percentages material quickly.",
      "references": [
        {
          "title": "Amazon S3 FAQs",
          "url": "https://aws.amazon.com/s3/faqs/"
        },
        {
          "title": "Google Cloud Storage pricing",
          "url": "https://cloud.google.com/storage/pricing"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "reason-m006",
      "type": "multi-select",
      "question": "Which indicate an estimate may be off by orders of magnitude?",
      "options": [
        "Unusually round numbers",
        "No consideration of overhead",
        "Latency under speed-of-light limits",
        "Linear extrapolation of exponential process"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Missing overhead, physics violations, and wrong growth model cause large errors. Round numbers alone aren't red flags.",
      "detailedExplanation": "Run a one-by-one validity check and discard options that depend on unstated conditions. Growth planning should show today vs horizon requirements side by side because compounding makes small percentages material quickly.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "reason-m007",
      "type": "multi-select",
      "question": "Which are valid simplifications in back-of-envelope estimates?",
      "options": [
        "Ignore metadata overhead",
        "Round to nearest power of 10",
        "Assume uniform distribution",
        "Ignore network latency"
      ],
      "correctIndices": [1, 2],
      "explanation": "Rounding and uniform distribution are acceptable simplifications. Ignoring overhead and latency can cause significant errors.",
      "detailedExplanation": "Avoid grouped guessing: test every option directly against the system boundary condition. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-m008",
      "type": "multi-select",
      "question": "Which capacity claims need immediate verification?",
      "options": [
        "Single DB handles all writes",
        "Cache hit rate above 99%",
        "Zero message loss",
        "Sub-second failover"
      ],
      "correctIndices": [0, 1, 2, 3],
      "explanation": "All of these are strong claims that need evidence. They're possible but shouldn't be assumed.",
      "detailedExplanation": "Use independent validation per option to prevent partial truths from slipping into the final set. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-t001",
      "type": "two-stage",
      "stages": [
        {
          "question": "A proposal claims 10ms average latency for a request that crosses 3 services. Is this reasonable if each service is 5ms?",
          "options": [
            "Yes, services run in parallel",
            "No, minimum is 15ms sequential",
            "Yes, if caching is used",
            "Depends on network latency"
          ],
          "correct": 1,
          "explanation": "3 sequential services × 5ms = 15ms minimum. 10ms is impossible without parallelization.",
          "detailedExplanation": "Use the earlier stage as the boundary condition and solve this step under that constraint. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "What additional time should you budget for network hops between services?",
          "options": [
            "~0.1ms per hop",
            "~1ms per hop",
            "~10ms per hop",
            "~100ms per hop"
          ],
          "correct": 1,
          "explanation": "Same-datacenter network hops are typically 0.5-2ms each.",
          "detailedExplanation": "Use the result from the previous step as a hard constraint, then evaluate this stage against it. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load."
        }
      ],
      "explanation": "Always account for network latency between services.",
      "detailedExplanation": "Handle this as a sequential constraint problem where each stage narrows the feasible design space. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    },
    {
      "id": "reason-t002",
      "type": "two-stage",
      "stages": [
        {
          "question": "Proposal claims 1 GB storage for 1 million user sessions at 10 KB each. Is this correct?",
          "options": [
            "Yes, math checks out",
            "No, need 10 GB",
            "No, need 100 MB",
            "Depends on compression"
          ],
          "correct": 1,
          "explanation": "1M × 10 KB = 10 GB, not 1 GB. Off by 10×.",
          "detailedExplanation": "Translate the prior stage outcome into an operational check before selecting the next move. Storage estimates should separate raw data, replication, and retention horizon so capacity and cost risk are visible early."
        },
        {
          "question": "If sessions also need 20% overhead for indexes, actual storage needed?",
          "options": ["10 GB", "12 GB", "15 GB", "20 GB"],
          "correct": 1,
          "explanation": "10 GB × 1.2 = 12 GB with index overhead.",
          "detailedExplanation": "Carry the prior stage assumptions forward and revalidate units before making the next decision. Data model choices are strongest when grounded in query paths, write amplification, and index/storage overhead."
        }
      ],
      "explanation": "Always verify arithmetic and add overhead factors.",
      "detailedExplanation": "Solve this as a chained decision where step two must inherit the constraints from step one. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-t003",
      "type": "two-stage",
      "stages": [
        {
          "question": "Design claims 99.99% availability with monthly maintenance windows of 4 hours. What's the actual max availability?",
          "options": ["99.99%", "99.95%", "99.5%", "99%"],
          "correct": 2,
          "explanation": "4 hours/month = 48 hours/year. 48/8760 = 0.55% downtime = 99.45% availability max.",
          "detailedExplanation": "Keep the previous conclusion fixed, then test this stage for consistency and scale. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them."
        },
        {
          "question": "To achieve 99.99% (52 min/year), max maintenance window per month?",
          "options": ["~1 minute", "~4 minutes", "~15 minutes", "~1 hour"],
          "correct": 1,
          "explanation": "52 min/year ÷ 12 months = 4.3 minutes per month max.",
          "detailedExplanation": "This stage is best solved by propagating the earlier result and validating edge conditions. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "explanation": "Planned downtime counts against availability SLAs.",
      "detailedExplanation": "Think of this as staged reasoning: outcome one becomes input constraint for outcome two. Translate target percentages into concrete time or request budgets, then test whether incident frequency and recovery speed can actually satisfy them.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "AWS Pricing Calculator",
          "url": "https://calculator.aws/#/"
        }
      ]
    },
    {
      "id": "reason-t004",
      "type": "two-stage",
      "stages": [
        {
          "question": "Estimate claims 100 Mbps sufficient for 1000 concurrent video streams at 5 Mbps each. Actual bandwidth needed?",
          "options": ["100 Mbps", "500 Mbps", "5 Gbps", "50 Gbps"],
          "correct": 2,
          "explanation": "1000 × 5 Mbps = 5000 Mbps = 5 Gbps. Estimate is 50× too low.",
          "detailedExplanation": "Advance from the first-stage output, then verify that this decision still respects the same limits. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load."
        },
        {
          "question": "How many concurrent streams can 100 Mbps actually support at 5 Mbps each?",
          "options": ["2 streams", "10 streams", "20 streams", "100 streams"],
          "correct": 2,
          "explanation": "100 Mbps ÷ 5 Mbps = 20 streams max.",
          "detailedExplanation": "Treat this as a continuation step: preserve earlier constraints and recheck feasibility. Compute sizing should distinguish average vs peak demand and map both to per-node limits and scaling triggers."
        }
      ],
      "explanation": "Bandwidth requirements scale linearly with concurrent streams.",
      "detailedExplanation": "The key is continuity across stages: preserve prior constraints and re-evaluate under the new question. Bandwidth planning should normalize units first, then include protocol overhead and peak multipliers to avoid underestimating production load.",
      "references": [
        {
          "title": "Cloudflare CDN and cache concepts",
          "url": "https://developers.cloudflare.com/cache/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "Azure bandwidth pricing",
          "url": "https://azure.microsoft.com/pricing/details/bandwidth/"
        }
      ]
    }
  ]
}
