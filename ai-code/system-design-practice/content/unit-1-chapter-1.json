{
  "unit": 1,
  "unitTitle": "Estimation",
  "chapter": 1,
  "chapterTitle": "Reference Numbers",
  "chapterDescription": "Rapid recall of key numbers for back-of-envelope calculations",
  "problems": [
    {
      "id": "ref-001",
      "type": "numeric-input",
      "question": "How many seconds are in one day?",
      "answer": 86400,
      "unit": "seconds",
      "tolerance": 0.1,
      "explanation": "24 hours × 60 min × 60 sec = 86,400 seconds. Round to ~100K for quick estimates.",
      "detailedExplanation": "The '86,400 seconds in a day' number appears constantly in system design. Whenever you convert daily metrics to per-second rates (DAU to QPS), you divide by 86,400. The shortcut: round to ~10<sup>5</sup> (100K) for quick mental math—it's only ~15% off. In interviews, saying '≈100K seconds per day' instantly signals you know your reference numbers."
    },
    {
      "id": "ref-002",
      "type": "numeric-input",
      "question": "How many seconds are in one month (30 days)?",
      "answer": 2600000,
      "unit": "seconds",
      "tolerance": 0.2,
      "explanation": "30 days × 86,400 = 2,592,000 seconds. Round to ~2.6M for quick estimates.",
      "detailedExplanation": "2.6 million seconds per month is the key conversion factor for monthly capacity planning. When a PM says 'we need to handle X million requests per month,' divide by 2.6M to get QPS. This number also helps estimate storage growth: if each request generates 1 KB of data, then 2.6M requests/month ≈ 2.6 GB/month of storage."
    },
    {
      "id": "ref-003",
      "type": "numeric-input",
      "question": "How many seconds are in one year?",
      "answer": 31500000,
      "unit": "seconds",
      "tolerance": 0.15,
      "explanation": "365 days × 86,400 = 31,536,000 seconds. Round to ~30M for quick estimates.",
      "detailedExplanation": "~30 million seconds per year is useful for annual capacity projections and SLA calculations. For example, 99.99% availability means 0.01% × 31.5M ≈ 3,150 seconds ≈ 52.5 minutes of allowed downtime per year. When estimating yearly storage needs, multiply your per-second data rate by 30M."
    },
    {
      "id": "ref-004",
      "type": "multiple-choice",
      "question": "At 1 QPS (query per second), approximately how many requests do you handle per month?",
      "options": ["260K", "2.6M", "26M", "260M"],
      "correct": 1,
      "explanation": "1 QPS × 2.6M seconds/month = 2.6M requests per month.",
      "detailedExplanation": "This conversion bridges per-second thinking (capacity planning) and per-month thinking (business metrics, billing). If your service runs at 100 QPS, that's 260M requests/month. At 1 QPS, your service generates 2.6M log entries per month—knowing this helps you estimate logging storage costs and choose the right log retention policy."
    },
    {
      "id": "ref-005",
      "type": "multiple-choice",
      "question": "1 million DAU with 10 requests per user per day translates to approximately what QPS?",
      "options": ["10 QPS", "100 QPS", "1,000 QPS", "10,000 QPS"],
      "correct": 1,
      "explanation": "1M users × 10 req = 10M req/day ÷ 86,400 sec = ~115 QPS ≈ 100 QPS.",
      "detailedExplanation": "The DAU-to-QPS conversion is one of the most common interview calculations. The key insight: divide total daily requests by ~100K (seconds/day). A useful shortcut: 1M daily requests ≈ 12 QPS. Also consider peak-to-average ratio—peak traffic is typically 2-5x the average, so 100 QPS average might mean 200-500 QPS peak. Always size infrastructure for peak, not average."
    },
    {
      "id": "ref-006",
      "type": "ordering",
      "question": "Rank these storage units from smallest to largest.",
      "items": ["TB", "MB", "PB", "GB", "KB"],
      "correctOrder": [4, 1, 3, 0, 2],
      "explanation": "KB (10^3) < MB (10^6) < GB (10^9) < TB (10^12) < PB (10^15).",
      "detailedExplanation": "Each step in the storage hierarchy is 1,000x (or 1,024x in binary). The standard prefix sequence: <strong>K</strong>ilo, <strong>M</strong>ega, <strong>G</strong>iga, <strong>T</strong>era, <strong>P</strong>eta—each 1,000x the previous. In interviews, you'll frequently convert between these: 1 PB = 1,000 TB = 1M GB. Modern data warehouses commonly deal in PB; a single server's RAM is typically measured in GB."
    },
    {
      "id": "ref-007",
      "type": "numeric-input",
      "question": "What is 2^10 (approximately)?",
      "answer": 1024,
      "tolerance": 0.05,
      "explanation": "2^10 = 1,024 ≈ 1 thousand. This is the basis for binary storage units.",
      "detailedExplanation": "2<sup>10</sup> = 1,024 is the foundation of all binary-to-decimal conversions—it's why 1 KB ≈ 1,000 bytes. The ~2.4% error from rounding 1,024 to 1,000 compounds at higher powers but remains small enough for interview estimates. Chain this building block: 2<sup>20</sup> ≈ 1M, 2<sup>30</sup> ≈ 1B, 2<sup>40</sup> ≈ 1T."
    },
    {
      "id": "ref-008",
      "type": "numeric-input",
      "question": "What is 2^20 (approximately)?",
      "answer": 1048576,
      "tolerance": 0.1,
      "explanation": "2^20 = 1,048,576 ≈ 1 million. This is 1 MB in binary terms.",
      "detailedExplanation": "2<sup>20</sup> is the basis for megabyte-scale calculations. In practice, it helps you quickly size data structures: a million-entry hash map with 64-byte keys and 8-byte pointers uses ~72 MB—small enough to fit comfortably in a single server's memory. This kind of quick sizing lets you answer 'does this fit in RAM?' during interviews."
    },
    {
      "id": "ref-009",
      "type": "multiple-choice",
      "question": "What is the maximum value of a 32-bit unsigned integer (2^32)?",
      "options": ["~400 million", "~4 billion", "~40 billion", "~400 billion"],
      "correct": 1,
      "explanation": "2^32 = 4,294,967,296 ≈ 4 billion. This limits things like IPv4 addresses.",
      "detailedExplanation": "The 4-billion limit of 32-bit integers explains many real-world constraints. IPv4's 4.3 billion address limit drove the transition to IPv6. Auto-incrementing 32-bit primary keys overflow at ~4B rows. When your system could exceed ~4B of anything, you need 64-bit identifiers."
    },
    {
      "id": "ref-010",
      "type": "ordering",
      "question": "Rank these operations from fastest to slowest.",
      "items": [
        "RAM access",
        "L1 cache",
        "SSD random read",
        "HDD random read",
        "L2 cache"
      ],
      "correctOrder": [1, 4, 0, 2, 3],
      "explanation": "L1 (~1ns) < L2 (~4ns) < RAM (~100ns) < SSD (~100μs) < HDD (~10ms).",
      "detailedExplanation": "This latency hierarchy is arguably the most important reference in system design. The key ratios: L1→L2 is ~4x, L2→RAM is ~25x, RAM→SSD is ~1,000x, SSD→HDD is ~100x. In interviews, knowing these ratios helps justify design decisions like 'we need a cache because going to disk would be 100,000x slower than memory.'"
    },
    {
      "id": "ref-011",
      "type": "numeric-input",
      "question": "What is the typical RAM access latency in nanoseconds?",
      "answer": 100,
      "unit": "ns",
      "tolerance": 0.5,
      "explanation": "RAM access is approximately 100 nanoseconds.",
      "detailedExplanation": "100 nanoseconds for RAM access means a single server can do ~10M random memory lookups per second. In-memory databases like Redis exploit this: with data in RAM, response times are dominated by network overhead (~0.5ms same-datacenter), not data access. This is why caching is so effective—moving data from disk (SSD at ~100μs, HDD at ~10ms) to nanosecond-range memory yields 1,000-100,000x speedups."
    },
    {
      "id": "ref-012",
      "type": "numeric-input",
      "question": "What is the typical SSD random read latency in microseconds?",
      "answer": 100,
      "unit": "μs",
      "tolerance": 0.5,
      "explanation": "SSD random read is approximately 100 microseconds (0.1 ms).",
      "detailedExplanation": "SSD random read at 100 microseconds means ~10K random reads per second per drive. This is why database IOPS matter: a query touching 100 random pages needs 10ms on SSD. NVMe SSDs can be 3-5x faster (~20-30μs), which is why cloud providers offer NVMe instance storage for I/O-intensive workloads. For sequential reads, SSDs are much faster—throughput of 500 MB/s to 3+ GB/s."
    },
    {
      "id": "ref-013",
      "type": "numeric-input",
      "question": "What is the typical HDD random read latency in milliseconds?",
      "answer": 10,
      "unit": "ms",
      "tolerance": 0.5,
      "explanation": "HDD random read is approximately 10 milliseconds due to mechanical seek time.",
      "detailedExplanation": "10ms HDD latency comes from physical constraints: the disk must spin to the right sector (rotational latency, ~4ms at 7200 RPM) and the head must seek (~5-6ms). This limits HDDs to ~100-150 random IOPS. HDDs are still used for bulk storage where sequential access dominates (log files, backups, data lakes) because their cost per GB is 3-5x lower than SSDs."
    },
    {
      "id": "ref-014",
      "type": "multiple-choice",
      "question": "What is the typical round-trip time within the same datacenter?",
      "options": ["0.05 ms", "0.5 ms", "5 ms", "50 ms"],
      "correct": 1,
      "explanation": "Same-datacenter RTT is typically around 0.5 ms (500 microseconds).",
      "detailedExplanation": "Same-datacenter RTT of ~0.5ms is the baseline cost of any network call between services. A request that fans out to 10 microservices sequentially adds ~5ms just from network overhead. This is why fan-out architectures prefer parallel calls, and why co-locating tightly-coupled services in the same availability zone matters for latency-sensitive paths."
    },
    {
      "id": "ref-015",
      "type": "multiple-choice",
      "question": "What is the typical round-trip time for cross-region communication?",
      "options": ["5-15 ms", "50-150 ms", "500-1500 ms", "5-15 seconds"],
      "correct": 1,
      "explanation": "Cross-region RTT is typically 50-150 ms depending on geographic distance.",
      "detailedExplanation": "Cross-region latency is dominated by the speed of light in fiber optic cable (~200,000 km/s, or ~5μs/km). Fiber doesn't run in straight lines, so US East-to-West is ~7,000+ km of actual fiber path, yielding ~35ms one-way and ~70ms RTT with routing overhead. This physical constraint is why globally distributed systems need data replication close to users and why strong consistency across regions is expensive—each consensus round costs a full RTT."
    },
    {
      "id": "ref-016",
      "type": "multi-select",
      "question": "Which operations are typically in the nanosecond range?",
      "options": [
        "L1 cache access",
        "RAM access",
        "SSD random read",
        "L2 cache access"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "L1 (~1ns), L2 (~4ns), and RAM (~100ns) are nanosecond operations. SSD is microseconds.",
      "detailedExplanation": "Knowing which operations are in the nanosecond range tells you what's 'free' in system design. L1, L2, and RAM operations are all fast enough that a CPU can do millions per second. The jump from nanoseconds (memory) to microseconds (SSD) is where careful optimization begins, and the jump to milliseconds (HDD, network) is where architectural decisions like caching and data locality become critical."
    },
    {
      "id": "ref-017",
      "type": "numeric-input",
      "question": "What is the typical sequential read throughput of an SSD in MB/s?",
      "answer": 500,
      "unit": "MB/s",
      "tolerance": 0.5,
      "explanation": "SSD sequential read is typically 500 MB/s to 3 GB/s for modern NVMe drives.",
      "detailedExplanation": "SSD sequential read at 500 MB/s (SATA) to 3+ GB/s (NVMe) means you can scan 1 GB of data in 0.3-2 seconds. This matters for analytics workloads: a full table scan of a 100 GB table takes ~30-200 seconds from SSD. This is why columnar storage formats (Parquet, ORC) are popular for analytics—they let you read only the columns you need, dramatically reducing I/O."
    },
    {
      "id": "ref-018",
      "type": "multiple-choice",
      "question": "What throughput does a 1 Gbps network provide in MB/s?",
      "options": ["12.5 MB/s", "125 MB/s", "1,250 MB/s", "1 MB/s"],
      "correct": 1,
      "explanation": "1 Gbps = 1,000 Mbps ÷ 8 = 125 MB/s (bits to bytes).",
      "detailedExplanation": "The bits-to-bytes conversion (÷8) catches many people in interviews. A 10 Gbps network link delivers 1.25 GB/s of data throughput. This matters for data transfer estimates: moving 1 TB over a 1 Gbps link takes ~2.2 hours. For very large data migrations (petabytes), it's often faster to physically ship hard drives—AWS Snowball exists for exactly this reason."
    },
    {
      "id": "ref-019",
      "type": "multiple-choice",
      "question": "How much downtime per year does 99.9% availability (three 9s) allow?",
      "options": ["52.6 minutes", "8.76 hours", "3.65 days", "5.26 minutes"],
      "correct": 1,
      "explanation": "99.9% uptime = 0.1% downtime = 0.001 × 365 × 24 = 8.76 hours/year.",
      "detailedExplanation": "Three 9s (99.9%) is a common SLA for non-critical services. It allows ~8.76 hours of downtime per year, or ~43 minutes per month. Most startups initially target three 9s because achieving four 9s requires significantly more investment in redundancy, monitoring, and on-call response. Each additional 9 roughly requires 10x more engineering effort and infrastructure cost."
    },
    {
      "id": "ref-020",
      "type": "multiple-choice",
      "question": "How much downtime per year does 99.99% availability (four 9s) allow?",
      "options": ["5.26 minutes", "52.6 minutes", "8.76 hours", "87.6 minutes"],
      "correct": 1,
      "explanation": "99.99% uptime = 0.01% downtime = 0.0001 × 365 × 24 × 60 = 52.6 min/year.",
      "detailedExplanation": "Four 9s (99.99%) is the standard target for critical production services at large companies. At 52.6 minutes per year, you can afford at most one major incident per quarter. Achieving this typically requires: automated failover (manual response is too slow), redundant components with no single point of failure, health checks and circuit breakers, and 24/7 on-call with sub-5-minute response time."
    },
    {
      "id": "ref-021",
      "type": "ordering",
      "question": "Rank these availability levels from least to most reliable.",
      "items": ["99.99%", "99%", "99.999%", "99.9%"],
      "correctOrder": [1, 3, 0, 2],
      "explanation": "99% (3.65 days down) < 99.9% (8.76 hrs) < 99.99% (52.6 min) < 99.999% (5.26 min).",
      "detailedExplanation": "Each additional 9 reduces allowed downtime by 10x. For compound systems, availabilities multiply: two services in series at 99.9% each give 99.9% × 99.9% = 99.8% combined. This is why minimizing dependencies in the critical path matters—every additional service in the chain reduces overall availability. Redundant (parallel) components improve availability: two 99% services in active-active give 1 - (0.01)² = 99.99%."
    },
    {
      "id": "ref-022",
      "type": "numeric-input",
      "question": "How many operations per second can a single Redis instance typically handle?",
      "answer": 100000,
      "unit": "ops/sec",
      "tolerance": 0.5,
      "explanation": "Redis can handle 100K+ operations per second on a single instance.",
      "detailedExplanation": "Redis achieves 100K+ ops/sec because its command execution is single-threaded (no lock contention), in-memory (nanosecond access), and uses efficient data structures. At 100K ops/sec, one Redis instance handles the cache needs of most services. The bottleneck is usually network bandwidth, not CPU. For higher throughput, Redis Cluster shards across multiple nodes."
    },
    {
      "id": "ref-023",
      "type": "multiple-choice",
      "question": "What is the typical QPS range a single web server can handle?",
      "options": ["10-100", "100-1K", "1K-10K", "10K-100K"],
      "correct": 2,
      "explanation": "A typical web server handles 1K-10K QPS depending on workload complexity.",
      "detailedExplanation": "A single web server handling 1K-10K QPS varies widely by workload: simple API proxying might hit 10K QPS, while request handling with database queries might only manage 1K QPS. This number drives horizontal scaling calculations: if you need 50K QPS, plan for 5-50 servers plus headroom. Always account for 2-3x headroom for traffic spikes and graceful degradation."
    },
    {
      "id": "ref-024",
      "type": "multiple-choice",
      "question": "What is the typical cache hit rate for a well-configured CDN?",
      "options": ["50-70%", "70-85%", "90-99%", "99.9%+"],
      "correct": 2,
      "explanation": "A well-configured CDN typically achieves 90-99% cache hit rates for static content.",
      "detailedExplanation": "A 95% CDN cache hit rate means only 5% of requests reach your origin servers. For a service handling 100K QPS at the edge, the origin only sees 5K QPS—a 20x reduction. Cache hit rate depends on content cardinality: a site with 1,000 static assets achieves near-99%, while a site with millions of personalized pages might only reach 70-80%. Tuning cache keys and TTLs is how you improve hit rates."
    },
    {
      "id": "ref-025",
      "type": "numeric-input",
      "question": "How many bytes is a UUID?",
      "answer": 16,
      "unit": "bytes",
      "tolerance": "exact",
      "explanation": "A UUID is 128 bits = 16 bytes.",
      "detailedExplanation": "UUIDs are 128 bits (16 bytes), typically displayed as 36 characters with hyphens (e.g., <code>550e8400-e29b-41d4-a716-446655440000</code>). In storage estimates, use 16 bytes for binary storage or 36 bytes for string representation. UUIDs are popular for distributed ID generation because they don't require coordination, but they're larger than auto-incrementing integers (4-8 bytes) and their randomness can hurt B-tree index performance."
    },
    {
      "id": "ref-026",
      "type": "numeric-input",
      "question": "How many bytes is an IPv4 address?",
      "answer": 4,
      "unit": "bytes",
      "tolerance": "exact",
      "explanation": "IPv4 addresses are 32 bits = 4 bytes (e.g., 192.168.1.1).",
      "detailedExplanation": "IPv4's 4-byte (32-bit) address space gives ~4.3 billion addresses, which seemed infinite in the 1980s but is now exhausted. This drove NAT adoption (letting multiple devices share one public IP). In system design, IPv4 addresses are relevant for estimating IP-based rate limiting storage, understanding why CDNs use anycast, and calculating the size of network access control lists."
    },
    {
      "id": "ref-027",
      "type": "numeric-input",
      "question": "How many bytes is an IPv6 address?",
      "answer": 16,
      "unit": "bytes",
      "tolerance": "exact",
      "explanation": "IPv6 addresses are 128 bits = 16 bytes.",
      "detailedExplanation": "IPv6 uses 128 bits (16 bytes), providing 3.4 × 10<sup>38</sup> addresses—more than 10<sup>28</sup> per person on Earth, effectively unlimited for any practical purpose. In system design, IPv6 matters for storage sizing (IP-based logs are 4x larger than IPv4), firewall rule storage, and future-proofing network infrastructure. Dual-stack systems supporting both protocols need to account for the larger address size in their data models."
    },
    {
      "id": "ref-028",
      "type": "multiple-choice",
      "question": "What is the typical size of a tweet-length text message?",
      "options": ["30 bytes", "300 bytes", "3 KB", "30 KB"],
      "correct": 1,
      "explanation": "A tweet (280 chars) with metadata is roughly 300 bytes.",
      "detailedExplanation": "300 bytes per short text message is a useful baseline for sizing text-heavy systems. For a chat app with 1M messages/day: 1M × 300B = 300 MB/day ≈ 9 GB/month ≈ 108 GB/year—easily fits on a single database. Add metadata (user IDs, timestamps, indexes) and the number roughly doubles. This kind of quick sizing helps you decide early whether you need sharding or if a single node suffices."
    },
    {
      "id": "ref-029",
      "type": "multiple-choice",
      "question": "What is the typical size of a full-resolution smartphone photo?",
      "options": ["100 KB", "500 KB", "1-5 MB", "10-20 MB"],
      "correct": 2,
      "explanation": "Full-size photos from modern smartphones are typically 1-5 MB compressed.",
      "detailedExplanation": "Photo size heavily impacts storage system design. For Instagram-scale (100M photos/day × 3 MB average): that's 300 TB/day of raw storage. This is why photo services use aggressive compression, multiple resolution variants (thumbnail, medium, full), object storage (S3) instead of databases, and CDNs for serving. The metadata (user, location, tags) is tiny (~1 KB) compared to the image itself."
    },
    {
      "id": "ref-030",
      "type": "multiple-choice",
      "question": "What is the typical size of 1 hour of 1080p video?",
      "options": ["100-300 MB", "500-800 MB", "1-3 GB", "5-10 GB"],
      "correct": 2,
      "explanation": "1 hour of 1080p video is typically 1-3 GB depending on compression.",
      "detailedExplanation": "Video is the largest common data type in system design. YouTube ingests 500+ hours of video per minute. At ~2 GB/hour, that's ~60 TB/day of raw uploads. Services transcode each video into multiple bitrates and resolutions (adaptive bitrate streaming), multiplying storage by 5-10x. This is why video platforms are among the largest storage consumers globally and rely heavily on CDN edge caching."
    },
    {
      "id": "ref-031",
      "type": "ordering",
      "question": "Rank these data sizes from smallest to largest.",
      "items": [
        "Full photo",
        "UUID",
        "Video thumbnail",
        "API response",
        "Tweet text"
      ],
      "correctOrder": [1, 4, 3, 2, 0],
      "explanation": "UUID (16B) < Tweet (~300B) < API response (1-10KB) < Thumbnail (10-50KB) < Photo (1-5MB).",
      "detailedExplanation": "Knowing relative data sizes lets you quickly estimate storage needs. The hierarchy spans 5 orders of magnitude: UUID (16B) → text snippet (300B) → API response (1-10 KB) → thumbnail (10-50 KB) → full photo (1-5 MB). A billion UUIDs is ~16 GB (fits in RAM); a billion photos is ~3 PB (needs distributed object storage). These size intuitions help you choose the right storage tier in interviews."
    },
    {
      "id": "ref-032",
      "type": "numeric-input",
      "question": "If you have 100 QPS, how many requests per month?",
      "answer": 260000000,
      "tolerance": 0.15,
      "explanation": "100 QPS × 2.6M sec/month = 260M requests per month.",
      "detailedExplanation": "This scaling calculation is essential for capacity planning and cost estimation. Cloud services often price per-million requests: at 260M requests/month, an API Gateway at $3.50/million costs ~$910/month. This number also helps with database sizing: if each request writes a 1 KB row, 260M rows/month = 260 GB/month of data growth, which informs your sharding and retention strategies."
    },
    {
      "id": "ref-033",
      "type": "multiple-choice",
      "question": "What does 2^30 approximately equal?",
      "options": ["1 million", "10 million", "100 million", "1 billion"],
      "correct": 3,
      "explanation": "2^30 = 1,073,741,824 ≈ 1 billion. This is approximately 1 GB.",
      "detailedExplanation": "2<sup>30</sup> ≈ 1 billion (1 GB in binary) is useful for quick memory calculations. A server with 64 GB RAM has ~64 billion bytes. If each cached object is 1 KB, you can cache ~64 million objects in memory. This kind of back-of-envelope math helps you answer 'does this fit in memory?' questions in interviews without needing a calculator."
    },
    {
      "id": "ref-034",
      "type": "multiple-choice",
      "question": "What does 2^40 approximately equal?",
      "options": ["1 billion", "10 billion", "100 billion", "1 trillion"],
      "correct": 3,
      "explanation": "2^40 ≈ 1 trillion. This is approximately 1 TB.",
      "detailedExplanation": "2<sup>40</sup> ≈ 1 trillion (1 TB in binary) sets the boundary for single-machine storage. Modern servers can have up to 16-32 TB of SSD storage. If your dataset exceeds ~10 TB, you likely need distributed storage or sharding. This reference also helps with ID space calculations: a 40-bit counter can represent ~1 trillion unique values before wrapping."
    },
    {
      "id": "ref-035",
      "type": "multi-select",
      "question": "Which of these numbers are approximately 1 million?",
      "options": [
        "Seconds in a day",
        "2^20",
        "Seconds in a month",
        "1 MB in bytes"
      ],
      "correctIndices": [1, 3],
      "explanation": "2^20 ≈ 1M and 1 MB = 1M bytes. Seconds/day is ~86K, seconds/month is ~2.6M.",
      "detailedExplanation": "Anchoring reference numbers to 'approximately 1 million' helps with quick sizing. Since 1 MB = 1M bytes: 1M users × 1 KB profile = 1 GB total, which easily fits in memory. Similarly, 1M DAU generating 10 requests each = 10M requests/day ≈ 120 QPS. These 'million-anchored' building blocks let you compose larger estimates without losing track of orders of magnitude."
    },
    {
      "id": "ref-036",
      "type": "numeric-input",
      "question": "What is L1 cache latency in nanoseconds?",
      "answer": 1,
      "unit": "ns",
      "tolerance": 0.5,
      "explanation": "L1 cache is the fastest cache tier at approximately 1 nanosecond.",
      "detailedExplanation": "L1 cache at ~1ns represents the fastest data access possible on modern hardware. L1 is typically 32-64 KB per CPU core—tiny but blazingly fast. CPU-bound processing at L1 speeds can handle billions of simple operations per second. The key takeaway for system design: computation is rarely the bottleneck—I/O (disk, network) almost always is."
    },
    {
      "id": "ref-037",
      "type": "numeric-input",
      "question": "What is L2 cache latency in nanoseconds?",
      "answer": 4,
      "unit": "ns",
      "tolerance": 0.5,
      "explanation": "L2 cache is approximately 4 nanoseconds.",
      "detailedExplanation": "L2 cache at ~4ns is 256 KB to 1 MB per core. The 4x slowdown from L1 comes from the larger cache size requiring more complex lookups. In practice, L2 vs L1 rarely matters for system design interviews—what matters is the massive gap between on-chip caches (1-4ns) and main memory (100ns). This 25-100x gap is why CPU cache efficiency matters for high-performance systems like databases."
    },
    {
      "id": "ref-038",
      "type": "multiple-choice",
      "question": "How many times slower is SSD access compared to RAM?",
      "options": ["10x", "100x", "1,000x", "10,000x"],
      "correct": 2,
      "explanation": "RAM ~100ns, SSD ~100μs. 100μs / 100ns = 1,000x slower.",
      "detailedExplanation": "The 1,000x gap between RAM and SSD is the single most important ratio in system design. It's why caching works: serving from cache (RAM, ~100ns) vs database on disk (SSD, ~100μs) is 1,000x faster for cached data. Even a modest 90% cache hit rate means 90% of requests are 1,000x faster. This ratio justifies the cost premium of RAM (roughly 20-50x more per GB than SSD) for hot data."
    },
    {
      "id": "ref-039",
      "type": "multiple-choice",
      "question": "How many times slower is HDD access compared to SSD?",
      "options": ["10x", "100x", "1,000x", "10,000x"],
      "correct": 1,
      "explanation": "SSD ~100μs, HDD ~10ms. 10ms / 100μs = 100x slower.",
      "detailedExplanation": "The 100x gap between SSD and HDD comes primarily from mechanical seek time. For random access workloads (database queries), this means ~100 IOPS (HDD) vs ~10,000 IOPS (SSD). However, for sequential access (streaming video, log files), the gap narrows significantly because HDD doesn't need to seek. This is why HDDs remain cost-effective for append-heavy and sequential-read workloads like archival storage."
    },
    {
      "id": "ref-040",
      "type": "ordering",
      "question": "Rank these throughputs from lowest to highest.",
      "items": [
        "HDD sequential",
        "10 Gbps network",
        "SSD sequential",
        "1 Gbps network"
      ],
      "correctOrder": [3, 0, 2, 1],
      "explanation": "1Gbps (125MB/s) < HDD (100-200MB/s) < SSD (500MB/s-3GB/s) < 10Gbps (1.25GB/s).",
      "detailedExplanation": "This hierarchy has a surprising result: HDD sequential reads (~150 MB/s) are faster than 1 Gbps network (125 MB/s). This means transferring large datasets locally from disk is faster than pulling them over a standard network link. It's why Hadoop was designed around data locality—moving computation to data is faster than moving data to computation. For 10 Gbps networks (1.25 GB/s), network transfer exceeds SATA SSD speeds, though NVMe SSDs (3+ GB/s) can still outpace it."
    },
    {
      "id": "ref-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need sub-millisecond access times. Which storage tier is appropriate?",
          "options": ["SSD", "RAM", "HDD", "Network storage"],
          "correct": 1,
          "explanation": "RAM access (~100ns) is well under 1ms. SSD (~100μs) is borderline."
        },
        {
          "question": "Given RAM access, what's the order of magnitude latency difference compared to HDD?",
          "options": [
            "1,000x faster",
            "10,000x faster",
            "100,000x faster",
            "1,000,000x faster"
          ],
          "correct": 2,
          "explanation": "RAM ~100ns, HDD ~10ms. 10ms / 100ns = 100,000x faster.",
          "detailedExplanation": "The 100,000x gap between RAM and HDD makes them suited for fundamentally different access patterns. RAM is for hot, randomly-accessed data (caches, session stores, real-time indexes). HDD is for cold, sequentially-accessed data (archives, logs, backups). SSDs sit in between for warm data with moderate random access. Choosing the right storage tier for each data class is a core system design skill."
        }
      ],
      "explanation": "Understanding the latency hierarchy is crucial for system design decisions."
    },
    {
      "id": "ref-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Your service has 10M DAU with 5 requests per user per day. What's your approximate QPS?",
          "options": ["~60 QPS", "~600 QPS", "~6,000 QPS", "~60,000 QPS"],
          "correct": 1,
          "explanation": "10M × 5 = 50M req/day ÷ 86,400 sec ≈ 580 QPS ≈ 600 QPS."
        },
        {
          "question": "At this QPS, approximately how many requests per month?",
          "options": ["~150M", "~1.5B", "~15B", "~150B"],
          "correct": 1,
          "explanation": "600 QPS × 2.6M sec/month ≈ 1.56 billion requests per month.",
          "detailedExplanation": "The three-step conversion chain—DAU → daily requests → QPS → monthly volume—is a foundational interview pattern. Always start from the user-facing metric (DAU) and work toward infrastructure metrics (QPS, storage). Remember to account for peak traffic: if average QPS is 600, peak might be 2,000-3,000 QPS during busy hours. Size your infrastructure for peak, not average, or use auto-scaling."
        }
      ],
      "explanation": "Converting between DAU, QPS, and monthly requests is a fundamental estimation skill."
    },
    {
      "id": "ref-043",
      "type": "multiple-choice",
      "question": "What throughput can Kafka typically achieve per broker?",
      "options": [
        "1K-10K msg/sec",
        "10K-100K msg/sec",
        "100K-1M msg/sec",
        "1M-10M msg/sec"
      ],
      "correct": 2,
      "explanation": "Kafka can handle 100K-1M messages per second per broker.",
      "detailedExplanation": "Kafka achieves high throughput through sequential disk writes (append-only logs), batching, zero-copy network transfer, and horizontal partitioning. A 3-broker Kafka cluster can handle 300K-3M messages/second. In system design, Kafka is the default choice for high-throughput event streaming, log aggregation, and decoupling producers from consumers. Its throughput advantage over traditional message brokers like RabbitMQ (~10K-50K msg/s) makes it ideal for large-scale data pipelines."
    },
    {
      "id": "ref-044",
      "type": "numeric-input",
      "question": "How many bytes is a 64-bit timestamp?",
      "answer": 8,
      "unit": "bytes",
      "tolerance": "exact",
      "explanation": "64 bits ÷ 8 bits/byte = 8 bytes.",
      "detailedExplanation": "8 bytes for a timestamp is a common overhead in data models. Every log entry, event record, and database row typically has at least one timestamp. In storage sizing: 1 billion events with an 8-byte timestamp = 8 GB just for timestamps. 64-bit millisecond-precision Unix timestamps won't overflow until the year 292,278,994—effectively infinite for practical purposes."
    },
    {
      "id": "ref-045",
      "type": "multiple-choice",
      "question": "What is the typical size of a compressed image thumbnail?",
      "options": ["1-5 KB", "10-50 KB", "100-200 KB", "500 KB - 1 MB"],
      "correct": 1,
      "explanation": "Compressed thumbnails are typically 10-50 KB.",
      "detailedExplanation": "Thumbnails at 10-50 KB are small enough to be served efficiently from cache and loaded quickly on mobile networks. A feed showing 20 thumbnails loads ~200 KB-1 MB of images—fast even on 3G. This is why image-heavy services generate thumbnails at upload time rather than resizing on the fly. Typical systems store 3-5 size variants per image: thumbnail (50 KB), medium (200 KB), and full (1-5 MB)."
    },
    {
      "id": "ref-046",
      "type": "multi-select",
      "question": "Which are in the millisecond range for latency?",
      "options": [
        "HDD random read",
        "Same datacenter RTT",
        "Cross-region RTT",
        "SSD random read"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "HDD (~10ms), same-DC RTT (~0.5ms), cross-region (~50-150ms) are all milliseconds. SSD is microseconds.",
      "detailedExplanation": "The millisecond range spans three very different operations: same-DC network (~0.5ms), HDD random read (~10ms), and cross-region network (~50-150ms). Millisecond-range latencies are 'noticeable but acceptable' for most user-facing operations. The latency budget for an API response is typically 100-500ms, so you can afford a few millisecond-range operations per request but not dozens—this is why minimizing serial dependencies matters."
    },
    {
      "id": "ref-047",
      "type": "ordering",
      "question": "Rank these time durations from shortest to longest.",
      "items": ["1 microsecond", "1 millisecond", "1 nanosecond", "1 second"],
      "correctOrder": [2, 0, 1, 3],
      "explanation": "1ns < 1μs (1000ns) < 1ms (1000μs) < 1s (1000ms).",
      "detailedExplanation": "The SI prefix chain nano→micro→milli→unit has 1,000x between each step. A useful mnemonic: <strong>n</strong>s (billionths), <strong>μ</strong>s (millionths), <strong>m</strong>s (thousandths), <strong>s</strong> (ones). In system design conversations, always specify the unit—'100 microseconds' and '100 milliseconds' differ by 1,000x, and confusing them leads to wildly wrong capacity estimates."
    },
    {
      "id": "ref-048",
      "type": "multiple-choice",
      "question": "What is the typical size range of a JSON API response?",
      "options": ["10-100 bytes", "100-500 bytes", "1-10 KB", "50-100 KB"],
      "correct": 2,
      "explanation": "Typical JSON API responses are 1-10 KB.",
      "detailedExplanation": "1-10 KB per API response is a useful default for estimating bandwidth and storage. At 10K QPS with 5 KB average responses: 50 MB/s outbound bandwidth. For response caching: 1 million cached responses × 5 KB = 5 GB—easily fits in Redis. Larger payloads (>100 KB) often indicate the need for pagination, field filtering (GraphQL-style), or response compression (gzip typically achieves 70-80% compression on JSON)."
    },
    {
      "id": "ref-049",
      "type": "numeric-input",
      "question": "At 1,000 QPS, how many requests per month (in billions)?",
      "answer": 2.6,
      "unit": "billion",
      "tolerance": 0.2,
      "explanation": "1,000 QPS × 2.6M sec/month = 2.6 billion requests per month.",
      "detailedExplanation": "2.6 billion requests per month at 1K QPS is a useful threshold for infrastructure decisions. At this scale, you likely need multiple application servers behind a load balancer, database read replicas or caching, and monitoring for latency percentiles (p99, p999). Many databases can handle this read volume on a single node with proper indexing, but write-heavy workloads at this scale often require sharding."
    },
    {
      "id": "ref-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "You need 99.99% availability. How many minutes of downtime per year is allowed?",
          "options": ["~5 minutes", "~53 minutes", "~8.8 hours", "~3.6 days"],
          "correct": 1,
          "explanation": "99.99% = 52.6 minutes downtime/year."
        },
        {
          "question": "If each incident takes 15 minutes to resolve, how many incidents can you have per year?",
          "options": [
            "~3 incidents",
            "~10 incidents",
            "~35 incidents",
            "~100 incidents"
          ],
          "correct": 0,
          "explanation": "52.6 min ÷ 15 min/incident ≈ 3.5 incidents maximum per year.",
          "detailedExplanation": "This calculation reveals why high availability is so hard operationally. With only ~3 incidents allowed per year at 99.99%, every deployment, infrastructure change, and dependency failure matters. Teams achieving four 9s typically use: canary deployments (catch bugs before full rollout), automated rollback (recover in minutes, not hours), redundant dependencies (no single external service can take you down), and error budgets (explicitly trading reliability for development velocity)."
        }
      ],
      "explanation": "Availability targets directly constrain your incident response capabilities."
    }
  ]
}
