{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 5,
  "chapterTitle": "Chat Core Messaging Architecture",
  "chapterDescription": "Decompose end-to-end chat messaging flow across send, route, store, deliver, and ack paths.",
  "problems": [
    {
      "id": "cd-cc-001",
      "type": "multiple-choice",
      "question": "Case Alpha: message send API. Dominant risk is out-of-order delivery in group conversations. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Use per-conversation ordering keys with idempotent delivery semantics.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In Chat Core Messaging Architecture, message send API fails mainly through out-of-order delivery in group conversations. The best choice is \"Use per-conversation ordering keys with idempotent delivery semantics\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "Read this as a scenario about \"message send API\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-002",
      "type": "multiple-choice",
      "question": "Case Beta: connection/session router. Dominant risk is duplicate delivery during reconnect storms. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Separate durable message commit from best-effort realtime push fanout.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For connection/session router, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Separate durable message commit from best-effort realtime push fanout\" outperforms the alternatives because it targets duplicate delivery during reconnect storms and preserves safe recovery behavior. It is also the most compatible with Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "The key clue in this question is \"connection/session router\". Discard growth plans without a clear trigger point for capacity action. Show present and projected demand side by side so scaling deadlines are visible early. Common pitfall: forecasting volume without resource thresholds.",
      "references": [
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        },
        {
          "title": "Rule of 72",
          "url": "https://www.investopedia.com/terms/r/ruleof72.asp"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-003",
      "type": "multiple-choice",
      "question": "Case Gamma: group chat fanout worker. Dominant risk is offline message backlog growth during outages. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Persist delivery state transitions so reconnect logic can resume safely.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat group chat fanout worker as a reliability-control decision, not an averages-only optimization. \"Persist delivery state transitions so reconnect logic can resume safely\" is correct since it mitigates offline message backlog growth during outages while keeping containment local. The decision remains valid given: Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "detailedExplanation": "Start from \"group chat fanout worker\", then pressure-test the result against the options. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cc-004",
      "type": "multiple-choice",
      "question": "Case Delta: message persistence log. Dominant risk is session stickiness causing uneven gateway load. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use adaptive group fanout strategy (small-group push, large-group pull)."
      ],
      "correct": 3,
      "explanation": "Message persistence log should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Use adaptive group fanout strategy (small-group push, large-group pull)\" is strongest because it directly addresses session stickiness causing uneven gateway load and improves repeatability under stress. This aligns with the extra condition (A previous rollback fixed averages but not tail impact).",
      "detailedExplanation": "If you keep \"message persistence log\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: offline inbox service. Dominant risk is ack loss leading to false undelivered state. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Implement ack dedupe and replay-safe retry windows for unstable clients.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In Chat Core Messaging Architecture, offline inbox service fails mainly through ack loss leading to false undelivered state. The best choice is \"Implement ack dedupe and replay-safe retry windows for unstable clients\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: User trust risk is highest on this path.",
      "detailedExplanation": "The core signal here is \"offline inbox service\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-006",
      "type": "multiple-choice",
      "question": "Case Zeta: delivery ack processor. Dominant risk is fanout overload for large group channels. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Partition gateway sessions by stable user key and rebalance with drain controls.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "For delivery ack processor, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Partition gateway sessions by stable user key and rebalance with drain controls\" outperforms the alternatives because it targets fanout overload for large group channels and preserves safe recovery behavior. It is also the most compatible with A shared dependency has uncertain health right now.",
      "detailedExplanation": "Use \"delivery ack processor\" as your starting point, then verify tradeoffs carefully. Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-007",
      "type": "multiple-choice",
      "question": "Case Eta: typing indicator stream. Dominant risk is history query latency spikes on hot chats. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Serve history from optimized recent-window store plus archival fallback.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Treat typing indicator stream as a reliability-control decision, not an averages-only optimization. \"Serve history from optimized recent-window store plus archival fallback\" is correct since it mitigates history query latency spikes on hot chats while keeping containment local. The decision remains valid given: The change must preserve cost discipline during peak.",
      "detailedExplanation": "This prompt is really about \"typing indicator stream\". Reject designs that improve throughput while weakening reliability guarantees. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-008",
      "type": "multiple-choice",
      "question": "Case Theta: message history fetch API. Dominant risk is delivery retries triggering duplicate notifications. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Protect offline inbox throughput with per-tenant quotas and backpressure."
      ],
      "correct": 3,
      "explanation": "Message history fetch API should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Protect offline inbox throughput with per-tenant quotas and backpressure\" is strongest because it directly addresses delivery retries triggering duplicate notifications and improves repeatability under stress. This aligns with the extra condition (Telemetry shows risk concentrated in one partition class).",
      "detailedExplanation": "The decision turns on \"message history fetch API\". Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cc-009",
      "type": "multiple-choice",
      "question": "Case Iota: message ordering coordinator. Dominant risk is presence and delivery paths contending for same infra. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Isolate typing/presence ephemeral streams from critical delivery workflows.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "In Chat Core Messaging Architecture, message ordering coordinator fails mainly through presence and delivery paths contending for same infra. The best choice is \"Isolate typing/presence ephemeral streams from critical delivery workflows\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "Read this as a scenario about \"message ordering coordinator\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-010",
      "type": "multiple-choice",
      "question": "Case Kappa: cross-device delivery service. Dominant risk is cross-region routing drift for roaming users. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use regional affinity with explicit cross-region forwarding contracts for roamers.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat cross-device delivery service as a reliability-control decision, not an averages-only optimization. \"Use regional affinity with explicit cross-region forwarding contracts for roamers\" is correct since it mitigates cross-region routing drift for roaming users while keeping containment local. The decision remains valid given: Current runbooks are missing explicit ownership for this boundary.",
      "detailedExplanation": "Start from \"cross-device delivery service\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-011",
      "type": "multiple-choice",
      "question": "Case Lambda: message send API. Dominant risk is out-of-order delivery in group conversations. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use per-conversation ordering keys with idempotent delivery semantics.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Message send API should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Use per-conversation ordering keys with idempotent delivery semantics\" is strongest because it directly addresses out-of-order delivery in group conversations and improves repeatability under stress. This aligns with the extra condition (A cross-region path recently changed behavior after migration).",
      "detailedExplanation": "The key clue in this question is \"message send API\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-012",
      "type": "multiple-choice",
      "question": "Case Mu: connection/session router. Dominant risk is duplicate delivery during reconnect storms. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Separate durable message commit from best-effort realtime push fanout."
      ],
      "correct": 3,
      "explanation": "In Chat Core Messaging Architecture, connection/session router fails mainly through duplicate delivery during reconnect storms. The best choice is \"Separate durable message commit from best-effort realtime push fanout\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "Read this as a scenario about \"connection/session router\". Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-013",
      "type": "multiple-choice",
      "question": "Case Nu: group chat fanout worker. Dominant risk is offline message backlog growth during outages. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Persist delivery state transitions so reconnect logic can resume safely.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For group chat fanout worker, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Persist delivery state transitions so reconnect logic can resume safely\" outperforms the alternatives because it targets offline message backlog growth during outages and preserves safe recovery behavior. It is also the most compatible with Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "The decision turns on \"group chat fanout worker\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "cd-cc-014",
      "type": "multiple-choice",
      "question": "Case Xi: message persistence log. Dominant risk is session stickiness causing uneven gateway load. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use adaptive group fanout strategy (small-group push, large-group pull).",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat message persistence log as a reliability-control decision, not an averages-only optimization. \"Use adaptive group fanout strategy (small-group push, large-group pull)\" is correct since it mitigates session stickiness causing uneven gateway load while keeping containment local. The decision remains valid given: A partial failure is masking itself as success in metrics.",
      "detailedExplanation": "This prompt is really about \"message persistence log\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-015",
      "type": "multiple-choice",
      "question": "Case Omicron: offline inbox service. Dominant risk is ack loss leading to false undelivered state. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Implement ack dedupe and replay-safe retry windows for unstable clients.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Offline inbox service should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Implement ack dedupe and replay-safe retry windows for unstable clients\" is strongest because it directly addresses ack loss leading to false undelivered state and improves repeatability under stress. This aligns with the extra condition (This fix must hold under celebrity or campaign spike conditions).",
      "detailedExplanation": "Use \"offline inbox service\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-016",
      "type": "multiple-choice",
      "question": "Case Pi: delivery ack processor. Dominant risk is fanout overload for large group channels. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Partition gateway sessions by stable user key and rebalance with drain controls."
      ],
      "correct": 3,
      "explanation": "In Chat Core Messaging Architecture, delivery ack processor fails mainly through fanout overload for large group channels. The best choice is \"Partition gateway sessions by stable user key and rebalance with drain controls\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "The core signal here is \"delivery ack processor\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-017",
      "type": "multiple-choice",
      "question": "Case Rho: typing indicator stream. Dominant risk is history query latency spikes on hot chats. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Serve history from optimized recent-window store plus archival fallback.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "For typing indicator stream, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Serve history from optimized recent-window store plus archival fallback\" outperforms the alternatives because it targets history query latency spikes on hot chats and preserves safe recovery behavior. It is also the most compatible with On-call requested a reversible operational first step.",
      "detailedExplanation": "If you keep \"typing indicator stream\" in view, the correct answer separates faster. Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-018",
      "type": "multiple-choice",
      "question": "Case Sigma: message history fetch API. Dominant risk is delivery retries triggering duplicate notifications. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Protect offline inbox throughput with per-tenant quotas and backpressure.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Treat message history fetch API as a reliability-control decision, not an averages-only optimization. \"Protect offline inbox throughput with per-tenant quotas and backpressure\" is correct since it mitigates delivery retries triggering duplicate notifications while keeping containment local. The decision remains valid given: The system mixes strict and eventual paths with unclear contracts.",
      "detailedExplanation": "Start from \"message history fetch API\", then pressure-test the result against the options. Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-019",
      "type": "multiple-choice",
      "question": "Case Tau: message ordering coordinator. Dominant risk is presence and delivery paths contending for same infra. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Isolate typing/presence ephemeral streams from critical delivery workflows.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "Message ordering coordinator should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Isolate typing/presence ephemeral streams from critical delivery workflows\" is strongest because it directly addresses presence and delivery paths contending for same infra and improves repeatability under stress. This aligns with the extra condition (A hot-key pattern is likely from real traffic skew).",
      "detailedExplanation": "The key clue in this question is \"message ordering coordinator\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: cross-device delivery service. Dominant risk is cross-region routing drift for roaming users. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use regional affinity with explicit cross-region forwarding contracts for roamers."
      ],
      "correct": 3,
      "explanation": "For cross-device delivery service, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Use regional affinity with explicit cross-region forwarding contracts for roamers\" outperforms the alternatives because it targets cross-region routing drift for roaming users and preserves safe recovery behavior. It is also the most compatible with The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "Use \"cross-device delivery service\" as your starting point, then verify tradeoffs carefully. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-021",
      "type": "multiple-choice",
      "question": "Case Phi: message send API. Dominant risk is out-of-order delivery in group conversations. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Use per-conversation ordering keys with idempotent delivery semantics.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat message send API as a reliability-control decision, not an averages-only optimization. \"Use per-conversation ordering keys with idempotent delivery semantics\" is correct since it mitigates out-of-order delivery in group conversations while keeping containment local. The decision remains valid given: Compliance requires explicit behavior for edge-case failures.",
      "detailedExplanation": "This prompt is really about \"message send API\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-022",
      "type": "multiple-choice",
      "question": "Case Chi: connection/session router. Dominant risk is duplicate delivery during reconnect storms. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Separate durable message commit from best-effort realtime push fanout.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Connection/session router should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Separate durable message commit from best-effort realtime push fanout\" is strongest because it directly addresses duplicate delivery during reconnect storms and improves repeatability under stress. This aligns with the extra condition (This boundary has failed during the last two game days).",
      "detailedExplanation": "If you keep \"connection/session router\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-023",
      "type": "multiple-choice",
      "question": "Case Psi: group chat fanout worker. Dominant risk is offline message backlog growth during outages. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Persist delivery state transitions so reconnect logic can resume safely.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Chat Core Messaging Architecture, group chat fanout worker fails mainly through offline message backlog growth during outages. The best choice is \"Persist delivery state transitions so reconnect logic can resume safely\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "The core signal here is \"group chat fanout worker\". Prefer the choice that balances hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "cd-cc-024",
      "type": "multiple-choice",
      "question": "Case Omega: message persistence log. Dominant risk is session stickiness causing uneven gateway load. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Use adaptive group fanout strategy (small-group push, large-group pull)."
      ],
      "correct": 3,
      "explanation": "For message persistence log, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Use adaptive group fanout strategy (small-group push, large-group pull)\" outperforms the alternatives because it targets session stickiness causing uneven gateway load and preserves safe recovery behavior. It is also the most compatible with Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "The key clue in this question is \"message persistence log\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-025",
      "type": "multiple-choice",
      "question": "Case Atlas: offline inbox service. Dominant risk is ack loss leading to false undelivered state. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Implement ack dedupe and replay-safe retry windows for unstable clients.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat offline inbox service as a reliability-control decision, not an averages-only optimization. \"Implement ack dedupe and replay-safe retry windows for unstable clients\" is correct since it mitigates ack loss leading to false undelivered state while keeping containment local. The decision remains valid given: The fix should avoid broad architectural rewrites this quarter.",
      "detailedExplanation": "Start from \"offline inbox service\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-026",
      "type": "multiple-choice",
      "question": "Case Nova: delivery ack processor. Dominant risk is fanout overload for large group channels. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Partition gateway sessions by stable user key and rebalance with drain controls.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "Delivery ack processor should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Partition gateway sessions by stable user key and rebalance with drain controls\" is strongest because it directly addresses fanout overload for large group channels and improves repeatability under stress. This aligns with the extra condition (Current metrics hide per-tenant variance that matters).",
      "detailedExplanation": "The decision turns on \"delivery ack processor\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-027",
      "type": "multiple-choice",
      "question": "Case Orion: typing indicator stream. Dominant risk is history query latency spikes on hot chats. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Serve history from optimized recent-window store plus archival fallback.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "In Chat Core Messaging Architecture, typing indicator stream fails mainly through history query latency spikes on hot chats. The best choice is \"Serve history from optimized recent-window store plus archival fallback\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The fallback path is under-tested in production-like load.",
      "detailedExplanation": "Read this as a scenario about \"typing indicator stream\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-028",
      "type": "multiple-choice",
      "question": "Case Vega: message history fetch API. Dominant risk is delivery retries triggering duplicate notifications. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Protect offline inbox throughput with per-tenant quotas and backpressure."
      ],
      "correct": 3,
      "explanation": "For message history fetch API, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Protect offline inbox throughput with per-tenant quotas and backpressure\" outperforms the alternatives because it targets delivery retries triggering duplicate notifications and preserves safe recovery behavior. It is also the most compatible with A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "Use \"message history fetch API\" as your starting point, then verify tradeoffs carefully. Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-029",
      "type": "multiple-choice",
      "question": "Case Helios: message ordering coordinator. Dominant risk is presence and delivery paths contending for same infra. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Isolate typing/presence ephemeral streams from critical delivery workflows.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Treat message ordering coordinator as a reliability-control decision, not an averages-only optimization. \"Isolate typing/presence ephemeral streams from critical delivery workflows\" is correct since it mitigates presence and delivery paths contending for same infra while keeping containment local. The decision remains valid given: The system must preserve critical events over bulk traffic.",
      "detailedExplanation": "This prompt is really about \"message ordering coordinator\". Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-030",
      "type": "multiple-choice",
      "question": "Case Aurora: cross-device delivery service. Dominant risk is cross-region routing drift for roaming users. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use regional affinity with explicit cross-region forwarding contracts for roamers.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In Chat Core Messaging Architecture, cross-device delivery service fails mainly through cross-region routing drift for roaming users. The best choice is \"Use regional affinity with explicit cross-region forwarding contracts for roamers\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "Read this as a scenario about \"cross-device delivery service\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: message send API. Dominant risk is out-of-order delivery in group conversations. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Use per-conversation ordering keys with idempotent delivery semantics.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For message send API, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Use per-conversation ordering keys with idempotent delivery semantics\" outperforms the alternatives because it targets out-of-order delivery in group conversations and preserves safe recovery behavior. It is also the most compatible with The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "The decision turns on \"message send API\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-032",
      "type": "multiple-choice",
      "question": "Case Pulse: connection/session router. Dominant risk is duplicate delivery during reconnect storms. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents.",
        "Separate durable message commit from best-effort realtime push fanout."
      ],
      "correct": 3,
      "explanation": "Treat connection/session router as a reliability-control decision, not an averages-only optimization. \"Separate durable message commit from best-effort realtime push fanout\" is correct since it mitigates duplicate delivery during reconnect storms while keeping containment local. The decision remains valid given: Operational complexity is rising faster than team onboarding.",
      "detailedExplanation": "Start from \"connection/session router\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-033",
      "type": "multiple-choice",
      "question": "Case Forge: group chat fanout worker. Dominant risk is offline message backlog growth during outages. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Persist delivery state transitions so reconnect logic can resume safely.",
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 0,
      "explanation": "Group chat fanout worker should be solved at the failure boundary named in Chat Core Messaging Architecture. \"Persist delivery state transitions so reconnect logic can resume safely\" is strongest because it directly addresses offline message backlog growth during outages and improves repeatability under stress. This aligns with the extra condition (Stakeholders need clear trade-off rationale in the postmortem).",
      "detailedExplanation": "The key clue in this question is \"group chat fanout worker\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    },
    {
      "id": "cd-cc-034",
      "type": "multiple-choice",
      "question": "Case Harbor: message persistence log. Dominant risk is session stickiness causing uneven gateway load. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Use adaptive group fanout strategy (small-group push, large-group pull).",
        "Optimize only average latency and defer reliability controls until later.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 1,
      "explanation": "In Chat Core Messaging Architecture, message persistence log fails mainly through session stickiness causing uneven gateway load. The best choice is \"Use adaptive group fanout strategy (small-group push, large-group pull)\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "The core signal here is \"message persistence log\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-035",
      "type": "multiple-choice",
      "question": "Case Vector: offline inbox service. Dominant risk is ack loss leading to false undelivered state. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply a single global policy across all paths without boundary differentiation.",
        "Optimize only average latency and defer reliability controls until later.",
        "Implement ack dedupe and replay-safe retry windows for unstable clients.",
        "Rely on manual intervention as the primary mitigation for repeated incidents."
      ],
      "correct": 2,
      "explanation": "For offline inbox service, prefer the option that prevents reoccurrence in Chat Core Messaging Architecture. \"Implement ack dedupe and replay-safe retry windows for unstable clients\" outperforms the alternatives because it targets ack loss leading to false undelivered state and preserves safe recovery behavior. It is also the most compatible with Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "If you keep \"offline inbox service\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message send API: signal points to session stickiness causing uneven gateway load. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around message send API mismatches session stickiness causing uneven gateway load, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Chat Core Messaging Architecture: for Incident review for message send API: signal points to session stickiness causing uneven gateway load, \"The current decomposition around message send API mismatches session stickiness causing uneven gateway load, creating repeated failures\" is correct because it addresses session stickiness causing uneven gateway load and improves controllability.",
          "detailedExplanation": "Start from \"incident review for message send API: signal points to session stickiness causing\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for message send API: signal points to\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Partition gateway sessions by stable user key and rebalance with drain controls.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Partition gateway sessions by stable user key and rebalance with drain controls\" best matches With root cause identified for \"incident review for message send API: signal points to\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Core Messaging Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for connection/session router: signal points to ack loss leading to false undelivered state. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around connection/session router mismatches ack loss leading to false undelivered state, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Chat Core Messaging Architecture, the best answer is \"The current decomposition around connection/session router mismatches ack loss leading to false undelivered state, creating repeated failures\". It is the option most directly aligned to ack loss leading to false undelivered state while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"incident review for connection/session router: signal points to ack loss leading to\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for connection/session router: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Serve history from optimized recent-window store plus archival fallback.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Chat Core Messaging Architecture: for After diagnosing \"incident review for connection/session router: signal\", which next change should be prioritized first, \"Serve history from optimized recent-window store plus archival fallback\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Use \"chat Core Messaging Architecture\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for group chat fanout worker: signal points to fanout overload for large group channels. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around group chat fanout worker mismatches fanout overload for large group channels, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Incident review for group chat fanout worker: signal points to fanout overload for large group channels is a two-step reliability decision. At stage 1, \"The current decomposition around group chat fanout worker mismatches fanout overload for large group channels, creating repeated failures\" wins because it balances immediate containment with long-term prevention around fanout overload for large group channels.",
          "detailedExplanation": "Read this as a scenario about \"incident review for group chat fanout worker: signal points to fanout overload for\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident review for group chat fanout worker: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Protect offline inbox throughput with per-tenant quotas and backpressure."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Chat Core Messaging Architecture, the best answer is \"Protect offline inbox throughput with per-tenant quotas and backpressure\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"chat Core Messaging Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message persistence log: signal points to history query latency spikes on hot chats. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around message persistence log mismatches history query latency spikes on hot chats, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around message persistence log mismatches history query latency spikes on hot chats, creating repeated failures\" best matches Incident review for message persistence log: signal points to history query latency spikes on hot chats by targeting history query latency spikes on hot chats and lowering repeat risk.",
          "detailedExplanation": "Use \"incident review for message persistence log: signal points to history query latency\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for message persistence log: signal\", what first move gives the best reliability impact?",
          "options": [
            "Isolate typing/presence ephemeral streams from critical delivery workflows.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "For \"incident review for message persistence log: signal\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Isolate typing/presence ephemeral streams from critical delivery workflows\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Core Messaging Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for offline inbox service: signal points to delivery retries triggering duplicate notifications. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around offline inbox service mismatches delivery retries triggering duplicate notifications, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Chat Core Messaging Architecture, the best answer is \"The current decomposition around offline inbox service mismatches delivery retries triggering duplicate notifications, creating repeated failures\". It is the option most directly aligned to delivery retries triggering duplicate notifications while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"incident review for offline inbox service: signal points to delivery retries triggering\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for offline inbox service: signal\" is diagnosed, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Use regional affinity with explicit cross-region forwarding contracts for roamers.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Chat Core Messaging Architecture: for Now that \"incident review for offline inbox service: signal\" is diagnosed, what is the highest-leverage change to make now, \"Use regional affinity with explicit cross-region forwarding contracts for roamers\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"chat Core Messaging Architecture\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for delivery ack processor: signal points to presence and delivery paths contending for same infra. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around delivery ack processor mismatches presence and delivery paths contending for same infra, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Incident review for delivery ack processor: signal points to presence and delivery paths contending for same infra is a two-step reliability decision. At stage 1, \"The current decomposition around delivery ack processor mismatches presence and delivery paths contending for same infra, creating repeated failures\" wins because it balances immediate containment with long-term prevention around presence and delivery paths contending for same infra.",
          "detailedExplanation": "The core signal here is \"incident review for delivery ack processor: signal points to presence and delivery\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for delivery ack processor: signal\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Use per-conversation ordering keys with idempotent delivery semantics.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Chat Core Messaging Architecture, the best answer is \"Use per-conversation ordering keys with idempotent delivery semantics\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Core Messaging Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for typing indicator stream: signal points to cross-region routing drift for roaming users. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around typing indicator stream mismatches cross-region routing drift for roaming users, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around typing indicator stream mismatches cross-region routing drift for roaming users, creating repeated failures\" best matches Incident review for typing indicator stream: signal points to cross-region routing drift for roaming users by targeting cross-region routing drift for roaming users and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"incident review for typing indicator stream: signal points to cross-region routing\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for typing indicator stream: signal\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Separate durable message commit from best-effort realtime push fanout."
          ],
          "correct": 3,
          "explanation": "After diagnosing \"incident review for typing indicator stream: signal\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Separate durable message commit from best-effort realtime push fanout\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"chat Core Messaging Architecture\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message history fetch API: signal points to out-of-order delivery in group conversations. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around message history fetch API mismatches out-of-order delivery in group conversations, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Chat Core Messaging Architecture: for Incident review for message history fetch API: signal points to out-of-order delivery in group conversations, \"The current decomposition around message history fetch API mismatches out-of-order delivery in group conversations, creating repeated failures\" is correct because it addresses out-of-order delivery in group conversations and improves controllability.",
          "detailedExplanation": "Start from \"incident review for message history fetch API: signal points to out-of-order delivery\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With root cause identified for \"incident review for message history fetch API: signal\", which next change should be prioritized first?",
          "options": [
            "Persist delivery state transitions so reconnect logic can resume safely.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Persist delivery state transitions so reconnect logic can resume safely\" best matches With root cause identified for \"incident review for message history fetch API: signal\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Core Messaging Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message ordering coordinator: signal points to duplicate delivery during reconnect storms. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around message ordering coordinator mismatches duplicate delivery during reconnect storms, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Chat Core Messaging Architecture, the best answer is \"The current decomposition around message ordering coordinator mismatches duplicate delivery during reconnect storms, creating repeated failures\". It is the option most directly aligned to duplicate delivery during reconnect storms while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"incident review for message ordering coordinator: signal points to duplicate delivery\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for message ordering coordinator:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Use adaptive group fanout strategy (small-group push, large-group pull).",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Chat Core Messaging Architecture: for For \"incident review for message ordering coordinator:\", which immediate adjustment best addresses the risk, \"Use adaptive group fanout strategy (small-group push, large-group pull)\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Core Messaging Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for cross-device delivery service: signal points to offline message backlog growth during outages. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around cross-device delivery service mismatches offline message backlog growth during outages, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Incident review for cross-device delivery service: signal points to offline message backlog growth during outages is a two-step reliability decision. At stage 1, \"The current decomposition around cross-device delivery service mismatches offline message backlog growth during outages, creating repeated failures\" wins because it balances immediate containment with long-term prevention around offline message backlog growth during outages.",
          "detailedExplanation": "Read this as a scenario about \"incident review for cross-device delivery service: signal points to offline message\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for cross-device delivery service:\", which next change should be prioritized first?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Implement ack dedupe and replay-safe retry windows for unstable clients.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Chat Core Messaging Architecture, the best answer is \"Implement ack dedupe and replay-safe retry windows for unstable clients\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"chat Core Messaging Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message send API: signal points to session stickiness causing uneven gateway load. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around message send API mismatches session stickiness causing uneven gateway load, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around message send API mismatches session stickiness causing uneven gateway load, creating repeated failures\" best matches Incident review for message send API: signal points to session stickiness causing uneven gateway load by targeting session stickiness causing uneven gateway load and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident review for message send API: signal points to session stickiness causing\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for message send API: signal points to\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Partition gateway sessions by stable user key and rebalance with drain controls."
          ],
          "correct": 3,
          "explanation": "With diagnosis confirmed in \"incident review for message send API: signal points to\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Partition gateway sessions by stable user key and rebalance with drain controls\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Core Messaging Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for connection/session router: signal points to ack loss leading to false undelivered state. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around connection/session router mismatches ack loss leading to false undelivered state, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Stage 1 in Chat Core Messaging Architecture: for Incident review for connection/session router: signal points to ack loss leading to false undelivered state, \"The current decomposition around connection/session router mismatches ack loss leading to false undelivered state, creating repeated failures\" is correct because it addresses ack loss leading to false undelivered state and improves controllability.",
          "detailedExplanation": "This prompt is really about \"incident review for connection/session router: signal points to ack loss leading to\". Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Using the diagnosis from \"incident review for connection/session router: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Serve history from optimized recent-window store plus archival fallback.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Serve history from optimized recent-window store plus archival fallback\" best matches Using the diagnosis from \"incident review for connection/session router: signal\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"chat Core Messaging Architecture\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for group chat fanout worker: signal points to fanout overload for large group channels. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around group chat fanout worker mismatches fanout overload for large group channels, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Chat Core Messaging Architecture, the best answer is \"The current decomposition around group chat fanout worker mismatches fanout overload for large group channels, creating repeated failures\". It is the option most directly aligned to fanout overload for large group channels while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"incident review for group chat fanout worker: signal points to fanout overload for\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for group chat fanout worker: signal\" is diagnosed, which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Protect offline inbox throughput with per-tenant quotas and backpressure.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Chat Core Messaging Architecture: for Now that \"incident review for group chat fanout worker: signal\" is diagnosed, which next step is strongest under current constraints, \"Protect offline inbox throughput with per-tenant quotas and backpressure\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"chat Core Messaging Architecture\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message persistence log: signal points to history query latency spikes on hot chats. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around message persistence log mismatches history query latency spikes on hot chats, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "Incident review for message persistence log: signal points to history query latency spikes on hot chats is a two-step reliability decision. At stage 1, \"The current decomposition around message persistence log mismatches history query latency spikes on hot chats, creating repeated failures\" wins because it balances immediate containment with long-term prevention around history query latency spikes on hot chats.",
          "detailedExplanation": "The core signal here is \"incident review for message persistence log: signal points to history query latency\". Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for message persistence log: signal\" scenario, which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Isolate typing/presence ephemeral streams from critical delivery workflows.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "For stage 2 in Chat Core Messaging Architecture, the best answer is \"Isolate typing/presence ephemeral streams from critical delivery workflows\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Core Messaging Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for offline inbox service: signal points to delivery retries triggering duplicate notifications. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around offline inbox service mismatches delivery retries triggering duplicate notifications, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Chat Core Messaging Architecture: for Incident review for offline inbox service: signal points to delivery retries triggering duplicate notifications, \"The current decomposition around offline inbox service mismatches delivery retries triggering duplicate notifications, creating repeated failures\" is correct because it addresses delivery retries triggering duplicate notifications and improves controllability.",
          "detailedExplanation": "Start from \"incident review for offline inbox service: signal points to delivery retries triggering\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for offline inbox service: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Use regional affinity with explicit cross-region forwarding contracts for roamers."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use regional affinity with explicit cross-region forwarding contracts for roamers\" best matches With root cause identified for \"incident review for offline inbox service: signal\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Core Messaging Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for delivery ack processor: signal points to presence and delivery paths contending for same infra. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around delivery ack processor mismatches presence and delivery paths contending for same infra, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Chat Core Messaging Architecture, the best answer is \"The current decomposition around delivery ack processor mismatches presence and delivery paths contending for same infra, creating repeated failures\". It is the option most directly aligned to presence and delivery paths contending for same infra while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"incident review for delivery ack processor: signal points to presence and delivery\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for delivery ack processor: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Use per-conversation ordering keys with idempotent delivery semantics.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Chat Core Messaging Architecture: for After diagnosing \"incident review for delivery ack processor: signal\", which immediate adjustment best addresses the risk, \"Use per-conversation ordering keys with idempotent delivery semantics\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Use \"chat Core Messaging Architecture\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for typing indicator stream: signal points to cross-region routing drift for roaming users. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around typing indicator stream mismatches cross-region routing drift for roaming users, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for typing indicator stream: signal points to cross-region routing drift for roaming users is a two-step reliability decision. At stage 1, \"The current decomposition around typing indicator stream mismatches cross-region routing drift for roaming users, creating repeated failures\" wins because it balances immediate containment with long-term prevention around cross-region routing drift for roaming users.",
          "detailedExplanation": "The core signal here is \"incident review for typing indicator stream: signal points to cross-region routing\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for typing indicator stream: signal\" scenario, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Separate durable message commit from best-effort realtime push fanout.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Core Messaging Architecture, the best answer is \"Separate durable message commit from best-effort realtime push fanout\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Use \"after diagnosis, what is the strongest next change\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"chat Core Messaging Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message history fetch API: signal points to out-of-order delivery in group conversations. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around message history fetch API mismatches out-of-order delivery in group conversations, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around message history fetch API mismatches out-of-order delivery in group conversations, creating repeated failures\" best matches Incident review for message history fetch API: signal points to out-of-order delivery in group conversations by targeting out-of-order delivery in group conversations and lowering repeat risk.",
          "detailedExplanation": "The key clue in this question is \"incident review for message history fetch API: signal points to out-of-order delivery\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for message history fetch API: signal\" is diagnosed, what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Persist delivery state transitions so reconnect logic can resume safely.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "Now that \"incident review for message history fetch API: signal\" is diagnosed, what should change first before wider rollout is a two-step reliability decision. At stage 2, \"Persist delivery state transitions so reconnect logic can resume safely\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Read this as a scenario about \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "If you keep \"chat Core Messaging Architecture\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message ordering coordinator: signal points to duplicate delivery during reconnect storms. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around message ordering coordinator mismatches duplicate delivery during reconnect storms, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Chat Core Messaging Architecture: for Incident review for message ordering coordinator: signal points to duplicate delivery during reconnect storms, \"The current decomposition around message ordering coordinator mismatches duplicate delivery during reconnect storms, creating repeated failures\" is correct because it addresses duplicate delivery during reconnect storms and improves controllability.",
          "detailedExplanation": "This prompt is really about \"incident review for message ordering coordinator: signal points to duplicate delivery\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for message ordering coordinator:\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Use adaptive group fanout strategy (small-group push, large-group pull)."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use adaptive group fanout strategy (small-group push, large-group pull)\" best matches Using the diagnosis from \"incident review for message ordering coordinator:\", which next step is strongest under current constraints by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "If you keep \"after diagnosis, what is the strongest next change\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"chat Core Messaging Architecture\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for cross-device delivery service: signal points to offline message backlog growth during outages. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around cross-device delivery service mismatches offline message backlog growth during outages, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Chat Core Messaging Architecture, the best answer is \"The current decomposition around cross-device delivery service mismatches offline message backlog growth during outages, creating repeated failures\". It is the option most directly aligned to offline message backlog growth during outages while preserving safe follow-on actions.",
          "detailedExplanation": "If you keep \"incident review for cross-device delivery service: signal points to offline message\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for cross-device delivery service:\", which next change should be prioritized first?",
          "options": [
            "Implement ack dedupe and replay-safe retry windows for unstable clients.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Chat Core Messaging Architecture: for With diagnosis confirmed in \"incident review for cross-device delivery service:\", which next change should be prioritized first, \"Implement ack dedupe and replay-safe retry windows for unstable clients\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Core Messaging Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message send API: signal points to session stickiness causing uneven gateway load. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around message send API mismatches session stickiness causing uneven gateway load, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "Incident review for message send API: signal points to session stickiness causing uneven gateway load is a two-step reliability decision. At stage 1, \"The current decomposition around message send API mismatches session stickiness causing uneven gateway load, creating repeated failures\" wins because it balances immediate containment with long-term prevention around session stickiness causing uneven gateway load.",
          "detailedExplanation": "Read this as a scenario about \"incident review for message send API: signal points to session stickiness causing\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for message send API: signal points to\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Partition gateway sessions by stable user key and rebalance with drain controls.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Chat Core Messaging Architecture, the best answer is \"Partition gateway sessions by stable user key and rebalance with drain controls\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "The key clue in this question is \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"chat Core Messaging Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for connection/session router: signal points to ack loss leading to false undelivered state. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "The current decomposition around connection/session router mismatches ack loss leading to false undelivered state, creating repeated failures.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around connection/session router mismatches ack loss leading to false undelivered state, creating repeated failures\" best matches Incident review for connection/session router: signal points to ack loss leading to false undelivered state by targeting ack loss leading to false undelivered state and lowering repeat risk.",
          "detailedExplanation": "Use \"incident review for connection/session router: signal points to ack loss leading to\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for connection/session router: signal\", which next step is strongest under current constraints?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Serve history from optimized recent-window store plus archival fallback.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 2,
          "explanation": "For \"incident review for connection/session router: signal\", which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Serve history from optimized recent-window store plus archival fallback\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "The core signal here is \"after diagnosis, what is the strongest next change\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"chat Core Messaging Architecture\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for group chat fanout worker: signal points to fanout overload for large group channels. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The current decomposition around group chat fanout worker mismatches fanout overload for large group channels, creating repeated failures.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Chat Core Messaging Architecture: for Incident review for group chat fanout worker: signal points to fanout overload for large group channels, \"The current decomposition around group chat fanout worker mismatches fanout overload for large group channels, creating repeated failures\" is correct because it addresses fanout overload for large group channels and improves controllability.",
          "detailedExplanation": "Start from \"incident review for group chat fanout worker: signal points to fanout overload for\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With root cause identified for \"incident review for group chat fanout worker: signal\", what should change first before wider rollout?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward.",
            "Protect offline inbox throughput with per-tenant quotas and backpressure."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Protect offline inbox throughput with per-tenant quotas and backpressure\" best matches With root cause identified for \"incident review for group chat fanout worker: signal\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "The decision turns on \"after diagnosis, what is the strongest next change\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "This prompt is really about \"chat Core Messaging Architecture\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for message persistence log: signal points to history query latency spikes on hot chats. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible.",
            "The current decomposition around message persistence log mismatches history query latency spikes on hot chats, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Chat Core Messaging Architecture, the best answer is \"The current decomposition around message persistence log mismatches history query latency spikes on hot chats, creating repeated failures\". It is the option most directly aligned to history query latency spikes on hot chats while preserving safe follow-on actions.",
          "detailedExplanation": "The decision turns on \"incident review for message persistence log: signal points to history query latency\". Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for message persistence log: signal\", what first move gives the best reliability impact?",
          "options": [
            "Isolate typing/presence ephemeral streams from critical delivery workflows.",
            "Disable safeguards temporarily to increase short-term throughput.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Chat Core Messaging Architecture: for After diagnosing \"incident review for message persistence log: signal\", what first move gives the best reliability impact, \"Isolate typing/presence ephemeral streams from critical delivery workflows\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Start from \"after diagnosis, what is the strongest next change\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Use \"chat Core Messaging Architecture\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for offline inbox service: signal points to delivery retries triggering duplicate notifications. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around offline inbox service mismatches delivery retries triggering duplicate notifications, creating repeated failures.",
            "This is normal variance and not a design/control issue.",
            "More retries alone will resolve this reliably without architectural changes.",
            "The root cause is unknown, so no targeted diagnosis is possible."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around offline inbox service mismatches delivery retries triggering duplicate notifications, creating repeated failures\" best matches Incident review for offline inbox service: signal points to delivery retries triggering duplicate notifications by targeting delivery retries triggering duplicate notifications and lowering repeat risk.",
          "detailedExplanation": "If you keep \"incident review for offline inbox service: signal points to delivery retries triggering\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for offline inbox service: signal\", which immediate adjustment best addresses the risk?",
          "options": [
            "Disable safeguards temporarily to increase short-term throughput.",
            "Use regional affinity with explicit cross-region forwarding contracts for roamers.",
            "Delay design changes and rely on current runbook only.",
            "Expand traffic first, then analyze incidents afterward."
          ],
          "correct": 1,
          "explanation": "With diagnosis confirmed in \"incident review for offline inbox service: signal\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Use regional affinity with explicit cross-region forwarding contracts for roamers\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "This prompt is really about \"after diagnosis, what is the strongest next change\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"chat Core Messaging Architecture\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-061",
      "type": "multi-select",
      "question": "For this scenario, which signals best identify decomposition boundary mistakes? (Select all that apply)",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Chat Core Messaging Architecture: The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Start from \"signals best identify decomposition boundary mistakes? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-062",
      "type": "multi-select",
      "question": "For this scenario, which controls improve safety on critical write paths? (Select all that apply)",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "For this scenario, which controls improve safety on critical write paths is intentionally multi-dimensional in Chat Core Messaging Architecture. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The decision turns on \"controls improve safety on critical write paths? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-063",
      "type": "multi-select",
      "question": "For this scenario, which practices reduce hot-key or hot-partition impact? (Select all that apply)",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Chat Core Messaging Architecture, For this scenario, which practices reduce hot-key or hot-partition impact needs layered controls, not one silver bullet. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Read this as a scenario about \"practices reduce hot-key or hot-partition impact? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cc-064",
      "type": "multi-select",
      "question": "For this scenario, what improves reliability when mixing sync and async paths? (Select all that apply)",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For For this scenario, what improves reliability when mixing sync and async paths, the highest-signal answer is a bundle of controls. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Use \"improves reliability when mixing sync and async paths? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cc-065",
      "type": "multi-select",
      "question": "For this scenario, which choices usually lower operational risk at scale? (Select all that apply)",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Chat Core Messaging Architecture: The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "This prompt is really about \"choices usually lower operational risk at scale? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-066",
      "type": "multi-select",
      "question": "For this scenario, what should be explicit in API/service contracts for this design? (Select all that apply)",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For this scenario, what should be explicit in API/service contracts for this design is intentionally multi-dimensional in Chat Core Messaging Architecture. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "If you keep \"be explicit in API/service contracts for this design? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-067",
      "type": "multi-select",
      "question": "For this scenario, which anti-patterns often cause incident recurrence? (Select all that apply)",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "In Chat Core Messaging Architecture, For this scenario, which anti-patterns often cause incident recurrence needs layered controls, not one silver bullet. The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The core signal here is \"anti-patterns often cause incident recurrence? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-068",
      "type": "multi-select",
      "question": "For this scenario, what increases confidence before broad traffic rollout? (Select all that apply)",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For For this scenario, what increases confidence before broad traffic rollout, the highest-signal answer is a bundle of controls. The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The key clue in this question is \"increases confidence before broad traffic rollout? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-069",
      "type": "multi-select",
      "question": "For this scenario, which controls protect high-priority traffic during spikes? (Select all that apply)",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Chat Core Messaging Architecture: The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Start from \"controls protect high-priority traffic during spikes? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-070",
      "type": "multi-select",
      "question": "For this scenario, which telemetry dimensions are most actionable for design triage? (Select all that apply)",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Chat Core Messaging Architecture, For this scenario, which telemetry dimensions are most actionable for design triage needs layered controls, not one silver bullet. The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "The core signal here is \"telemetry dimensions are most actionable for design triage? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-071",
      "type": "multi-select",
      "question": "For this scenario, which governance actions improve cross-team reliability ownership? (Select all that apply)",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For For this scenario, which governance actions improve cross-team reliability ownership, the highest-signal answer is a bundle of controls. The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "If you keep \"governance actions improve cross-team reliability ownership? (Select all that apply)\" in view, the correct answer separates faster. Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-072",
      "type": "multi-select",
      "question": "For this scenario, what helps prevent retry amplification cascades? (Select all that apply)",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Chat Core Messaging Architecture: The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "This prompt is really about \"helps prevent retry amplification cascades? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "cd-cc-073",
      "type": "multi-select",
      "question": "For this scenario, which fallback strategies are strong when dependencies degrade? (Select all that apply)",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For this scenario, which fallback strategies are strong when dependencies degrade is intentionally multi-dimensional in Chat Core Messaging Architecture. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Use \"fallback strategies are strong when dependencies degrade? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-074",
      "type": "multi-select",
      "question": "For this scenario, what reduces data-quality regressions in eventual pipelines? (Select all that apply)",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "In Chat Core Messaging Architecture, For this scenario, what reduces data-quality regressions in eventual pipelines needs layered controls, not one silver bullet. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Read this as a scenario about \"reduces data-quality regressions in eventual pipelines? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-075",
      "type": "multi-select",
      "question": "For this scenario, which runbook components improve incident execution quality? (Select all that apply)",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For For this scenario, which runbook components improve incident execution quality, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "The decision turns on \"runbook components improve incident execution quality? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-076",
      "type": "multi-select",
      "question": "For this scenario, which architecture choices improve blast-radius containment? (Select all that apply)",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "This is a composition question for Chat Core Messaging Architecture: The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Start from \"architecture choices improve blast-radius containment? (Select all that apply)\", then pressure-test the result against the options. Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-077",
      "type": "multi-select",
      "question": "For this scenario, what evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For this scenario, what evidence demonstrates a fix worked beyond short-term recovery is intentionally multi-dimensional in Chat Core Messaging Architecture. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "The key clue in this question is \"evidence demonstrates a fix worked beyond short-term recovery? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-078",
      "type": "numeric-input",
      "question": "If a critical path handles 5,400,000 requests/day and 0.18% fail SLO, what is failures/day?",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "For If a critical path handles 5,400,000 requests/day and 0, the computed target in Chat Core Messaging Architecture is 9720 requests. Responses within +/-3% indicate sound sizing judgment.",
      "detailedExplanation": "The core signal here is \"critical path handles 5,400,000 requests/day and 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 5,400 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-079",
      "type": "numeric-input",
      "question": "If queue ingest is 2,200 events/min and drain is 2,530 events/min, what is net drain rate?",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for If queue ingest is 2,200 events/min and drain is 2,530 events/min, what is net drain rate: 330 events/min. Answers within +/-0% show correct directional reasoning for Chat Core Messaging Architecture.",
      "detailedExplanation": "If you keep \"queue ingest is 2,200 events/min and drain is 2,530 events/min\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Numbers such as 2,200 and 2,530 should be normalized first so downstream reasoning stays consistent. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-080",
      "type": "numeric-input",
      "question": "If retries add 0.28 extra attempts at 75,000 req/sec, what is effective attempts/sec?",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Chat Core Messaging Architecture expects quick quantitative triage: If retries add 0 evaluates to 96000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "The decision turns on \"retries add 0\". Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 0.28 and 75,000 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-081",
      "type": "numeric-input",
      "question": "If failover takes 16s and occurs 24 times/day, what is total failover seconds/day?",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "For If failover takes 16s and occurs 24 times/day, what is total failover seconds/day, the computed target in Chat Core Messaging Architecture is 384 seconds. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "Read this as a scenario about \"failover takes 16s and occurs 24 times/day\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 16s and 24 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-082",
      "type": "numeric-input",
      "question": "If target p99 is 650ms; observed p99 is 845ms, what is percent over target?",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for If target p99 is 650ms; observed p99 is 845ms, what is percent over target: 30 %. Answers within +/-30% show correct directional reasoning for Chat Core Messaging Architecture.",
      "detailedExplanation": "The key clue in this question is \"target p99 is 650ms\". Keep every transformation in one unit system and check order of magnitude at the end. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Keep quantities like 650ms and 845ms in aligned units before selecting an answer. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-083",
      "type": "numeric-input",
      "question": "Evaluate this prompt: if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "The operational math for Evaluate this prompt: if 34% of 130,000 req/min are high-priority, how many high-priority req/min gives 44200 requests/min. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "Start from \"if 34% of 130,000 req/min are high-priority, how many high-priority req/min\", then pressure-test the result against the options. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 34 and 130,000 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-084",
      "type": "numeric-input",
      "question": "If error rate drops from 1.0% to 0.22%, what is percent reduction?",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Chat Core Messaging Architecture expects quick quantitative triage: If error rate drops from 1 evaluates to 78 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "If you keep \"error rate drops from 1\" in view, the correct answer separates faster. Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 1.0 and 0.22 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-085",
      "type": "numeric-input",
      "question": "If a 9-node quorum cluster requires majority writes, what is minimum acknowledgements?",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "For If a 9-node quorum cluster requires majority writes, what is minimum acknowledgements, the computed target in Chat Core Messaging Architecture is 5 acks. Responses within +/-0% indicate sound sizing judgment.",
      "detailedExplanation": "The core signal here is \"9-node quorum cluster requires majority writes\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. Numbers such as 9 and 5 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring conflict resolution behavior.",
      "references": [
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        },
        {
          "title": "In Search of an Understandable Consensus Algorithm (Raft)",
          "url": "https://raft.github.io/raft.pdf"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-086",
      "type": "numeric-input",
      "question": "If backlog is 56,000 tasks with net drain 350 tasks/min, what is minutes to clear?",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for If backlog is 56,000 tasks with net drain 350 tasks/min, what is minutes to clear: 160 minutes. Answers within +/-0% show correct directional reasoning for Chat Core Messaging Architecture.",
      "detailedExplanation": "Use \"backlog is 56,000 tasks with net drain 350 tasks/min\" as your starting point, then verify tradeoffs carefully. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-087",
      "type": "numeric-input",
      "question": "If a fleet has 18 zones and 3 are unavailable, what is percent remaining available?",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "The operational math for If a fleet has 18 zones and 3 are unavailable, what is percent remaining available gives 83.33 %. In interview pacing, hitting this value within +/-30% is the pass condition.",
      "detailedExplanation": "This prompt is really about \"fleet has 18 zones and 3 are unavailable\". Keep every transformation in one unit system and check order of magnitude at the end. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 18 and 3 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-088",
      "type": "numeric-input",
      "question": "If mTTR improved from 52 min to 34 min, what is percent reduction?",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Chat Core Messaging Architecture expects quick quantitative triage: If mTTR improved from 52 min to 34 min, what is percent reduction evaluates to 34.62 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "The decision turns on \"mTTR improved from 52 min to 34 min\". Normalize units before computing so conversion mistakes do not propagate. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 52 min and 34 min appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-089",
      "type": "numeric-input",
      "question": "Evaluate this prompt: if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "For Evaluate this prompt: if 11% of 2,800,000 daily ops need manual checks, checks/day, the computed target in Chat Core Messaging Architecture is 308000 operations. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Read this as a scenario about \"if 11% of 2,800,000 daily ops need manual checks, checks/day\". Keep every transformation in one unit system and check order of magnitude at the end. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 11 and 2,800 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-090",
      "type": "ordering",
      "question": "Order a classic-design decomposition workflow. (chat core messaging architecture lens)",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Chat Core Messaging Architecture emphasizes safe recovery order. Beginning at Identify critical user journey and invariants and finishing at Validate with load/failure drills and refine keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Start from \"order a classic-design decomposition workflow\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Cross-check with anchor numbers to test plausibility before finalizing. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-091",
      "type": "ordering",
      "question": "From a chat core messaging architecture viewpoint, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Explicit boundaries with contracts must happen before Implicit coupling with no ownership. That ordering matches incident-safe flow in Chat Core Messaging Architecture.",
      "detailedExplanation": "The key clue in this question is \"order by increasing design risk\". Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-092",
      "type": "ordering",
      "question": "Considering chat core messaging architecture, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Core Messaging Architecture should start with Scope blast radius and affected flows and end with Run recurrence checks and hardening actions. Considering chat core messaging architecture, order safe incident mitigation steps rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Read this as a scenario about \"order safe incident mitigation steps\". Build the rank from biggest differences first, then refine with adjacent checks. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "cd-cc-093",
      "type": "ordering",
      "question": "In this chat core messaging architecture context, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For In this chat core messaging architecture context, order by increasing retry-control maturity, the correct ordering runs from Fixed immediate retries to Jittered retries with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "The decision turns on \"order by increasing retry-control maturity\". Place obvious extremes first, then sort the middle by pairwise comparison. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "cd-cc-094",
      "type": "ordering",
      "question": "Within chat core messaging architecture, order fallback sophistication.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Chat Core Messaging Architecture emphasizes safe recovery order. Beginning at Implicit fallback behavior and finishing at Policy-driven automated fallback with tests keeps blast radius controlled while restoring service.",
      "detailedExplanation": "This prompt is really about \"order fallback sophistication\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-095",
      "type": "ordering",
      "question": "For chat core messaging architecture, order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Host health check only must happen before Staged shift plus failback rehearsal and rollback gates. That ordering matches incident-safe flow in Chat Core Messaging Architecture.",
      "detailedExplanation": "Use \"order failover validation rigor\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-096",
      "type": "ordering",
      "question": "Put these in order of increasing blast radius.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Core Messaging Architecture should start with Single process failure and end with Cross-region control-plane failure. Put these in order of increasing blast radius rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "The core signal here is \"order by increasing blast radius\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-097",
      "type": "ordering",
      "question": "Order data-path durability confidence. Use a chat core messaging architecture perspective.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order data-path durability confidence, the correct ordering runs from In-memory only acknowledgment to Replicated durable write plus replay/integrity verification. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "If you keep \"order data-path durability confidence\" in view, the correct answer separates faster. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-098",
      "type": "ordering",
      "question": "Order by increasing operational discipline. (chat core messaging architecture lens)",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Chat Core Messaging Architecture emphasizes safe recovery order. Beginning at Ad hoc incident response and finishing at Role-based response plus action closure tracking keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Start from \"order by increasing operational discipline\", then pressure-test the result against the options. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-099",
      "type": "ordering",
      "question": "From a chat core messaging architecture viewpoint, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Canary small cohort must happen before Finalize runbook and ownership updates. That ordering matches incident-safe flow in Chat Core Messaging Architecture.",
      "detailedExplanation": "The key clue in this question is \"order rollout safety for major design changes\". Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    },
    {
      "id": "cd-cc-100",
      "type": "ordering",
      "question": "Considering chat core messaging architecture, order evidence strength for fix success.",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Chat Core Messaging Architecture should start with Single successful test run and end with Sustained recovery plus failure-drill pass. Considering chat core messaging architecture, order evidence strength for fix success rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Read this as a scenario about \"order evidence strength for fix success\". Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        },
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ]
    }
  ]
}
