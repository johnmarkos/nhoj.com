{
  "unit": 10,
  "unitTitle": "Classic Designs Decomposed",
  "chapter": 2,
  "chapterTitle": "Twitter/X Timeline Ranking, Serving & Reliability",
  "chapterDescription": "Decompose feed ranking pipeline, serving stack, cache hierarchy, and resilience controls for timeline quality.",
  "problems": [
    {
      "id": "cd-tr-001",
      "type": "multiple-choice",
      "question": "Case Alpha: candidate generation service. Dominant risk is ranking latency spikes from feature-store contention. Which next move is strongest? The team needs a mitigation that can be canaried in under an hour.",
      "options": [
        "Cap candidate set sizes per request class and use staged ranking to bound tail latency.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat candidate generation service as a reliability-control decision, not an averages-only optimization. \"Cap candidate set sizes per request class and use staged ranking to bound tail latency\" is correct since it mitigates ranking latency spikes from feature-store contention while keeping containment local. The decision remains valid given: The team needs a mitigation that can be canaried in under an hour.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-002",
      "type": "multiple-choice",
      "question": "Case Beta: feature fetch layer. Dominant risk is stale features producing low-quality feed order. Which next move is strongest? Recent traffic growth exposed this bottleneck repeatedly.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Separate online features from offline enrichments with explicit freshness contracts.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For feature fetch layer, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Separate online features from offline enrichments with explicit freshness contracts\" outperforms the alternatives because it targets stale features producing low-quality feed order and preserves safe recovery behavior. It is also the most compatible with Recent traffic growth exposed this bottleneck repeatedly.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-003",
      "type": "multiple-choice",
      "question": "Case Gamma: ranking model inference path. Dominant risk is cache stampede during model rollout. Which next move is strongest? Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Protect cache layers with jittered refresh and request coalescing to avoid stampedes.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, ranking model inference path fails mainly through cache stampede during model rollout. The best choice is \"Protect cache layers with jittered refresh and request coalescing to avoid stampedes\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Leadership asked for a fix that reduces recurrence, not just MTTR.",
      "detailedExplanation": "Generalize from ranking model inference path to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-004",
      "type": "multiple-choice",
      "question": "Case Delta: timeline serving API. Dominant risk is degraded dependency returning incomplete candidate sets. Which next move is strongest? A previous rollback fixed averages but not tail impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Define degraded ranking modes that preserve safety and basic relevance under dependency failure."
      ],
      "correct": 3,
      "explanation": "Timeline serving API should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\" is strongest because it directly addresses degraded dependency returning incomplete candidate sets and improves repeatability under stress. This aligns with the extra condition (A previous rollback fixed averages but not tail impact).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prefer the approach that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-005",
      "type": "multiple-choice",
      "question": "Case Epsilon: ranking feature cache. Dominant risk is feature/schema mismatch causing ranking failures. Which next move is strongest? User trust risk is highest on this path.",
      "options": [
        "Version ranking features and enforce strict schema checks at inference boundaries.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat ranking feature cache as a reliability-control decision, not an averages-only optimization. \"Version ranking features and enforce strict schema checks at inference boundaries\" is correct since it mitigates feature/schema mismatch causing ranking failures while keeping containment local. The decision remains valid given: User trust risk is highest on this path.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-006",
      "type": "multiple-choice",
      "question": "Case Zeta: freshness reconciliation worker. Dominant risk is fallback path overused and hurting engagement. Which next move is strongest? A shared dependency has uncertain health right now.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Route premium/critical traffic through protected capacity pools during ranking incidents.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "For freshness reconciliation worker, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Route premium/critical traffic through protected capacity pools during ranking incidents\" outperforms the alternatives because it targets fallback path overused and hurting engagement and preserves safe recovery behavior. It is also the most compatible with A shared dependency has uncertain health right now.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. In real systems, reject approaches that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-007",
      "type": "multiple-choice",
      "question": "Case Eta: safety filtering stage. Dominant risk is priority traffic starved by background recompute jobs. Which next move is strongest? The change must preserve cost discipline during peak.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, safety filtering stage fails mainly through priority traffic starved by background recompute jobs. The best choice is \"Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The change must preserve cost discipline during peak.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-008",
      "type": "multiple-choice",
      "question": "Case Theta: feed quality metrics pipeline. Dominant risk is invalidation gaps after moderation or delete events. Which next move is strongest? Telemetry shows risk concentrated in one partition class.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs."
      ],
      "correct": 3,
      "explanation": "Feed quality metrics pipeline should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs\" is strongest because it directly addresses invalidation gaps after moderation or delete events and improves repeatability under stress. This aligns with the extra condition (Telemetry shows risk concentrated in one partition class).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Prefer approaches that balance hit rate with clear staleness and invalidation behavior. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-009",
      "type": "multiple-choice",
      "question": "Case Iota: multi-tenant ranking cluster. Dominant risk is unbounded candidate expansion increasing cost and latency. Which next move is strongest? The product team accepts graceful degradation, not silent corruption.",
      "options": [
        "Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Treat multi-tenant ranking cluster as a reliability-control decision, not an averages-only optimization. \"Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality\" is correct since it mitigates unbounded candidate expansion increasing cost and latency while keeping containment local. The decision remains valid given: The product team accepts graceful degradation, not silent corruption.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-010",
      "type": "multiple-choice",
      "question": "Case Kappa: degraded ranking fallback path. Dominant risk is regional serving drift in freshness and quality. Which next move is strongest? Current runbooks are missing explicit ownership for this boundary.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Validate model rollouts with canary cohorts and automatic rollback gates.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, degraded ranking fallback path fails mainly through regional serving drift in freshness and quality. The best choice is \"Validate model rollouts with canary cohorts and automatic rollback gates\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Current runbooks are missing explicit ownership for this boundary.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-011",
      "type": "multiple-choice",
      "question": "Case Lambda: candidate generation service. Dominant risk is ranking latency spikes from feature-store contention. Which next move is strongest? A cross-region path recently changed behavior after migration.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Cap candidate set sizes per request class and use staged ranking to bound tail latency.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Candidate generation service should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Cap candidate set sizes per request class and use staged ranking to bound tail latency\" is strongest because it directly addresses ranking latency spikes from feature-store contention and improves repeatability under stress. This aligns with the extra condition (A cross-region path recently changed behavior after migration).",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-012",
      "type": "multiple-choice",
      "question": "Case Mu: feature fetch layer. Dominant risk is stale features producing low-quality feed order. Which next move is strongest? Client retries are already elevated and can amplify mistakes.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Separate online features from offline enrichments with explicit freshness contracts."
      ],
      "correct": 3,
      "explanation": "Treat feature fetch layer as a reliability-control decision, not an averages-only optimization. \"Separate online features from offline enrichments with explicit freshness contracts\" is correct since it mitigates stale features producing low-quality feed order while keeping containment local. The decision remains valid given: Client retries are already elevated and can amplify mistakes.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard cache tactics that hide consistency bugs under high load. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-013",
      "type": "multiple-choice",
      "question": "Case Nu: ranking model inference path. Dominant risk is cache stampede during model rollout. Which next move is strongest? Capacity headroom exists but only in specific pools.",
      "options": [
        "Protect cache layers with jittered refresh and request coalescing to avoid stampedes.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For ranking model inference path, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Protect cache layers with jittered refresh and request coalescing to avoid stampedes\" outperforms the alternatives because it targets cache stampede during model rollout and preserves safe recovery behavior. It is also the most compatible with Capacity headroom exists but only in specific pools.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Discard cache tactics that hide consistency bugs under high load. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-014",
      "type": "multiple-choice",
      "question": "Case Xi: timeline serving API. Dominant risk is degraded dependency returning incomplete candidate sets. Which next move is strongest? A partial failure is masking itself as success in metrics.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Define degraded ranking modes that preserve safety and basic relevance under dependency failure.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, timeline serving API fails mainly through degraded dependency returning incomplete candidate sets. The best choice is \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: A partial failure is masking itself as success in metrics.",
      "detailedExplanation": "Generalize from timeline serving API to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-015",
      "type": "multiple-choice",
      "question": "Case Omicron: ranking feature cache. Dominant risk is feature/schema mismatch causing ranking failures. Which next move is strongest? This fix must hold under celebrity or campaign spike conditions.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Version ranking features and enforce strict schema checks at inference boundaries.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Ranking feature cache should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Version ranking features and enforce strict schema checks at inference boundaries\" is strongest because it directly addresses feature/schema mismatch causing ranking failures and improves repeatability under stress. This aligns with the extra condition (This fix must hold under celebrity or campaign spike conditions).",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-016",
      "type": "multiple-choice",
      "question": "Case Pi: freshness reconciliation worker. Dominant risk is fallback path overused and hurting engagement. Which next move is strongest? SLO burn suggests immediate containment plus follow-up hardening.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Route premium/critical traffic through protected capacity pools during ranking incidents."
      ],
      "correct": 3,
      "explanation": "Treat freshness reconciliation worker as a reliability-control decision, not an averages-only optimization. \"Route premium/critical traffic through protected capacity pools during ranking incidents\" is correct since it mitigates fallback path overused and hurting engagement while keeping containment local. The decision remains valid given: SLO burn suggests immediate containment plus follow-up hardening.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that sound good in general but do not reduce concrete reliability risk. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-017",
      "type": "multiple-choice",
      "question": "Case Rho: safety filtering stage. Dominant risk is priority traffic starved by background recompute jobs. Which next move is strongest? On-call requested a reversible operational first step.",
      "options": [
        "Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "For safety filtering stage, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation\" outperforms the alternatives because it targets priority traffic starved by background recompute jobs and preserves safe recovery behavior. It is also the most compatible with On-call requested a reversible operational first step.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject approaches that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-018",
      "type": "multiple-choice",
      "question": "Case Sigma: feed quality metrics pipeline. Dominant risk is invalidation gaps after moderation or delete events. Which next move is strongest? The system mixes strict and eventual paths with unclear contracts.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, feed quality metrics pipeline fails mainly through invalidation gaps after moderation or delete events. The best choice is \"Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The system mixes strict and eventual paths with unclear contracts.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-019",
      "type": "multiple-choice",
      "question": "Case Tau: multi-tenant ranking cluster. Dominant risk is unbounded candidate expansion increasing cost and latency. Which next move is strongest? A hot-key pattern is likely from real traffic skew.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Multi-tenant ranking cluster should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality\" is strongest because it directly addresses unbounded candidate expansion increasing cost and latency and improves repeatability under stress. This aligns with the extra condition (A hot-key pattern is likely from real traffic skew).",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-020",
      "type": "multiple-choice",
      "question": "Case Upsilon: degraded ranking fallback path. Dominant risk is regional serving drift in freshness and quality. Which next move is strongest? The path must remain mobile-latency friendly under stress.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Validate model rollouts with canary cohorts and automatic rollback gates."
      ],
      "correct": 3,
      "explanation": "For degraded ranking fallback path, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Validate model rollouts with canary cohorts and automatic rollback gates\" outperforms the alternatives because it targets regional serving drift in freshness and quality and preserves safe recovery behavior. It is also the most compatible with The path must remain mobile-latency friendly under stress.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-021",
      "type": "multiple-choice",
      "question": "Case Phi: candidate generation service. Dominant risk is ranking latency spikes from feature-store contention. Which next move is strongest? Compliance requires explicit behavior for edge-case failures.",
      "options": [
        "Cap candidate set sizes per request class and use staged ranking to bound tail latency.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, candidate generation service fails mainly through ranking latency spikes from feature-store contention. The best choice is \"Cap candidate set sizes per request class and use staged ranking to bound tail latency\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Compliance requires explicit behavior for edge-case failures.",
      "detailedExplanation": "Generalize from candidate generation service to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-022",
      "type": "multiple-choice",
      "question": "Case Chi: feature fetch layer. Dominant risk is stale features producing low-quality feed order. Which next move is strongest? This boundary has failed during the last two game days.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Separate online features from offline enrichments with explicit freshness contracts.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Feature fetch layer should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Separate online features from offline enrichments with explicit freshness contracts\" is strongest because it directly addresses stale features producing low-quality feed order and improves repeatability under stress. This aligns with the extra condition (This boundary has failed during the last two game days).",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-023",
      "type": "multiple-choice",
      "question": "Case Psi: ranking model inference path. Dominant risk is cache stampede during model rollout. Which next move is strongest? A cache invalidation gap has customer-visible impact.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Protect cache layers with jittered refresh and request coalescing to avoid stampedes.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat ranking model inference path as a reliability-control decision, not an averages-only optimization. \"Protect cache layers with jittered refresh and request coalescing to avoid stampedes\" is correct since it mitigates cache stampede during model rollout while keeping containment local. The decision remains valid given: A cache invalidation gap has customer-visible impact.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Reject approaches that improve speed but weaken freshness or invalidation correctness. A strong caching answer names staleness limits, invalidation behavior, and keying strategy. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-024",
      "type": "multiple-choice",
      "question": "Case Omega: timeline serving API. Dominant risk is degraded dependency returning incomplete candidate sets. Which next move is strongest? Dependency retries currently exceed safe server limits.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Define degraded ranking modes that preserve safety and basic relevance under dependency failure."
      ],
      "correct": 3,
      "explanation": "For timeline serving API, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\" outperforms the alternatives because it targets degraded dependency returning incomplete candidate sets and preserves safe recovery behavior. It is also the most compatible with Dependency retries currently exceed safe server limits.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Prefer the approach that keeps client behavior explicit while preserving evolvability. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-025",
      "type": "multiple-choice",
      "question": "Case Atlas: ranking feature cache. Dominant risk is feature/schema mismatch causing ranking failures. Which next move is strongest? The fix should avoid broad architectural rewrites this quarter.",
      "options": [
        "Version ranking features and enforce strict schema checks at inference boundaries.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, ranking feature cache fails mainly through feature/schema mismatch causing ranking failures. The best choice is \"Version ranking features and enforce strict schema checks at inference boundaries\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The fix should avoid broad architectural rewrites this quarter.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-026",
      "type": "multiple-choice",
      "question": "Case Nova: freshness reconciliation worker. Dominant risk is fallback path overused and hurting engagement. Which next move is strongest? Current metrics hide per-tenant variance that matters.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Route premium/critical traffic through protected capacity pools during ranking incidents.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Freshness reconciliation worker should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Route premium/critical traffic through protected capacity pools during ranking incidents\" is strongest because it directly addresses fallback path overused and hurting engagement and improves repeatability under stress. This aligns with the extra condition (Current metrics hide per-tenant variance that matters).",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-027",
      "type": "multiple-choice",
      "question": "Case Orion: safety filtering stage. Dominant risk is priority traffic starved by background recompute jobs. Which next move is strongest? The fallback path is under-tested in production-like load.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "Treat safety filtering stage as a reliability-control decision, not an averages-only optimization. \"Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation\" is correct since it mitigates priority traffic starved by background recompute jobs while keeping containment local. The decision remains valid given: The fallback path is under-tested in production-like load.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. In real systems, reject plans that assume linear scaling across shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-028",
      "type": "multiple-choice",
      "question": "Case Vega: feed quality metrics pipeline. Dominant risk is invalidation gaps after moderation or delete events. Which next move is strongest? A control-plane issue is bleeding into data-plane reliability.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs."
      ],
      "correct": 3,
      "explanation": "For feed quality metrics pipeline, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs\" outperforms the alternatives because it targets invalidation gaps after moderation or delete events and preserves safe recovery behavior. It is also the most compatible with A control-plane issue is bleeding into data-plane reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: hot-key skew causing uneven load.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-029",
      "type": "multiple-choice",
      "question": "Case Helios: multi-tenant ranking cluster. Dominant risk is unbounded candidate expansion increasing cost and latency. Which next move is strongest? The system must preserve critical events over bulk traffic.",
      "options": [
        "Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, multi-tenant ranking cluster fails mainly through unbounded candidate expansion increasing cost and latency. The best choice is \"Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: The system must preserve critical events over bulk traffic.",
      "detailedExplanation": "Generalize from multi-tenant ranking cluster to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Prefer approaches that map cleanly to throughput, concurrency, and scaling limits. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-030",
      "type": "multiple-choice",
      "question": "Case Aurora: degraded ranking fallback path. Dominant risk is regional serving drift in freshness and quality. Which next move is strongest? Recent deploys changed queue and timeout settings together.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Validate model rollouts with canary cohorts and automatic rollback gates.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat degraded ranking fallback path as a reliability-control decision, not an averages-only optimization. \"Validate model rollouts with canary cohorts and automatic rollback gates\" is correct since it mitigates regional serving drift in freshness and quality while keeping containment local. The decision remains valid given: Recent deploys changed queue and timeout settings together.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Prefer approaches that directly address failure mode, recovery path, and blast radius. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-031",
      "type": "multiple-choice",
      "question": "Case Nimbus: candidate generation service. Dominant risk is ranking latency spikes from feature-store contention. Which next move is strongest? The edge layer is healthy but origin saturation is growing.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Cap candidate set sizes per request class and use staged ranking to bound tail latency.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For candidate generation service, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Cap candidate set sizes per request class and use staged ranking to bound tail latency\" outperforms the alternatives because it targets ranking latency spikes from feature-store contention and preserves safe recovery behavior. It is also the most compatible with The edge layer is healthy but origin saturation is growing.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. In real systems, reject plans that assume linear scaling across shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-032",
      "type": "multiple-choice",
      "question": "Case Pulse: feature fetch layer. Dominant risk is stale features producing low-quality feed order. Which next move is strongest? Operational complexity is rising faster than team onboarding.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred.",
        "Separate online features from offline enrichments with explicit freshness contracts."
      ],
      "correct": 3,
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, feature fetch layer fails mainly through stale features producing low-quality feed order. The best choice is \"Separate online features from offline enrichments with explicit freshness contracts\" because it lowers recurrence without expanding blast radius. It also fits the interview constraint: Operational complexity is rising faster than team onboarding.",
      "detailedExplanation": "Generalize from feature fetch layer to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Discard cache tactics that hide consistency bugs under high load. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-033",
      "type": "multiple-choice",
      "question": "Case Forge: ranking model inference path. Dominant risk is cache stampede during model rollout. Which next move is strongest? Stakeholders need clear trade-off rationale in the postmortem.",
      "options": [
        "Protect cache layers with jittered refresh and request coalescing to avoid stampedes.",
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 0,
      "explanation": "Ranking model inference path should be solved at the failure boundary named in Twitter/X Timeline Ranking, Serving & Reliability. \"Protect cache layers with jittered refresh and request coalescing to avoid stampedes\" is strongest because it directly addresses cache stampede during model rollout and improves repeatability under stress. This aligns with the extra condition (Stakeholders need clear trade-off rationale in the postmortem).",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Reject approaches that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-034",
      "type": "multiple-choice",
      "question": "Case Harbor: timeline serving API. Dominant risk is degraded dependency returning incomplete candidate sets. Which next move is strongest? A localized failure is at risk of becoming cross-region.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Define degraded ranking modes that preserve safety and basic relevance under dependency failure.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 1,
      "explanation": "Treat timeline serving API as a reliability-control decision, not an averages-only optimization. \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\" is correct since it mitigates degraded dependency returning incomplete candidate sets while keeping containment local. The decision remains valid given: A localized failure is at risk of becoming cross-region.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-035",
      "type": "multiple-choice",
      "question": "Case Vector: ranking feature cache. Dominant risk is feature/schema mismatch causing ranking failures. Which next move is strongest? Recovery sequencing matters as much as immediate containment.",
      "options": [
        "Apply one shared policy across all paths first, then introduce boundary-specific controls only where error budgets are exceeded.",
        "Optimize average latency first and defer reliability controls until sustained p95 error-budget burn appears.",
        "Version ranking features and enforce strict schema checks at inference boundaries.",
        "Use operator-driven mitigations as the primary response for recurring incidents while automation remains deferred."
      ],
      "correct": 2,
      "explanation": "For ranking feature cache, prefer the option that prevents reoccurrence in Twitter/X Timeline Ranking, Serving & Reliability. \"Version ranking features and enforce strict schema checks at inference boundaries\" outperforms the alternatives because it targets feature/schema mismatch causing ranking failures and preserves safe recovery behavior. It is also the most compatible with Recovery sequencing matters as much as immediate containment.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Prioritize the approach that best protects reliability objectives under stated failure conditions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for candidate generation service: signal points to degraded dependency returning incomplete candidate sets. Support tickets confirm this pattern is recurring. What is the primary diagnosis?",
          "options": [
            "The current decomposition around candidate generation service mismatches degraded dependency returning incomplete candidate sets, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Incident review for candidate generation service: signal points to degraded dependency returning incomplete candidate sets is a two-step reliability decision. At stage 1, \"The current decomposition around candidate generation service mismatches degraded dependency returning incomplete candidate sets, creating repeated failures\" wins because it balances immediate containment with long-term prevention around degraded dependency returning incomplete candidate sets.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for candidate generation service:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Route premium/critical traffic through protected capacity pools during ranking incidents.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "For stage 2 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"Route premium/critical traffic through protected capacity pools during ranking incidents\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for feature fetch layer: signal points to feature/schema mismatch causing ranking failures. The incident timeline shows policy mismatch across services. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around feature fetch layer mismatches feature/schema mismatch causing ranking failures, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around feature fetch layer mismatches feature/schema mismatch causing ranking failures, creating repeated failures\" best matches Incident review for feature fetch layer: signal points to feature/schema mismatch causing ranking failures by targeting feature/schema mismatch causing ranking failures and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "In the \"incident review for feature fetch layer: signal points\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In the \"incident review for feature fetch layer: signal points\" scenario, which next step is strongest under current constraints is a two-step reliability decision. At stage 2, \"Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ranking model inference path: signal points to fallback path overused and hurting engagement. Last failover drill reproduced the same weakness. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around ranking model inference path mismatches fallback path overused and hurting engagement, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Stage 1 in Twitter/X Timeline Ranking, Serving & Reliability: for Incident review for ranking model inference path: signal points to fallback path overused and hurting engagement, \"The current decomposition around ranking model inference path mismatches fallback path overused and hurting engagement, creating repeated failures\" is correct because it addresses fallback path overused and hurting engagement and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After diagnosing \"incident review for ranking model inference path:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs."
          ],
          "correct": 3,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs\" best matches After diagnosing \"incident review for ranking model inference path:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for timeline serving API: signal points to priority traffic starved by background recompute jobs. A recent schema change increased failure surface area. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around timeline serving API mismatches priority traffic starved by background recompute jobs, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "For stage 1 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"The current decomposition around timeline serving API mismatches priority traffic starved by background recompute jobs, creating repeated failures\". It is the option most directly aligned to priority traffic starved by background recompute jobs while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for timeline serving API: signal points\", which next step is strongest under current constraints?",
          "options": [
            "Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Stage 2 in Twitter/X Timeline Ranking, Serving & Reliability: for With root cause identified for \"incident review for timeline serving API: signal points\", which next step is strongest under current constraints, \"Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ranking feature cache: signal points to invalidation gaps after moderation or delete events. A dependency team changed defaults without shared review. What is the primary diagnosis?",
          "options": [
            "The current decomposition around ranking feature cache mismatches invalidation gaps after moderation or delete events, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around ranking feature cache mismatches invalidation gaps after moderation or delete events, creating repeated failures\" best matches Incident review for ranking feature cache: signal points to invalidation gaps after moderation or delete events by targeting invalidation gaps after moderation or delete events and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for ranking feature cache: signal\", what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Validate model rollouts with canary cohorts and automatic rollback gates.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Using the diagnosis from \"incident review for ranking feature cache: signal\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Validate model rollouts with canary cohorts and automatic rollback gates\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for freshness reconciliation worker: signal points to unbounded candidate expansion increasing cost and latency. Alert noise is obscuring root-cause signals. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around freshness reconciliation worker mismatches unbounded candidate expansion increasing cost and latency, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Twitter/X Timeline Ranking, Serving & Reliability: for Incident review for freshness reconciliation worker: signal points to unbounded candidate expansion increasing cost and latency, \"The current decomposition around freshness reconciliation worker mismatches unbounded candidate expansion increasing cost and latency, creating repeated failures\" is correct because it addresses unbounded candidate expansion increasing cost and latency and improves controllability.",
          "detailedExplanation": "Generalize from incident review for freshness reconciliation worker: signal points to unbounded to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for freshness reconciliation worker:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Cap candidate set sizes per request class and use staged ranking to bound tail latency.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Cap candidate set sizes per request class and use staged ranking to bound tail latency\" best matches With diagnosis confirmed in \"incident review for freshness reconciliation worker:\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for safety filtering stage: signal points to regional serving drift in freshness and quality. Backlog growth started before CPU looked saturated. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around safety filtering stage mismatches regional serving drift in freshness and quality, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"The current decomposition around safety filtering stage mismatches regional serving drift in freshness and quality, creating repeated failures\". It is the option most directly aligned to regional serving drift in freshness and quality while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "In the \"incident review for safety filtering stage: signal\" scenario, which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Separate online features from offline enrichments with explicit freshness contracts."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Twitter/X Timeline Ranking, Serving & Reliability: for In the \"incident review for safety filtering stage: signal\" scenario, which next step is strongest under current constraints, \"Separate online features from offline enrichments with explicit freshness contracts\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for feed quality metrics pipeline: signal points to ranking latency spikes from feature-store contention. Canary data points already indicate partial regression. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around feed quality metrics pipeline mismatches ranking latency spikes from feature-store contention, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for feed quality metrics pipeline: signal points to ranking latency spikes from feature-store contention is a two-step reliability decision. At stage 1, \"The current decomposition around feed quality metrics pipeline mismatches ranking latency spikes from feature-store contention, creating repeated failures\" wins because it balances immediate containment with long-term prevention around ranking latency spikes from feature-store contention.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Now that \"incident review for feed quality metrics pipeline:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Protect cache layers with jittered refresh and request coalescing to avoid stampedes.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"Protect cache layers with jittered refresh and request coalescing to avoid stampedes\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-tenant ranking cluster: signal points to stale features producing low-quality feed order. A single hot tenant now dominates this workload. What is the primary diagnosis?",
          "options": [
            "The current decomposition around multi-tenant ranking cluster mismatches stale features producing low-quality feed order, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around multi-tenant ranking cluster mismatches stale features producing low-quality feed order, creating repeated failures\" best matches Incident review for multi-tenant ranking cluster: signal points to stale features producing low-quality feed order by targeting stale features producing low-quality feed order and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for multi-tenant ranking cluster:\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Define degraded ranking modes that preserve safety and basic relevance under dependency failure.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "With root cause identified for \"incident review for multi-tenant ranking cluster:\", what first move gives the best reliability impact is a two-step reliability decision. At stage 2, \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for degraded ranking fallback path: signal points to cache stampede during model rollout. The current fallback behavior is undocumented for clients. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around degraded ranking fallback path mismatches cache stampede during model rollout, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Twitter/X Timeline Ranking, Serving & Reliability: for Incident review for degraded ranking fallback path: signal points to cache stampede during model rollout, \"The current decomposition around degraded ranking fallback path mismatches cache stampede during model rollout, creating repeated failures\" is correct because it addresses cache stampede during model rollout and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After diagnosing \"incident review for degraded ranking fallback path:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Version ranking features and enforce strict schema checks at inference boundaries.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Version ranking features and enforce strict schema checks at inference boundaries\" best matches After diagnosing \"incident review for degraded ranking fallback path:\", what should change first before wider rollout by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for candidate generation service: signal points to degraded dependency returning incomplete candidate sets. Traffic composition changed after a mobile release. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around candidate generation service mismatches degraded dependency returning incomplete candidate sets, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "For stage 1 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"The current decomposition around candidate generation service mismatches degraded dependency returning incomplete candidate sets, creating repeated failures\". It is the option most directly aligned to degraded dependency returning incomplete candidate sets while preserving safe follow-on actions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Given the diagnosis in \"incident review for candidate generation service:\", which next step is strongest under current constraints?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Route premium/critical traffic through protected capacity pools during ranking incidents."
          ],
          "correct": 3,
          "explanation": "Stage 2 in Twitter/X Timeline Ranking, Serving & Reliability: for Given the diagnosis in \"incident review for candidate generation service:\", which next step is strongest under current constraints, \"Route premium/critical traffic through protected capacity pools during ranking incidents\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for feature fetch layer: signal points to feature/schema mismatch causing ranking failures. This path is business-critical during daily peaks. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around feature fetch layer mismatches feature/schema mismatch causing ranking failures, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "Incident review for feature fetch layer: signal points to feature/schema mismatch causing ranking failures is a two-step reliability decision. At stage 1, \"The current decomposition around feature fetch layer mismatches feature/schema mismatch causing ranking failures, creating repeated failures\" wins because it balances immediate containment with long-term prevention around feature/schema mismatch causing ranking failures.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "For \"incident review for feature fetch layer: signal points\", which immediate adjustment best addresses the risk?",
          "options": [
            "Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For stage 2 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize from twitter/X Timeline Ranking, Serving & Reliability to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ranking model inference path: signal points to fallback path overused and hurting engagement. The same class of issue appeared in a prior quarter. What is the primary diagnosis?",
          "options": [
            "The current decomposition around ranking model inference path mismatches fallback path overused and hurting engagement, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around ranking model inference path mismatches fallback path overused and hurting engagement, creating repeated failures\" best matches Incident review for ranking model inference path: signal points to fallback path overused and hurting engagement by targeting fallback path overused and hurting engagement and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Using the diagnosis from \"incident review for ranking model inference path:\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Using the diagnosis from \"incident review for ranking model inference path:\", which next change should be prioritized first is a two-step reliability decision. At stage 2, \"Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for timeline serving API: signal points to priority traffic starved by background recompute jobs. Error budget policy now requires corrective action. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around timeline serving API mismatches priority traffic starved by background recompute jobs, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "Stage 1 in Twitter/X Timeline Ranking, Serving & Reliability: for Incident review for timeline serving API: signal points to priority traffic starved by background recompute jobs, \"The current decomposition around timeline serving API mismatches priority traffic starved by background recompute jobs, creating repeated failures\" is correct because it addresses priority traffic starved by background recompute jobs and improves controllability.",
          "detailedExplanation": "Generalize from incident review for timeline serving API: signal points to priority traffic starved by to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for timeline serving API: signal points\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality\" best matches With diagnosis confirmed in \"incident review for timeline serving API: signal points\", which next change should be prioritized first by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ranking feature cache: signal points to invalidation gaps after moderation or delete events. The mitigation needs explicit rollback checkpoints. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around ranking feature cache mismatches invalidation gaps after moderation or delete events, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Incident review for ranking feature cache: signal points to invalidation gaps after moderation or delete events is a two-step reliability decision. At stage 1, \"The current decomposition around ranking feature cache mismatches invalidation gaps after moderation or delete events, creating repeated failures\" wins because it balances immediate containment with long-term prevention around invalidation gaps after moderation or delete events.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "For \"incident review for ranking feature cache: signal\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Validate model rollouts with canary cohorts and automatic rollback gates."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"Validate model rollouts with canary cohorts and automatic rollback gates\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize from twitter/X Timeline Ranking, Serving & Reliability to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for freshness reconciliation worker: signal points to unbounded candidate expansion increasing cost and latency. Some retries are deterministic failures and should not repeat. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around freshness reconciliation worker mismatches unbounded candidate expansion increasing cost and latency, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around freshness reconciliation worker mismatches unbounded candidate expansion increasing cost and latency, creating repeated failures\" best matches Incident review for freshness reconciliation worker: signal points to unbounded candidate expansion increasing cost and latency by targeting unbounded candidate expansion increasing cost and latency and lowering repeat risk.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for freshness reconciliation worker:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Cap candidate set sizes per request class and use staged ranking to bound tail latency.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For \"incident review for freshness reconciliation worker:\", which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Cap candidate set sizes per request class and use staged ranking to bound tail latency\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for safety filtering stage: signal points to regional serving drift in freshness and quality. A read/write boundary is currently coupled too tightly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around safety filtering stage mismatches regional serving drift in freshness and quality, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Twitter/X Timeline Ranking, Serving & Reliability: for Incident review for safety filtering stage: signal points to regional serving drift in freshness and quality, \"The current decomposition around safety filtering stage mismatches regional serving drift in freshness and quality, creating repeated failures\" is correct because it addresses regional serving drift in freshness and quality and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "In the \"incident review for safety filtering stage: signal\" scenario, what is the highest-leverage change to make now?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Separate online features from offline enrichments with explicit freshness contracts.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Separate online features from offline enrichments with explicit freshness contracts\" best matches In the \"incident review for safety filtering stage: signal\" scenario, what is the highest-leverage change to make now by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for feed quality metrics pipeline: signal points to ranking latency spikes from feature-store contention. Global averages hide one region with severe tail latency. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around feed quality metrics pipeline mismatches ranking latency spikes from feature-store contention, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"The current decomposition around feed quality metrics pipeline mismatches ranking latency spikes from feature-store contention, creating repeated failures\". It is the option most directly aligned to ranking latency spikes from feature-store contention while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After diagnosing \"incident review for feed quality metrics pipeline:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Protect cache layers with jittered refresh and request coalescing to avoid stampedes.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Twitter/X Timeline Ranking, Serving & Reliability: for After diagnosing \"incident review for feed quality metrics pipeline:\", what should change first before wider rollout, \"Protect cache layers with jittered refresh and request coalescing to avoid stampedes\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Generalize from after diagnosis, what is the strongest next change to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for multi-tenant ranking cluster: signal points to stale features producing low-quality feed order. Multiple teams are changing this path concurrently. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around multi-tenant ranking cluster mismatches stale features producing low-quality feed order, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Incident review for multi-tenant ranking cluster: signal points to stale features producing low-quality feed order is a two-step reliability decision. At stage 1, \"The current decomposition around multi-tenant ranking cluster mismatches stale features producing low-quality feed order, creating repeated failures\" wins because it balances immediate containment with long-term prevention around stale features producing low-quality feed order.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "Using the diagnosis from \"incident review for multi-tenant ranking cluster:\", which next change should be prioritized first?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Define degraded ranking modes that preserve safety and basic relevance under dependency failure."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"Define degraded ranking modes that preserve safety and basic relevance under dependency failure\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for degraded ranking fallback path: signal points to cache stampede during model rollout. Only one zone currently has reliable spare capacity. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around degraded ranking fallback path mismatches cache stampede during model rollout, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around degraded ranking fallback path mismatches cache stampede during model rollout, creating repeated failures\" best matches Incident review for degraded ranking fallback path: signal points to cache stampede during model rollout by targeting cache stampede during model rollout and lowering repeat risk.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Now that \"incident review for degraded ranking fallback path:\" is diagnosed, which immediate adjustment best addresses the risk?",
          "options": [
            "Version ranking features and enforce strict schema checks at inference boundaries.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "Now that \"incident review for degraded ranking fallback path:\" is diagnosed, which immediate adjustment best addresses the risk is a two-step reliability decision. At stage 2, \"Version ranking features and enforce strict schema checks at inference boundaries\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for candidate generation service: signal points to degraded dependency returning incomplete candidate sets. Observability exists, but ownership on action is unclear. What is the primary diagnosis?",
          "options": [
            "The current decomposition around candidate generation service mismatches degraded dependency returning incomplete candidate sets, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "Stage 1 in Twitter/X Timeline Ranking, Serving & Reliability: for Incident review for candidate generation service: signal points to degraded dependency returning incomplete candidate sets, \"The current decomposition around candidate generation service mismatches degraded dependency returning incomplete candidate sets, creating repeated failures\" is correct because it addresses degraded dependency returning incomplete candidate sets and improves controllability.",
          "detailedExplanation": "Generalize from incident review for candidate generation service: signal points to degraded dependency to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "Given the diagnosis in \"incident review for candidate generation service:\", which immediate adjustment best addresses the risk?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Route premium/critical traffic through protected capacity pools during ranking incidents.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "In stage 2, prioritize evidence-driven mitigation. \"Route premium/critical traffic through protected capacity pools during ranking incidents\" best matches Given the diagnosis in \"incident review for candidate generation service:\", which immediate adjustment best addresses the risk by targeting the stated failure path and lowering repeat risk.",
          "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for feature fetch layer: signal points to feature/schema mismatch causing ranking failures. This risk compounds when demand spikes suddenly. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "The current decomposition around feature fetch layer mismatches feature/schema mismatch causing ranking failures, creating repeated failures.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 1,
          "explanation": "For stage 1 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"The current decomposition around feature fetch layer mismatches feature/schema mismatch causing ranking failures, creating repeated failures\". It is the option most directly aligned to feature/schema mismatch causing ranking failures while preserving safe follow-on actions.",
          "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Do not reset assumptions between stages; carry forward prior constraints directly. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "With diagnosis confirmed in \"incident review for feature fetch layer: signal points\", what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 2,
          "explanation": "Stage 2 in Twitter/X Timeline Ranking, Serving & Reliability: for With diagnosis confirmed in \"incident review for feature fetch layer: signal points\", what first move gives the best reliability impact, \"Prioritize moderation and delete invalidations ahead of non-critical ranking recomputation\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ranking model inference path: signal points to fallback path overused and hurting engagement. The team prefers smallest high-leverage change first. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "The current decomposition around ranking model inference path mismatches fallback path overused and hurting engagement, creating repeated failures.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 2,
          "explanation": "Incident review for ranking model inference path: signal points to fallback path overused and hurting engagement is a two-step reliability decision. At stage 1, \"The current decomposition around ranking model inference path mismatches fallback path overused and hurting engagement, creating repeated failures\" wins because it balances immediate containment with long-term prevention around fallback path overused and hurting engagement.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "With root cause identified for \"incident review for ranking model inference path:\", what should change first before wider rollout?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes.",
            "Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs."
          ],
          "correct": 3,
          "explanation": "For stage 2 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"Use multi-level fallbacks (recent, popular, followed) with explicit quality trade-offs\". It is the option most directly aligned to the current signal while preserving safe follow-on actions.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "Generalize from twitter/X Timeline Ranking, Serving & Reliability to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for timeline serving API: signal points to priority traffic starved by background recompute jobs. Known unknowns remain around stale cache windows. What is the primary diagnosis?",
          "options": [
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel.",
            "The current decomposition around timeline serving API mismatches priority traffic starved by background recompute jobs, creating repeated failures."
          ],
          "correct": 3,
          "explanation": "In stage 1, prioritize evidence-driven mitigation. \"The current decomposition around timeline serving API mismatches priority traffic starved by background recompute jobs, creating repeated failures\" best matches Incident review for timeline serving API: signal points to priority traffic starved by background recompute jobs by targeting priority traffic starved by background recompute jobs and lowering repeat risk.",
          "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "For \"incident review for timeline serving API: signal points\", what is the highest-leverage change to make now?",
          "options": [
            "Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality.",
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 0,
          "explanation": "For \"incident review for timeline serving API: signal points\", what is the highest-leverage change to make now is a two-step reliability decision. At stage 2, \"Track quality and latency SLIs together so reliability fixes do not silently degrade feed quality\" wins because it balances immediate containment with long-term prevention around the detected pattern.",
          "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "Incident review for ranking feature cache: signal points to invalidation gaps after moderation or delete events. A rollback is possible but expensive if used too broadly. What is the primary diagnosis?",
          "options": [
            "The current decomposition around ranking feature cache mismatches invalidation gaps after moderation or delete events, creating repeated failures.",
            "Treat this as variance within current confidence bands and extend observation before changing control-path behavior.",
            "Increase retry/backoff budgets globally as the first response, deferring topology changes unless multi-window error trends worsen.",
            "Delay targeted mitigation until root-cause confidence is higher, while applying broad low-risk safeguards in parallel."
          ],
          "correct": 0,
          "explanation": "For stage 1 in Twitter/X Timeline Ranking, Serving & Reliability, the best answer is \"The current decomposition around ranking feature cache mismatches invalidation gaps after moderation or delete events, creating repeated failures\". It is the option most directly aligned to invalidation gaps after moderation or delete events while preserving safe follow-on actions.",
          "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "Now that \"incident review for ranking feature cache: signal\" is diagnosed, what first move gives the best reliability impact?",
          "options": [
            "Temporarily relax guardrail thresholds to recover throughput, then tighten controls after backlog and latency normalize.",
            "Validate model rollouts with canary cohorts and automatic rollback gates.",
            "Keep architecture unchanged and focus on stricter runbook execution before introducing control-path redesign.",
            "Continue gradual traffic expansion to gather stronger signal before committing to control-path changes."
          ],
          "correct": 1,
          "explanation": "Stage 2 in Twitter/X Timeline Ranking, Serving & Reliability: for Now that \"incident review for ranking feature cache: signal\" is diagnosed, what first move gives the best reliability impact, \"Validate model rollouts with canary cohorts and automatic rollback gates\" is correct because it addresses the incident signal and improves controllability.",
          "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-061",
      "type": "multi-select",
      "question": "Select all that apply: which signals best identify decomposition boundary mistakes.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, Select all that apply: which signals best identify decomposition boundary mistakes needs layered controls, not one silver bullet. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-062",
      "type": "multi-select",
      "question": "Select all that apply: which controls improve safety on critical write paths.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Select all that apply: which controls improve safety on critical write paths is intentionally multi-dimensional in Twitter/X Timeline Ranking, Serving & Reliability. The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Evaluate each candidate approach independently under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-063",
      "type": "multi-select",
      "question": "Select all that apply: which practices reduce hot-key or hot-partition impact.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "This is a composition question for Twitter/X Timeline Ranking, Serving & Reliability: The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-064",
      "type": "multi-select",
      "question": "Select all that apply: what improves reliability when mixing sync and async paths.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Select all that apply: what improves reliability when mixing sync and async paths, the highest-signal answer is a bundle of controls. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-065",
      "type": "multi-select",
      "question": "Select all that apply: which choices usually lower operational risk at scale.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, Select all that apply: which choices usually lower operational risk at scale needs layered controls, not one silver bullet. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize from choices usually lower operational risk at scale? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-066",
      "type": "multi-select",
      "question": "Select all that apply: what should be explicit in API/service contracts for this design.",
      "options": [
        "Per-endpoint consistency/freshness policy",
        "Explicit fallback behavior matrix",
        "Implicit behavior based on team memory",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all that apply: what should be explicit in API/service contracts for this design is intentionally multi-dimensional in Twitter/X Timeline Ranking, Serving & Reliability. The correct combination is Per-endpoint consistency/freshness policy, Explicit fallback behavior matrix, and Implicit behavior based on team memory. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-067",
      "type": "multi-select",
      "question": "Select all that apply: which anti-patterns often cause incident recurrence.",
      "options": [
        "Dependency saturation by class",
        "Only global averages without segmentation",
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "This is a composition question for Twitter/X Timeline Ranking, Serving & Reliability: The correct combination is Dependency saturation by class, Per-boundary latency/error telemetry, and Fault-domain segmented dashboards. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-068",
      "type": "multi-select",
      "question": "Select all that apply: what increases confidence before broad traffic rollout.",
      "options": [
        "Unbounded retries during overload",
        "Idempotency enforcement on retries",
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "For Select all that apply: what increases confidence before broad traffic rollout, the highest-signal answer is a bundle of controls. The correct combination is Idempotency enforcement on retries, Explicit timeout budgets per hop, and Priority-aware admission controls. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-069",
      "type": "multi-select",
      "question": "Select all that apply: which controls protect high-priority traffic during spikes.",
      "options": [
        "Adaptive sharding for hot entities",
        "Key-level replication for skewed load",
        "Queue isolation for heavy producers",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, Select all that apply: which controls protect high-priority traffic during spikes needs layered controls, not one silver bullet. The correct combination is Adaptive sharding for hot entities, Key-level replication for skewed load, and Queue isolation for heavy producers. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-070",
      "type": "multi-select",
      "question": "Select all that apply: which telemetry dimensions are most actionable for design triage.",
      "options": [
        "Async outbox for side effects",
        "Replay-safe dedupe processing",
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Twitter/X Timeline Ranking, Serving & Reliability: The correct combination is Async outbox for side effects, Replay-safe dedupe processing, and Transactional boundaries for critical writes. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Evaluate each candidate approach independently under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-071",
      "type": "multi-select",
      "question": "Select all that apply: which governance actions improve cross-team reliability ownership.",
      "options": [
        "Capacity headroom by priority class",
        "Manual ad hoc response only",
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria"
      ],
      "correctIndices": [0, 2, 3],
      "explanation": "For Select all that apply: which governance actions improve cross-team reliability ownership, the highest-signal answer is a bundle of controls. The correct combination is Capacity headroom by priority class, Canary rollout with rollback gates, and Runbook ownership and abort criteria. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Validate each proposed control independently and avoid partially true claims that fail under realistic load. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-072",
      "type": "multi-select",
      "question": "Select all that apply: what helps prevent retry amplification cascades.",
      "options": [
        "Implicit behavior based on team memory",
        "Contracted degraded-mode semantics",
        "Per-endpoint consistency/freshness policy",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, Select all that apply: what helps prevent retry amplification cascades needs layered controls, not one silver bullet. The correct combination is Implicit behavior based on team memory, Contracted degraded-mode semantics, and Per-endpoint consistency/freshness policy. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Tie decisions to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-073",
      "type": "multi-select",
      "question": "Select all that apply: which fallback strategies are strong when dependencies degrade.",
      "options": [
        "Per-boundary latency/error telemetry",
        "Fault-domain segmented dashboards",
        "Dependency saturation by class",
        "Only global averages without segmentation"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all that apply: which fallback strategies are strong when dependencies degrade is intentionally multi-dimensional in Twitter/X Timeline Ranking, Serving & Reliability. The correct combination is Per-boundary latency/error telemetry, Fault-domain segmented dashboards, and Dependency saturation by class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-074",
      "type": "multi-select",
      "question": "Select all that apply: what reduces data-quality regressions in eventual pipelines.",
      "options": [
        "Explicit timeout budgets per hop",
        "Priority-aware admission controls",
        "Unbounded retries during overload",
        "Idempotency enforcement on retries"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "This is a composition question for Twitter/X Timeline Ranking, Serving & Reliability: The correct combination is Explicit timeout budgets per hop, Priority-aware admission controls, and Idempotency enforcement on retries. The set works because each element covers a different failure-management gap.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Evaluate each candidate approach independently under the same constraints. Strong answers connect quorum/coordination settings to concrete correctness goals. Common pitfall: misreading quorum behavior during failures.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-075",
      "type": "multi-select",
      "question": "Select all that apply: which runbook components improve incident execution quality.",
      "options": [
        "Queue isolation for heavy producers",
        "Single partition for all traffic classes",
        "Adaptive sharding for hot entities",
        "Assume healthy behavior globally without domain-specific evidence"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "For Select all that apply: which runbook components improve incident execution quality, the highest-signal answer is a bundle of controls. The correct combination is Queue isolation for heavy producers, Single partition for all traffic classes, and Adaptive sharding for hot entities. That bundle reduces both immediate incident cost and repeat incidence.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Validate each proposed control independently and avoid partially true claims that fail under realistic load. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-076",
      "type": "multi-select",
      "question": "Select all that apply: which architecture choices improve blast-radius containment.",
      "options": [
        "Direct dual-write without reconciliation",
        "Transactional boundaries for critical writes",
        "Async outbox for side effects",
        "Replay-safe dedupe processing"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "In Twitter/X Timeline Ranking, Serving & Reliability, Select all that apply: which architecture choices improve blast-radius containment needs layered controls, not one silver bullet. The correct combination is Transactional boundaries for critical writes, Async outbox for side effects, and Replay-safe dedupe processing. Together they improve detection, containment, and recovery quality.",
      "detailedExplanation": "Generalize from architecture choices improve blast-radius containment? (Select all that apply) to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Anchor decisions in explicit constraints, invariants, and observable failure signals rather than intuition. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-077",
      "type": "multi-select",
      "question": "Select all that apply: what evidence demonstrates a fix worked beyond short-term recovery.",
      "options": [
        "Canary rollout with rollback gates",
        "Runbook ownership and abort criteria",
        "Capacity headroom by priority class",
        "Manual ad hoc response only"
      ],
      "correctIndices": [0, 1, 2],
      "explanation": "Select all that apply: what evidence demonstrates a fix worked beyond short-term recovery is intentionally multi-dimensional in Twitter/X Timeline Ranking, Serving & Reliability. The correct combination is Canary rollout with rollback gates, Runbook ownership and abort criteria, and Capacity headroom by priority class. Selecting all of them shows tradeoff-aware reliability reasoning.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Evaluate each candidate approach independently under the same constraints. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-078",
      "type": "numeric-input",
      "question": "Given a critical path handles 5,400,000 requests/day and 0.18% fail SLO, failures/day?",
      "answer": 9720,
      "unit": "requests",
      "tolerance": 0.03,
      "explanation": "The operational math for Given a critical path handles 5,400,000 requests/day and 0 gives 9720 requests. In interview pacing, hitting this value within +/-3% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 5,400 and 000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-079",
      "type": "numeric-input",
      "question": "Given queue ingest is 2,200 events/min and drain is 2,530 events/min, net drain rate?",
      "answer": 330,
      "unit": "events/min",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Given queue ingest is 2,200 events/min and drain is 2,530 events/min, net drain rate: 330 events/min. Answers within +/-0% show correct directional reasoning for Twitter/X Timeline Ranking, Serving & Reliability.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Keep every transformation in one unit system and check order of magnitude at the end. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Keep quantities like 2,200 and 2,530 in aligned units before deciding on an implementation approach. Common pitfall: consumer lag growth under burst traffic.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-080",
      "type": "numeric-input",
      "question": "Given retries add 0.28 extra attempts at 75,000 req/sec, effective attempts/sec?",
      "answer": 96000,
      "unit": "attempts/sec",
      "tolerance": 0.02,
      "explanation": "Twitter/X Timeline Ranking, Serving & Reliability expects quick quantitative triage: Given retries add 0 evaluates to 96000 attempts/sec. Any answer within +/-2% is acceptable.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 0.28 and 75,000 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-081",
      "type": "numeric-input",
      "question": "Given failover takes 16s and occurs 24 times/day, total failover seconds/day?",
      "answer": 384,
      "unit": "seconds",
      "tolerance": 0,
      "explanation": "The operational math for Given failover takes 16s and occurs 24 times/day, total failover seconds/day gives 384 seconds. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Numbers such as 16s and 24 should be normalized first so downstream reasoning stays consistent. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-082",
      "type": "numeric-input",
      "question": "Given target p99 is 650ms; observed p99 is 845ms, percent over target?",
      "answer": 30,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Use first-pass reliability arithmetic for Given target p99 is 650ms; observed p99 is 845ms, percent over target: 30 %. Answers within +/-30% show correct directional reasoning for Twitter/X Timeline Ranking, Serving & Reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Keep every transformation in one unit system and check order of magnitude at the end. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Keep quantities like 650ms and 845ms in aligned units before deciding on an implementation approach. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-083",
      "type": "numeric-input",
      "question": "In this scenario, if 34% of 130,000 req/min are high-priority, how many high-priority req/min?",
      "answer": 44200,
      "unit": "requests/min",
      "tolerance": 0.02,
      "explanation": "For In this scenario, if 34% of 130,000 req/min are high-priority, how many high-priority req/min, the computed target in Twitter/X Timeline Ranking, Serving & Reliability is 44200 requests/min. Responses within +/-2% indicate sound sizing judgment.",
      "detailedExplanation": "Generalize from if 34% of 130,000 req/min are high-priority, how many high-priority req/min to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. Capacity reasoning should separate average and peak demand, then map both to saturation limits. If values like 34 and 130,000 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-084",
      "type": "numeric-input",
      "question": "Given error rate drops from 1.0% to 0.22%, percent reduction?",
      "answer": 78,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Twitter/X Timeline Ranking, Serving & Reliability expects quick quantitative triage: Given error rate drops from 1 evaluates to 78 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 1.0 and 0.22 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-085",
      "type": "numeric-input",
      "question": "Given a 9-node quorum cluster requires majority writes, minimum acknowledgements?",
      "answer": 5,
      "unit": "acks",
      "tolerance": 0,
      "explanation": "The operational math for Given a 9-node quorum cluster requires majority writes, minimum acknowledgements gives 5 acks. In interview pacing, hitting this value within +/-0% is the pass condition.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Normalize units before computing so conversion mistakes do not propagate. Pick based on invariants and acceptable anomalies, then justify latency/availability tradeoffs. If values like 9 and 5 appear, convert them into one unit basis before comparison. Common pitfall: using weak consistency for strict invariants.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "CAP theorem",
          "url": "https://en.wikipedia.org/wiki/CAP_theorem"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-086",
      "type": "numeric-input",
      "question": "Given backlog is 56,000 tasks with net drain 350 tasks/min, minutes to clear?",
      "answer": 160,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "Use first-pass reliability arithmetic for Given backlog is 56,000 tasks with net drain 350 tasks/min, minutes to clear: 160 minutes. Answers within +/-0% show correct directional reasoning for Twitter/X Timeline Ranking, Serving & Reliability.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 56,000 and 350 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-087",
      "type": "numeric-input",
      "question": "Given a fleet has 18 zones and 3 are unavailable, percent remaining available?",
      "answer": 83.33,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "For Given a fleet has 18 zones and 3 are unavailable, percent remaining available, the computed target in Twitter/X Timeline Ranking, Serving & Reliability is 83.33 %. Responses within +/-30% indicate sound sizing judgment.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Keep every transformation in one unit system and check order of magnitude at the end. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 18 and 3 in aligned units before deciding on an implementation approach. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-088",
      "type": "numeric-input",
      "question": "Given mTTR improved from 52 min to 34 min, percent reduction?",
      "answer": 34.62,
      "unit": "%",
      "tolerance": 0.3,
      "explanation": "Twitter/X Timeline Ranking, Serving & Reliability expects quick quantitative triage: Given mTTR improved from 52 min to 34 min, percent reduction evaluates to 34.62 %. Any answer within +/-30% is acceptable.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Normalize units before computing so conversion mistakes do not propagate. Tie decisions to concrete operational outcomes, not abstract reliability language. If values like 52 min and 34 min appear, convert them into one unit basis before comparison. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-089",
      "type": "numeric-input",
      "question": "In this scenario, if 11% of 2,800,000 daily ops need manual checks, checks/day?",
      "answer": 308000,
      "unit": "operations",
      "tolerance": 0.02,
      "explanation": "The operational math for In this scenario, if 11% of 2,800,000 daily ops need manual checks, checks/day gives 308000 operations. In interview pacing, hitting this value within +/-2% is the pass condition.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 11 and 2,800 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-090",
      "type": "ordering",
      "question": "Considering twitter/x timeline ranking, serving & reliability, order a classic-design decomposition workflow.",
      "items": [
        "Identify critical user journey and invariants",
        "Split request path into atomic components",
        "Assign reliability/scale controls per boundary",
        "Validate with load/failure drills and refine"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Twitter/X Timeline Ranking, Serving & Reliability should start with Identify critical user journey and invariants and end with Validate with load/failure drills and refine. Considering twitter/x timeline ranking, serving & reliability, order a classic-design decomposition workflow rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. A harsh sanity check should identify which assumption is most likely wrong. Common pitfall: accepting implausible outputs because arithmetic is clean.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-091",
      "type": "ordering",
      "question": "From a twitter/x timeline ranking, serving & reliability viewpoint, order by increasing design risk.",
      "items": [
        "Explicit boundaries with contracts",
        "Shared dependency with safeguards",
        "Shared dependency without safeguards",
        "Implicit coupling with no ownership"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Explicit boundaries with contracts must happen before Implicit coupling with no ownership. That ordering matches incident-safe flow in Twitter/X Timeline Ranking, Serving & Reliability.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-092",
      "type": "ordering",
      "question": "Within twitter/x timeline ranking, serving & reliability, order safe incident mitigation steps.",
      "items": [
        "Scope blast radius and affected flows",
        "Contain with guardrails and admission controls",
        "Apply targeted root-cause fix",
        "Run recurrence checks and hardening actions"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Twitter/X Timeline Ranking, Serving & Reliability emphasizes safe recovery order. Beginning at Scope blast radius and affected flows and finishing at Run recurrence checks and hardening actions keeps blast radius controlled while restoring service.",
      "detailedExplanation": "For related interview or production problems, make the decision around the dominant constraint instead of broad platform-wide changes. Place obvious extremes first, then sort the middle by pairwise comparison. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-093",
      "type": "ordering",
      "question": "In this twitter/x timeline ranking, serving & reliability context, order by increasing retry-control maturity.",
      "items": [
        "Fixed immediate retries",
        "Capped exponential retries",
        "Capped retries with jitter",
        "Jittered retries with retry budgets and telemetry"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For In this twitter/x timeline ranking, serving & reliability context, order by increasing retry-control maturity, the correct ordering runs from Fixed immediate retries to Jittered retries with retry budgets and telemetry. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview and production incidents, classify the dominant failure mode first, then choose the earliest intervention that materially reduces user-facing risk. Place obvious extremes first, then sort the middle by pairwise comparison. A strong real-world approach explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-094",
      "type": "ordering",
      "question": "Order fallback sophistication. Focus on twitter/x timeline ranking, serving & reliability tradeoffs.",
      "items": [
        "Implicit fallback behavior",
        "Manual fallback toggles",
        "Documented fallback matrix",
        "Policy-driven automated fallback with tests"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Twitter/X Timeline Ranking, Serving & Reliability should start with Implicit fallback behavior and end with Policy-driven automated fallback with tests. Order fallback sophistication rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "Generalize from order fallback sophistication to the underlying invariant and failure mode, then compare approaches by risk reduction, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-095",
      "type": "ordering",
      "question": "For twitter/x timeline ranking, serving & reliability, order failover validation rigor.",
      "items": [
        "Host health check only",
        "Health plus freshness checks",
        "Health/freshness plus staged traffic shift",
        "Staged shift plus failback rehearsal and rollback gates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Host health check only must happen before Staged shift plus failback rehearsal and rollback gates. That ordering matches incident-safe flow in Twitter/X Timeline Ranking, Serving & Reliability.",
      "detailedExplanation": "Generalize this scenario to the underlying systems skill: identify the invariant to protect, the load/failure pattern, and the first control that changes outcomes. Place obvious extremes first, then sort the middle by pairwise comparison. Map the approach to measurable reliability impact such as error-budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-096",
      "type": "ordering",
      "question": "Order these by blast radius, low to high.",
      "items": [
        "Single process failure",
        "Single node failure",
        "Single zone failure",
        "Cross-region control-plane failure"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Twitter/X Timeline Ranking, Serving & Reliability emphasizes safe recovery order. Beginning at Single process failure and finishing at Cross-region control-plane failure keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Build the rank from biggest differences first, then refine with adjacent checks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-097",
      "type": "ordering",
      "question": "Order data-path durability confidence. Use a twitter/x timeline ranking, serving & reliability perspective.",
      "items": [
        "In-memory only acknowledgment",
        "Single durable write",
        "Replicated durable write",
        "Replicated durable write plus replay/integrity verification"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "For Order data-path durability confidence, the correct ordering runs from In-memory only acknowledgment to Replicated durable write plus replay/integrity verification. This progression minimizes rework and reduces risk during active incident response.",
      "detailedExplanation": "For related interview or production problems, distinguish core signal from background noise before selecting mitigations. Place obvious extremes first, then sort the middle by pairwise comparison. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-098",
      "type": "ordering",
      "question": "Considering twitter/x timeline ranking, serving & reliability, order by increasing operational discipline.",
      "items": [
        "Ad hoc incident response",
        "Named responder roles",
        "Role-based response with timeline",
        "Role-based response plus action closure tracking"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "The sequence in Twitter/X Timeline Ranking, Serving & Reliability should start with Ad hoc incident response and end with Role-based response plus action closure tracking. Considering twitter/x timeline ranking, serving & reliability, order by increasing operational discipline rewards stabilization-first execution before broader tuning.",
      "detailedExplanation": "For related interview or production problems, identify the highest-signal symptom early and map it to the smallest high-leverage control change. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-099",
      "type": "ordering",
      "question": "From a twitter/x timeline ranking, serving & reliability viewpoint, order rollout safety for major design changes.",
      "items": [
        "Canary small cohort",
        "Monitor guardrail metrics",
        "Expand traffic gradually",
        "Finalize runbook and ownership updates"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Order these steps by operational dependency, not preference: Canary small cohort must happen before Finalize runbook and ownership updates. That ordering matches incident-safe flow in Twitter/X Timeline Ranking, Serving & Reliability.",
      "detailedExplanation": "Begin by naming the dominant constraint, then pressure-test candidate approaches against reliability, latency, and operability trade-offs. Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    },
    {
      "id": "cd-tr-100",
      "type": "ordering",
      "question": "Order evidence strength for fix success. (twitter/x timeline ranking, serving & reliability lens)",
      "items": [
        "Single successful test run",
        "Short canary stability",
        "Sustained SLO recovery in production",
        "Sustained recovery plus failure-drill pass"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Twitter/X Timeline Ranking, Serving & Reliability emphasizes safe recovery order. Beginning at Single successful test run and finishing at Sustained recovery plus failure-drill pass keeps blast radius controlled while restoring service.",
      "detailedExplanation": "Anchor on the dominant constraint and evaluate approaches by blast radius, reversibility, and operational cost. Order by relative scale and bottleneck effect, then validate neighboring items. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ],
      "tags": [
        "classic-designs",
        "twitter-x-timeline-ranking-serving-and-reliability"
      ],
      "difficulty": "staff-level"
    }
  ]
}
