{
  "unit": 7,
  "unitTitle": "Scaling Compute",
  "chapter": 1,
  "chapterTitle": "Load Balancing Fundamentals",
  "chapterDescription": "Core load balancing decisions across L4/L7 routing, health checks, stickiness, failover behavior, and production traffic management.",
  "problems": [
    {
      "id": "sc-lb-001",
      "type": "multiple-choice",
      "question": "A checkout API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Traffic is steady but p99 spikes during GC events.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use round robin because simple even distribution when request cost is similar",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "round robin is suitable here because simple even distribution when request cost is similar. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "If you keep \"checkout API has 8 healthy instances and request cost is variable\" in view, the correct answer separates faster. Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 8 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-002",
      "type": "multiple-choice",
      "question": "A image upload API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Request sizes vary 20x between tenants.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use least connections because better for uneven request duration because it tracks active load"
      ],
      "correct": 3,
      "explanation": "least connections is suitable here because better for uneven request duration because it tracks active load. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "This prompt is really about \"image upload API has 8 healthy instances and request cost is variable\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Numbers such as 8 and 20x should be normalized first so downstream reasoning stays consistent. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-003",
      "type": "multiple-choice",
      "question": "A search API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? A few endpoints hold connections for 30+ seconds.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use least response time because reacts to backend latency differences",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "least response time is suitable here because reacts to backend latency differences. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "Use \"search API has 8 healthy instances and request cost is variable\" as your starting point, then verify tradeoffs carefully. Discard options that weaken contract clarity or compatibility over time. Interface decisions should be justified by contract stability and client impact over time. Keep quantities like 8 and 30 in aligned units before selecting an answer. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-004",
      "type": "multiple-choice",
      "question": "A mobile gateway has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Background exports share the same backend pool.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use weighted round robin because supports heterogeneous backend capacity"
      ],
      "correct": 3,
      "explanation": "weighted round robin is suitable here because supports heterogeneous backend capacity. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "Read this as a scenario about \"mobile gateway has 8 healthy instances and request cost is variable\". Discard plans that assume linear scaling despite shared bottlenecks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 8 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-005",
      "type": "multiple-choice",
      "question": "A billing service has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? One AZ has 15% fewer healthy instances today.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use round robin because simple even distribution when request cost is similar",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "round robin is suitable here because simple even distribution when request cost is similar. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "The decision turns on \"billing service has 8 healthy instances and request cost is variable\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 8 and 15 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-006",
      "type": "multiple-choice",
      "question": "A inventory API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Queue depth on some instances grows faster than others.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use least connections because better for uneven request duration because it tracks active load"
      ],
      "correct": 3,
      "explanation": "least connections is suitable here because better for uneven request duration because it tracks active load. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "Start from \"inventory API has 8 healthy instances and request cost is variable\", then pressure-test the result against the options. Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Numbers such as 8 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-lb-007",
      "type": "multiple-choice",
      "question": "A profile service has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Bot traffic causes bursty arrival patterns.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use least response time because reacts to backend latency differences",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "least response time is suitable here because reacts to backend latency differences. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "The key clue in this question is \"profile service has 8 healthy instances and request cost is variable\". Eliminate options that fail under peak load or saturation conditions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 8 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-008",
      "type": "multiple-choice",
      "question": "A recommendation API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Cache-miss requests are much heavier than cache hits.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use weighted round robin because supports heterogeneous backend capacity"
      ],
      "correct": 3,
      "explanation": "weighted round robin is suitable here because supports heterogeneous backend capacity. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "The core signal here is \"recommendation API has 8 healthy instances and request cost is variable\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Numbers such as 8 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        }
      ]
    },
    {
      "id": "sc-lb-009",
      "type": "multiple-choice",
      "question": "A admin dashboard has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Periodic cron bursts hit at minute boundaries.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use round robin because simple even distribution when request cost is similar",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "round robin is suitable here because simple even distribution when request cost is similar. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "If you keep \"admin dashboard has 8 healthy instances and request cost is variable\" in view, the correct answer separates faster. Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 8 appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-010",
      "type": "multiple-choice",
      "question": "A notification API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Long-lived HTTP/2 streams coexist with short RPC calls.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use least connections because better for uneven request duration because it tracks active load"
      ],
      "correct": 3,
      "explanation": "least connections is suitable here because better for uneven request duration because it tracks active load. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "The key clue in this question is \"notification API has 8 healthy instances and request cost is variable\". Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. If values like 8 and 2 appear, convert them into one unit basis before comparison. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-011",
      "type": "multiple-choice",
      "question": "A auth edge has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? A canary backend has half the CPU of stable nodes.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use least response time because reacts to backend latency differences",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "least response time is suitable here because reacts to backend latency differences. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "Start from \"auth edge has 8 healthy instances and request cost is variable\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Keep quantities like 8 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-012",
      "type": "multiple-choice",
      "question": "A stream metadata API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? TLS handshake cost dominates a subset of requests.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use weighted round robin because supports heterogeneous backend capacity"
      ],
      "correct": 3,
      "explanation": "weighted round robin is suitable here because supports heterogeneous backend capacity. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "The decision turns on \"stream metadata API has 8 healthy instances and request cost is variable\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Keep quantities like 8 in aligned units before selecting an answer. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-013",
      "type": "multiple-choice",
      "question": "A checkout API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Partial dependency outage affects only write paths.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use round robin because simple even distribution when request cost is similar",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "round robin is suitable here because simple even distribution when request cost is similar. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "Read this as a scenario about \"checkout API has 8 healthy instances and request cost is variable\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. If values like 8 appear, convert them into one unit basis before comparison. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-014",
      "type": "multiple-choice",
      "question": "A image upload API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Mobile clients retry aggressively on 5xx.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use least connections because better for uneven request duration because it tracks active load"
      ],
      "correct": 3,
      "explanation": "least connections is suitable here because better for uneven request duration because it tracks active load. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "Use \"image upload API has 8 healthy instances and request cost is variable\" as your starting point, then verify tradeoffs carefully. Prioritize the option that best protects the reliability objective under the stated failure conditions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. If values like 8 appear, convert them into one unit basis before comparison. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        }
      ]
    },
    {
      "id": "sc-lb-015",
      "type": "multiple-choice",
      "question": "A search API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Regional failover shifted 30% extra traffic into this pool.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use least response time because reacts to backend latency differences",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "least response time is suitable here because reacts to backend latency differences. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "This prompt is really about \"search API has 8 healthy instances and request cost is variable\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 8 and 30 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-016",
      "type": "multiple-choice",
      "question": "A mobile gateway has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Some tenants require strict per-user affinity.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use weighted round robin because supports heterogeneous backend capacity"
      ],
      "correct": 3,
      "explanation": "weighted round robin is suitable here because supports heterogeneous backend capacity. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "If you keep \"mobile gateway has 8 healthy instances and request cost is variable\" in view, the correct answer separates faster. Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 8 in aligned units before selecting an answer. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-017",
      "type": "multiple-choice",
      "question": "A billing service has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Blue/green deploy is draining 20% of nodes.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use round robin because simple even distribution when request cost is similar",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "round robin is suitable here because simple even distribution when request cost is similar. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "The core signal here is \"billing service has 8 healthy instances and request cost is variable\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Keep quantities like 8 and 20 in aligned units before selecting an answer. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-018",
      "type": "multiple-choice",
      "question": "A inventory API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? Observed backend latency variance doubled this week.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use least connections because better for uneven request duration because it tracks active load"
      ],
      "correct": 3,
      "explanation": "least connections is suitable here because better for uneven request duration because it tracks active load. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "The key clue in this question is \"inventory API has 8 healthy instances and request cost is variable\". Discard options that weaken contract clarity or compatibility over time. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Keep quantities like 8 in aligned units before selecting an answer. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-019",
      "type": "multiple-choice",
      "question": "A profile service has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? One shard serves premium users with heavier workloads.",
      "options": [
        "Route randomly and ignore backend saturation",
        "Use least response time because reacts to backend latency differences",
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead"
      ],
      "correct": 1,
      "explanation": "least response time is suitable here because reacts to backend latency differences. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "Start from \"profile service has 8 healthy instances and request cost is variable\", then pressure-test the result against the options. Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 8 appear, convert them into one unit basis before comparison. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-020",
      "type": "multiple-choice",
      "question": "A recommendation API has 8 healthy instances and request cost is variable. Which load-balancing choice is strongest as a default? API gateway now supports request hedging for reads.",
      "options": [
        "Pin all traffic to one backend to preserve cache warmth",
        "Disable health checks to reduce LB overhead",
        "Route randomly and ignore backend saturation",
        "Use weighted round robin because supports heterogeneous backend capacity"
      ],
      "correct": 3,
      "explanation": "weighted round robin is suitable here because supports heterogeneous backend capacity. The alternatives either remove resilience or ignore load signals.",
      "detailedExplanation": "This prompt is really about \"recommendation API has 8 healthy instances and request cost is variable\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Numbers such as 8 should be normalized first so downstream reasoning stays consistent. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-021",
      "type": "multiple-choice",
      "question": "A team requires URL path based routing (`/api/*` vs `/static/*`) at the balancer. What layer capability is required?",
      "options": [
        "Anycast without proxying",
        "L4 TCP load balancing",
        "L7 HTTP-aware load balancing",
        "DNS round robin only"
      ],
      "correct": 2,
      "explanation": "Path/header-based policies require L7 awareness of HTTP semantics.",
      "detailedExplanation": "Use \"team requires URL path based routing (`/api/*` vs `/static/*`) at the balancer\" as your starting point, then verify tradeoffs carefully. Discard options that weaken contract clarity or compatibility over time. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-022",
      "type": "multiple-choice",
      "question": "An LB marks backend healthy only if TCP connects, but app returns many 500s. What is the most direct fix?",
      "options": [
        "Use application-level health checks for dependency readiness",
        "Disable checks to avoid false positives",
        "Switch to sticky sessions",
        "Increase idle timeout only"
      ],
      "correct": 0,
      "explanation": "TCP reachability is insufficient; health checks should verify app-level readiness.",
      "detailedExplanation": "The core signal here is \"lB marks backend healthy only if TCP connects, but app returns many 500s\". Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 500s appear, convert them into one unit basis before comparison. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-023",
      "type": "multiple-choice",
      "question": "During deploys, in-flight requests fail when instances are terminated immediately. Best mitigation?",
      "options": [
        "Reduce replica count first",
        "Terminate faster to finish rollout",
        "Connection draining/graceful deregistration before removal",
        "Force all requests to retry twice"
      ],
      "correct": 2,
      "explanation": "Draining allows in-flight requests to complete before backend removal.",
      "detailedExplanation": "If you keep \"during deploys, in-flight requests fail when instances are terminated immediately\" in view, the correct answer separates faster. Discard plans that assume linear scaling despite shared bottlenecks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-024",
      "type": "multiple-choice",
      "question": "A product requires user session affinity for short-lived cart state kept in memory. Which trade-off is true?",
      "options": [
        "Sticky sessions eliminate all failover risk",
        "Sticky sessions are required for stateless services",
        "Sticky sessions reduce need for health checks",
        "Sticky sessions improve locality but reduce rebalance flexibility on node failure"
      ],
      "correct": 3,
      "explanation": "Affinity helps cache/session locality but can create uneven load and harder failover.",
      "detailedExplanation": "Start from \"product requires user session affinity for short-lived cart state kept in memory\", then pressure-test the result against the options. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-025",
      "type": "multiple-choice",
      "question": "What does fail-open behavior usually mean for a load balancer?",
      "options": [
        "Bypass observability",
        "Reject all traffic when uncertain about backend health",
        "Continue routing to potentially unhealthy backends to preserve availability",
        "Disable retries globally"
      ],
      "correct": 2,
      "explanation": "Fail-open favors availability over strict correctness when health signal confidence drops.",
      "detailedExplanation": "The key clue in this question is \"fail-open behavior usually mean for a load balancer\". Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-026",
      "type": "multiple-choice",
      "question": "What is the strongest reason to terminate TLS at the edge load balancer?",
      "options": [
        "Centralizes certificate management and enables L7 routing/inspection",
        "It guarantees zero latency overhead",
        "It prevents need for mTLS anywhere",
        "It removes all encryption requirements internally"
      ],
      "correct": 0,
      "explanation": "Edge termination simplifies cert operations and unlocks HTTP-aware policies.",
      "detailedExplanation": "Read this as a scenario about \"the strongest reason to terminate TLS at the edge load balancer\". Eliminate designs that create ambiguous API semantics or brittle versioning paths. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: ambiguous contracts that hide behavior changes.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-027",
      "type": "multiple-choice",
      "question": "A single regional load balancer is a critical SPOF. Which architecture is the standard mitigation?",
      "options": [
        "Disable autoscaling",
        "Add retries at clients only",
        "Run redundant LB instances/zones with health-checked failover",
        "Increase one LB size vertically forever"
      ],
      "correct": 2,
      "explanation": "Redundant LB planes remove single-node failure risk.",
      "detailedExplanation": "The decision turns on \"single regional load balancer is a critical SPOF\". Prefer the choice that maps cleanly to throughput, concurrency, and scaling limits. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-028",
      "type": "multiple-choice",
      "question": "Backend CPUs are low, but request latency is rising due to long queues per instance. Which LB algorithm often helps first?",
      "options": [
        "Least connections / least outstanding requests",
        "Hash by client IP",
        "Random with equal weight",
        "Round robin static"
      ],
      "correct": 0,
      "explanation": "Queue-aware algorithms avoid sending more work to already-busy instances.",
      "detailedExplanation": "This prompt is really about \"backend CPUs are low, but request latency is rising due to long queues per instance\". Prefer the choice that keeps ordering/acknowledgment behavior predictable under failure. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: ordering loss during partition or replay changes.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-029",
      "type": "multiple-choice",
      "question": "A system needs deterministic per-user routing to improve cache hit ratio without full sticky sessions. Which approach fits?",
      "options": [
        "Disable caching",
        "Consistent hashing by user key",
        "Random routing",
        "Global FIFO at LB"
      ],
      "correct": 1,
      "explanation": "Consistent hashing gives stable placement with less movement on membership changes.",
      "detailedExplanation": "Use \"system needs deterministic per-user routing to improve cache hit ratio without full\" as your starting point, then verify tradeoffs carefully. Reject options that improve speed but weaken freshness or invalidation correctness. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-030",
      "type": "multiple-choice",
      "question": "What is a typical downside of client-side load balancing compared with centralized server-side LB?",
      "options": [
        "Clients must maintain fresher backend membership/health data",
        "Lower request latency always",
        "No retry logic needed",
        "No need for service discovery"
      ],
      "correct": 0,
      "explanation": "Client-side balancing shifts discovery/health complexity into every caller.",
      "detailedExplanation": "If you keep \"a typical downside of client-side load balancing compared with centralized server-side\" in view, the correct answer separates faster. Eliminate options that fail under peak load or saturation conditions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-031",
      "type": "multiple-choice",
      "question": "If one backend has 2x CPU capacity, which policy best uses it?",
      "options": [
        "Use only DNS randomization",
        "Equal round robin weights",
        "Weighted routing reflecting backend capacity",
        "Disable that backend to avoid skew"
      ],
      "correct": 2,
      "explanation": "Weighted policies map traffic to heterogeneous capacity.",
      "detailedExplanation": "The core signal here is \"if one backend has 2x CPU capacity, which policy best uses it\". Eliminate options that fail under peak load or saturation conditions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 2x should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-032",
      "type": "multiple-choice",
      "question": "What is the key risk when health checks are too aggressive (short interval + low timeout)?",
      "options": [
        "Instance flapping and unnecessary traffic churn",
        "Higher cache hit ratio",
        "Guaranteed lower latency",
        "Zero false positives"
      ],
      "correct": 0,
      "explanation": "Overly strict probes can oscillate healthy nodes in/out under transient jitter.",
      "detailedExplanation": "Use \"the key risk when health checks are too aggressive (short interval + low timeout)\" as your starting point, then verify tradeoffs carefully. Reject choices that sound good generally but do not reduce the concrete reliability risk in this scenario. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-033",
      "type": "multiple-choice",
      "question": "A CDN fronts static assets, but dynamic API traffic still needs origin balancing. Why?",
      "options": [
        "Only DNS is needed for APIs",
        "CDN replaces all backend balancing",
        "Dynamic requests still require origin-level load distribution and failover",
        "CDN makes health checks obsolete"
      ],
      "correct": 2,
      "explanation": "CDNs reduce edge load but do not eliminate balancing needs for origin APIs.",
      "detailedExplanation": "This prompt is really about \"cDN fronts static assets, but dynamic API traffic still needs origin balancing\". Prefer the choice that keeps client behavior explicit while preserving evolvability. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-034",
      "type": "multiple-choice",
      "question": "What is the most accurate statement about DNS round-robin for load balancing?",
      "options": [
        "It is simple but limited by DNS caching and slower failover dynamics",
        "It replaces layer-7 policy routing",
        "It guarantees equal load always",
        "It provides per-request health-aware balancing"
      ],
      "correct": 0,
      "explanation": "DNS balancing is coarse and cache-influenced; health/failover behavior is less precise.",
      "detailedExplanation": "The decision turns on \"the most accurate statement about DNS round-robin for load balancing\". Eliminate answers that do not directly address the failure mode, recovery path, or blast radius. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-035",
      "type": "multiple-choice",
      "question": "You need to canary 5% traffic to v2 based on request headers. Best place?",
      "options": [
        "Kernel socket backlog",
        "L4 network ACL",
        "L7 load balancer/proxy routing rules",
        "DNS TTL changes only"
      ],
      "correct": 2,
      "explanation": "Header-aware split routing is an L7 capability.",
      "detailedExplanation": "Read this as a scenario about \"you need to canary 5% traffic to v2 based on request headers\". Discard plans that assume linear scaling despite shared bottlenecks. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Keep quantities like 5 in aligned units before selecting an answer. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-036",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a flash-sale checkout API. First symptom: tail latency doubles under promo traffic. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because latency spikes during promotions",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node"
          ],
          "correct": 1,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "If you keep \"you operate a flash-sale checkout API\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the mitigation that improves distribution without sacrificing resilience.",
          "options": [
            "Remove observability labels from requests",
            "Adopt least connections with queue-aware routing",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend"
          ],
          "correct": 1,
          "explanation": "least connections with queue-aware routing addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "This prompt is really about \"after confirming imbalance, which mitigation is the strongest next move? Choose the\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: assuming exactly-once without idempotency."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"load Balancing Fundamentals\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-037",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a photo upload gateway. First symptom: long-lived uploads starve short requests. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Investigate load-balancer policy because frequent backend restarts in deploy windows",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day"
          ],
          "correct": 0,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "This prompt is really about \"you operate a photo upload gateway\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the next action that is safest for production rollout.",
          "options": [
            "Adopt graceful draining before deregistration",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests"
          ],
          "correct": 0,
          "explanation": "graceful draining before deregistration addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "If you keep \"after confirming imbalance, which mitigation is the strongest next move? Pick the next\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"load Balancing Fundamentals\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-038",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a fintech transfer API. First symptom: one zone intermittently degrades network quality. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because one AZ shows intermittent packet loss"
          ],
          "correct": 3,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The key clue in this question is \"you operate a fintech transfer API\". Do not reset assumptions between stages; carry forward prior constraints directly. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Prioritize the fix with the best latency-to-risk trade-off.",
          "options": [
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt zonal health checks with zone-aware failover"
          ],
          "correct": 3,
          "explanation": "zonal health checks with zone-aware failover addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Read this as a scenario about \"after confirming imbalance, which mitigation is the strongest next move? Prioritize the\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "If you keep \"load Balancing Fundamentals\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-039",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a gaming matchmaking service. First symptom: specific cohorts overload a subset of nodes. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because hot users overload a subset of nodes",
            "Disable health checks to reduce control plane noise"
          ],
          "correct": 2,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The core signal here is \"you operate a gaming matchmaking service\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the control that addresses root cause rather than symptoms.",
          "options": [
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt consistent hashing with virtual nodes",
            "Rely on client retries only with no routing changes"
          ],
          "correct": 2,
          "explanation": "consistent hashing with virtual nodes addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Use \"after confirming imbalance, which mitigation is the strongest next move? Pick the\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The core signal here is \"load Balancing Fundamentals\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-040",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a back-office reporting API. First symptom: sticky workflows fail after node rotation. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because session-bound workflows break after re-route",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node"
          ],
          "correct": 1,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Read this as a scenario about \"you operate a back-office reporting API\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Select the option that reduces tail risk under failure.",
          "options": [
            "Remove observability labels from requests",
            "Adopt externalize session state to shared store",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend"
          ],
          "correct": 1,
          "explanation": "externalize session state to shared store addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The key clue in this question is \"after confirming imbalance, which mitigation is the strongest next move? Select the\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"load Balancing Fundamentals\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-041",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a public search endpoint. First symptom: p99 grows while average latency stays flat. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Investigate load-balancer policy because latency spikes during promotions",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day"
          ],
          "correct": 0,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Use \"you operate a public search endpoint\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: retry storms during partial failure."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the mitigation that scales with future traffic growth.",
          "options": [
            "Adopt least connections with queue-aware routing",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests"
          ],
          "correct": 0,
          "explanation": "least connections with queue-aware routing addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The core signal here is \"after confirming imbalance, which mitigation is the strongest next move? Choose the\". Do not reset assumptions between stages; carry forward prior constraints directly. Throughput is only one part; replay behavior and consumer lag handling matter equally. Common pitfall: ordering loss during partition or replay changes."
        }
      ],
      "detailedExplanation": "The decision turns on \"load Balancing Fundamentals\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-042",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a B2B webhook ingress service. First symptom: retry bursts create uneven target pressure. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because frequent backend restarts in deploy windows"
          ],
          "correct": 3,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "This prompt is really about \"you operate a B2B webhook ingress service\". Do not reset assumptions between stages; carry forward prior constraints directly. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the action that preserves debuggability and rollback safety.",
          "options": [
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt graceful draining before deregistration"
          ],
          "correct": 3,
          "explanation": "graceful draining before deregistration addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "If you keep \"after confirming imbalance, which mitigation is the strongest next move? Pick the\" in view, the correct answer separates faster. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"load Balancing Fundamentals\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-043",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a feature-flag delivery API. First symptom: canary nodes show elevated 5xx during rollout. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because one AZ shows intermittent packet loss",
            "Disable health checks to reduce control plane noise"
          ],
          "correct": 2,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "If you keep \"you operate a feature-flag delivery API\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Select the fix that improves both fairness and throughput.",
          "options": [
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt zonal health checks with zone-aware failover",
            "Rely on client retries only with no routing changes"
          ],
          "correct": 2,
          "explanation": "zonal health checks with zone-aware failover addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "This prompt is really about \"after confirming imbalance, which mitigation is the strongest next move? Select the fix\". Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"load Balancing Fundamentals\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-044",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a catalog browse API. First symptom: cache-miss traffic clusters on a few instances. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because hot users overload a subset of nodes",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node"
          ],
          "correct": 1,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The core signal here is \"you operate a catalog browse API\". Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the option that best contains blast radius.",
          "options": [
            "Remove observability labels from requests",
            "Adopt consistent hashing with virtual nodes",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend"
          ],
          "correct": 1,
          "explanation": "consistent hashing with virtual nodes addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Use \"after confirming imbalance, which mitigation is the strongest next move? Choose the\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "The core signal here is \"load Balancing Fundamentals\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-045",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate an auth token introspection service. First symptom: TLS handshake overhead spikes CPU on select backends. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Investigate load-balancer policy because session-bound workflows break after re-route",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day"
          ],
          "correct": 0,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The key clue in this question is \"you operate an auth token introspection service\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the change that aligns with stateless scaling goals.",
          "options": [
            "Adopt externalize session state to shared store",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests"
          ],
          "correct": 0,
          "explanation": "externalize session state to shared store addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Read this as a scenario about \"after confirming imbalance, which mitigation is the strongest next move? Pick the\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "If you keep \"load Balancing Fundamentals\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-046",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a document rendering API. First symptom: slow jobs accumulate on already-busy targets. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because latency spikes during promotions"
          ],
          "correct": 3,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Start from \"you operate a document rendering API\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the action that avoids retry amplification.",
          "options": [
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt least connections with queue-aware routing"
          ],
          "correct": 3,
          "explanation": "least connections with queue-aware routing addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The decision turns on \"after confirming imbalance, which mitigation is the strongest next move? Choose the\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"load Balancing Fundamentals\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-047",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a travel pricing service. First symptom: weighted routing no longer matches real capacity. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because frequent backend restarts in deploy windows",
            "Disable health checks to reduce control plane noise"
          ],
          "correct": 2,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The decision turns on \"you operate a travel pricing service\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the mitigation with strongest SLO protection.",
          "options": [
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt graceful draining before deregistration",
            "Rely on client retries only with no routing changes"
          ],
          "correct": 2,
          "explanation": "graceful draining before deregistration addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Start from \"after confirming imbalance, which mitigation is the strongest next move? Pick the\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: retry storms during partial failure."
        }
      ],
      "detailedExplanation": "Use \"load Balancing Fundamentals\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-048",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a social feed read API. First symptom: cross-zone forwarding increased after maintenance. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because one AZ shows intermittent packet loss",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node"
          ],
          "correct": 1,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Read this as a scenario about \"you operate a social feed read API\". Solve this as chained reasoning where stage two must respect stage one assumptions. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the step that most directly reduces flapping.",
          "options": [
            "Remove observability labels from requests",
            "Adopt zonal health checks with zone-aware failover",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend"
          ],
          "correct": 1,
          "explanation": "zonal health checks with zone-aware failover addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The key clue in this question is \"after confirming imbalance, which mitigation is the strongest next move? Choose the\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"load Balancing Fundamentals\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-049",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate an IoT command API. First symptom: regional failover shifted unexpected load into one pool. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Investigate load-balancer policy because hot users overload a subset of nodes",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day"
          ],
          "correct": 0,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Use \"you operate an IoT command API\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the fix with lowest operational complexity for on-call teams.",
          "options": [
            "Adopt consistent hashing with virtual nodes",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests"
          ],
          "correct": 0,
          "explanation": "consistent hashing with virtual nodes addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The core signal here is \"after confirming imbalance, which mitigation is the strongest next move? Pick the fix\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        }
      ],
      "detailedExplanation": "The decision turns on \"load Balancing Fundamentals\". Solve this as chained reasoning where stage two must respect stage one assumptions. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-050",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a loyalty points service. First symptom: health checks pass but business requests fail. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because session-bound workflows break after re-route"
          ],
          "correct": 3,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The decision turns on \"you operate a loyalty points service\". Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the option that improves failover behavior under zonal loss.",
          "options": [
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt externalize session state to shared store"
          ],
          "correct": 3,
          "explanation": "externalize session state to shared store addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Start from \"after confirming imbalance, which mitigation is the strongest next move? Choose the\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "Use \"load Balancing Fundamentals\" as your starting point, then verify tradeoffs carefully. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-051",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a fraud scoring endpoint. First symptom: dependency saturation causes large response-time variance. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because latency spikes during promotions",
            "Disable health checks to reduce control plane noise"
          ],
          "correct": 2,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Start from \"you operate a fraud scoring endpoint\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the approach that supports heterogenous backend capacity.",
          "options": [
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt least connections with queue-aware routing",
            "Rely on client retries only with no routing changes"
          ],
          "correct": 2,
          "explanation": "least connections with queue-aware routing addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The decision turns on \"after confirming imbalance, which mitigation is the strongest next move? Pick the\". Do not reset assumptions between stages; carry forward prior constraints directly. The important tradeoffs are delivery semantics, ordering scope, and backpressure under failure. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "This prompt is really about \"load Balancing Fundamentals\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-052",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate an order history API. First symptom: new nodes appear healthy but serve cold-cache latency. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because frequent backend restarts in deploy windows",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node"
          ],
          "correct": 1,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The key clue in this question is \"you operate an order history API\". Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the action that best handles long-tail request variance.",
          "options": [
            "Remove observability labels from requests",
            "Adopt graceful draining before deregistration",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend"
          ],
          "correct": 1,
          "explanation": "graceful draining before deregistration addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Read this as a scenario about \"after confirming imbalance, which mitigation is the strongest next move? Choose the\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "If you keep \"load Balancing Fundamentals\" in view, the correct answer separates faster. Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-053",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a customer support API. First symptom: backend flapping causes frequent routing churn. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Investigate load-balancer policy because one AZ shows intermittent packet loss",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day"
          ],
          "correct": 0,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The core signal here is \"you operate a customer support API\". Solve this as chained reasoning where stage two must respect stage one assumptions. Prioritize explicit semantics and upgrade safety, not just short-term convenience. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the response that keeps canary risk bounded.",
          "options": [
            "Adopt zonal health checks with zone-aware failover",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests"
          ],
          "correct": 0,
          "explanation": "zonal health checks with zone-aware failover addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Use \"after confirming imbalance, which mitigation is the strongest next move? Pick the\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "The core signal here is \"load Balancing Fundamentals\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-054",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a payments dispute API. First symptom: drain windows are shorter than request lifetimes. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because hot users overload a subset of nodes"
          ],
          "correct": 3,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "If you keep \"you operate a payments dispute API\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the mitigation that reduces persistent skew fastest.",
          "options": [
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt consistent hashing with virtual nodes"
          ],
          "correct": 3,
          "explanation": "consistent hashing with virtual nodes addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "This prompt is really about \"after confirming imbalance, which mitigation is the strongest next move? Choose the\". Do not reset assumptions between stages; carry forward prior constraints directly. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "The key clue in this question is \"load Balancing Fundamentals\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-055",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a map tile metadata API. First symptom: bot traffic generates burst patterns at minute boundaries. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because session-bound workflows break after re-route",
            "Disable health checks to reduce control plane noise"
          ],
          "correct": 2,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "This prompt is really about \"you operate a map tile metadata API\". Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the control that improves readiness signal quality.",
          "options": [
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt externalize session state to shared store",
            "Rely on client retries only with no routing changes"
          ],
          "correct": 2,
          "explanation": "externalize session state to shared store addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "If you keep \"after confirming imbalance, which mitigation is the strongest next move? Pick the\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Start from \"load Balancing Fundamentals\", then pressure-test the result against the options. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-056",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a recommendation candidate API. First symptom: hash-key skew sends hot users to few nodes. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because latency spikes during promotions",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node"
          ],
          "correct": 1,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Use \"you operate a recommendation candidate API\" as your starting point, then verify tradeoffs carefully. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the option that best supports graceful deployments.",
          "options": [
            "Remove observability labels from requests",
            "Adopt least connections with queue-aware routing",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend"
          ],
          "correct": 1,
          "explanation": "least connections with queue-aware routing addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The core signal here is \"after confirming imbalance, which mitigation is the strongest next move? Choose the\". Do not reset assumptions between stages; carry forward prior constraints directly. A good message-system answer defines guarantees clearly for both producer and consumer paths. Common pitfall: consumer lag growth under burst traffic."
        }
      ],
      "detailedExplanation": "The decision turns on \"load Balancing Fundamentals\". Solve this as chained reasoning where stage two must respect stage one assumptions. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-057",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a real-time bidding edge. First symptom: gRPC streams and unary calls compete unfairly. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Investigate load-balancer policy because frequent backend restarts in deploy windows",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day"
          ],
          "correct": 0,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Read this as a scenario about \"you operate a real-time bidding edge\". Solve this as chained reasoning where stage two must respect stage one assumptions. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: interface design coupled too tightly to internal implementation."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the change that preserves per-user correctness constraints.",
          "options": [
            "Adopt graceful draining before deregistration",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests"
          ],
          "correct": 0,
          "explanation": "graceful draining before deregistration addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The key clue in this question is \"after confirming imbalance, which mitigation is the strongest next move? Pick the\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks."
        }
      ],
      "detailedExplanation": "Read this as a scenario about \"load Balancing Fundamentals\". Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-058",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a merchant onboarding API. First symptom: request mix changed after new partner launch. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because one AZ shows intermittent packet loss"
          ],
          "correct": 3,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "The decision turns on \"you operate a merchant onboarding API\". Do not reset assumptions between stages; carry forward prior constraints directly. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: ambiguous contracts that hide behavior changes."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the mitigation that prevents control-plane thrash.",
          "options": [
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt zonal health checks with zone-aware failover"
          ],
          "correct": 3,
          "explanation": "zonal health checks with zone-aware failover addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "Start from \"after confirming imbalance, which mitigation is the strongest next move? Choose the\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof."
        }
      ],
      "detailedExplanation": "Use \"load Balancing Fundamentals\" as your starting point, then verify tradeoffs carefully. Solve this as chained reasoning where stage two must respect stage one assumptions. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-059",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate a subscription billing API. First symptom: retry amplification appears during transient faults. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Route all traffic to the newest node",
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because hot users overload a subset of nodes",
            "Disable health checks to reduce control plane noise"
          ],
          "correct": 2,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "Start from \"you operate a subscription billing API\", then pressure-test the result against the options. Solve this as chained reasoning where stage two must respect stage one assumptions. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: assuming recovery speed without operational proof."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Pick the step that improves both correctness and availability.",
          "options": [
            "Pin traffic permanently to one backend",
            "Remove observability labels from requests",
            "Adopt consistent hashing with virtual nodes",
            "Rely on client retries only with no routing changes"
          ],
          "correct": 2,
          "explanation": "consistent hashing with virtual nodes addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "The decision turns on \"after confirming imbalance, which mitigation is the strongest next move? Pick the step\". Do not reset assumptions between stages; carry forward prior constraints directly. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: meeting average goals while missing tail-risk."
        }
      ],
      "detailedExplanation": "This prompt is really about \"load Balancing Fundamentals\". Do not reset assumptions between stages; carry forward prior constraints directly. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-060",
      "type": "two-stage",
      "stages": [
        {
          "question": "You operate an internal graph query API. First symptom: tail regressions correlate with stale LB membership. What is the most likely control-plane area to evaluate first?",
          "options": [
            "Increase DNS TTL to 1 day",
            "Investigate load-balancer policy because session-bound workflows break after re-route",
            "Disable health checks to reduce control plane noise",
            "Route all traffic to the newest node"
          ],
          "correct": 1,
          "explanation": "Start with request distribution and health policy before tuning unrelated layers.",
          "detailedExplanation": "This prompt is really about \"you operate an internal graph query API\". Do not reset assumptions between stages; carry forward prior constraints directly. Interface decisions should be justified by contract stability and client impact over time. Common pitfall: breaking clients during version evolution."
        },
        {
          "question": "After confirming imbalance, which mitigation is the strongest next move? Choose the fix most robust to uncertain failure attribution.",
          "options": [
            "Remove observability labels from requests",
            "Adopt externalize session state to shared store",
            "Rely on client retries only with no routing changes",
            "Pin traffic permanently to one backend"
          ],
          "correct": 1,
          "explanation": "externalize session state to shared store addresses the failure mode directly while preserving scalable routing behavior.",
          "detailedExplanation": "If you keep \"after confirming imbalance, which mitigation is the strongest next move? Choose the fix\" in view, the correct answer separates faster. Keep stage continuity explicit: the first-step outcome is a hard input to the next step. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization."
        }
      ],
      "detailedExplanation": "Start from \"load Balancing Fundamentals\", then pressure-test the result against the options. Do not reset assumptions between stages; carry forward prior constraints directly. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-061",
      "type": "multi-select",
      "question": "Which factors should influence choosing L4 vs L7 load balancing? (Select all that apply)",
      "options": [
        "Office seating layout",
        "Need for header/path routing",
        "TLS termination strategy",
        "Required protocol awareness"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Routing logic, TLS handling, and protocol semantics drive L4/L7 choice.",
      "detailedExplanation": "The key clue in this question is \"factors should influence choosing L4 vs L7 load balancing? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-062",
      "type": "multi-select",
      "question": "Healthy production load balancing usually includes which controls? (Select all that apply)",
      "options": [
        "Graceful connection draining",
        "Per-backend metrics",
        "No failure budget policy",
        "Active health checks"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Checks, draining, and metrics are core reliability controls.",
      "detailedExplanation": "Read this as a scenario about \"healthy production load balancing usually includes which controls? (Select all that\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-063",
      "type": "multi-select",
      "question": "Sticky sessions can cause which risks? (Select all that apply)",
      "options": [
        "Guaranteed statelessness",
        "Uneven load under skewed users",
        "Harder failover when node dies",
        "Reduced cross-node rebalance flexibility"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Affinity improves locality but can hurt balancing and resilience.",
      "detailedExplanation": "The decision turns on \"sticky sessions can cause which risks? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-064",
      "type": "multi-select",
      "question": "Signals of unhealthy balancing quality include which? (Select all that apply)",
      "options": [
        "Persistent tail-latency despite spare cluster CPU",
        "Frequent backend flapping",
        "Perfectly stable p95 and no errors",
        "High variance in backend utilization"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Skew, tail lag, and flapping point to policy or health signal issues.",
      "detailedExplanation": "This prompt is really about \"signals of unhealthy balancing quality include which? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-065",
      "type": "multi-select",
      "question": "When using weighted balancing, what should weights reflect? (Select all that apply)",
      "options": [
        "Alphabetical hostname order",
        "Relative backend capacity",
        "Observed performance constraints",
        "Intentional traffic shaping goals"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Weights should encode capacity/performance/business routing intent.",
      "detailedExplanation": "Use \"using weighted balancing, what should weights reflect? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-066",
      "type": "multi-select",
      "question": "What are valid reasons to keep retries bounded at clients and proxies? (Select all that apply)",
      "options": [
        "Protect overloaded backends",
        "Preserve latency SLOs",
        "Guarantee infinite success",
        "Avoid retry storms"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Bounded retries prevent amplification during incidents.",
      "detailedExplanation": "The core signal here is \"valid reasons to keep retries bounded at clients and proxies? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-067",
      "type": "multi-select",
      "question": "Which controls improve zonal resilience with load balancers? (Select all that apply)",
      "options": [
        "Single-zone only deployment",
        "Zone-aware health checks",
        "Cross-zone failover policy",
        "Capacity headroom per zone"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Zonal checks/failover/headroom reduce blast radius from AZ issues.",
      "detailedExplanation": "If you keep \"controls improve zonal resilience with load balancers? (Select all that apply)\" in view, the correct answer separates faster. Treat every option as a separate true/false test under the same constraints. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-068",
      "type": "multi-select",
      "question": "For canary releases at the load balancer, which practices are strong? (Select all that apply)",
      "options": [
        "Automatic rollback thresholds",
        "Segment-specific routing rules",
        "Immediate 100% cutover by default",
        "Small initial traffic percentage"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Gradual exposure with guardrails lowers deployment risk.",
      "detailedExplanation": "Start from \"for canary releases at the load balancer, which practices are strong? (Select all that\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-069",
      "type": "multi-select",
      "question": "Important LB observability dimensions include which? (Select all that apply)",
      "options": [
        "Only host disk size",
        "Per-target success/error rates",
        "Target selection distribution",
        "End-to-end latency percentiles"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Distribution, latency, and success metrics reveal balancing quality.",
      "detailedExplanation": "The key clue in this question is \"important LB observability dimensions include which? (Select all that apply)\". Treat every option as a separate true/false test under the same constraints. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-070",
      "type": "multi-select",
      "question": "Reasons to externalize session state from app memory include which? (Select all that apply)",
      "options": [
        "Reduce reliance on sticky sessions",
        "Simplify failover across instances",
        "Increase node-local coupling",
        "Enable stateless horizontal scaling"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "External state improves elasticity and resilience.",
      "detailedExplanation": "The decision turns on \"reasons to externalize session state from app memory include which? (Select all that\". Treat every option as a separate true/false test under the same constraints. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-071",
      "type": "multi-select",
      "question": "What can make DNS-based balancing insufficient alone? (Select all that apply)",
      "options": [
        "Native header-based routing",
        "Client-side DNS caching",
        "Lack of request-level health awareness",
        "Slow failover reaction to node failure"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "DNS is coarse-grained and cache-affected; it lacks fine request controls.",
      "detailedExplanation": "Read this as a scenario about \"make DNS-based balancing insufficient alone? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Cache design quality is mostly about correctness boundaries, not only hit rate. Common pitfall: stale data despite high hit rates.",
      "references": [
        {
          "title": "Redis Documentation",
          "url": "https://redis.io/docs/latest/"
        },
        {
          "title": "Cache-Aside pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/cache-aside"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-072",
      "type": "multi-select",
      "question": "What should a robust LB health endpoint validate? (Select all that apply)",
      "options": [
        "Ability to serve core requests",
        "Resource exhaustion guardrails",
        "Marketing copy freshness",
        "Critical dependency readiness"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Health endpoints should represent real serving readiness for critical path traffic.",
      "detailedExplanation": "The key clue in this question is \"a robust LB health endpoint validate? (Select all that apply)\". Validate each option independently; do not select statements that are only partially true. Good API choices balance client ergonomics, compatibility, and long-term evolvability. Common pitfall: interface design coupled too tightly to internal implementation.",
      "references": [
        {
          "title": "RFC 9110: HTTP Semantics",
          "url": "https://www.rfc-editor.org/rfc/rfc9110"
        },
        {
          "title": "gRPC Documentation",
          "url": "https://grpc.io/docs/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-073",
      "type": "multi-select",
      "question": "Common causes of load imbalance include which? (Select all that apply)",
      "options": [
        "Uniform request cost and perfect signals",
        "Unequal backend capacity with equal weights",
        "Hash skew from low-cardinality keys",
        "Slow target deregistration"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Capacity mismatch, skew, and stale membership often drive imbalance.",
      "detailedExplanation": "Start from \"common causes of load imbalance include which? (Select all that apply)\", then pressure-test the result against the options. Treat every option as a separate true/false test under the same constraints. Treat freshness policy and invalidation paths as first-class constraints. Common pitfall: invalidation races under concurrent writes.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-074",
      "type": "multi-select",
      "question": "Which are valid trade-offs when terminating TLS at edge LB? (Select all that apply)",
      "options": [
        "Potential internal trust-boundary requirements",
        "L7 policy enablement",
        "Automatic elimination of authn/authz",
        "Centralized cert lifecycle"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Edge TLS helps operations and routing but does not replace identity controls.",
      "detailedExplanation": "If you keep \"valid trade-offs when terminating TLS at edge LB? (Select all that apply)\" in view, the correct answer separates faster. Validate each option independently; do not select statements that are only partially true. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-075",
      "type": "multi-select",
      "question": "When should fail-closed behavior be preferred? (Select all that apply)",
      "options": [
        "Any consumer social feed",
        "Safety-critical correctness dominates availability",
        "Regulated operations require strict backend health certainty",
        "Serving stale/incorrect responses is unacceptable"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Fail-closed is for domains where bad responses are worse than temporary unavailability.",
      "detailedExplanation": "The core signal here is \"fail-closed behavior be preferred? (Select all that apply)\". Avoid pattern guessing and evaluate each candidate directly against the scenario. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-076",
      "type": "multi-select",
      "question": "Good rollback triggers for LB canaries include which? (Select all that apply)",
      "options": [
        "Tail latency regression",
        "Saturation increase at target pool",
        "Weekend calendar date",
        "Error rate regression"
      ],
      "correctIndices": [0, 1, 3],
      "explanation": "Rollbacks should react to measurable risk signals, not arbitrary dates.",
      "detailedExplanation": "Use \"good rollback triggers for LB canaries include which? (Select all that apply)\" as your starting point, then verify tradeoffs carefully. Avoid pattern guessing and evaluate each candidate directly against the scenario. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-077",
      "type": "multi-select",
      "question": "For client-side load balancing, clients must typically handle which concerns? (Select all that apply)",
      "options": [
        "Datacenter power distribution",
        "Service discovery updates",
        "Endpoint health filtering",
        "Retry/backoff semantics"
      ],
      "correctIndices": [1, 2, 3],
      "explanation": "Client balancers absorb control-plane responsibilities directly.",
      "detailedExplanation": "This prompt is really about \"for client-side load balancing, clients must typically handle which concerns? (Select\". Validate each option independently; do not select statements that are only partially true. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-078",
      "type": "numeric-input",
      "question": "An LB receives 24,000 requests/second and has 12 equally capable backends. Approximate requests/second per backend?",
      "answer": 2000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "24,000 / 12 = 2,000 rps per backend.",
      "detailedExplanation": "The decision turns on \"lB receives 24,000 requests/second and has 12 equally capable backends\". Normalize units before computing so conversion mistakes do not propagate. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. If values like 24,000 and 12 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-079",
      "type": "numeric-input",
      "question": "Traffic jumps to 36,000 rps while still on 12 backends. What is per-backend load?",
      "answer": 3000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "36,000 / 12 = 3,000 rps.",
      "detailedExplanation": "Read this as a scenario about \"traffic jumps to 36,000 rps while still on 12 backends\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 36,000 rps and 12 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-080",
      "type": "numeric-input",
      "question": "Each backend can safely do 2,500 rps. At 36,000 total rps, minimum backend count needed?",
      "answer": 15,
      "unit": "instances",
      "tolerance": 0,
      "explanation": "36,000 / 2,500 = 14.4, so round up to 15.",
      "detailedExplanation": "The core signal here is \"each backend can safely do 2,500 rps\". Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 2,500 rps and 36,000 appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-081",
      "type": "numeric-input",
      "question": "Health checks run every 5s with timeout 1s; 180 backends are probed. Approximate probes per minute?",
      "answer": 2160,
      "unit": "probes",
      "tolerance": 0,
      "explanation": "Each backend: 12 probes/min. 180 * 12 = 2,160 probes/min.",
      "detailedExplanation": "If you keep \"health checks run every 5s with timeout 1s\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 5s and 1s should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-082",
      "type": "numeric-input",
      "question": "If 3% of 40,000 rps requires expensive auth checks, how many rps hit that path?",
      "answer": 1200,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.03 * 40,000 = 1,200 rps.",
      "detailedExplanation": "This prompt is really about \"if 3% of 40,000 rps requires expensive auth checks, how many rps hit that path\". Normalize units before computing so conversion mistakes do not propagate. A strong compute answer links throughput targets to concurrency and scaling triggers. If values like 3 and 40,000 rps appear, convert them into one unit basis before comparison. Common pitfall: sizing to average and failing at peak.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-083",
      "type": "numeric-input",
      "question": "A canary gets 5% of 18,000 rps. What canary traffic rate is expected?",
      "answer": 900,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "0.05 * 18,000 = 900 rps.",
      "detailedExplanation": "Use \"canary gets 5% of 18,000 rps\" as your starting point, then verify tradeoffs carefully. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Numbers such as 5 and 18,000 rps should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-084",
      "type": "numeric-input",
      "question": "A backend pool has weights 1:2:3 across three targets with 12,000 rps total. Approximate rps to heaviest target?",
      "answer": 6000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "Total weight 6; heaviest gets 3/6 of traffic = 6,000 rps.",
      "detailedExplanation": "Read this as a scenario about \"backend pool has weights 1:2:3 across three targets with 12,000 rps total\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. A strong compute answer links throughput targets to concurrency and scaling triggers. Numbers such as 1 and 2 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-085",
      "type": "numeric-input",
      "question": "Client retry policy can add up to 0.2 extra requests per original request. At 25,000 original rps, what is worst-case effective rps?",
      "answer": 30000,
      "unit": "rps",
      "tolerance": 0.01,
      "explanation": "25,000 * (1 + 0.2) = 30,000 rps effective load.",
      "detailedExplanation": "The decision turns on \"client retry policy can add up to 0\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Numbers such as 0.2 and 25,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-lb-086",
      "type": "numeric-input",
      "question": "LB target has p95 latency 180ms and queueing adds 40ms. Approximate new p95?",
      "answer": 220,
      "unit": "ms",
      "tolerance": 0.02,
      "explanation": "Latency adds to roughly 220ms at p95 in this simplified model.",
      "detailedExplanation": "Start from \"lB target has p95 latency 180ms and queueing adds 40ms\", then pressure-test the result against the options. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Tie the decision to concrete operational outcomes, not abstract reliability language. Numbers such as 180ms and 40ms should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Percentile",
          "url": "https://en.wikipedia.org/wiki/Percentile"
        }
      ]
    },
    {
      "id": "sc-lb-087",
      "type": "numeric-input",
      "question": "To keep 30% headroom for 28,000 rps demand, what capacity should pool provide?",
      "answer": 40000,
      "unit": "rps",
      "tolerance": 0.03,
      "explanation": "Required capacity = 28,000 / 0.7 = 40,000 rps.",
      "detailedExplanation": "The key clue in this question is \"to keep 30% headroom for 28,000 rps demand, what capacity should pool provide\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 30 and 28,000 rps should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-088",
      "type": "numeric-input",
      "question": "A deploy drains 2 instances/minute from a 20-instance pool. How many minutes to drain all instances (if no replacements)?",
      "answer": 10,
      "unit": "minutes",
      "tolerance": 0,
      "explanation": "20 / 2 = 10 minutes.",
      "detailedExplanation": "The core signal here is \"deploy drains 2 instances/minute from a 20-instance pool\". Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Numbers such as 2 and 20 should be normalized first so downstream reasoning stays consistent. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-089",
      "type": "numeric-input",
      "question": "Error budget allows 0.1% failed requests over 5,000,000 requests/day. Max failed requests/day?",
      "answer": 5000,
      "unit": "requests",
      "tolerance": 0,
      "explanation": "0.001 * 5,000,000 = 5,000.",
      "detailedExplanation": "If you keep \"error budget allows 0\" in view, the correct answer separates faster. Write the unit conversion path explicitly, then calculate, then sanity-check magnitude. The strongest answer explains how failure mode, mitigation speed, and blast radius interact. Numbers such as 0.1 and 5,000 should be normalized first so downstream reasoning stays consistent. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-090",
      "type": "ordering",
      "question": "Order a safe target removal flow during deployment.",
      "items": [
        "Mark instance as draining",
        "Stop new request routing to instance",
        "Finish in-flight requests",
        "Deregister instance"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Drain first, stop new traffic, let in-flight complete, then remove target.",
      "detailedExplanation": "The key clue in this question is \"order a safe target removal flow during deployment\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-091",
      "type": "ordering",
      "question": "Order by increasing routing sophistication.",
      "items": [
        "DNS round robin",
        "L4 TCP balancing",
        "L7 HTTP balancing",
        "L7 with header/path + canary policies"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Each stage adds finer request semantics and control.",
      "detailedExplanation": "Start from \"order by increasing routing sophistication\", then pressure-test the result against the options. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-092",
      "type": "ordering",
      "question": "Order canary rollout steps.",
      "items": [
        "Route 1-5% traffic to canary",
        "Monitor guardrail metrics",
        "Increase traffic gradually",
        "Promote or rollback"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Small exposure then measured expansion with explicit exit criteria.",
      "detailedExplanation": "The decision turns on \"order canary rollout steps\". Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-093",
      "type": "ordering",
      "question": "Order incident triage for sudden LB-induced 5xx spike.",
      "items": [
        "Confirm scope and affected routes",
        "Check target health and selection skew",
        "Apply mitigation (drain bad targets/rollback rule)",
        "Run post-incident fix plan"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Scope first, diagnose, mitigate, then follow-up remediation.",
      "detailedExplanation": "Read this as a scenario about \"order incident triage for sudden LB-induced 5xx spike\". Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: meeting average goals while missing tail-risk.",
      "references": [
        {
          "title": "Google SRE Book: Service Level Objectives",
          "url": "https://sre.google/sre-book/service-level-objectives/"
        },
        {
          "title": "Google SRE Book: Embracing Risk (Error Budgets)",
          "url": "https://sre.google/sre-book/embracing-risk/"
        },
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        }
      ]
    },
    {
      "id": "sc-lb-094",
      "type": "ordering",
      "question": "Order health-check strictness from loosest to strictest.",
      "items": [
        "TCP connect only",
        "HTTP 200 on /healthz",
        "HTTP readiness with dependency checks",
        "Synthetic transaction health test"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Strictness rises as checks better represent real serving readiness.",
      "detailedExplanation": "Use \"order health-check strictness from loosest to strictest\" as your starting point, then verify tradeoffs carefully. Build the rank from biggest differences first, then refine with adjacent checks. Capacity reasoning should separate average and peak demand, then map both to saturation limits. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-095",
      "type": "ordering",
      "question": "Order by likely failover reaction speed (slowest to fastest).",
      "items": [
        "DNS TTL-based failover",
        "Passive error-based target eviction",
        "Active periodic health checks",
        "Per-request circuit-breaker style routing"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Coarser DNS updates are slower; request-level controls react fastest.",
      "detailedExplanation": "This prompt is really about \"order by likely failover reaction speed (slowest to fastest)\". Place obvious extremes first, then sort the middle by pairwise comparison. Map the choice to measurable reliability impact such as error budget burn and recovery behavior. Common pitfall: retry storms during partial failure.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "RFC 1035: Domain names - implementation and specification",
          "url": "https://www.rfc-editor.org/rfc/rfc1035"
        }
      ]
    },
    {
      "id": "sc-lb-096",
      "type": "ordering",
      "question": "Order controls to reduce retry amplification.",
      "items": [
        "Set bounded retry count",
        "Add exponential backoff",
        "Add jitter",
        "Enforce retry budget/hedging guardrails"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Bound retries first, then spread attempts, then cap aggregate retry pressure.",
      "detailedExplanation": "If you keep \"order controls to reduce retry amplification\" in view, the correct answer separates faster. Place obvious extremes first, then sort the middle by pairwise comparison. Tie the decision to concrete operational outcomes, not abstract reliability language. Common pitfall: assuming recovery speed without operational proof.",
      "references": [
        {
          "title": "Apache Kafka documentation",
          "url": "https://kafka.apache.org/documentation/"
        },
        {
          "title": "RabbitMQ Tutorials",
          "url": "https://www.rabbitmq.com/tutorials"
        },
        {
          "title": "Retry pattern",
          "url": "https://learn.microsoft.com/en-us/azure/architecture/patterns/retry"
        }
      ]
    },
    {
      "id": "sc-lb-097",
      "type": "ordering",
      "question": "Order by increasing session externalization maturity.",
      "items": [
        "In-memory per-node sessions",
        "Sticky sessions at LB",
        "Shared session store",
        "Stateless token/session-minimized design"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Maturity moves from node coupling toward stateless scaling.",
      "detailedExplanation": "The core signal here is \"order by increasing session externalization maturity\". Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: ignoring queueing effects at high utilization.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-098",
      "type": "ordering",
      "question": "Order balancing policies by dependence on real-time load telemetry (least to most).",
      "items": [
        "Round robin",
        "Weighted round robin",
        "Least connections",
        "Least response time"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Policies progress from static to dynamic telemetry-driven decisions.",
      "detailedExplanation": "The key clue in this question is \"order balancing policies by dependence on real-time load telemetry (least to most)\". Order by relative scale and bottleneck effect, then validate neighboring items. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-099",
      "type": "ordering",
      "question": "Order steps for introducing weighted balancing safely.",
      "items": [
        "Measure baseline per-target metrics",
        "Assign initial weights by capacity",
        "Watch skew/error regressions",
        "Tune weights iteratively"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Start from measurement, apply cautiously, then iterate from observed behavior.",
      "detailedExplanation": "Start from \"order steps for introducing weighted balancing safely\", then pressure-test the result against the options. Place obvious extremes first, then sort the middle by pairwise comparison. Sizing decisions are better when headroom and bottleneck behavior are stated explicitly. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "System Design Primer: Back-of-the-envelope estimation",
          "url": "https://github.com/donnemartin/system-design-primer#back-of-the-envelope-estimation"
        }
      ]
    },
    {
      "id": "sc-lb-100",
      "type": "ordering",
      "question": "Order capacity management loop for LB pools.",
      "items": [
        "Track demand and utilization",
        "Forecast near-term peaks",
        "Pre-scale with headroom",
        "Validate post-peak and adjust model"
      ],
      "correctOrder": [0, 1, 2, 3],
      "explanation": "Measure, forecast, act, then learn and refine.",
      "detailedExplanation": "If you keep \"order capacity management loop for LB pools\" in view, the correct answer separates faster. Build the rank from biggest differences first, then refine with adjacent checks. A strong compute answer links throughput targets to concurrency and scaling triggers. Common pitfall: assuming linear scaling through shared bottlenecks.",
      "references": [
        {
          "title": "Little's law",
          "url": "https://en.wikipedia.org/wiki/Little%27s_law"
        },
        {
          "title": "NGINX Load Balancing",
          "url": "https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/"
        },
        {
          "title": "Compound annual growth rate",
          "url": "https://www.investopedia.com/terms/c/cagr.asp"
        }
      ]
    }
  ]
}
